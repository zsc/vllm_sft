# ç¬¬ 10 ç« ï¼šè®­ç»ƒå´©æºƒä¸ NaN é—®é¢˜

è®­ç»ƒè¿‡ç¨‹ä¸­çªç„¶å‡ºç° Loss çˆ†ç‚¸æˆ– NaNï¼Œæ˜¯æ¯ä¸ª VLM å·¥ç¨‹å¸ˆçš„å™©æ¢¦ã€‚ä¸€ä¸ªåŸæœ¬æ­£å¸¸è¿è¡Œçš„è®­ç»ƒï¼Œå¯èƒ½åœ¨å‡ ä¸ª step å†…å½»åº•å´©æºƒï¼Œæµªè´¹æ•°å¤©çš„è®¡ç®—èµ„æºã€‚æœ¬ç« å°†ç³»ç»Ÿä»‹ç»è®­ç»ƒä¸ç¨³å®šçš„æ ¹æœ¬åŸå› ã€å¿«é€Ÿè¯Šæ–­æ–¹æ³•ï¼Œä»¥åŠç»è¿‡å®æˆ˜æ£€éªŒçš„è§£å†³æ–¹æ¡ˆã€‚æˆ‘ä»¬å°†å­¦ä¹ å¦‚ä½•åœ¨ 5 åˆ†é’Ÿå†…å®šä½é—®é¢˜ï¼ŒæŒæ¡æ··åˆç²¾åº¦è®­ç»ƒçš„ç¨³å®šæ€§æŠ€å·§ï¼Œå¹¶å»ºç«‹å®Œå–„çš„å®¹é”™æœºåˆ¶ã€‚

## 10.1 Loss çˆ†ç‚¸çš„ 5 åˆ†é’Ÿæ’æŸ¥æµç¨‹

å½“è®­ç»ƒ Loss çªç„¶é£™å‡æˆ–å‡ºç° NaN æ—¶ï¼Œæ—¶é—´å°±æ˜¯é‡‘é’±ã€‚ä»¥ä¸‹æ˜¯ç»è¿‡å¤§é‡å®è·µæ€»ç»“çš„å¿«é€Ÿè¯Šæ–­æµç¨‹ï¼š

### 10.1.1 ç¬¬ä¸€æ­¥ï¼šç«‹å³ä¿å­˜ç°åœºï¼ˆ30 ç§’ï¼‰

```python
# ç´§æ€¥ä¿å­˜å½“å‰çŠ¶æ€
torch.save({
    'step': current_step,
    'model_state': model.state_dict(),
    'optimizer_state': optimizer.state_dict(),
    'loss_history': loss_history[-100:],  # æœ€è¿‘100ä¸ªstepçš„loss
    'grad_norm_history': grad_norm_history[-100:],
}, f'debug_checkpoint_step_{current_step}.pt')
```

### 10.1.2 ç¬¬äºŒæ­¥ï¼šæ£€æŸ¥ Loss æ›²çº¿æ¨¡å¼ï¼ˆ1 åˆ†é’Ÿï¼‰

Loss çˆ†ç‚¸é€šå¸¸æœ‰ä¸‰ç§æ¨¡å¼ï¼Œæ¯ç§å¯¹åº”ä¸åŒçš„åŸå› ï¼š

```
æ¨¡å¼ 1: çªç„¶è·³è·ƒ
Loss: 2.1 â†’ 2.0 â†’ 1.9 â†’ 8734.5 â†’ NaN
åŸå› : å•ä¸ªå¼‚å¸¸æ ·æœ¬æˆ–æ•°å€¼æº¢å‡º

æ¨¡å¼ 2: æŒ‡æ•°å¢é•¿
Loss: 2.1 â†’ 2.3 â†’ 2.8 â†’ 4.5 â†’ 12.3 â†’ 89.7 â†’ NaN
åŸå› : å­¦ä¹ ç‡è¿‡å¤§æˆ–æ¢¯åº¦ç´¯ç§¯é”™è¯¯

æ¨¡å¼ 3: éœ‡è¡å‘æ•£
Loss: 2.1 â†’ 1.8 â†’ 2.5 â†’ 1.6 â†’ 3.2 â†’ 1.4 â†’ 5.8 â†’ NaN
åŸå› : ä¼˜åŒ–å™¨çŠ¶æ€æŸåæˆ–æ•°å€¼ä¸ç¨³å®š
```

### 10.1.3 ç¬¬ä¸‰æ­¥ï¼šå®šä½é—®é¢˜å±‚çº§ï¼ˆ2 åˆ†é’Ÿï¼‰

ä½¿ç”¨ä»¥ä¸‹ä»£ç å¿«é€Ÿå®šä½é—®é¢˜å‘ç”Ÿçš„å±‚çº§ï¼š

```python
def check_model_health(model):
    """å¿«é€Ÿæ£€æŸ¥æ¨¡å‹å„å±‚çš„å¥åº·çŠ¶æ€"""
    issues = []
    
    for name, param in model.named_parameters():
        # æ£€æŸ¥å‚æ•°æœ¬èº«
        if torch.isnan(param).any():
            issues.append(f"NaN in parameter: {name}")
        if torch.isinf(param).any():
            issues.append(f"Inf in parameter: {name}")
        
        # æ£€æŸ¥æ¢¯åº¦
        if param.grad is not None:
            if torch.isnan(param.grad).any():
                issues.append(f"NaN in gradient: {name}")
            if torch.isinf(param.grad).any():
                issues.append(f"Inf in gradient: {name}")
            
            # æ£€æŸ¥æ¢¯åº¦èŒƒæ•°
            grad_norm = param.grad.norm().item()
            if grad_norm > 1000:
                issues.append(f"Large gradient norm ({grad_norm:.2f}): {name}")
    
    return issues
```

### 10.1.4 ç¬¬å››æ­¥ï¼šæ£€æŸ¥å…³é”®æ•°å€¼ï¼ˆ1.5 åˆ†é’Ÿï¼‰

VLM è®­ç»ƒä¸­æœ€å®¹æ˜“å‡ºé—®é¢˜çš„æ•°å€¼è®¡ç®—ï¼š

1. **æ³¨æ„åŠ›åˆ†æ•°**ï¼š
```python
# æ£€æŸ¥æ³¨æ„åŠ›æƒé‡
attention_weights = torch.softmax(scores / math.sqrt(d_k), dim=-1)
if (attention_weights == 0).all(dim=-1).any():
    print("è­¦å‘Šï¼šå‡ºç°å…¨é›¶æ³¨æ„åŠ›æƒé‡ï¼ˆæ•°å€¼ä¸‹æº¢ï¼‰")
if torch.isnan(attention_weights).any():
    print("è­¦å‘Šï¼šæ³¨æ„åŠ›æƒé‡åŒ…å« NaN")
```

2. **æŸå¤±å‡½æ•°ä¸­çš„ log æ“ä½œ**ï¼š
```python
# æ·»åŠ æ•°å€¼ç¨³å®šæ€§
logits = model(inputs)
# é”™è¯¯ï¼šå¯èƒ½å¯¼è‡´ log(0)
loss = -torch.log(probs[target])
# æ­£ç¡®ï¼šæ·»åŠ  epsilon
loss = -torch.log(probs[target] + 1e-8)
```

3. **LayerNorm çš„é™¤æ³•**ï¼š
```python
# æ£€æŸ¥ LayerNorm æ˜¯å¦ç¨³å®š
def stable_layer_norm(x, eps=1e-5):
    mean = x.mean(dim=-1, keepdim=True)
    var = x.var(dim=-1, keepdim=True, unbiased=False)
    # ç¡®ä¿æ–¹å·®ä¸ä¸ºé›¶
    return (x - mean) / torch.sqrt(var + eps)
```

### 10.1.5 ç´§æ€¥å¤„ç†å†³ç­–æ ‘

```
å‘ç° Loss çˆ†ç‚¸/NaN
â”‚
â”œâ”€ æ˜¯å¦åœ¨å‰ 1000 æ­¥å†…ï¼Ÿ
â”‚  â”œâ”€ æ˜¯ â†’ æ£€æŸ¥åˆå§‹åŒ–å’Œå­¦ä¹ ç‡é¢„çƒ­
â”‚  â””â”€ å¦ â†’ ç»§ç»­è¯Šæ–­
â”‚
â”œâ”€ æ˜¯å¦ä½¿ç”¨æ··åˆç²¾åº¦ï¼Ÿ
â”‚  â”œâ”€ æ˜¯ â†’ æ£€æŸ¥ loss scaling å’Œ dtype è½¬æ¢
â”‚  â””â”€ å¦ â†’ æ£€æŸ¥æ•°å€¼æº¢å‡º
â”‚
â”œâ”€ æ˜¯å¦æœ‰å¼‚å¸¸å¤§çš„æ¢¯åº¦ï¼Ÿ
â”‚  â”œâ”€ æ˜¯ â†’ é™ä½å­¦ä¹ ç‡æˆ–å¢å¼º gradient clipping
â”‚  â””â”€ å¦ â†’ æ£€æŸ¥æ•°æ®å’ŒæŸå¤±å‡½æ•°
â”‚
â””â”€ æ˜¯å¦å¯ä»¥ä» checkpoint æ¢å¤ï¼Ÿ
   â”œâ”€ æ˜¯ â†’ è°ƒæ•´è¶…å‚æ•°åæ¢å¤è®­ç»ƒ
   â””â”€ å¦ â†’ é™çº§åˆ°æ›´ä¿å®ˆçš„é…ç½®é‡æ–°å¼€å§‹
```

## 10.2 æ¢¯åº¦ç›‘æ§ä¸å¼‚å¸¸å€¼å®šä½

### 10.2.1 å®æ—¶æ¢¯åº¦ç›‘æ§ç³»ç»Ÿ

å»ºç«‹å®Œå–„çš„æ¢¯åº¦ç›‘æ§æ˜¯é¢„é˜²è®­ç»ƒå´©æºƒçš„ç¬¬ä¸€é“é˜²çº¿ï¼š

```python
class GradientMonitor:
    """æ¢¯åº¦ç›‘æ§å™¨ï¼Œå®æ—¶è·Ÿè¸ªæ¢¯åº¦ç»Ÿè®¡ä¿¡æ¯"""
    
    def __init__(self, model, logger=None):
        self.model = model
        self.logger = logger
        self.history = defaultdict(list)
        self.anomaly_threshold = {
            'max_norm': 100.0,
            'min_norm': 1e-8,
            'nan_tolerance': 0,
        }
    
    def check_gradients(self, step):
        """æ£€æŸ¥å½“å‰æ­¥çš„æ¢¯åº¦å¥åº·çŠ¶æ€"""
        alerts = []
        
        for name, param in self.model.named_parameters():
            if param.grad is None:
                continue
            
            grad = param.grad.data
            
            # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
            grad_norm = grad.norm().item()
            grad_mean = grad.mean().item()
            grad_std = grad.std().item()
            
            # è®°å½•å†å²
            self.history[name].append({
                'step': step,
                'norm': grad_norm,
                'mean': grad_mean,
                'std': grad_std
            })
            
            # å¼‚å¸¸æ£€æµ‹
            if torch.isnan(grad).any():
                alerts.append(f"Step {step}: NaN gradient in {name}")
            
            if grad_norm > self.anomaly_threshold['max_norm']:
                alerts.append(f"Step {step}: Large gradient norm {grad_norm:.2f} in {name}")
            
            if grad_norm < self.anomaly_threshold['min_norm'] and grad_norm > 0:
                alerts.append(f"Step {step}: Vanishing gradient {grad_norm:.2e} in {name}")
        
        return alerts
```

### 10.2.2 æ¢¯åº¦å¼‚å¸¸çš„æ ¹æºåˆ†æ

ä¸åŒå±‚çš„æ¢¯åº¦å¼‚å¸¸å¾€å¾€æŒ‡å‘ä¸åŒçš„é—®é¢˜ï¼š

1. **è§†è§‰ç¼–ç å™¨å±‚çš„æ¢¯åº¦çˆ†ç‚¸**
   - åŸå› ï¼šå›¾åƒé¢„å¤„ç†é”™è¯¯ï¼ˆå¦‚æœªå½’ä¸€åŒ–ï¼‰
   - è§£å†³ï¼šæ£€æŸ¥å›¾åƒè¾“å…¥èŒƒå›´ï¼Œç¡®ä¿åœ¨ [-1, 1] æˆ– [0, 1]

2. **æŠ•å½±å±‚çš„æ¢¯åº¦æ¶ˆå¤±**
   - åŸå› ï¼šç»´åº¦ä¸åŒ¹é…æˆ–åˆå§‹åŒ–ä¸å½“
   - è§£å†³ï¼šä½¿ç”¨ Xavier æˆ– Kaiming åˆå§‹åŒ–

3. **è¯­è¨€æ¨¡å‹å±‚çš„æ¢¯åº¦éœ‡è¡**
   - åŸå› ï¼šåºåˆ—é•¿åº¦å˜åŒ–è¿‡å¤§æˆ– padding ç­–ç•¥ä¸å½“
   - è§£å†³ï¼šä½¿ç”¨åŠ¨æ€ padding å’Œæ³¨æ„åŠ› mask

### 10.2.3 é«˜çº§æ¢¯åº¦åˆ†æå·¥å…·

```python
def analyze_gradient_flow(model, input_batch, target_batch):
    """åˆ†ææ¢¯åº¦åœ¨æ¨¡å‹ä¸­çš„æµåŠ¨æƒ…å†µ"""
    
    model.zero_grad()
    output = model(input_batch)
    loss = compute_loss(output, target_batch)
    loss.backward()
    
    # æ”¶é›†æ¯å±‚çš„æ¢¯åº¦ä¿¡æ¯
    gradient_flow = []
    for name, param in model.named_parameters():
        if param.grad is not None:
            grad_data = param.grad.data
            gradient_flow.append({
                'layer': name,
                'grad_norm': grad_data.norm().item(),
                'grad_mean': grad_data.mean().item(),
                'grad_max': grad_data.max().item(),
                'grad_min': grad_data.min().item(),
                'percent_zeros': (grad_data == 0).float().mean().item() * 100
            })
    
    # å¯è§†åŒ–æ¢¯åº¦æµ
    import matplotlib.pyplot as plt
    
    layers = [g['layer'].split('.')[-1] for g in gradient_flow]
    grad_norms = [g['grad_norm'] for g in gradient_flow]
    
    plt.figure(figsize=(12, 6))
    plt.semilogy(grad_norms, label='Gradient Norm')
    plt.xticks(range(len(layers)), layers, rotation=45, ha='right')
    plt.xlabel('Layers')
    plt.ylabel('Gradient Norm (log scale)')
    plt.title('Gradient Flow Through Network')
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.show()
    
    return gradient_flow
```

## 10.3 æ··åˆç²¾åº¦è®­ç»ƒçš„ç¨³å®šæ€§æŠ€å·§

### 10.3.1 FP16 vs BF16 çš„é€‰æ‹©

æ··åˆç²¾åº¦è®­ç»ƒæ˜¯æå‡è®­ç»ƒé€Ÿåº¦çš„å…³é”®ï¼Œä½†ä¹Ÿæ˜¯ç¨³å®šæ€§é—®é¢˜çš„ä¸»è¦æ¥æºï¼š

```
FP16 (åŠç²¾åº¦æµ®ç‚¹)
â”œâ”€ ä¼˜ç‚¹ï¼šç¡¬ä»¶æ”¯æŒå¹¿æ³›ï¼Œé€Ÿåº¦å¿«
â”œâ”€ ç¼ºç‚¹ï¼šæ•°å€¼èŒƒå›´å° (Â±65,504)ï¼Œå®¹æ˜“æº¢å‡º
â””â”€ é€‚ç”¨ï¼šç¨³å®šçš„æ¨¡å‹ï¼Œå……åˆ†çš„ loss scaling

BF16 (Brain Float 16)
â”œâ”€ ä¼˜ç‚¹ï¼šæ•°å€¼èŒƒå›´å¤§ (Â±3.4Ã—10^38)ï¼Œä¸FP32ç›¸åŒ
â”œâ”€ ç¼ºç‚¹ï¼šç²¾åº¦è¾ƒä½ï¼Œéœ€è¦æ–°ç¡¬ä»¶ï¼ˆA100+ï¼‰
â””â”€ é€‚ç”¨ï¼šå¤§æ¨¡å‹è®­ç»ƒï¼Œæ•°å€¼ç¨³å®šæ€§è¦æ±‚é«˜
```

### 10.3.2 åŠ¨æ€ Loss Scaling ç­–ç•¥

```python
class DynamicLossScaler:
    """è‡ªé€‚åº”çš„ loss scalingï¼Œé˜²æ­¢æ¢¯åº¦ä¸‹æº¢/ä¸Šæº¢"""
    
    def __init__(self, init_scale=2**16, scale_factor=2.0, 
                 scale_window=2000, tolerance=0.05):
        self.scale = init_scale
        self.scale_factor = scale_factor
        self.scale_window = scale_window
        self.tolerance = tolerance
        self.overflow_counter = 0
        self.step_counter = 0
    
    def scale_loss(self, loss):
        """æ”¾å¤§lossé˜²æ­¢æ¢¯åº¦ä¸‹æº¢"""
        return loss * self.scale
    
    def unscale_gradients(self, optimizer):
        """ç¼©å°æ¢¯åº¦åˆ°æ­£ç¡®èŒƒå›´"""
        for group in optimizer.param_groups:
            for param in group['params']:
                if param.grad is not None:
                    param.grad.data.div_(self.scale)
    
    def update_scale(self, overflow):
        """æ ¹æ®æº¢å‡ºæƒ…å†µåŠ¨æ€è°ƒæ•´scale"""
        if overflow:
            # å‘ç”Ÿæº¢å‡ºï¼Œå‡å°scale
            self.scale /= self.scale_factor
            self.overflow_counter += 1
            print(f"Gradient overflow! Reducing scale to {self.scale}")
            return True
        
        self.step_counter += 1
        if self.step_counter % self.scale_window == 0:
            # é•¿æ—¶é—´æ— æº¢å‡ºï¼Œå°è¯•å¢å¤§scale
            self.scale *= self.scale_factor
            print(f"Increasing scale to {self.scale}")
        
        return False
```

### 10.3.3 å…³é”®å±‚çš„ç²¾åº¦ä¿æŠ¤

æŸäº›å±‚å¿…é¡»ä¿æŒ FP32 ç²¾åº¦ä»¥ç¡®ä¿ç¨³å®šæ€§ï¼š

```python
def configure_mixed_precision(model):
    """é…ç½®æ··åˆç²¾åº¦è®­ç»ƒçš„å±‚çº§ç²¾åº¦"""
    
    # å§‹ç»ˆä¿æŒ FP32 çš„å±‚
    fp32_layers = [
        'layer_norm',      # LayerNorm å¯¹ç²¾åº¦æ•æ„Ÿ
        'softmax',         # Softmax éœ€è¦é«˜ç²¾åº¦
        'loss',            # æŸå¤±è®¡ç®—
        'positional',      # ä½ç½®ç¼–ç 
    ]
    
    for name, module in model.named_modules():
        # æ£€æŸ¥æ˜¯å¦éœ€è¦FP32
        need_fp32 = any(fp_layer in name.lower() 
                        for fp_layer in fp32_layers)
        
        if need_fp32:
            # å¼ºåˆ¶ä½¿ç”¨FP32
            module.float()
            for param in module.parameters():
                param.data = param.data.float()
        else:
            # å¯ä»¥ä½¿ç”¨FP16/BF16
            module.half()  # or module.bfloat16()
    
    return model
```

### 10.3.4 æ¢¯åº¦ç´¯ç§¯ä¸æ··åˆç²¾åº¦çš„äº¤äº’

```python
def stable_gradient_accumulation(model, optimizer, data_loader, 
                                accumulation_steps=4):
    """ç¨³å®šçš„æ¢¯åº¦ç´¯ç§¯å®ç°"""
    
    scaler = torch.cuda.amp.GradScaler()
    accumulated_loss = 0
    
    for step, batch in enumerate(data_loader):
        # åˆ¤æ–­æ˜¯å¦æ˜¯ç´¯ç§¯çš„æœ€åä¸€æ­¥
        is_accumulation_boundary = (step + 1) % accumulation_steps == 0
        
        with torch.cuda.amp.autocast():
            outputs = model(batch['input'])
            loss = compute_loss(outputs, batch['target'])
            # é‡è¦ï¼šé™¤ä»¥ç´¯ç§¯æ­¥æ•°
            loss = loss / accumulation_steps
        
        # Scale losså¹¶åå‘ä¼ æ’­
        scaler.scale(loss).backward()
        accumulated_loss += loss.item()
        
        if is_accumulation_boundary:
            # æ¢¯åº¦è£å‰ªï¼ˆåœ¨unscaleä¹‹åï¼Œstepä¹‹å‰ï¼‰
            scaler.unscale_(optimizer)
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            
            # ä¼˜åŒ–å™¨æ­¥è¿›
            scaler.step(optimizer)
            scaler.update()
            optimizer.zero_grad()
            
            # è®°å½•
            print(f"Step {step}: Loss = {accumulated_loss:.4f}")
            accumulated_loss = 0
```

## 10.4 Checkpoint æ¢å¤ä¸æ–­ç‚¹ç»­è®­

### 10.4.1 å®Œæ•´çš„ Checkpoint ç³»ç»Ÿ

```python
class CheckpointManager:
    """å…¨é¢çš„æ£€æŸ¥ç‚¹ç®¡ç†å™¨"""
    
    def __init__(self, save_dir, keep_last_n=3, save_interval=1000):
        self.save_dir = save_dir
        self.keep_last_n = keep_last_n
        self.save_interval = save_interval
        self.checkpoints = []
    
    def save_checkpoint(self, model, optimizer, scheduler, 
                       epoch, step, metrics, extra_state=None):
        """ä¿å­˜å®Œæ•´çš„è®­ç»ƒçŠ¶æ€"""
        
        checkpoint = {
            'epoch': epoch,
            'step': step,
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'scheduler_state_dict': scheduler.state_dict() if scheduler else None,
            'metrics': metrics,
            'rng_state': {
                'python': random.getstate(),
                'numpy': np.random.get_state(),
                'torch': torch.get_rng_state(),
                'cuda': torch.cuda.get_rng_state_all(),
            },
            'timestamp': datetime.now().isoformat(),
        }
        
        if extra_state:
            checkpoint['extra_state'] = extra_state
        
        # ä¿å­˜checkpoint
        checkpoint_path = os.path.join(
            self.save_dir, 
            f'checkpoint_step_{step}.pt'
        )
        torch.save(checkpoint, checkpoint_path)
        self.checkpoints.append(checkpoint_path)
        
        # æ¸…ç†æ—§çš„checkpoints
        if len(self.checkpoints) > self.keep_last_n:
            old_checkpoint = self.checkpoints.pop(0)
            if os.path.exists(old_checkpoint):
                os.remove(old_checkpoint)
        
        return checkpoint_path
    
    def load_checkpoint(self, checkpoint_path, model, optimizer=None, 
                       scheduler=None, strict=True):
        """æ¢å¤è®­ç»ƒçŠ¶æ€"""
        
        checkpoint = torch.load(checkpoint_path, map_location='cpu')
        
        # æ¢å¤æ¨¡å‹
        model.load_state_dict(checkpoint['model_state_dict'], strict=strict)
        
        # æ¢å¤ä¼˜åŒ–å™¨
        if optimizer and 'optimizer_state_dict' in checkpoint:
            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        
        # æ¢å¤å­¦ä¹ ç‡è°ƒåº¦å™¨
        if scheduler and 'scheduler_state_dict' in checkpoint:
            scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
        
        # æ¢å¤éšæœºæ•°çŠ¶æ€
        if 'rng_state' in checkpoint:
            random.setstate(checkpoint['rng_state']['python'])
            np.random.set_state(checkpoint['rng_state']['numpy'])
            torch.set_rng_state(checkpoint['rng_state']['torch'])
            torch.cuda.set_rng_state_all(checkpoint['rng_state']['cuda'])
        
        return checkpoint
```

### 10.4.2 æ–­ç‚¹ç»­è®­çš„æœ€ä½³å®è·µ

```python
def resume_training(checkpoint_path, model, optimizer, data_loader):
    """å®‰å…¨çš„æ–­ç‚¹ç»­è®­æµç¨‹"""
    
    # 1. åŠ è½½checkpoint
    checkpoint = torch.load(checkpoint_path)
    model.load_state_dict(checkpoint['model_state_dict'])
    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
    
    # 2. æ¢å¤åˆ°æ­£ç¡®çš„æ•°æ®ä½ç½®
    start_epoch = checkpoint['epoch']
    start_step = checkpoint['step']
    
    # 3. éªŒè¯æ¢å¤æ˜¯å¦æˆåŠŸ
    validation_batch = next(iter(data_loader))
    with torch.no_grad():
        output = model(validation_batch['input'])
        loss = compute_loss(output, validation_batch['target'])
    
    print(f"Validation loss after resume: {loss.item():.4f}")
    
    # 4. æ£€æŸ¥æ˜¯å¦éœ€è¦é™çº§é…ç½®
    if checkpoint.get('crashed', False):
        print("Previous training crashed. Applying conservative settings...")
        # é™ä½å­¦ä¹ ç‡
        for param_group in optimizer.param_groups:
            param_group['lr'] *= 0.5
        # å¢å¼ºæ¢¯åº¦è£å‰ª
        max_grad_norm = 0.5
    else:
        max_grad_norm = 1.0
    
    return start_epoch, start_step, max_grad_norm
```

### 10.4.3 å´©æºƒæ¢å¤ç­–ç•¥

```python
class CrashRecoveryTrainer:
    """å…·æœ‰å´©æºƒæ¢å¤èƒ½åŠ›çš„è®­ç»ƒå™¨"""
    
    def __init__(self, model, config):
        self.model = model
        self.config = config
        self.crash_counter = 0
        self.max_crashes = 3
    
    def train_with_recovery(self, data_loader):
        """å¸¦è‡ªåŠ¨æ¢å¤çš„è®­ç»ƒå¾ªç¯"""
        
        while self.crash_counter < self.max_crashes:
            try:
                # æ­£å¸¸è®­ç»ƒ
                self._train_epoch(data_loader)
                self.crash_counter = 0  # é‡ç½®è®¡æ•°å™¨
                
            except (RuntimeError, ValueError) as e:
                self.crash_counter += 1
                print(f"Training crashed (attempt {self.crash_counter}/{self.max_crashes}): {e}")
                
                # å´©æºƒæ¢å¤ç­–ç•¥
                recovery_actions = self._get_recovery_strategy(e)
                for action in recovery_actions:
                    action()
                
                # ä»æœ€è¿‘çš„checkpointæ¢å¤
                if self.last_checkpoint:
                    self.load_checkpoint(self.last_checkpoint)
                else:
                    print("No checkpoint available, restarting training...")
                    self._reinitialize_training()
    
    def _get_recovery_strategy(self, error):
        """æ ¹æ®é”™è¯¯ç±»å‹ç¡®å®šæ¢å¤ç­–ç•¥"""
        
        strategies = []
        
        if "CUDA out of memory" in str(error):
            strategies.append(self._reduce_batch_size)
            strategies.append(self._enable_gradient_checkpointing)
        
        elif "nan" in str(error).lower():
            strategies.append(self._reduce_learning_rate)
            strategies.append(self._reset_optimizer_state)
            strategies.append(self._switch_to_fp32)
        
        elif "gradient" in str(error).lower():
            strategies.append(self._enhance_gradient_clipping)
            strategies.append(self._reduce_accumulation_steps)
        
        return strategies
```

## æœ¬ç« å°ç»“

åœ¨æœ¬ç« ä¸­ï¼Œæˆ‘ä»¬ç³»ç»Ÿå­¦ä¹ äº† VLM è®­ç»ƒä¸­å´©æºƒå’Œ NaN é—®é¢˜çš„è¯Šæ–­ä¸è§£å†³æ–¹æ³•ï¼š

### æ ¸å¿ƒçŸ¥è¯†ç‚¹

1. **5åˆ†é’Ÿå¿«é€Ÿè¯Šæ–­æµç¨‹**
   - ä¿å­˜ç°åœº â†’ åˆ†æLossæ¨¡å¼ â†’ å®šä½é—®é¢˜å±‚ â†’ æ£€æŸ¥å…³é”®æ•°å€¼ â†’ ç´§æ€¥å¤„ç†
   - ä¸‰ç§å…¸å‹çš„ Loss çˆ†ç‚¸æ¨¡å¼ï¼šçªç„¶è·³è·ƒã€æŒ‡æ•°å¢é•¿ã€éœ‡è¡å‘æ•£
   - ä¸åŒæ¨¡å¼å¯¹åº”ä¸åŒçš„æ ¹æœ¬åŸå› å’Œè§£å†³æ–¹æ¡ˆ

2. **æ¢¯åº¦ç›‘æ§ä½“ç³»**
   - å®æ—¶æ¢¯åº¦ç»Ÿè®¡ï¼šèŒƒæ•°ã€å‡å€¼ã€æ ‡å‡†å·®ã€é›¶å€¼æ¯”ä¾‹
   - å±‚çº§æ¢¯åº¦åˆ†æï¼šè§†è§‰ç¼–ç å™¨ã€æŠ•å½±å±‚ã€è¯­è¨€æ¨¡å‹çš„ç‰¹å¾
   - æ¢¯åº¦æµå¯è§†åŒ–ï¼šå¿«é€Ÿå®šä½æ¢¯åº¦æ¶ˆå¤±æˆ–çˆ†ç‚¸çš„ä½ç½®

3. **æ··åˆç²¾åº¦è®­ç»ƒç¨³å®šæ€§**
   - FP16 vs BF16 çš„æƒè¡¡ï¼šæ•°å€¼èŒƒå›´ vs ç²¾åº¦
   - åŠ¨æ€ Loss Scalingï¼šè‡ªé€‚åº”è°ƒæ•´é˜²æ­¢æº¢å‡º
   - å…³é”®å±‚ç²¾åº¦ä¿æŠ¤ï¼šLayerNormã€Softmax å¿…é¡» FP32
   - æ¢¯åº¦ç´¯ç§¯çš„æ­£ç¡®å®ç°ï¼šé˜²æ­¢ç²¾åº¦æŸå¤±ç´¯ç§¯

4. **Checkpoint ä¸å®¹é”™æœºåˆ¶**
   - å®Œæ•´çŠ¶æ€ä¿å­˜ï¼šæ¨¡å‹ã€ä¼˜åŒ–å™¨ã€è°ƒåº¦å™¨ã€éšæœºæ•°ç§å­
   - æ™ºèƒ½æ¢å¤ç­–ç•¥ï¼šæ ¹æ®å´©æºƒç±»å‹è‡ªåŠ¨è°ƒæ•´é…ç½®
   - å´©æºƒè®¡æ•°å™¨ï¼šé¿å…æ— é™å¾ªç¯ï¼Œè®¾ç½®æœ€å¤§é‡è¯•æ¬¡æ•°

### å…³é”®å…¬å¼

1. **æ¢¯åº¦èŒƒæ•°è®¡ç®—**ï¼š
   $$\|\nabla\|_2 = \sqrt{\sum_{i} g_i^2}$$

2. **Loss Scaling åŸç†**ï¼š
   $$\nabla_{\text{scaled}} = \text{scale} \times \nabla_{\text{original}}$$
   $$\nabla_{\text{final}} = \nabla_{\text{scaled}} / \text{scale}$$

3. **æ¢¯åº¦è£å‰ª**ï¼š
   $$\nabla_{\text{clipped}} = \begin{cases}
   \nabla & \text{if } \|\nabla\| \leq \text{max\_norm} \\
   \nabla \times \frac{\text{max\_norm}}{\|\nabla\|} & \text{otherwise}
   \end{cases}$$

4. **æ•°å€¼ç¨³å®šçš„ Softmax**ï¼š
   $$\text{softmax}(x_i) = \frac{e^{x_i - \max(x)}}{\sum_j e^{x_j - \max(x)}}$$

## ç»ƒä¹ é¢˜

### åŸºç¡€é¢˜

**ç»ƒä¹  10.1ï¼šLoss æ¨¡å¼è¯†åˆ«**
ç»™å®šä»¥ä¸‹ Loss åºåˆ—ï¼Œåˆ¤æ–­å±äºå“ªç§çˆ†ç‚¸æ¨¡å¼å¹¶åˆ†æå¯èƒ½çš„åŸå› ï¼š
```
åºåˆ—A: 1.8, 1.7, 1.6, 1.5, 1.4, 87234.5, NaN
åºåˆ—B: 2.1, 2.2, 2.5, 3.1, 4.8, 9.2, 23.5, 156.7, NaN
åºåˆ—C: 2.0, 1.8, 2.2, 1.6, 2.5, 1.4, 3.2, 1.2, 5.8, NaN
```

ğŸ’¡ **æç¤º**ï¼šå›é¡¾10.1.2èŠ‚çš„ä¸‰ç§æ¨¡å¼ç‰¹å¾

<details>
<summary>ğŸ“ å‚è€ƒç­”æ¡ˆ</summary>

- **åºåˆ—A**ï¼šçªç„¶è·³è·ƒæ¨¡å¼ã€‚Lossä»1.4ç›´æ¥è·³åˆ°87234.5ï¼Œè¡¨æ˜é‡åˆ°äº†å¼‚å¸¸æ ·æœ¬æˆ–æ•°å€¼æº¢å‡ºã€‚å¯èƒ½åŸå› ï¼š
  - æ•°æ®é›†ä¸­å­˜åœ¨å¼‚å¸¸æ ·æœ¬ï¼ˆå¦‚æ ‡ç­¾é”™è¯¯ï¼‰
  - é™¤é›¶é”™è¯¯æˆ–log(0)æ“ä½œ
  - æ³¨æ„åŠ›è®¡ç®—ä¸­çš„æ•°å€¼æº¢å‡º

- **åºåˆ—B**ï¼šæŒ‡æ•°å¢é•¿æ¨¡å¼ã€‚Losså‘ˆæŒ‡æ•°çº§å¢é•¿ï¼Œæ¯æ­¥å¤§çº¦ç¿»å€ã€‚å¯èƒ½åŸå› ï¼š
  - å­¦ä¹ ç‡è¿‡å¤§å¯¼è‡´å‚æ•°æ›´æ–°è¿‡æ¿€
  - æ¢¯åº¦ç´¯ç§¯å®ç°é”™è¯¯ï¼ˆå¿˜è®°é™¤ä»¥ç´¯ç§¯æ­¥æ•°ï¼‰
  - ä¼˜åŒ–å™¨momentumè®¾ç½®ä¸å½“

- **åºåˆ—C**ï¼šéœ‡è¡å‘æ•£æ¨¡å¼ã€‚Lossåœ¨ä¸‹é™å’Œä¸Šå‡ä¹‹é—´éœ‡è¡ï¼ŒæŒ¯å¹…é€æ¸å¢å¤§ã€‚å¯èƒ½åŸå› ï¼š
  - ä¼˜åŒ–å™¨çŠ¶æ€æŸåï¼ˆå¦‚Adamçš„äºŒé˜¶çŸ©ä¼°è®¡ï¼‰
  - æ‰¹æ¬¡é—´æ•°æ®åˆ†å¸ƒå·®å¼‚è¿‡å¤§
  - å­¦ä¹ ç‡è°ƒåº¦å™¨é…ç½®é”™è¯¯
</details>

**ç»ƒä¹  10.2ï¼šæ¢¯åº¦è£å‰ªé˜ˆå€¼é€‰æ‹©**
ä½ çš„æ¨¡å‹æ­£å¸¸è®­ç»ƒæ—¶æ¢¯åº¦èŒƒæ•°åœ¨ 0.5-2.0 ä¹‹é—´ï¼Œå¶å°”ä¼šè¾¾åˆ° 10-20ã€‚åº”è¯¥å¦‚ä½•è®¾ç½®æ¢¯åº¦è£å‰ªçš„é˜ˆå€¼ï¼Ÿå¦‚æœè®¾ç½®ä¸º 1.0 ä¼šå‘ç”Ÿä»€ä¹ˆï¼Ÿè®¾ç½®ä¸º 100 å‘¢ï¼Ÿ

ğŸ’¡ **æç¤º**ï¼šè€ƒè™‘æ¢¯åº¦è£å‰ªå¯¹æ”¶æ•›é€Ÿåº¦å’Œç¨³å®šæ€§çš„å½±å“

<details>
<summary>ğŸ“ å‚è€ƒç­”æ¡ˆ</summary>

åˆç†çš„æ¢¯åº¦è£å‰ªé˜ˆå€¼åº”è¯¥è®¾ç½®ä¸º **5.0-10.0**ï¼ŒåŸå› å¦‚ä¸‹ï¼š

- **è®¾ç½®ä¸º 1.0 çš„é—®é¢˜**ï¼š
  - ä¼šé¢‘ç¹è§¦å‘è£å‰ªï¼ˆæ­£å¸¸æ¢¯åº¦å°±æœ‰2.0ï¼‰
  - äººä¸ºé™åˆ¶äº†æ¨¡å‹çš„å­¦ä¹ èƒ½åŠ›
  - å¯èƒ½å¯¼è‡´æ”¶æ•›å˜æ…¢æˆ–æ— æ³•æ”¶æ•›åˆ°æœ€ä¼˜è§£
  - ç›¸å½“äºå¼ºåˆ¶é™ä½äº†æœ‰æ•ˆå­¦ä¹ ç‡

- **è®¾ç½®ä¸º 100 çš„é—®é¢˜**ï¼š
  - åŸºæœ¬ä¸ä¼šè§¦å‘ï¼ˆæ­£å¸¸æœ€å¤§å€¼æ‰20ï¼‰
  - å¤±å»äº†é˜²æ­¢æ¢¯åº¦çˆ†ç‚¸çš„ä¿æŠ¤ä½œç”¨
  - å½“çœŸæ­£å‡ºç°å¼‚å¸¸æ—¶æ— æ³•åŠæ—¶é˜»æ­¢

- **æ¨èç­–ç•¥**ï¼š
  1. åˆå§‹è®¾ç½®ä¸ºæ­£å¸¸æœ€å¤§å€¼çš„ 2-3 å€ï¼ˆå¦‚ 5.0ï¼‰
  2. ç›‘æ§è£å‰ªé¢‘ç‡ï¼Œå¦‚æœé¢‘ç¹è§¦å‘åˆ™é€‚å½“æé«˜
  3. å¯¹ä¸åŒå±‚ä½¿ç”¨ä¸åŒé˜ˆå€¼ï¼ˆè§†è§‰ç¼–ç å™¨å¯ä»¥æ›´å¤§ï¼‰
</details>

**ç»ƒä¹  10.3ï¼šæ··åˆç²¾åº¦æ•°å€¼èŒƒå›´**
è®¡ç®—å¹¶æ¯”è¾ƒ FP16 å’Œ BF16 èƒ½è¡¨ç¤ºçš„æœ€å¤§æœ€å°æ­£æ•°ã€‚ä¸ºä»€ä¹ˆ BF16 æ›´ä¸å®¹æ˜“å‡ºç°æ¢¯åº¦ä¸‹æº¢ï¼Ÿ

ğŸ’¡ **æç¤º**ï¼šæŸ¥é˜… IEEE 754 æ ‡å‡†ä¸­çš„æµ®ç‚¹æ•°æ ¼å¼å®šä¹‰

<details>
<summary>ğŸ“ å‚è€ƒç­”æ¡ˆ</summary>

**FP16ï¼ˆåŠç²¾åº¦ï¼‰**ï¼š
- æ ¼å¼ï¼š1ä½ç¬¦å· + 5ä½æŒ‡æ•° + 10ä½å°¾æ•°
- æœ€å¤§å€¼ï¼š65,504
- æœ€å°æ­£è§„å€¼ï¼š6.10 Ã— 10^-5
- æœ€å°éæ­£è§„å€¼ï¼š5.96 Ã— 10^-8

**BF16ï¼ˆBrain Float 16ï¼‰**ï¼š
- æ ¼å¼ï¼š1ä½ç¬¦å· + 8ä½æŒ‡æ•° + 7ä½å°¾æ•°
- æœ€å¤§å€¼ï¼š3.39 Ã— 10^38ï¼ˆä¸FP32ç›¸åŒï¼‰
- æœ€å°æ­£è§„å€¼ï¼š1.18 Ã— 10^-38
- æœ€å°éæ­£è§„å€¼ï¼š9.18 Ã— 10^-41

**BF16 ä¸æ˜“æ¢¯åº¦ä¸‹æº¢çš„åŸå› **ï¼š
1. æŒ‡æ•°ä½æ•°å¤šï¼ˆ8ä½ vs 5ä½ï¼‰ï¼Œæ•°å€¼èŒƒå›´å¤§
2. å¯ä»¥è¡¨ç¤ºæå°çš„æ¢¯åº¦å€¼è€Œä¸ä¼šç›´æ¥å˜ä¸º0
3. ä¸FP32çš„æ•°å€¼èŒƒå›´ä¸€è‡´ï¼Œè½¬æ¢æ—¶ä¸ä¼šæº¢å‡º
4. ä»£ä»·æ˜¯å°¾æ•°ç²¾åº¦é™ä½ï¼ˆ7ä½ vs 10ä½ï¼‰ï¼Œä½†æ·±åº¦å­¦ä¹ ä¸­é€šå¸¸å¯æ¥å—
</details>

### æŒ‘æˆ˜é¢˜

**ç»ƒä¹  10.4ï¼šè®¾è®¡è‡ªé€‚åº”æ¢¯åº¦è£å‰ªç®—æ³•**
æ ‡å‡†çš„æ¢¯åº¦è£å‰ªä½¿ç”¨å›ºå®šé˜ˆå€¼ï¼Œè¯·è®¾è®¡ä¸€ä¸ªè‡ªé€‚åº”ç®—æ³•ï¼Œæ ¹æ®å†å²æ¢¯åº¦ç»Ÿè®¡åŠ¨æ€è°ƒæ•´è£å‰ªé˜ˆå€¼ã€‚è¦æ±‚ï¼š
1. èƒ½å¤Ÿé€‚åº”è®­ç»ƒè¿‡ç¨‹ä¸­æ¢¯åº¦èŒƒæ•°çš„è‡ªç„¶å˜åŒ–
2. ä»ç„¶èƒ½å¤Ÿæ£€æµ‹å’Œå¤„ç†å¼‚å¸¸å€¼
3. ç»™å‡ºä¼ªä»£ç å®ç°

ğŸ’¡ **æç¤º**ï¼šå¯ä»¥ä½¿ç”¨ç§»åŠ¨å¹³å‡å’Œæ ‡å‡†å·®

<details>
<summary>ğŸ“ å‚è€ƒç­”æ¡ˆ</summary>

```python
class AdaptiveGradientClipper:
    def __init__(self, percentile=99.5, history_size=1000, 
                 min_threshold=1.0, max_threshold=100.0):
        self.percentile = percentile
        self.history = deque(maxlen=history_size)
        self.min_threshold = min_threshold
        self.max_threshold = max_threshold
        
    def compute_threshold(self):
        if len(self.history) < 100:  # åˆå§‹é˜¶æ®µä½¿ç”¨å›ºå®šå€¼
            return 10.0
        
        # æ–¹æ³•1ï¼šåŸºäºç™¾åˆ†ä½æ•°
        threshold = np.percentile(self.history, self.percentile)
        
        # æ–¹æ³•2ï¼šåŸºäºå‡å€¼å’Œæ ‡å‡†å·®ï¼ˆ3-sigmaè§„åˆ™ï¼‰
        # mean = np.mean(self.history)
        # std = np.std(self.history)
        # threshold = mean + 3 * std
        
        # é™åˆ¶åœ¨åˆç†èŒƒå›´å†…
        threshold = np.clip(threshold, self.min_threshold, self.max_threshold)
        return threshold
    
    def clip_gradients(self, model):
        # è®¡ç®—å½“å‰æ¢¯åº¦èŒƒæ•°
        total_norm = 0
        for p in model.parameters():
            if p.grad is not None:
                total_norm += p.grad.data.norm(2).item() ** 2
        total_norm = total_norm ** 0.5
        
        # æ›´æ–°å†å²
        self.history.append(total_norm)
        
        # è®¡ç®—è‡ªé€‚åº”é˜ˆå€¼
        clip_value = self.compute_threshold()
        
        # æ‰§è¡Œè£å‰ª
        if total_norm > clip_value:
            clip_coef = clip_value / (total_norm + 1e-6)
            for p in model.parameters():
                if p.grad is not None:
                    p.grad.data.mul_(clip_coef)
            return True, clip_value
        
        return False, clip_value
```

**ä¼˜åŠ¿**ï¼š
1. è‡ªåŠ¨é€‚åº”ä¸åŒè®­ç»ƒé˜¶æ®µçš„æ¢¯åº¦èŒƒå›´
2. é¿å…å›ºå®šé˜ˆå€¼è¿‡æ¾æˆ–è¿‡ç´§
3. åŸºäºç»Ÿè®¡çš„å¼‚å¸¸æ£€æµ‹æ›´é²æ£’
</details>

**ç»ƒä¹  10.5ï¼šå®ç°æ¢¯åº¦å¼‚å¸¸å®šä½å™¨**
è®¾è®¡ä¸€ä¸ªå·¥å…·ï¼Œå½“æ£€æµ‹åˆ° NaN æ¢¯åº¦æ—¶ï¼Œèƒ½å¤Ÿå¿«é€Ÿå®šä½æ˜¯å“ªä¸ªæ“ä½œäº§ç”Ÿçš„ NaNï¼Œå¹¶ç»™å‡ºå¯èƒ½çš„åŸå› ã€‚è€ƒè™‘ VLM ä¸­çš„ç‰¹æ®Šæƒ…å†µã€‚

ğŸ’¡ **æç¤º**ï¼šä½¿ç”¨ PyTorch çš„ autograd å¼‚å¸¸æ£€æµ‹æ¨¡å¼

<details>
<summary>ğŸ“ å‚è€ƒç­”æ¡ˆ</summary>

```python
class NaNGradientLocator:
    def __init__(self, model):
        self.model = model
        self.forward_hooks = []
        self.backward_hooks = []
        self.problematic_layers = []
        
    def enable_detection(self):
        """å¯ç”¨NaNæ£€æµ‹"""
        torch.autograd.set_detect_anomaly(True)
        
        # æ³¨å†Œå‰å‘é’©å­
        for name, module in self.model.named_modules():
            handle = module.register_forward_hook(
                self._make_forward_hook(name)
            )
            self.forward_hooks.append(handle)
            
            # æ³¨å†Œåå‘é’©å­
            handle = module.register_backward_hook(
                self._make_backward_hook(name)
            )
            self.backward_hooks.append(handle)
    
    def _make_forward_hook(self, layer_name):
        def hook(module, input, output):
            # æ£€æŸ¥è¾“å…¥
            for i, inp in enumerate(input):
                if torch.is_tensor(inp) and torch.isnan(inp).any():
                    self.problematic_layers.append({
                        'layer': layer_name,
                        'type': 'forward_input',
                        'index': i,
                        'stage': 'forward'
                    })
            
            # æ£€æŸ¥è¾“å‡º
            if torch.is_tensor(output) and torch.isnan(output).any():
                # VLMç‰¹æ®Šæ£€æŸ¥
                if 'attention' in layer_name.lower():
                    # æ£€æŸ¥æ³¨æ„åŠ›åˆ†æ•°
                    print(f"NaN in attention layer {layer_name}")
                    print("å¯èƒ½åŸå› ï¼š1) åºåˆ—é•¿åº¦è¿‡é•¿å¯¼è‡´æ•°å€¼æº¢å‡º")
                    print("         2) æ³¨æ„åŠ›maskè®¾ç½®é”™è¯¯")
                    
                elif 'vision' in layer_name.lower():
                    print(f"NaN in vision layer {layer_name}")
                    print("å¯èƒ½åŸå› ï¼š1) å›¾åƒæœªå½’ä¸€åŒ–")
                    print("         2) å›¾åƒåŒ…å«å¼‚å¸¸å€¼ï¼ˆå…¨é»‘/å…¨ç™½ï¼‰")
                    
                elif 'proj' in layer_name.lower():
                    print(f"NaN in projection layer {layer_name}")
                    print("å¯èƒ½åŸå› ï¼š1) ç»´åº¦ä¸åŒ¹é…")
                    print("         2) åˆå§‹åŒ–ä¸å½“")
                
                self.problematic_layers.append({
                    'layer': layer_name,
                    'type': 'forward_output',
                    'stage': 'forward'
                })
        return hook
    
    def _make_backward_hook(self, layer_name):
        def hook(module, grad_input, grad_output):
            # æ£€æŸ¥æ¢¯åº¦è¾“å‡º
            for i, grad in enumerate(grad_output):
                if grad is not None and torch.isnan(grad).any():
                    self.problematic_layers.append({
                        'layer': layer_name,
                        'type': 'grad_output',
                        'index': i,
                        'stage': 'backward'
                    })
                    
                    # åˆ†æå…·ä½“åŸå› 
                    self._analyze_nan_cause(layer_name, module, grad)
        return hook
    
    def _analyze_nan_cause(self, layer_name, module, grad):
        """åˆ†æNaNçš„å…·ä½“åŸå› """
        
        # æ£€æŸ¥å¸¸è§æ“ä½œ
        if isinstance(module, nn.LayerNorm):
            print(f"LayerNorm {layer_name}: æ£€æŸ¥è¾“å…¥æ–¹å·®æ˜¯å¦ä¸º0")
            
        elif isinstance(module, nn.Softmax):
            print(f"Softmax {layer_name}: æ£€æŸ¥æ˜¯å¦æœ‰-infè¾“å…¥å¯¼è‡´exp(x)=0")
            
        elif 'loss' in layer_name.lower():
            print(f"Loss layer {layer_name}: æ£€æŸ¥log(0)æˆ–é™¤é›¶")
            
        # ç»™å‡ºä¿®å¤å»ºè®®
        print("\nå»ºè®®ä¿®å¤æ–¹æ¡ˆ:")
        print("1. æ·»åŠ epsilon: x + 1e-8")
        print("2. ä½¿ç”¨torch.clampé™åˆ¶èŒƒå›´")
        print("3. æ£€æŸ¥æ•°æ®é¢„å¤„ç†æµç¨‹")
        print("4. é™ä½å­¦ä¹ ç‡æˆ–ä½¿ç”¨æ¢¯åº¦è£å‰ª")
    
    def get_report(self):
        """ç”Ÿæˆè¯Šæ–­æŠ¥å‘Š"""
        if not self.problematic_layers:
            return "æœªæ£€æµ‹åˆ°NaN"
        
        report = "NaNæ¢¯åº¦è¯Šæ–­æŠ¥å‘Š\n" + "="*50 + "\n"
        
        # æŒ‰å‡ºç°é¡ºåºæ’åº
        for issue in self.problematic_layers:
            report += f"\nå±‚: {issue['layer']}\n"
            report += f"ç±»å‹: {issue['type']}\n"
            report += f"é˜¶æ®µ: {issue['stage']}\n"
            report += "-"*30 + "\n"
        
        # ç»™å‡ºæœ€å¯èƒ½çš„æ ¹å› 
        first_issue = self.problematic_layers[0]
        report += f"\næœ€å¯èƒ½çš„æ ¹å› : {first_issue['layer']}å±‚çš„{first_issue['type']}\n"
        
        return report
```

è¿™ä¸ªå·¥å…·èƒ½å¤Ÿï¼š
1. ç²¾ç¡®å®šä½äº§ç”ŸNaNçš„å±‚å’Œæ“ä½œ
2. åŒºåˆ†å‰å‘å’Œåå‘ä¼ æ’­ä¸­çš„NaN
3. é’ˆå¯¹VLMç‰¹æœ‰ç»„ä»¶ç»™å‡ºè¯Šæ–­
4. æä¾›å…·ä½“çš„ä¿®å¤å»ºè®®
</details>

**ç»ƒä¹  10.6ï¼šå´©æºƒé¢„æµ‹ç³»ç»Ÿ**
è®¾è®¡ä¸€ä¸ªç³»ç»Ÿï¼Œèƒ½å¤Ÿåœ¨è®­ç»ƒçœŸæ­£å´©æºƒå‰ 10-20 æ­¥é¢„æµ‹å³å°†å‘ç”Ÿçš„å´©æºƒï¼Œå¹¶è‡ªåŠ¨é‡‡å–é¢„é˜²æªæ–½ã€‚

ğŸ’¡ **æç¤º**ï¼šç›‘æ§å¤šä¸ªæŒ‡æ ‡çš„è¶‹åŠ¿å˜åŒ–

<details>
<summary>ğŸ“ å‚è€ƒç­”æ¡ˆ</summary>

```python
class CrashPredictor:
    def __init__(self, window_size=20, alert_threshold=0.8):
        self.window_size = window_size
        self.alert_threshold = alert_threshold
        self.metrics_history = defaultdict(lambda: deque(maxlen=window_size))
        self.crash_probability = 0
        
    def update_metrics(self, step, loss, grad_norm, learning_rate):
        """æ›´æ–°ç›‘æ§æŒ‡æ ‡"""
        
        # è®°å½•åŸå§‹æŒ‡æ ‡
        self.metrics_history['loss'].append(loss)
        self.metrics_history['grad_norm'].append(grad_norm)
        self.metrics_history['lr'].append(learning_rate)
        
        # è®¡ç®—å¯¼æ•°æŒ‡æ ‡
        if len(self.metrics_history['loss']) > 1:
            loss_delta = loss - self.metrics_history['loss'][-2]
            self.metrics_history['loss_delta'].append(loss_delta)
            
            # äºŒé˜¶å¯¼æ•°ï¼ˆåŠ é€Ÿåº¦ï¼‰
            if len(self.metrics_history['loss_delta']) > 1:
                loss_accel = loss_delta - self.metrics_history['loss_delta'][-2]
                self.metrics_history['loss_accel'].append(loss_accel)
        
        # é¢„æµ‹å´©æºƒæ¦‚ç‡
        self.crash_probability = self._predict_crash()
        
        return self.crash_probability
    
    def _predict_crash(self):
        """åŸºäºå¤šä¸ªä¿¡å·é¢„æµ‹å´©æºƒæ¦‚ç‡"""
        
        signals = []
        
        # ä¿¡å·1ï¼šLossè¿ç»­å¢é•¿
        if len(self.metrics_history['loss']) >= 3:
            recent_losses = list(self.metrics_history['loss'])[-3:]
            if all(recent_losses[i] < recent_losses[i+1] 
                   for i in range(len(recent_losses)-1)):
                signals.append(0.3)
        
        # ä¿¡å·2ï¼šLosså¢é•¿åŠ é€Ÿ
        if len(self.metrics_history['loss_accel']) >= 2:
            recent_accel = list(self.metrics_history['loss_accel'])[-2:]
            if all(a > 0 and a > self.metrics_history['loss'][-1] * 0.1 
                   for a in recent_accel):
                signals.append(0.4)
        
        # ä¿¡å·3ï¼šæ¢¯åº¦èŒƒæ•°æŒ‡æ•°å¢é•¿
        if len(self.metrics_history['grad_norm']) >= 3:
            recent_grads = list(self.metrics_history['grad_norm'])[-3:]
            if recent_grads[-1] > recent_grads[0] * 5:
                signals.append(0.5)
        
        # ä¿¡å·4ï¼šæ¢¯åº¦èŒƒæ•°è¶…è¿‡å†å²99åˆ†ä½
        if len(self.metrics_history['grad_norm']) >= self.window_size:
            threshold = np.percentile(self.metrics_history['grad_norm'], 99)
            if self.metrics_history['grad_norm'][-1] > threshold * 2:
                signals.append(0.6)
        
        # ç»¼åˆæ‰€æœ‰ä¿¡å·
        if not signals:
            return 0.0
        
        # ä½¿ç”¨æ¦‚ç‡ç»„åˆå…¬å¼
        combined_prob = 1.0
        for signal in signals:
            combined_prob *= (1 - signal)
        crash_prob = 1 - combined_prob
        
        return crash_prob
    
    def get_preventive_action(self):
        """æ ¹æ®å´©æºƒæ¦‚ç‡è¿”å›é¢„é˜²æªæ–½"""
        
        if self.crash_probability < 0.3:
            return None
        
        actions = []
        
        if self.crash_probability >= 0.3:
            actions.append(('save_checkpoint', 'Preventive checkpoint'))
        
        if self.crash_probability >= 0.5:
            actions.append(('reduce_lr', 0.5))  # é™ä½å­¦ä¹ ç‡50%
            actions.append(('increase_grad_clip', 0.5))  # åŠ å¼ºæ¢¯åº¦è£å‰ª
        
        if self.crash_probability >= 0.7:
            actions.append(('reduce_batch_size', 0.5))  # å‡å°batch size
            actions.append(('switch_to_fp32', True))  # åˆ‡æ¢åˆ°FP32
        
        if self.crash_probability >= 0.9:
            actions.append(('emergency_stop', True))  # ç´§æ€¥åœæ­¢
            
        return actions
    
    def apply_preventive_actions(self, actions, model, optimizer, config):
        """åº”ç”¨é¢„é˜²æªæ–½"""
        
        for action, param in actions:
            if action == 'save_checkpoint':
                save_emergency_checkpoint(model, optimizer, param)
                
            elif action == 'reduce_lr':
                for param_group in optimizer.param_groups:
                    param_group['lr'] *= param
                print(f"é™ä½å­¦ä¹ ç‡åˆ° {param_group['lr']}")
                
            elif action == 'increase_grad_clip':
                config.grad_clip_norm *= param
                print(f"åŠ å¼ºæ¢¯åº¦è£å‰ªåˆ° {config.grad_clip_norm}")
                
            elif action == 'reduce_batch_size':
                config.batch_size = int(config.batch_size * param)
                print(f"å‡å°batch sizeåˆ° {config.batch_size}")
                
            elif action == 'switch_to_fp32':
                model.float()
                print("åˆ‡æ¢åˆ°FP32ç²¾åº¦")
                
            elif action == 'emergency_stop':
                print("æ£€æµ‹åˆ°å³å°†å´©æºƒï¼Œç´§æ€¥åœæ­¢è®­ç»ƒï¼")
                return False  # åœæ­¢è®­ç»ƒ
        
        return True  # ç»§ç»­è®­ç»ƒ
```

è¯¥ç³»ç»Ÿçš„ç‰¹ç‚¹ï¼š
1. å¤šæŒ‡æ ‡è”åˆç›‘æ§ï¼ˆlossã€æ¢¯åº¦ã€å­¦ä¹ ç‡ï¼‰
2. åŸºäºè¶‹åŠ¿è€Œéå•ç‚¹å€¼åˆ¤æ–­
3. åˆ†çº§å“åº”æœºåˆ¶
4. é¢„é˜²æªæ–½é€’è¿›å¼å¢å¼º
5. ä¿ç•™ç´§æ€¥åœæ­¢é€‰é¡¹é¿å…èµ„æºæµªè´¹
</details>

## å¸¸è§é™·é˜±ä¸é”™è¯¯

### 1. å¿½è§†æ—©æœŸä¿¡å·
âŒ **é”™è¯¯**ï¼šç­‰åˆ° Loss å®Œå…¨å˜æˆ NaN æ‰å¤„ç†
âœ… **æ­£ç¡®**ï¼šåœ¨ Loss å¼€å§‹å¼‚å¸¸å¢é•¿æ—¶å°±ä»‹å…¥

### 2. è¿‡åº¦ä¾èµ–è‡ªåŠ¨æ··åˆç²¾åº¦
âŒ **é”™è¯¯**ï¼šå®Œå…¨ä¿¡ä»» AMP çš„ loss scaling
âœ… **æ­£ç¡®**ï¼šæ‰‹åŠ¨æ£€æŸ¥å…³é”®æ“ä½œçš„æ•°å€¼èŒƒå›´

### 3. Checkpoint ä¸å®Œæ•´
âŒ **é”™è¯¯**ï¼šåªä¿å­˜æ¨¡å‹æƒé‡
âœ… **æ­£ç¡®**ï¼šä¿å­˜å®Œæ•´è®­ç»ƒçŠ¶æ€ï¼ˆåŒ…æ‹¬ä¼˜åŒ–å™¨ã€éšæœºæ•°ç§å­ï¼‰

### 4. æ¢¯åº¦è£å‰ªæ—¶æœºé”™è¯¯
âŒ **é”™è¯¯**ï¼šåœ¨ loss.backward() ä¹‹å‰è£å‰ª
âœ… **æ­£ç¡®**ï¼šåœ¨ backward ä¹‹åã€optimizer.step() ä¹‹å‰è£å‰ª

### 5. å¿½ç•¥æ•°æ®é—®é¢˜
âŒ **é”™è¯¯**ï¼šåªå…³æ³¨æ¨¡å‹å’Œä¼˜åŒ–å™¨
âœ… **æ­£ç¡®**ï¼šæ£€æŸ¥æ•°æ®é¢„å¤„ç†ã€æ ‡ç­¾æ­£ç¡®æ€§ã€å¼‚å¸¸æ ·æœ¬

### 6. æ¢å¤è®­ç»ƒåä¸éªŒè¯
âŒ **é”™è¯¯**ï¼šåŠ è½½ checkpoint åç›´æ¥ç»§ç»­è®­ç»ƒ
âœ… **æ­£ç¡®**ï¼šå…ˆåœ¨éªŒè¯é›†ä¸Šæµ‹è¯•ï¼Œç¡®è®¤çŠ¶æ€æ­£ç¡®

## æœ€ä½³å®è·µæ£€æŸ¥æ¸…å•

### è®­ç»ƒå‰å‡†å¤‡
- [ ] é…ç½®å®Œæ•´çš„ checkpoint ä¿å­˜æœºåˆ¶
- [ ] è®¾ç½®åˆç†çš„æ¢¯åº¦è£å‰ªé˜ˆå€¼ï¼ˆåŸºäºå°è§„æ¨¡å®éªŒï¼‰
- [ ] å‡†å¤‡ FP32 é™çº§æ–¹æ¡ˆ
- [ ] å®ç°æ¢¯åº¦ç›‘æ§å’Œæ—¥å¿—è®°å½•
- [ ] éªŒè¯æ•°æ®åŠ è½½å’Œé¢„å¤„ç†æµç¨‹
- [ ] æµ‹è¯• checkpoint æ¢å¤æµç¨‹

### è®­ç»ƒä¸­ç›‘æ§
- [ ] æ¯ N æ­¥æ£€æŸ¥æ¢¯åº¦èŒƒæ•°åˆ†å¸ƒ
- [ ] ç›‘æ§ Loss çš„ä¸€é˜¶å’ŒäºŒé˜¶å¯¼æ•°
- [ ] å…³æ³¨å…³é”®å±‚çš„å‚æ•°å’Œæ¢¯åº¦ç»Ÿè®¡
- [ ] å®šæœŸä¿å­˜ checkpointï¼ˆè‡³å°‘æ¯å°æ—¶ï¼‰
- [ ] è®¾ç½®å¼‚å¸¸å€¼æŠ¥è­¦é˜ˆå€¼

### å´©æºƒåæ¢å¤
- [ ] åˆ†æå´©æºƒå‰çš„æ—¥å¿—å’ŒæŒ‡æ ‡
- [ ] è¯†åˆ«å´©æºƒæ¨¡å¼ï¼ˆçªå‘/æ¸è¿›/å‘¨æœŸï¼‰
- [ ] è°ƒæ•´é…ç½®ï¼ˆå­¦ä¹ ç‡ã€batch sizeã€ç²¾åº¦ï¼‰
- [ ] ä»æœ€è¿‘çš„ç¨³å®š checkpoint æ¢å¤
- [ ] éªŒè¯æ¢å¤åçš„æ¨¡å‹è¡Œä¸º
- [ ] è®°å½•é—®é¢˜å’Œè§£å†³æ–¹æ¡ˆä¾›æœªæ¥å‚è€ƒ

### é•¿æœŸä¼˜åŒ–
- [ ] å»ºç«‹å´©æºƒæ¡ˆä¾‹åº“
- [ ] æ€»ç»“ä¸åŒæ¨¡å‹æ¶æ„çš„ç¨³å®šæ€§ç‰¹ç‚¹
- [ ] ä¼˜åŒ–æ•°æ®ç®¡é“å‡å°‘å¼‚å¸¸æ ·æœ¬
- [ ] å®ç°è‡ªåŠ¨åŒ–çš„å´©æºƒæ£€æµ‹å’Œæ¢å¤
- [ ] å®šæœŸæ›´æ–°ç›‘æ§æŒ‡æ ‡å’Œé˜ˆå€¼
