# ç¬¬ 12 ç« ï¼šå¤šæœºå¤šå¡è°ƒè¯•åœ°ç‹±

å½“ä½ çš„ VLM è®­ç»ƒæ‰©å±•åˆ°å¤šæœºå¤šå¡æ—¶ï¼Œä½ å°†è¿›å…¥ä¸€ä¸ªå……æ»¡æŒ‘æˆ˜çš„è°ƒè¯•ä¸–ç•Œã€‚æœ¬ç« å°†ç³»ç»Ÿåœ°ä»‹ç»åˆ†å¸ƒå¼è®­ç»ƒä¸­æœ€å¸¸è§çš„é—®é¢˜å’Œè§£å†³æ–¹æ¡ˆï¼Œä» NCCL é€šä¿¡é”™è¯¯åˆ°è¿›ç¨‹æ­»é”ï¼Œä»å¼‚æ„ GPU æ··è®­åˆ°æ¡†æ¶é€‰æ‹©ï¼Œå¸®åŠ©ä½ å¿«é€Ÿå®šä½å’Œè§£å†³å¤šæœºè®­ç»ƒä¸­çš„å„ç§"åœ°ç‹±"çº§é—®é¢˜ã€‚æ— è®ºæ˜¯å‡Œæ™¨ä¸‰ç‚¹çš„ NCCL timeoutï¼Œè¿˜æ˜¯è«åå…¶å¦™çš„è¿›ç¨‹æŒ‚èµ·ï¼Œæœ¬ç« éƒ½èƒ½è®©ä½ åœ¨ 5 åˆ†é’Ÿå†…æ‰¾åˆ°è§£å†³æ€è·¯ã€‚

## 12.1 NCCL é”™è¯¯çš„å¸¸è§åŸå› ä¸è§£å†³

NCCLï¼ˆNVIDIA Collective Communications Libraryï¼‰æ˜¯å¤š GPU è®­ç»ƒçš„æ ¸å¿ƒé€šä¿¡åº“ã€‚å½“ä½ çœ‹åˆ° "NCCL Error" æ—¶ï¼Œä¸è¦æ…Œå¼ ï¼ŒæŒ‰ç…§ä»¥ä¸‹æµç¨‹ç³»ç»Ÿæ’æŸ¥ã€‚

### 12.1.1 å¿«é€Ÿè¯Šæ–­æµç¨‹

å½“é‡åˆ° NCCL é”™è¯¯æ—¶ï¼Œé¦–å…ˆæ‰§è¡Œä»¥ä¸‹è¯Šæ–­æ­¥éª¤ï¼š

**ç¬¬ä¸€æ­¥ï¼šå¯ç”¨è¯¦ç»†æ—¥å¿—**
```bash
export NCCL_DEBUG=INFO
export NCCL_DEBUG_SUBSYS=ALL
export TORCH_DISTRIBUTED_DEBUG=DETAIL
```

**ç¬¬äºŒæ­¥ï¼šæ£€æŸ¥åŸºç¡€è¿é€šæ€§**
```bash
# æ£€æŸ¥èŠ‚ç‚¹é—´ç½‘ç»œè¿é€šæ€§
ping <other_node_ip>
nc -zv <other_node_ip> 29500  # æ£€æŸ¥ç«¯å£æ˜¯å¦å¼€æ”¾

# æ£€æŸ¥ GPU å¯è§æ€§
nvidia-smi -L
echo $CUDA_VISIBLE_DEVICES
```

**ç¬¬ä¸‰æ­¥ï¼šè¿è¡Œ NCCL æµ‹è¯•**
```bash
# å•æœºæµ‹è¯•
python -m torch.distributed.run --nproc_per_node=8 --nnodes=1 test_nccl.py

# å¤šæœºæµ‹è¯•ï¼ˆåœ¨ä¸»èŠ‚ç‚¹è¿è¡Œï¼‰
python -m torch.distributed.run \
    --nproc_per_node=8 \
    --nnodes=2 \
    --node_rank=0 \
    --master_addr=<master_ip> \
    --master_port=29500 \
    test_nccl.py
```

### 12.1.2 å¸¸è§é”™è¯¯ç±»å‹ä¸è§£å†³æ–¹æ¡ˆ

**é”™è¯¯ 1ï¼šNCCL Init Failed**

ç—‡çŠ¶ï¼š
```
RuntimeError: NCCL error in: ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1191
unhandled system error, NCCL version 2.14.3
```

åŸå› åˆ†æï¼š
1. GPU ä¸å¯è§æˆ–ç¼–å·é”™è¯¯
2. NCCL ç‰ˆæœ¬ä¸å…¼å®¹
3. å…±äº«å†…å­˜ä¸è¶³

è§£å†³æ–¹æ¡ˆï¼š
```bash
# 1. ç¡®ä¿ GPU ç¼–å·æ­£ç¡®
export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7

# 2. å¢åŠ å…±äº«å†…å­˜
docker run --shm-size=32gb ...  # Docker ç¯å¢ƒ
# æˆ–ä¿®æ”¹ç³»ç»Ÿé…ç½®
echo "kernel.shmmax = 68719476736" >> /etc/sysctl.conf
echo "kernel.shmall = 4294967296" >> /etc/sysctl.conf
sysctl -p

# 3. é™çº§æˆ–å‡çº§ NCCL
pip install torch==2.0.1+cu118  # ä½¿ç”¨å…¼å®¹ç‰ˆæœ¬
```

**é”™è¯¯ 2ï¼šNCCL Timeout**

ç—‡çŠ¶ï¼š
```
torch.distributed.DistBackendError: NCCL timeout after 1800 seconds
```

åŸå› åˆ†æï¼š
1. ç½‘ç»œå¸¦å®½ä¸è¶³æˆ–å»¶è¿Ÿè¿‡é«˜
2. æŸä¸ªè¿›ç¨‹å¡ä½æˆ–å´©æºƒ
3. æ•°æ®ä¸å‡è¡¡å¯¼è‡´ç­‰å¾…

è§£å†³æ–¹æ¡ˆï¼š
```bash
# 1. å¢åŠ è¶…æ—¶æ—¶é—´
export NCCL_TIMEOUT=3600  # å•ä½ï¼šç§’
export NCCL_ASYNC_ERROR_HANDLING=1

# 2. ä½¿ç”¨æ›´å¿«çš„ç½‘ç»œåè®®
export NCCL_IB_DISABLE=0  # å¯ç”¨ InfiniBand
export NCCL_SOCKET_IFNAME=eth0  # æŒ‡å®šç½‘ç»œæ¥å£

# 3. æ£€æŸ¥è¿›ç¨‹çŠ¶æ€
ps aux | grep python  # æŸ¥çœ‹æ˜¯å¦æœ‰åƒµå°¸è¿›ç¨‹
htop  # æŸ¥çœ‹ CPU/å†…å­˜ä½¿ç”¨æƒ…å†µ
```

**é”™è¯¯ 3ï¼šNCCL AllReduce Failed**

ç—‡çŠ¶ï¼š
```
NCCL WARN Unhandled System Error while waiting for event
```

åŸå› åˆ†æï¼š
1. PCIe é€šä¿¡é—®é¢˜
2. NVLink é…ç½®é”™è¯¯
3. æ‹“æ‰‘ç»“æ„ä¸ä¼˜åŒ–

è§£å†³æ–¹æ¡ˆï¼š
```bash
# 1. æ£€æŸ¥ PCIe å’Œ NVLink çŠ¶æ€
nvidia-smi topo -m

# 2. ä¼˜åŒ– P2P é€šä¿¡
export NCCL_P2P_DISABLE=0
export NCCL_P2P_LEVEL=NVL  # ä½¿ç”¨ NVLink

# 3. è®¾ç½®åˆç†çš„ NCCL æ ‘å½¢æ‹“æ‰‘
export NCCL_TREE_THRESHOLD=0  # æ€»æ˜¯ä½¿ç”¨æ ‘å½¢ç®—æ³•
```

### 12.1.3 ç½‘ç»œé…ç½®ä¼˜åŒ–

**InfiniBand é…ç½®**

InfiniBand æ˜¯é«˜æ€§èƒ½è®¡ç®—çš„é¦–é€‰ç½‘ç»œï¼š

```bash
# æ£€æŸ¥ IB çŠ¶æ€
ibstat
ibping -S  # åœ¨ä¸€ä¸ªèŠ‚ç‚¹å¯åŠ¨æœåŠ¡å™¨
ibping -c <server_guid>  # åœ¨å¦ä¸€ä¸ªèŠ‚ç‚¹æµ‹è¯•

# NCCL IB é…ç½®
export NCCL_IB_HCA=mlx5_0,mlx5_1  # æŒ‡å®š HCA
export NCCL_IB_GID_INDEX=3  # RoCE ç¯å¢ƒéœ€è¦
export NCCL_IB_TC=106  # Traffic Class
export NCCL_IB_SL=0  # Service Level
```

**TCP ç½‘ç»œä¼˜åŒ–**

å½“åªæœ‰ä»¥å¤ªç½‘æ—¶çš„ä¼˜åŒ–ç­–ç•¥ï¼š

```bash
# 1. é€‰æ‹©æ­£ç¡®çš„ç½‘ç»œæ¥å£
export NCCL_SOCKET_IFNAME=eth0,eth1  # å¯ä»¥æŒ‡å®šå¤šä¸ª
export NCCL_SOCKET_NTHREADS=8  # å¢åŠ  socket çº¿ç¨‹æ•°
export NCCL_NSOCKS_PERTHREAD=4  # æ¯çº¿ç¨‹ socket æ•°

# 2. TCP ç¼“å†²åŒºä¼˜åŒ–
echo "net.core.rmem_max = 134217728" >> /etc/sysctl.conf
echo "net.core.wmem_max = 134217728" >> /etc/sysctl.conf
echo "net.ipv4.tcp_rmem = 4096 87380 134217728" >> /etc/sysctl.conf
echo "net.ipv4.tcp_wmem = 4096 65536 134217728" >> /etc/sysctl.conf
sysctl -p
```

### 12.1.4 ç¯å¢ƒå˜é‡é€ŸæŸ¥è¡¨

å…³é”® NCCL ç¯å¢ƒå˜é‡åŠå…¶ä½œç”¨ï¼š

```bash
# è°ƒè¯•ç›¸å…³
NCCL_DEBUG=INFO/WARN/ERROR  # æ—¥å¿—çº§åˆ«
NCCL_DEBUG_FILE=/path/to/log  # æ—¥å¿—è¾“å‡ºæ–‡ä»¶

# æ€§èƒ½è°ƒä¼˜
NCCL_BUFFSIZE=8388608  # ç¼“å†²åŒºå¤§å°ï¼ˆé»˜è®¤ 4MBï¼‰
NCCL_NTHREADS=512  # NCCL çº¿ç¨‹æ•°
NCCL_MAX_NCHANNELS=16  # æœ€å¤§é€šé“æ•°

# ç½‘ç»œé€‰æ‹©
NCCL_NET_GDR_LEVEL=5  # GPUDirect RDMA çº§åˆ«
NCCL_CROSS_NIC=1  # å…è®¸è·¨ NIC é€šä¿¡

# ç®—æ³•é€‰æ‹©
NCCL_ALGO=Ring/Tree/CollNet  # æŒ‡å®šç®—æ³•
NCCL_PROTO=LL/LL128/Simple  # åè®®é€‰æ‹©
```

### 12.1.5 å®æˆ˜æ¡ˆä¾‹ï¼š8Ã—A100 é›†ç¾¤è°ƒè¯•

çœŸå®æ¡ˆä¾‹ï¼šæŸå›¢é˜Ÿåœ¨ 2 èŠ‚ç‚¹ 8Ã—A100 é›†ç¾¤ä¸Šè®­ç»ƒ VLMï¼Œé‡åˆ°é—´æ­‡æ€§ NCCL é”™è¯¯ã€‚

**é—®é¢˜è¡¨ç°**ï¼š
- è®­ç»ƒè¿›è¡Œåˆ° 30% æ—¶éšæœºå‡ºç° NCCL timeout
- é”™è¯¯æ—¥å¿—æ˜¾ç¤º `unhandled cuda error`
- é‡å¯åèƒ½ç»§ç»­è®­ç»ƒä¸€æ®µæ—¶é—´

**æ’æŸ¥è¿‡ç¨‹**ï¼š
1. å¯ç”¨ NCCL_DEBUG=INFOï¼Œå‘ç°ç‰¹å®š GPU å¯¹é€šä¿¡è¶…æ—¶
2. nvidia-smi topo -m æ˜¾ç¤º GPU 6-7 ä¹‹é—´æ˜¯ PIX è¿æ¥ï¼ˆæœ€æ…¢ï¼‰
3. æ£€æŸ¥æ¸©åº¦æ—¥å¿—ï¼Œå‘ç° GPU 6 ç»å¸¸è§¦å‘æ¸©åº¦ä¿æŠ¤ï¼ˆthrottlingï¼‰

**è§£å†³æ–¹æ¡ˆ**ï¼š
```bash
# 1. è°ƒæ•´ GPU æ˜ å°„ï¼Œé¿å…ä½¿ç”¨é—®é¢˜ GPU å¯¹
export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5  # è·³è¿‡ 6,7

# 2. é™ä½åŠŸç‡ä¸Šé™é˜²æ­¢è¿‡çƒ­
nvidia-smi -pl 300  # è®¾ç½®åŠŸç‡ä¸Šé™ 300W

# 3. ä¼˜åŒ–é€šä¿¡æ¨¡å¼
export NCCL_P2P_LEVEL=PHB  # åªä½¿ç”¨åŒä¸€ PCIe æ¡¥ä¸‹çš„ P2P
export NCCL_ALGO=Tree  # ä½¿ç”¨æ ‘å½¢ç®—æ³•å‡å°‘ P2P ä¾èµ–

# 4. ç›‘æ§è„šæœ¬
watch -n 1 'nvidia-smi --query-gpu=index,name,temperature.gpu,power.draw --format=csv'
```

**æœ€ç»ˆæ•ˆæœ**ï¼š
- è®­ç»ƒç¨³å®šæ€§æå‡ï¼Œæœªå†å‡ºç° timeout
- è™½ç„¶ä½¿ç”¨ 6 ä¸ª GPUï¼Œä½†æ•´ä½“ååé‡åè€Œæå‡ 15%ï¼ˆé¿å…äº†é€šä¿¡ç“¶é¢ˆï¼‰

## 12.2 è¿›ç¨‹åŒæ­¥ä¸æ­»é”æ’æŸ¥

åˆ†å¸ƒå¼è®­ç»ƒä¸­ï¼Œè¿›ç¨‹åŒæ­¥é—®é¢˜æ˜¯ä»…æ¬¡äº NCCL é”™è¯¯çš„ç¬¬äºŒå¤§"æ€æ‰‹"ã€‚æœ¬èŠ‚å°†æ·±å…¥å‰–ææ­»é”çš„æˆå› å’Œå¿«é€Ÿå®šä½æ–¹æ³•ã€‚

### 12.2.1 åˆ†å¸ƒå¼è®­ç»ƒçš„åŒæ­¥æœºåˆ¶

ç†è§£åŒæ­¥ç‚¹æ˜¯æ’æŸ¥æ­»é”çš„åŸºç¡€ã€‚VLM è®­ç»ƒä¸­çš„ä¸»è¦åŒæ­¥ç‚¹ï¼š

**æ˜¾å¼åŒæ­¥ç‚¹**ï¼š
```python
# 1. Barrier åŒæ­¥
torch.distributed.barrier()  # æ‰€æœ‰è¿›ç¨‹å¿…é¡»åˆ°è¾¾

# 2. All-Reduce æ“ä½œ
torch.distributed.all_reduce(tensor)  # æ¢¯åº¦åŒæ­¥

# 3. Broadcast æ“ä½œ  
torch.distributed.broadcast(tensor, src=0)  # å‚æ•°å¹¿æ’­
```

**éšå¼åŒæ­¥ç‚¹**ï¼š
```python
# 1. ä¼˜åŒ–å™¨æ­¥è¿›
optimizer.step()  # DDP ä¼šè‡ªåŠ¨åŒæ­¥æ¢¯åº¦

# 2. æ¨¡å‹ä¿å­˜
if rank == 0:
    torch.save(model.state_dict(), path)
torch.distributed.barrier()  # ç­‰å¾…ä¿å­˜å®Œæˆ

# 3. æ•°æ®åŠ è½½
dataloader = DataLoader(dataset, sampler=DistributedSampler(dataset))
# DistributedSampler ç¡®ä¿å„è¿›ç¨‹æ•°æ®ä¸é‡å¤
```

### 12.2.2 æ­»é”çš„å…¸å‹åœºæ™¯

**åœºæ™¯ 1ï¼šæ¡ä»¶åˆ†æ”¯ä¸ä¸€è‡´**

é”™è¯¯ä»£ç ï¼š
```python
if rank == 0 and epoch % 10 == 0:
    # åªæœ‰ rank 0 æ‰§è¡ŒéªŒè¯
    val_loss = validate(model, val_loader)
    torch.distributed.broadcast(val_loss, src=0)  # æ­»é”ï¼å…¶ä»–è¿›ç¨‹æœªæ‰§è¡Œ
```

æ­£ç¡®åšæ³•ï¼š
```python
if epoch % 10 == 0:
    if rank == 0:
        val_loss = validate(model, val_loader)
    else:
        val_loss = torch.zeros(1).cuda()
    torch.distributed.broadcast(val_loss, src=0)  # æ‰€æœ‰è¿›ç¨‹éƒ½å‚ä¸
```

**åœºæ™¯ 2ï¼šæ•°æ®ä¸å‡è¡¡å¯¼è‡´çš„æ­»é”**

é—®é¢˜ä»£ç ï¼š
```python
for batch in dataloader:
    loss = model(batch)
    loss.backward()
    optimizer.step()  # æŸäº›è¿›ç¨‹æ•°æ®æå‰ç»“æŸï¼Œæœªå‚ä¸åŒæ­¥
```

è§£å†³æ–¹æ¡ˆï¼š
```python
# æ–¹æ¡ˆ 1ï¼šä½¿ç”¨ drop_last
dataloader = DataLoader(dataset, drop_last=True, ...)

# æ–¹æ¡ˆ 2ï¼šå¡«å……æ•°æ®
total_samples = len(dataset)
samples_per_rank = math.ceil(total_samples / world_size)
# ç¡®ä¿æ¯ä¸ªè¿›ç¨‹æœ‰ç›¸åŒæ•°é‡çš„æ‰¹æ¬¡
```

**åœºæ™¯ 3ï¼šå¼‚å¸¸å¤„ç†ä¸å½“**

å±é™©ä»£ç ï¼š
```python
try:
    output = model(input)
except Exception as e:
    print(f"Error on rank {rank}: {e}")
    continue  # è·³è¿‡è¿™ä¸ªæ‰¹æ¬¡ï¼Œä½†å…¶ä»–è¿›ç¨‹è¿˜åœ¨ç­‰å¾…åŒæ­¥ï¼
```

å®‰å…¨å¤„ç†ï¼š
```python
try:
    output = model(input)
except Exception as e:
    print(f"Error on rank {rank}: {e}")
    # é€šçŸ¥æ‰€æœ‰è¿›ç¨‹å‡ºç°å¼‚å¸¸
    error_flag = torch.tensor([1.0]).cuda()
    torch.distributed.all_reduce(error_flag)
    if error_flag.item() > 0:
        # æ‰€æœ‰è¿›ç¨‹ä¸€èµ·é€€å‡º
        cleanup_distributed()
        sys.exit(1)
```

### 12.2.3 æ­»é”å¿«é€Ÿè¯Šæ–­æ–¹æ³•

**æ–¹æ³• 1ï¼šæ·»åŠ è¶…æ—¶å’Œæ—¥å¿—**

```python
import functools
import torch.distributed as dist

def timeout_wrapper(func, timeout=300):
    @functools.wraps(func)
    def wrapper(*args, **kwargs):
        import signal
        
        def timeout_handler(signum, frame):
            raise TimeoutError(f"{func.__name__} timeout after {timeout}s")
        
        signal.signal(signal.SIGALRM, timeout_handler)
        signal.alarm(timeout)
        try:
            result = func(*args, **kwargs)
        finally:
            signal.alarm(0)
        return result
    return wrapper

# ä½¿ç”¨
@timeout_wrapper
def training_step(batch):
    # è®­ç»ƒä»£ç 
    pass
```

**æ–¹æ³• 2ï¼šè¿›ç¨‹çŠ¶æ€ç›‘æ§**

```python
import threading
import time

def monitor_thread(rank, interval=30):
    """ç›‘æ§çº¿ç¨‹ï¼Œå®šæœŸæ‰“å°è¿›ç¨‹çŠ¶æ€"""
    def run():
        step = 0
        while True:
            time.sleep(interval)
            print(f"[Rank {rank}] Heartbeat at step {step}, "
                  f"Memory: {torch.cuda.memory_allocated()/1e9:.2f}GB")
            step += 1
    
    thread = threading.Thread(target=run, daemon=True)
    thread.start()

# åœ¨è®­ç»ƒå¼€å§‹æ—¶å¯åŠ¨
monitor_thread(rank)
```

**æ–¹æ³• 3ï¼šä½¿ç”¨ py-spy åˆ†æ**

```bash
# å®‰è£… py-spy
pip install py-spy

# åˆ†ææŒ‚èµ·çš„è¿›ç¨‹
py-spy dump --pid <process_id>

# ç”Ÿæˆç«ç„°å›¾
py-spy record -d 30 -o profile.svg --pid <process_id>
```

### 12.2.4 Barrier è¶…æ—¶é—®é¢˜

**å¸¸è§åŸå› **ï¼š

1. **ä¸å‡åŒ€çš„è®¡ç®—è´Ÿè½½**
```python
# é—®é¢˜ï¼šrank 0 åšé¢å¤–çš„æ—¥å¿—è®°å½•
if rank == 0:
    # å¤æ‚çš„æ—¥å¿—è®¡ç®—
    log_metrics(model, epoch, step)
    
torch.distributed.barrier()  # rank 0 å¤ªæ…¢ï¼Œå…¶ä»–è¿›ç¨‹è¶…æ—¶
```

2. **I/O æ“ä½œä¸å½“**
```python
# é—®é¢˜ï¼šæ‰€æœ‰è¿›ç¨‹åŒæ—¶å†™å…¥
for rank in range(world_size):
    with open(f"log_{rank}.txt", "a") as f:
        f.write(metrics)  # æ–‡ä»¶ç³»ç»Ÿå‹åŠ›å¯¼è‡´æŸäº›è¿›ç¨‹é˜»å¡
```

**è§£å†³ç­–ç•¥**ï¼š

```python
# 1. ä½¿ç”¨å¼‚æ­¥ I/O
import asyncio

async def async_log(data, filename):
    async with aiofiles.open(filename, 'a') as f:
        await f.write(data)

# 2. é”™å¼€ I/O æ—¶æœº
if step % 100 == rank:  # ä¸åŒ rank åœ¨ä¸åŒæ­¥æ•°è®°å½•
    save_checkpoint(model, f"ckpt_{rank}_{step}.pt")

# 3. è®¾ç½®åˆç†çš„è¶…æ—¶
os.environ['TORCH_DISTRIBUTED_TIMEOUT'] = '3600'  # 1 å°æ—¶
```

### 12.2.5 å®æˆ˜æ¡ˆä¾‹ï¼š64 GPU è®­ç»ƒæ­»é”æ’æŸ¥

**èƒŒæ™¯**ï¼šæŸå›¢é˜Ÿä½¿ç”¨ 8 èŠ‚ç‚¹ Ã— 8 V100 è®­ç»ƒ 13B VLMï¼Œåœ¨ç¬¬ 1000 æ­¥çªç„¶æŒ‚èµ·ã€‚

**ç—‡çŠ¶**ï¼š
- æ‰€æœ‰ GPU åˆ©ç”¨ç‡é™ä¸º 0%
- CPU å ç”¨ç‡æ­£å¸¸
- æ— é”™è¯¯æ—¥å¿—è¾“å‡º

**æ’æŸ¥æ­¥éª¤**ï¼š

1. **ç¡®è®¤æŒ‚èµ·ä½ç½®**
```bash
# ä½¿ç”¨ gdb é™„åŠ åˆ°è¿›ç¨‹
gdb -p <pid>
(gdb) py-bt  # æŸ¥çœ‹ Python è°ƒç”¨æ ˆ

# å‘ç°å¡åœ¨ï¼š
File "torch/distributed/distributed_c10d.py", line 2838, in barrier
    work.wait()
```

2. **æ£€æŸ¥å„è¿›ç¨‹çŠ¶æ€**
```python
# æ·»åŠ è°ƒè¯•ä»£ç 
print(f"[Rank {rank}] Entering barrier at step {step}")
torch.distributed.barrier()
print(f"[Rank {rank}] Exited barrier")

# å‘ç° rank 43 æœªè¿›å…¥ barrier
```

3. **å®šä½é—®é¢˜ä»£ç **
```python
# åŸä»£ç 
if batch_idx > 0 and batch_idx % gradient_accumulation_steps == 0:
    optimizer.step()
    optimizer.zero_grad()

# é—®é¢˜ï¼šrank 43 çš„æ•°æ®å°‘ä¸€ä¸ª batchï¼Œæœªæ‰§è¡Œæœ€åä¸€æ¬¡ optimizer.step()
```

**è§£å†³æ–¹æ¡ˆ**ï¼š
```python
# 1. ç¡®ä¿æ‰€æœ‰ rank æ‰§è¡Œç›¸åŒæ¬¡æ•°çš„ä¼˜åŒ–æ­¥éª¤
total_steps = len(dataloader) // gradient_accumulation_steps
for step in range(total_steps):
    for _ in range(gradient_accumulation_steps):
        batch = next(iter(dataloader), None)
        if batch is not None:
            loss = model(batch)
            loss.backward()
    
    optimizer.step()
    optimizer.zero_grad()

# 2. æ·»åŠ åŒæ­¥æ£€æŸ¥ç‚¹
if step % 100 == 0:
    # åŒæ­¥æ£€æŸ¥ï¼Œç¡®ä¿æ‰€æœ‰è¿›ç¨‹è¿›åº¦ä¸€è‡´
    progress_tensor = torch.tensor([step], device='cuda')
    progress_list = [torch.zeros_like(progress_tensor) 
                    for _ in range(world_size)]
    torch.distributed.all_gather(progress_list, progress_tensor)
    
    if rank == 0:
        progress = [p.item() for p in progress_list]
        if len(set(progress)) > 1:
            print(f"Warning: Progress mismatch: {progress}")
```

## 12.3 ä¸åŒ GPU å‹å·æ··åˆè®­ç»ƒçš„å‘

åœ¨å®é™…ç”Ÿäº§ç¯å¢ƒä¸­ï¼Œä½ å¯èƒ½é¢ä¸´ A100 å’Œ V100 æ··ç”¨ã€3090 å’Œ 4090 å¹¶å­˜çš„æƒ…å†µã€‚å¼‚æ„ GPU è®­ç»ƒå……æ»¡æŒ‘æˆ˜ï¼Œæœ¬èŠ‚å°†æ­ç¤ºæ‰€æœ‰éšè—çš„é™·é˜±ã€‚

### 12.3.1 å¼‚æ„ GPU çš„ä¸»è¦æŒ‘æˆ˜

**ç¡¬ä»¶å·®å¼‚å¸¦æ¥çš„é—®é¢˜**ï¼š

| å·®å¼‚ç»´åº¦ | å½±å“ | å…¸å‹åœºæ™¯ |
|---------|------|---------|
| æ˜¾å­˜å¤§å° | OOM é£é™© | A100-80G vs A100-40G |
| è®¡ç®—èƒ½åŠ› | é€Ÿåº¦ç“¶é¢ˆ | V100 vs A100 (2.5x å·®è·) |
| ç²¾åº¦æ”¯æŒ | è®­ç»ƒä¸ç¨³å®š | 3090 (FP16) vs A100 (BF16) |
| äº’è”å¸¦å®½ | é€šä¿¡ç“¶é¢ˆ | NVLink vs PCIe |
| æ¶æ„å·®å¼‚ | åŠŸèƒ½ä¸å…¼å®¹ | Ampere vs Volta |

### 12.3.2 æ€§èƒ½ç“¶é¢ˆä¸è´Ÿè½½å‡è¡¡

**é—®é¢˜ 1ï¼šæœ¨æ¡¶æ•ˆåº”**

æœ€æ…¢çš„ GPU å†³å®šæ•´ä½“è®­ç»ƒé€Ÿåº¦ï¼š

```python
# è¯Šæ–­ä»£ç 
import time
import torch.distributed as dist

def measure_gpu_speed(rank, model, dummy_batch):
    torch.cuda.synchronize()
    start = time.time()
    
    for _ in range(100):
        output = model(dummy_batch)
        loss = output.mean()
        loss.backward()
    
    torch.cuda.synchronize()
    elapsed = time.time() - start
    
    # æ”¶é›†æ‰€æœ‰ GPU çš„æ—¶é—´
    times = [torch.zeros(1).cuda() for _ in range(dist.get_world_size())]
    torch.distributed.all_gather(times, torch.tensor([elapsed]).cuda())
    
    if rank == 0:
        times = [t.item() for t in times]
        print(f"GPU speeds: {times}")
        print(f"Slowest/Fastest ratio: {max(times)/min(times):.2f}x")
```

**è§£å†³æ–¹æ¡ˆï¼šåŠ¨æ€æ‰¹å¤§å°**

```python
class HeterogeneousDataLoader:
    def __init__(self, dataset, gpu_configs):
        """
        gpu_configs: {
            0: {'type': 'A100', 'memory': 80, 'batch_size': 8},
            1: {'type': 'V100', 'memory': 32, 'batch_size': 4},
        }
        """
        self.dataset = dataset
        self.gpu_configs = gpu_configs
        
    def get_batch_size(self, rank):
        # æ ¹æ® GPU èƒ½åŠ›åˆ†é…ä¸åŒæ‰¹å¤§å°
        return self.gpu_configs[rank]['batch_size']
    
    def create_loader(self, rank):
        batch_size = self.get_batch_size(rank)
        sampler = DistributedSampler(
            self.dataset,
            num_replicas=len(self.gpu_configs),
            rank=rank,
            shuffle=True
        )
        return DataLoader(
            self.dataset,
            batch_size=batch_size,
            sampler=sampler
        )
```

**é—®é¢˜ 2ï¼šæ˜¾å­˜ä¸å‡è¡¡**

```python
def adaptive_gradient_accumulation(rank, base_batch_size=8):
    """æ ¹æ® GPU æ˜¾å­˜åŠ¨æ€è°ƒæ•´æ¢¯åº¦ç´¯ç§¯"""
    gpu_memory = torch.cuda.get_device_properties(rank).total_memory
    
    if gpu_memory > 80 * 1024**3:  # 80GB
        micro_batch_size = base_batch_size
        accumulation_steps = 1
    elif gpu_memory > 40 * 1024**3:  # 40GB
        micro_batch_size = base_batch_size // 2
        accumulation_steps = 2
    elif gpu_memory > 24 * 1024**3:  # 24GB
        micro_batch_size = base_batch_size // 4
        accumulation_steps = 4
    else:  # 16GB or less
        micro_batch_size = 1
        accumulation_steps = base_batch_size
    
    return micro_batch_size, accumulation_steps
```

### 12.3.3 æ··åˆç²¾åº¦çš„å…¼å®¹æ€§é—®é¢˜

**BF16 vs FP16 æ··ç”¨é™·é˜±**ï¼š

```python
def setup_mixed_precision(rank):
    """å¤„ç†ä¸åŒ GPU çš„ç²¾åº¦å·®å¼‚"""
    gpu_name = torch.cuda.get_device_name(rank)
    
    if 'A100' in gpu_name or 'H100' in gpu_name:
        # æ”¯æŒ BF16
        dtype = torch.bfloat16
        use_bf16 = True
    else:
        # åªæ”¯æŒ FP16
        dtype = torch.float16
        use_bf16 = False
    
    # ç»Ÿä¸€ç²¾åº¦è®¾ç½®
    all_use_bf16 = torch.tensor([use_bf16], dtype=torch.bool).cuda()
    dist.all_reduce(all_use_bf16, op=dist.ReduceOp.MIN)
    
    if all_use_bf16.item():
        return torch.bfloat16
    else:
        # é™çº§åˆ° FP16
        if rank == 0:
            print("Warning: Falling back to FP16 due to hardware limitations")
        return torch.float16
```

**æ¢¯åº¦åŒæ­¥ç²¾åº¦é—®é¢˜**ï¼š

```python
class MixedPrecisionOptimizer:
    def __init__(self, optimizer, model, dtype):
        self.optimizer = optimizer
        self.model = model
        self.dtype = dtype
        self.grad_scaler = torch.cuda.amp.GradScaler(enabled=(dtype == torch.float16))
        
    def step(self):
        # æ¢¯åº¦è½¬æ¢åˆ°ç»Ÿä¸€ç²¾åº¦
        for param in self.model.parameters():
            if param.grad is not None:
                # ç¡®ä¿æ¢¯åº¦ç²¾åº¦ä¸€è‡´
                param.grad = param.grad.to(dtype=torch.float32)
        
        # åŒæ­¥å‰è½¬æ¢
        for param in self.model.parameters():
            if param.grad is not None:
                dist.all_reduce(param.grad)
                param.grad = param.grad / dist.get_world_size()
        
        # ä¼˜åŒ–å™¨æ­¥è¿›
        if self.dtype == torch.float16:
            self.grad_scaler.step(self.optimizer)
            self.grad_scaler.update()
        else:
            self.optimizer.step()
```

### 12.3.4 å®æˆ˜é…ç½®ç¤ºä¾‹

**åœºæ™¯ï¼š4Ã—A100-80G + 4Ã—V100-32G æ··åˆé›†ç¾¤**

```python
# é…ç½®æ–‡ä»¶ heterogeneous_config.yaml
gpu_groups:
  high_tier:  # A100-80G
    ranks: [0, 1, 2, 3]
    batch_size: 8
    gradient_accumulation: 1
    precision: bfloat16
    
  low_tier:  # V100-32G  
    ranks: [4, 5, 6, 7]
    batch_size: 3
    gradient_accumulation: 3
    precision: float16

communication:
  # åˆ†ç»„é€šä¿¡ç­–ç•¥
  allreduce_groups:
    - [0, 1, 2, 3]  # A100 å†…éƒ¨å…ˆåŒæ­¥
    - [4, 5, 6, 7]  # V100 å†…éƒ¨å…ˆåŒæ­¥
  
  # è·¨ç»„åŒæ­¥ä½¿ç”¨å¼‚æ­¥æ¨¡å¼
  cross_group_async: true
  
training:
  # ä½¿ç”¨ pipeline å¹¶è¡Œç¼“è§£ä¸å‡è¡¡
  pipeline_parallel: true
  pipeline_stages:
    - ranks: [0, 1]  # A100 å¤„ç†å‰é¢å±‚
    - ranks: [2, 3]  # A100 å¤„ç†ä¸­é—´å±‚
    - ranks: [4, 5, 6, 7]  # V100 å¤„ç†åé¢å±‚ï¼ˆè®¡ç®—é‡è¾ƒå°ï¼‰
```

å®ç°ä»£ç ï¼š

```python
class HeterogeneousTrainer:
    def __init__(self, config_path):
        self.config = yaml.load(open(config_path))
        self.rank = dist.get_rank()
        self.setup_gpu_group()
        
    def setup_gpu_group(self):
        # ç¡®å®šå½“å‰ GPU æ‰€å±ç»„
        for group_name, group_config in self.config['gpu_groups'].items():
            if self.rank in group_config['ranks']:
                self.group_name = group_name
                self.group_config = group_config
                break
        
        # åˆ›å»ºé€šä¿¡ç»„
        for group_ranks in self.config['communication']['allreduce_groups']:
            group = dist.new_group(ranks=group_ranks)
            if self.rank in group_ranks:
                self.local_group = group
    
    def train_step(self, batch):
        # æ ¹æ®ç»„é…ç½®è°ƒæ•´æ‰¹å¤§å°
        micro_batch = self._split_batch(batch, self.group_config['batch_size'])
        
        total_loss = 0
        for step in range(self.group_config['gradient_accumulation']):
            with torch.cuda.amp.autocast(
                dtype=getattr(torch, self.group_config['precision'])
            ):
                loss = self.model(micro_batch[step])
                loss = loss / self.group_config['gradient_accumulation']
            
            loss.backward()
            total_loss += loss.item()
        
        # åˆ†å±‚åŒæ­¥ç­–ç•¥
        self._hierarchical_allreduce()
        
        return total_loss
    
    def _hierarchical_allreduce(self):
        # ç¬¬ä¸€æ­¥ï¼šç»„å†…åŒæ­¥
        for param in self.model.parameters():
            if param.grad is not None:
                dist.all_reduce(param.grad, group=self.local_group)
        
        # ç¬¬äºŒæ­¥ï¼šè·¨ç»„åŒæ­¥ï¼ˆä»…ç»„é•¿å‚ä¸ï¼‰
        if self.rank == self.group_config['ranks'][0]:
            for param in self.model.parameters():
                if param.grad is not None:
                    dist.all_reduce(param.grad)  # å…¨å±€åŒæ­¥
        
        # ç¬¬ä¸‰æ­¥ï¼šç»„å†…å¹¿æ’­
        for param in self.model.parameters():
            if param.grad is not None:
                dist.broadcast(param.grad, 
                             src=self.group_config['ranks'][0],
                             group=self.local_group)
```

### 12.3.5 è°ƒè¯•æŠ€å·§ä¸ç›‘æ§

**å¼‚æ„é›†ç¾¤ç›‘æ§è„šæœ¬**ï¼š

```bash
#!/bin/bash
# monitor_heterogeneous.sh

while true; do
    echo "=== GPU Status at $(date) ==="
    
    # æ”¶é›†æ‰€æœ‰èŠ‚ç‚¹çš„ GPU ä¿¡æ¯
    for node in node1 node2; do
        echo "Node: $node"
        ssh $node "nvidia-smi --query-gpu=index,name,memory.used,memory.total,utilization.gpu,temperature.gpu,power.draw --format=csv"
    done
    
    # æ£€æŸ¥é€Ÿåº¦å·®å¼‚
    echo "=== Training Speed ==="
    tail -n 8 training.log | grep "step_time"
    
    # æ£€æŸ¥æ˜¯å¦æœ‰ OOM
    if grep -q "out of memory" training.log; then
        echo "WARNING: OOM detected!"
        grep "out of memory" training.log | tail -n 5
    fi
    
    sleep 30
done
```

## 12.4 FSDP vs DeepSpeed å®æˆ˜å¯¹æ¯”

é€‰æ‹© FSDP è¿˜æ˜¯ DeepSpeedï¼Ÿè¿™æ˜¯æ¯ä¸ªå¤§æ¨¡å‹è®­ç»ƒè€…éƒ½ä¼šé¢ä¸´çš„é—®é¢˜ã€‚æœ¬èŠ‚é€šè¿‡å®æˆ˜å¯¹æ¯”ï¼Œå¸®ä½ åšå‡ºæœ€ä½³é€‰æ‹©ã€‚

### 12.4.1 æ¶æ„å·®å¼‚ä¸è®¾è®¡ç†å¿µ

| ç‰¹æ€§ | FSDP | DeepSpeed |
|------|------|-----------|
| å¼€å‘è€… | Meta/PyTorch åŸç”Ÿ | Microsoft |
| é›†æˆåº¦ | PyTorch å†…ç½® | éœ€è¦é¢å¤–å®‰è£… |
| å­¦ä¹ æ›²çº¿ | ç›¸å¯¹ç®€å• | åŠŸèƒ½ä¸°å¯Œä½†å¤æ‚ |
| ä¼˜åŒ–å™¨çŠ¶æ€åˆ†ç‰‡ | âœ… | âœ… (ZeRO-2/3) |
| å‚æ•°åˆ†ç‰‡ | âœ… | âœ… (ZeRO-3) |
| æ¿€æ´»å€¼åˆ†ç‰‡ | éƒ¨åˆ†æ”¯æŒ | âœ… (ZeRO-R) |
| CPU Offload | âœ… | âœ… æ›´æˆç†Ÿ |
| æ··åˆç²¾åº¦ | åŸç”Ÿæ”¯æŒ | éœ€è¦é…ç½® |
| Pipeline å¹¶è¡Œ | âŒ | âœ… |

### 12.4.2 VLM è®­ç»ƒé…ç½®å¯¹æ¯”

**FSDP é…ç½®ç¤ºä¾‹**ï¼š

```python
from torch.distributed.fsdp import (
    FullyShardedDataParallel as FSDP,
    MixedPrecision,
    BackwardPrefetch,
    ShardingStrategy,
    CPUOffload
)
from torch.distributed.fsdp.wrap import (
    transformer_auto_wrap_policy,
    size_based_auto_wrap_policy
)

def setup_fsdp_for_vlm(model, vision_encoder, language_model):
    # æ··åˆç²¾åº¦é…ç½®
    mp_policy = MixedPrecision(
        param_dtype=torch.bfloat16,
        reduce_dtype=torch.bfloat16,
        buffer_dtype=torch.bfloat16,
        cast_forward_inputs=True
    )
    
    # è‡ªåŠ¨åŒ…è£…ç­–ç•¥ - å…³é”®ï¼
    auto_wrap_policy = functools.partial(
        transformer_auto_wrap_policy,
        transformer_layer_cls={
            # è§†è§‰ç¼–ç å™¨å±‚
            vision_encoder.__class__,
            # è¯­è¨€æ¨¡å‹å±‚  
            type(language_model.layers[0]),
        }
    )
    
    # CPU Offloadï¼ˆæ˜¾å­˜ä¸è¶³æ—¶å¯ç”¨ï¼‰
    cpu_offload = CPUOffload(offload_params=True)
    
    # FSDP é…ç½®
    model = FSDP(
        model,
        auto_wrap_policy=auto_wrap_policy,
        mixed_precision=mp_policy,
        sharding_strategy=ShardingStrategy.FULL_SHARD,  # å®Œå…¨åˆ†ç‰‡
        cpu_offload=cpu_offload,
        backward_prefetch=BackwardPrefetch.BACKWARD_PRE,  # é¢„å–ä¼˜åŒ–
        limit_all_gathers=True,  # é™åˆ¶ all-gather é˜²æ­¢ OOM
        use_orig_params=True,  # ä¿æŒåŸå§‹å‚æ•°ï¼ˆé‡è¦ï¼ï¼‰
    )
    
    return model
```

**DeepSpeed é…ç½®ç¤ºä¾‹**ï¼š

```json
{
    "train_batch_size": 64,
    "gradient_accumulation_steps": 8,
    "train_micro_batch_size_per_gpu": 2,
    
    "bf16": {
        "enabled": true
    },
    
    "zero_optimization": {
        "stage": 3,
        "offload_optimizer": {
            "device": "cpu",
            "pin_memory": true
        },
        "offload_param": {
            "device": "cpu",
            "pin_memory": true
        },
        "overlap_comm": true,
        "contiguous_gradients": true,
        "sub_group_size": 1e9,
        "reduce_bucket_size": 1e9,
        "stage3_prefetch_bucket_size": 1e8,
        "stage3_param_persistence_threshold": 1e6,
        "stage3_max_live_parameters": 1e9,
        "stage3_max_reuse_distance": 1e9,
        "stage3_gather_16bit_weights_on_model_save": true
    },
    
    "gradient_clipping": 1.0,
    
    "optimizer": {
        "type": "AdamW",
        "params": {
            "lr": 2e-5,
            "betas": [0.9, 0.999],
            "eps": 1e-8,
            "weight_decay": 0.01
        }
    },
    
    "scheduler": {
        "type": "WarmupCosineLR",
        "params": {
            "warmup_min_lr": 0,
            "warmup_max_lr": 2e-5,
            "warmup_num_steps": 1000,
            "total_num_steps": 10000
        }
    },
    
    "activation_checkpointing": {
        "partition_activations": true,
        "cpu_checkpointing": false,
        "contiguous_memory_optimization": false,
        "number_checkpoints": null,
        "synchronize_checkpoint_boundary": false,
        "profile": false
    }
}
```

### 12.4.3 æ€§èƒ½åŸºå‡†æµ‹è¯•

**æµ‹è¯•ç¯å¢ƒ**ï¼š
- æ¨¡å‹ï¼šLLaVA-13B
- ç¡¬ä»¶ï¼š8Ã—A100-40G
- æ•°æ®ï¼šå›¾æ–‡å¯¹ï¼Œæ‰¹å¤§å° 64

**æµ‹è¯•ç»“æœ**ï¼š

| æŒ‡æ ‡ | FSDP | DeepSpeed ZeRO-2 | DeepSpeed ZeRO-3 |
|------|------|-----------------|-----------------|
| ååé‡ (samples/s) | 28.5 | 31.2 | 26.8 |
| æ˜¾å­˜å ç”¨ (GB) | 35.2 | 32.1 | 28.9 |
| é€šä¿¡å¼€é”€ (%) | 18% | 15% | 22% |
| å¯åŠ¨æ—¶é—´ (s) | 45 | 62 | 78 |
| Checkpoint å¤§å° (GB) | 26 | 26 | 52 (åˆ†ç‰‡) |

**æ€§èƒ½åˆ†æä»£ç **ï¼š

```python
import time
import torch
from contextlib import contextmanager

@contextmanager
def profile_time(name):
    torch.cuda.synchronize()
    start = time.time()
    yield
    torch.cuda.synchronize()
    print(f"{name}: {time.time() - start:.2f}s")

def benchmark_training_step(model, batch, optimizer, use_fsdp=True):
    metrics = {}
    
    # Forward
    with profile_time("Forward"):
        output = model(batch)
        loss = output.loss
    metrics['loss'] = loss.item()
    
    # Backward
    with profile_time("Backward"):
        loss.backward()
    
    # Optimizer step
    with profile_time("Optimizer"):
        optimizer.step()
        optimizer.zero_grad()
    
    # å†…å­˜ç»Ÿè®¡
    metrics['memory_allocated'] = torch.cuda.memory_allocated() / 1e9
    metrics['memory_reserved'] = torch.cuda.memory_reserved() / 1e9
    
    if use_fsdp:
        # FSDP ç‰¹å®šæŒ‡æ ‡
        if hasattr(model, '_fsdp_wrapped_module'):
            metrics['communication_time'] = model._communication_time
    else:
        # DeepSpeed ç‰¹å®šæŒ‡æ ‡
        if hasattr(optimizer, 'timer_names'):
            for name in optimizer.timer_names:
                metrics[f'ds_{name}'] = optimizer.timers(name)
    
    return metrics
```

### 12.4.4 é€‰æ‹©å»ºè®®ä¸è¿ç§»æŒ‡å—

**ä½•æ—¶é€‰æ‹© FSDP**ï¼š
1. PyTorch åŸç”Ÿé¡¹ç›®ï¼Œä¸æƒ³å¼•å…¥é¢å¤–ä¾èµ–
2. æ¨¡å‹ç›¸å¯¹ç®€å•ï¼Œä¸éœ€è¦å¤æ‚çš„å¹¶è¡Œç­–ç•¥
3. å›¢é˜Ÿç†Ÿæ‚‰ PyTorchï¼Œå­¦ä¹ æˆæœ¬ä½
4. éœ€è¦ä¸å…¶ä»– PyTorch ç”Ÿæ€å·¥å…·é›†æˆ

**ä½•æ—¶é€‰æ‹© DeepSpeed**ï¼š
1. è¶…å¤§æ¨¡å‹ï¼ˆ>30Bï¼‰ï¼Œéœ€è¦æè‡´ä¼˜åŒ–
2. éœ€è¦ Pipeline å¹¶è¡Œç­‰é«˜çº§ç‰¹æ€§
3. æ··åˆè®­ç»ƒç¯å¢ƒï¼Œéœ€è¦æ›´ç»†ç²’åº¦æ§åˆ¶
4. å·²æœ‰ DeepSpeed ç»éªŒç§¯ç´¯

**ä» FSDP è¿ç§»åˆ° DeepSpeed**ï¼š

```python
# è¿ç§»æ£€æŸ¥æ¸…å•
migration_checklist = {
    "æ¨¡å‹åŒ…è£…": "FSDP(...) -> deepspeed.initialize(...)",
    "ä¼˜åŒ–å™¨": "éœ€è¦åœ¨ config ä¸­é…ç½®ï¼Œä¸èƒ½ç›´æ¥ä¼ å…¥",
    "æ¢¯åº¦ç´¯ç§¯": "è‡ªåŠ¨å¤„ç† -> éœ€è¦æ˜¾å¼é…ç½®",
    "Checkpoint": "torch.save -> model.save_checkpoint",
    "æ··åˆç²¾åº¦": "MixedPrecision -> fp16/bf16 config",
    "å­¦ä¹ ç‡è°ƒåº¦": "æ‰‹åŠ¨ -> é…ç½®æ–‡ä»¶",
}

# è¿ç§»ç¤ºä¾‹
def migrate_fsdp_to_deepspeed(fsdp_model, fsdp_optimizer):
    # 1. æå–åŸå§‹æ¨¡å‹
    if hasattr(fsdp_model, 'module'):
        base_model = fsdp_model.module
    else:
        base_model = fsdp_model
    
    # 2. åˆ›å»º DeepSpeed é…ç½®
    ds_config = {
        "train_batch_size": world_size * batch_size,
        "gradient_accumulation_steps": grad_acc_steps,
        "zero_optimization": {
            "stage": 3 if was_full_shard else 2,
            # å…¶ä»–é…ç½®...
        }
    }
    
    # 3. åˆå§‹åŒ– DeepSpeed
    model, optimizer, _, _ = deepspeed.initialize(
        model=base_model,
        config=ds_config,
        model_parameters=base_model.parameters()
    )
    
    return model, optimizer
```

### 12.4.5 å¸¸è§é—®é¢˜ä¸è§£å†³æ–¹æ¡ˆ

**é—®é¢˜ 1ï¼šFSDP OOM ä½† DeepSpeed æ­£å¸¸**

åŸå› ï¼šFSDP çš„ all-gather æ“ä½œå¯èƒ½å¯¼è‡´ç¬æ—¶æ˜¾å­˜å³°å€¼ã€‚

è§£å†³æ–¹æ¡ˆï¼š
```python
# FSDP é™åˆ¶ all-gather
model = FSDP(
    model,
    limit_all_gathers=True,
    forward_prefetch=True,  # å‰å‘é¢„å–
    backward_prefetch=BackwardPrefetch.BACKWARD_PRE,
)
```

**é—®é¢˜ 2ï¼šDeepSpeed è®­ç»ƒé€Ÿåº¦æ…¢**

åŸå› ï¼šZeRO-3 çš„å‚æ•°æ”¶é›†å¼€é”€ã€‚

è§£å†³æ–¹æ¡ˆï¼š
```python
# ä¼˜åŒ– ZeRO-3 é…ç½®
"zero_optimization": {
    "stage": 3,
    "stage3_max_live_parameters": 2e9,  # å¢åŠ ç¼“å­˜
    "stage3_max_reuse_distance": 2e9,
    "stage3_prefetch_bucket_size": 2e8,  # å¢åŠ é¢„å–
}
```

**é—®é¢˜ 3ï¼šCheckpoint ä¸å…¼å®¹**

```python
def convert_checkpoint(checkpoint_path, from_format="fsdp", to_format="deepspeed"):
    """è½¬æ¢ä¸åŒæ ¼å¼çš„ checkpoint"""
    
    if from_format == "fsdp" and to_format == "deepspeed":
        # FSDP -> DeepSpeed
        state_dict = torch.load(checkpoint_path)
        
        # FSDP å¯èƒ½æœ‰ _fsdp å‰ç¼€
        new_state_dict = {}
        for key, value in state_dict.items():
            new_key = key.replace("_fsdp_wrapped_module.", "")
            new_key = new_key.replace("_fpw_module.", "")
            new_state_dict[new_key] = value
        
        # DeepSpeed æœŸæœ›çš„æ ¼å¼
        ds_checkpoint = {
            "module": new_state_dict,
            "epoch": 0,
            "global_step": 0,
        }
        
        torch.save(ds_checkpoint, checkpoint_path.replace(".pt", "_ds.pt"))
    
    elif from_format == "deepspeed" and to_format == "fsdp":
        # DeepSpeed -> FSDP
        # DeepSpeed ZeRO-3 éœ€è¦å…ˆæ”¶é›†åˆ†ç‰‡
        from deepspeed.utils.zero_to_fp32 import convert_zero_checkpoint_to_fp32_state_dict
        
        convert_zero_checkpoint_to_fp32_state_dict(
            checkpoint_path,
            checkpoint_path.replace("ds", "fsdp.pt")
        )
```

## æœ¬ç« å°ç»“

å¤šæœºå¤šå¡è®­ç»ƒæ˜¯ VLM æ‰©å±•çš„å¿…ç»ä¹‹è·¯ï¼Œä½†ä¹Ÿå……æ»¡æŒ‘æˆ˜ã€‚æœ¬ç« ç³»ç»Ÿä»‹ç»äº†åˆ†å¸ƒå¼è®­ç»ƒä¸­æœ€å¸¸è§çš„å››ç±»é—®é¢˜ï¼š

1. **NCCL é€šä¿¡é”™è¯¯**ï¼šæŒæ¡äº†å¿«é€Ÿè¯Šæ–­æµç¨‹ã€ç¯å¢ƒå˜é‡é…ç½®å’Œç½‘ç»œä¼˜åŒ–ç­–ç•¥ã€‚è®°ä½ï¼Œå¤§éƒ¨åˆ† NCCL é”™è¯¯éƒ½å¯ä»¥é€šè¿‡æ­£ç¡®çš„ç¯å¢ƒå˜é‡å’Œç½‘ç»œé…ç½®è§£å†³ã€‚

2. **è¿›ç¨‹åŒæ­¥ä¸æ­»é”**ï¼šç†è§£äº†åˆ†å¸ƒå¼è®­ç»ƒçš„åŒæ­¥æœºåˆ¶ï¼Œå­¦ä¼šäº†è¯†åˆ«å’Œé¿å…æ­»é”çš„å…¸å‹åœºæ™¯ã€‚å…³é”®æ˜¯ç¡®ä¿æ‰€æœ‰è¿›ç¨‹æ‰§è¡Œç›¸åŒçš„é›†åˆé€šä¿¡æ“ä½œã€‚

3. **å¼‚æ„ GPU è®­ç»ƒ**ï¼šäº†è§£äº†æ··åˆ GPU è®­ç»ƒçš„æŒ‘æˆ˜å’Œè§£å†³æ–¹æ¡ˆã€‚æ ¸å¿ƒæ€æƒ³æ˜¯æ ¹æ®ç¡¬ä»¶èƒ½åŠ›åŠ¨æ€è°ƒæ•´æ‰¹å¤§å°å’Œæ¢¯åº¦ç´¯ç§¯ç­–ç•¥ã€‚

4. **FSDP vs DeepSpeed**ï¼šé€šè¿‡å®æˆ˜å¯¹æ¯”ï¼Œæ˜ç¡®äº†ä¸¤ç§æ¡†æ¶çš„ä¼˜åŠ£å’Œé€‚ç”¨åœºæ™¯ã€‚FSDP æ›´ç®€å•ç›´æ¥ï¼ŒDeepSpeed åŠŸèƒ½æ›´ä¸°å¯Œã€‚

**å…³é”®å…¬å¼å›é¡¾**ï¼š

æœ‰æ•ˆæ‰¹å¤§å°è®¡ç®—ï¼š
$$\text{Effective Batch Size} = \text{World Size} \times \text{Micro Batch Size} \times \text{Gradient Accumulation Steps}$$

é€šä¿¡æ—¶é—´ä¼°ç®—ï¼š
$$T_{\text{comm}} = \frac{\text{Data Size}}{\text{Bandwidth}} + \text{Latency} \times \text{Num Operations}$$

æ˜¾å­˜å ç”¨ï¼ˆZeRO-3ï¼‰ï¼š
$$M_{\text{per GPU}} = \frac{M_{\text{model}} + M_{\text{optimizer}} + M_{\text{gradients}}}{\text{World Size}} + M_{\text{activations}}$$

## ç»ƒä¹ é¢˜

### åŸºç¡€é¢˜

**ç»ƒä¹  12.1ï¼šNCCL ç¯å¢ƒå˜é‡é…ç½®**

ä½ çš„ 8 å¡ V100 æœåŠ¡å™¨è®­ç»ƒæ—¶ç»å¸¸å‡ºç° NCCL timeoutï¼Œè¯·å†™å‡ºå®Œæ•´çš„ç¯å¢ƒå˜é‡é…ç½®æ¥ä¼˜åŒ–é€šä¿¡ã€‚

ğŸ’¡ **æç¤º**ï¼šè€ƒè™‘è¶…æ—¶æ—¶é—´ã€æ—¥å¿—çº§åˆ«ã€P2P é€šä¿¡å’Œç½‘ç»œæ¥å£é€‰æ‹©ã€‚

<details>
<summary>ğŸ“ å‚è€ƒç­”æ¡ˆ</summary>

```bash
# å¢åŠ è¶…æ—¶æ—¶é—´
export NCCL_TIMEOUT=7200  # 2å°æ—¶

# å¯ç”¨è°ƒè¯•æ—¥å¿—
export NCCL_DEBUG=INFO
export NCCL_DEBUG_SUBSYS=INIT,GRAPH

# ä¼˜åŒ– P2P é€šä¿¡
export NCCL_P2P_DISABLE=0
export NCCL_P2P_LEVEL=NVL

# æŒ‡å®šç½‘ç»œæ¥å£
export NCCL_SOCKET_IFNAME=eth0
export NCCL_IB_DISABLE=1  # å¦‚æœæ²¡æœ‰ InfiniBand

# ä¼˜åŒ–ç¼“å†²åŒº
export NCCL_BUFFSIZE=8388608
export NCCL_NTHREADS=256

# æ ‘å½¢ç®—æ³•ä¼˜åŒ–
export NCCL_TREE_THRESHOLD=0
export NCCL_ALGO=Tree
```

è¿™å¥—é…ç½®å¢åŠ äº†è¶…æ—¶å®¹å¿åº¦ï¼Œå¯ç”¨äº†è¯¦ç»†æ—¥å¿—ä¾¿äºè°ƒè¯•ï¼Œä¼˜åŒ–äº† P2P å’Œç½‘ç»œé€šä¿¡ï¼Œé€‚åˆå¤§å¤šæ•° V100 é›†ç¾¤ã€‚
</details>

**ç»ƒä¹  12.2ï¼šæ­»é”è¯Šæ–­**

ä»¥ä¸‹ä»£ç åœ¨ 4 å¡è®­ç»ƒæ—¶ä¼šæ­»é”ï¼Œè¯·æ‰¾å‡ºåŸå› å¹¶ä¿®å¤ï¼š

```python
def validation_step(rank, model, val_loader):
    if rank == 0:
        model.eval()
        total_loss = 0
        for batch in val_loader:
            loss = model(batch).loss
            total_loss += loss.item()
        avg_loss = total_loss / len(val_loader)
        torch.distributed.broadcast(torch.tensor([avg_loss]).cuda(), src=0)
    return avg_loss
```

ğŸ’¡ **æç¤º**ï¼šè€ƒè™‘æ‰€æœ‰è¿›ç¨‹çš„æ‰§è¡Œè·¯å¾„ã€‚

<details>
<summary>ğŸ“ å‚è€ƒç­”æ¡ˆ</summary>

é—®é¢˜ï¼šåªæœ‰ rank 0 æ‰§è¡Œ broadcastï¼Œå…¶ä»–è¿›ç¨‹æ²¡æœ‰å¯¹åº”çš„ broadcast è°ƒç”¨ï¼Œå¯¼è‡´æ­»é”ã€‚

ä¿®å¤æ–¹æ¡ˆï¼š
```python
def validation_step(rank, model, val_loader):
    if rank == 0:
        model.eval()
        total_loss = 0
        for batch in val_loader:
            loss = model(batch).loss
            total_loss += loss.item()
        avg_loss = torch.tensor([total_loss / len(val_loader)]).cuda()
    else:
        avg_loss = torch.zeros(1).cuda()
    
    # æ‰€æœ‰è¿›ç¨‹éƒ½å‚ä¸ broadcast
    torch.distributed.broadcast(avg_loss, src=0)
    return avg_loss.item()
```
</details>

**ç»ƒä¹  12.3ï¼šå¼‚æ„ GPU æ‰¹å¤§å°è®¡ç®—**

ä½ æœ‰ 2 å¼  A100-80G å’Œ 2 å¼  V100-32Gï¼Œç›®æ ‡æ˜¯æ€»æ‰¹å¤§å° 64ã€‚è¯·è®¾è®¡æ¯ä¸ª GPU çš„ micro batch size å’Œæ¢¯åº¦ç´¯ç§¯æ­¥æ•°ã€‚

ğŸ’¡ **æç¤º**ï¼šA100 çš„è®¡ç®—èƒ½åŠ›çº¦æ˜¯ V100 çš„ 2.5 å€ã€‚

<details>
<summary>ğŸ“ å‚è€ƒç­”æ¡ˆ</summary>

æ ¹æ®æ˜¾å­˜å’Œè®¡ç®—èƒ½åŠ›åˆ†é…ï¼š

```python
config = {
    "A100-80G": {
        "micro_batch_size": 8,  # å……åˆ†åˆ©ç”¨æ˜¾å­˜
        "gradient_accumulation_steps": 2,
        "effective_batch_per_gpu": 16
    },
    "V100-32G": {
        "micro_batch_size": 3,  # æ˜¾å­˜é™åˆ¶
        "gradient_accumulation_steps": 5,  # è¡¥å¿å°æ‰¹æ¬¡
        "effective_batch_per_gpu": 15
    }
}

# éªŒè¯ï¼š
# 2 * 16 (A100) + 2 * 15 (V100) = 62 â‰ˆ 64
# å¯ä»¥é€šè¿‡è°ƒæ•´æœ€åä¸€ä¸ª batch æ¥ç²¾ç¡®è¾¾åˆ° 64
```

è¿™ç§é…ç½®å¹³è¡¡äº†æ˜¾å­˜ä½¿ç”¨å’Œè®¡ç®—æ•ˆç‡ï¼Œé¿å…äº†æœ¨æ¡¶æ•ˆåº”ã€‚
</details>

### æŒ‘æˆ˜é¢˜

**ç»ƒä¹  12.4ï¼šFSDP å†…å­˜ä¼˜åŒ–**

ä½ çš„ LLaVA-34B æ¨¡å‹åœ¨ 8Ã—A100-40G ä¸Šç”¨ FSDP è®­ç»ƒæ—¶ OOMã€‚è¯·æä¾›å®Œæ•´çš„ä¼˜åŒ–æ–¹æ¡ˆï¼ŒåŒ…æ‹¬é…ç½®å’Œä»£ç ã€‚

ğŸ’¡ **æç¤º**ï¼šè€ƒè™‘ CPU offloadã€æ¿€æ´»æ£€æŸ¥ç‚¹ã€åˆ†ç‰‡ç­–ç•¥ç­‰ã€‚

<details>
<summary>ğŸ“ å‚è€ƒç­”æ¡ˆ</summary>

```python
from torch.distributed.fsdp import (
    FullyShardedDataParallel as FSDP,
    MixedPrecision,
    BackwardPrefetch,
    ShardingStrategy,
    CPUOffload
)
from torch.distributed.algorithms._checkpoint.checkpoint_wrapper import (
    checkpoint_wrapper,
    CheckpointImpl,
    apply_activation_checkpointing
)

def optimize_fsdp_for_large_vlm(model):
    # 1. æ¿€æ´»æ£€æŸ¥ç‚¹
    check_fn = lambda m: isinstance(m, TransformerBlock)
    apply_activation_checkpointing(
        model,
        checkpoint_wrapper_fn=functools.partial(
            checkpoint_wrapper,
            checkpoint_impl=CheckpointImpl.NO_REENTRANT
        ),
        check_fn=check_fn
    )
    
    # 2. æ··åˆç²¾åº¦ - ä½¿ç”¨ BF16
    mp_policy = MixedPrecision(
        param_dtype=torch.bfloat16,
        reduce_dtype=torch.float32,  # æ¢¯åº¦ç”¨ FP32 æ›´ç¨³å®š
        buffer_dtype=torch.bfloat16
    )
    
    # 3. CPU Offload
    cpu_offload = CPUOffload(offload_params=True)
    
    # 4. ä¼˜åŒ–çš„åˆ†ç‰‡ç­–ç•¥
    model = FSDP(
        model,
        auto_wrap_policy=functools.partial(
            transformer_auto_wrap_policy,
            transformer_layer_cls={TransformerBlock},
        ),
        mixed_precision=mp_policy,
        sharding_strategy=ShardingStrategy.HYBRID_SHARD,  # æ··åˆåˆ†ç‰‡
        cpu_offload=cpu_offload,
        backward_prefetch=BackwardPrefetch.BACKWARD_PRE,
        limit_all_gathers=True,
        use_orig_params=True,
        sync_module_states=True,
        forward_prefetch=True
    )
    
    # 5. æ¢¯åº¦ç´¯ç§¯ + å° batch
    micro_batch_size = 1  # æå°æ‰¹æ¬¡
    gradient_accumulation_steps = 32
    
    return model, micro_batch_size, gradient_accumulation_steps
```

è¿™ä¸ªæ–¹æ¡ˆé€šè¿‡æ¿€æ´»æ£€æŸ¥ç‚¹å‡å°‘ 50% æ¿€æ´»å€¼å†…å­˜ï¼ŒCPU offload èŠ‚çœå‚æ•°å†…å­˜ï¼Œæ··åˆåˆ†ç‰‡ä¼˜åŒ–é€šä¿¡ï¼Œå¯ä»¥æˆåŠŸè®­ç»ƒ 34B æ¨¡å‹ã€‚
</details>

**ç»ƒä¹  12.5ï¼šåˆ†å¸ƒå¼è°ƒè¯•å·¥å…·è®¾è®¡**

è®¾è®¡ä¸€ä¸ªè°ƒè¯•å·¥å…·ï¼Œèƒ½å¤Ÿå®æ—¶ç›‘æ§å¤šæœºè®­ç»ƒçš„è¿›ç¨‹çŠ¶æ€ã€é€šä¿¡æ—¶é—´å’Œæ½œåœ¨æ­»é”ã€‚

ğŸ’¡ **æç¤º**ï¼šè€ƒè™‘å¿ƒè·³æœºåˆ¶ã€é€šä¿¡ hook å’Œå¼‚å¸¸æ£€æµ‹ã€‚

<details>
<summary>ğŸ“ å‚è€ƒç­”æ¡ˆ</summary>

```python
import threading
import time
import torch.distributed as dist
from collections import deque
from datetime import datetime

class DistributedDebugger:
    def __init__(self, rank, world_size, check_interval=10):
        self.rank = rank
        self.world_size = world_size
        self.check_interval = check_interval
        self.comm_times = deque(maxlen=100)
        self.last_heartbeat = time.time()
        self.deadlock_threshold = 300  # 5åˆ†é’Ÿ
        
        # æ³¨å†Œé€šä¿¡ hook
        self._register_comm_hooks()
        
        # å¯åŠ¨ç›‘æ§çº¿ç¨‹
        self._start_monitor()
    
    def _register_comm_hooks(self):
        """æ³¨å†Œé€šä¿¡é’©å­æ¥æµ‹é‡æ—¶é—´"""
        original_all_reduce = dist.all_reduce
        
        def timed_all_reduce(*args, **kwargs):
            start = time.time()
            result = original_all_reduce(*args, **kwargs)
            elapsed = time.time() - start
            self.comm_times.append(elapsed)
            
            if elapsed > 10:  # è¶…è¿‡10ç§’è­¦å‘Š
                print(f"[Rank {self.rank}] WARNING: all_reduce took {elapsed:.2f}s")
            
            return result
        
        dist.all_reduce = timed_all_reduce
    
    def _start_monitor(self):
        """å¯åŠ¨ç›‘æ§çº¿ç¨‹"""
        def monitor():
            while True:
                time.sleep(self.check_interval)
                self._check_health()
        
        thread = threading.Thread(target=monitor, daemon=True)
        thread.start()
    
    def _check_health(self):
        """å¥åº·æ£€æŸ¥"""
        current_time = time.time()
        
        # 1. æ£€æŸ¥å¿ƒè·³
        if current_time - self.last_heartbeat > self.deadlock_threshold:
            self._report_deadlock()
        
        # 2. ç»Ÿè®¡é€šä¿¡æ—¶é—´
        if self.comm_times:
            avg_comm = sum(self.comm_times) / len(self.comm_times)
            max_comm = max(self.comm_times)
            print(f"[Rank {self.rank}] Comm stats: avg={avg_comm:.3f}s, max={max_comm:.3f}s")
        
        # 3. å†…å­˜çŠ¶æ€
        mem_alloc = torch.cuda.memory_allocated() / 1e9
        mem_reserved = torch.cuda.memory_reserved() / 1e9
        print(f"[Rank {self.rank}] Memory: {mem_alloc:.2f}/{mem_reserved:.2f} GB")
        
        # 4. åŒæ­¥æ£€æŸ¥ï¼ˆå¯é€‰ï¼‰
        self._sync_check()
    
    def _sync_check(self):
        """æ£€æŸ¥æ‰€æœ‰è¿›ç¨‹æ˜¯å¦åŒæ­¥"""
        try:
            check_tensor = torch.tensor([self.rank], device='cuda')
            check_list = [torch.zeros_like(check_tensor) for _ in range(self.world_size)]
            dist.all_gather(check_list, check_tensor, timeout=datetime.timedelta(seconds=30))
            
            # éªŒè¯æ‰€æœ‰è¿›ç¨‹éƒ½å“åº”
            ranks = [t.item() for t in check_list]
            if sorted(ranks) != list(range(self.world_size)):
                print(f"[Rank {self.rank}] ERROR: Missing ranks in sync check: {ranks}")
        except Exception as e:
            print(f"[Rank {self.rank}] Sync check failed: {e}")
    
    def _report_deadlock(self):
        """æŠ¥å‘Šå¯èƒ½çš„æ­»é”"""
        import traceback
        print(f"[Rank {self.rank}] DEADLOCK WARNING!")
        print(f"Stack trace:")
        traceback.print_stack()
        
        # å¯é€‰ï¼šè§¦å‘ core dump
        import signal
        os.kill(os.getpid(), signal.SIGABRT)
    
    def update_heartbeat(self):
        """æ›´æ–°å¿ƒè·³æ—¶é—´ï¼ˆåœ¨è®­ç»ƒå¾ªç¯ä¸­è°ƒç”¨ï¼‰"""
        self.last_heartbeat = time.time()

# ä½¿ç”¨ç¤ºä¾‹
debugger = DistributedDebugger(rank, world_size)

for epoch in range(num_epochs):
    for batch in dataloader:
        debugger.update_heartbeat()  # æ›´æ–°å¿ƒè·³
        # è®­ç»ƒä»£ç ...
```

è¿™ä¸ªè°ƒè¯•å™¨æä¾›äº†å®æ—¶ç›‘æ§ã€æ­»é”æ£€æµ‹ã€é€šä¿¡æ€§èƒ½åˆ†æç­‰åŠŸèƒ½ï¼Œèƒ½å¤Ÿå¿«é€Ÿå®šä½åˆ†å¸ƒå¼è®­ç»ƒé—®é¢˜ã€‚
</details>

**ç»ƒä¹  12.6ï¼šæ··åˆå¹¶è¡Œç­–ç•¥è®¾è®¡**

ä¸º VLM-65B æ¨¡å‹è®¾è®¡ä¸€ä¸ªç»“åˆ FSDPã€Pipeline å¹¶è¡Œå’Œ Tensor å¹¶è¡Œçš„è®­ç»ƒæ–¹æ¡ˆï¼Œç¡¬ä»¶æ˜¯ 16Ã—A100-80Gï¼ˆ2 èŠ‚ç‚¹ï¼‰ã€‚

ğŸ’¡ **æç¤º**ï¼šè€ƒè™‘ä¸åŒå¹¶è¡Œç­–ç•¥çš„é€šä¿¡æ¨¡å¼å’Œå†…å­˜å ç”¨ã€‚

<details>
<summary>ğŸ“ å‚è€ƒç­”æ¡ˆ</summary>

```python
"""
æ··åˆå¹¶è¡Œç­–ç•¥è®¾è®¡ï¼š
- Tensor Parallel (TP): 4-way (èŠ‚ç‚¹å†…)
- Pipeline Parallel (PP): 2-way (è·¨èŠ‚ç‚¹)
- Data Parallel with FSDP: 2-way

æ€»å¹¶è¡Œåº¦: 4 Ã— 2 Ã— 2 = 16
"""

class HybridParallelVLM:
    def __init__(self, model_config):
        self.world_size = 16
        self.tp_size = 4  # èŠ‚ç‚¹å†… tensor parallel
        self.pp_size = 2  # pipeline stages
        self.dp_size = 2  # data parallel groups
        
        # åˆå§‹åŒ–è¿›ç¨‹ç»„
        self._init_process_groups()
        
        # æ„å»ºæ¨¡å‹
        self.model = self._build_model(model_config)
    
    def _init_process_groups(self):
        """åˆ›å»ºä¸åŒçš„è¿›ç¨‹ç»„"""
        rank = dist.get_rank()
        
        # Tensor Parallel ç»„ (åŒèŠ‚ç‚¹å†…)
        tp_ranks = [rank // 4 * 4 + i for i in range(4)]
        self.tp_group = dist.new_group(tp_ranks)
        
        # Pipeline Parallel ç»„ (è·¨èŠ‚ç‚¹)
        pp_ranks = [rank % 4 + i * 8 for i in range(2)]
        self.pp_group = dist.new_group(pp_ranks)
        
        # Data Parallel ç»„
        dp_ranks = [rank // 8 * 8 + rank % 4 + i * 4 
                   for i in range(2)]
        self.dp_group = dist.new_group(dp_ranks)
    
    def _build_model(self, config):
        """æ„å»ºæ··åˆå¹¶è¡Œæ¨¡å‹"""
        rank = dist.get_rank()
        
        # 1. æ¨¡å‹åˆ†å±‚ (Pipeline)
        if rank < 8:  # ç¬¬ä¸€ä¸ª pipeline stage
            layers = self._get_first_stage_layers(config)
        else:  # ç¬¬äºŒä¸ª pipeline stage
            layers = self._get_second_stage_layers(config)
        
        # 2. Tensor Parallel
        for layer in layers:
            if isinstance(layer, nn.Linear):
                layer = ColumnParallelLinear(layer, self.tp_group)
            elif isinstance(layer, nn.Embedding):
                layer = ParallelEmbedding(layer, self.tp_group)
        
        # 3. FSDP åŒ…è£…
        model = nn.Sequential(*layers)
        model = FSDP(
            model,
            process_group=self.dp_group,
            sharding_strategy=ShardingStrategy.FULL_SHARD,
            mixed_precision=MixedPrecision(
                param_dtype=torch.bfloat16,
                reduce_dtype=torch.float32
            ),
            limit_all_gathers=True
        )
        
        return model
    
    def train_step(self, batch):
        """æ··åˆå¹¶è¡Œè®­ç»ƒæ­¥éª¤"""
        # Pipeline parallel çš„ micro-batching
        micro_batches = self._split_batch(batch, self.pp_size)
        
        losses = []
        for micro_batch in micro_batches:
            # Forward (with pipeline)
            if self.is_first_stage():
                output = self.model(micro_batch)
                # å‘é€åˆ°ä¸‹ä¸€ä¸ª stage
                self._send_activations(output, target_stage=1)
            else:
                # æ¥æ”¶å‰ä¸€ä¸ª stage çš„æ¿€æ´»
                activations = self._recv_activations(source_stage=0)
                output = self.model(activations)
                loss = self.compute_loss(output, micro_batch['labels'])
                losses.append(loss)
            
            # Backward (reverse pipeline)
            if self.is_last_stage():
                loss.backward()
                # å‘é€æ¢¯åº¦åˆ°å‰ä¸€ä¸ª stage
                self._send_gradients(...)
            else:
                # æ¥æ”¶æ¢¯åº¦
                gradients = self._recv_gradients(...)
                output.backward(gradients)
        
        # FSDP ä¼šè‡ªåŠ¨å¤„ç†æ¢¯åº¦åŒæ­¥
        return sum(losses) / len(losses)

# é…ç½®ç¤ºä¾‹
config = {
    "model": {
        "hidden_size": 8192,
        "num_layers": 80,
        "num_heads": 64,
        "vocab_size": 32000
    },
    "training": {
        "micro_batch_size": 1,
        "gradient_accumulation": 8,
        "optimizer": "AdamW",
        "lr": 1e-4
    }
}

# å†…å­˜ä¼°ç®—
"""
æ¯ä¸ª GPU çš„æ¨¡å‹å‚æ•°ï¼š65B / 16 = 4B å‚æ•°
FP16 å­˜å‚¨ï¼š4B * 2 bytes = 8GB
ä¼˜åŒ–å™¨çŠ¶æ€ (AdamW)ï¼š8GB * 2 = 16GB
æ¿€æ´»å€¼ (with checkpointing)ï¼š~20GB
æ€»è®¡ï¼š~44GB < 80GB (å®‰å…¨)
"""
```

è¿™ä¸ªæ–¹æ¡ˆé€šè¿‡ä¸‰ç§å¹¶è¡Œç­–ç•¥çš„ç»„åˆï¼Œå®ç°äº† 65B æ¨¡å‹åœ¨ 16 å¡ä¸Šçš„é«˜æ•ˆè®­ç»ƒï¼Œæ¯ç§å¹¶è¡Œç­–ç•¥éƒ½é’ˆå¯¹å…¶æœ€é€‚åˆçš„ç»´åº¦è¿›è¡Œä¼˜åŒ–ã€‚
</details>

**ç»ƒä¹  12.7ï¼šé€šä¿¡ç“¶é¢ˆåˆ†æ**

ä½ çš„è®­ç»ƒåœ¨ scaling åˆ° 32 å¡åï¼Œæ•ˆç‡ä» 8 å¡çš„ 90% ä¸‹é™åˆ° 60%ã€‚è¯·åˆ†æå¯èƒ½çš„åŸå› å¹¶æä¾›ä¼˜åŒ–æ–¹æ¡ˆã€‚

ğŸ’¡ **æç¤º**ï¼šè€ƒè™‘é€šä¿¡æ‹“æ‰‘ã€æ¢¯åº¦åŒæ­¥ç­–ç•¥å’Œæ•°æ®åŠ è½½ã€‚

<details>
<summary>ğŸ“ å‚è€ƒç­”æ¡ˆ</summary>

å¯èƒ½åŸå› åˆ†æï¼š

1. **é€šä¿¡ç“¶é¢ˆå¢åŠ **
   - All-Reduce æ—¶é—´ âˆ log(N) Ã— æ•°æ®é‡
   - 32å¡çš„é€šä¿¡è½®æ•°æ¯”8å¡å¤š

2. **ç½‘ç»œæ‹“æ‰‘ä¸ä¼˜åŒ–**
   - è·¨èŠ‚ç‚¹é€šä¿¡å¸¦å®½å—é™
   - PCIe/NVLink æ‹“æ‰‘ä¸å‡è¡¡

3. **åŒæ­¥å¼€é”€**
   - Barrier ç­‰å¾…æ—¶é—´å¢åŠ 
   - æ•°æ®åŠ è½½ä¸å‡è¡¡åŠ å‰§

ä¼˜åŒ–æ–¹æ¡ˆï¼š

```python
# 1. æ¢¯åº¦å‹ç¼©
from torch.distributed.algorithms.ddp_comm_hooks import default_hooks

model.register_comm_hook(
    state=None,
    hook=default_hooks.fp16_compress_hook
)

# 2. åˆ†å±‚ All-Reduce
def hierarchical_allreduce(tensor, groups):
    # èŠ‚ç‚¹å†…å…ˆåŒæ­¥
    dist.all_reduce(tensor, group=groups['intra_node'])
    
    # èŠ‚ç‚¹é—´åŒæ­¥ï¼ˆä»… masterï¼‰
    if is_node_master():
        dist.all_reduce(tensor, group=groups['inter_node'])
    
    # èŠ‚ç‚¹å†…å¹¿æ’­
    dist.broadcast(tensor, src=node_master_rank, 
                  group=groups['intra_node'])

# 3. æ¢¯åº¦ç´¯ç§¯å¢åŠ 
gradient_accumulation_steps = 4  # 8å¡
gradient_accumulation_steps = 8  # 32å¡ï¼Œå‡å°‘åŒæ­¥é¢‘ç‡

# 4. å¼‚æ­¥æ•°æ®é¢„å–
class AsyncDataLoader:
    def __init__(self, dataset, num_workers=8):
        self.queue = Queue(maxsize=2)
        self.workers = []
        for _ in range(num_workers):
            w = Process(target=self._worker, args=(dataset,))
            w.start()
            self.workers.append(w)
    
    def _worker(self, dataset):
        while True:
            batch = dataset.get_next_batch()
            self.queue.put(batch)
    
    def __iter__(self):
        while True:
            yield self.queue.get()

# 5. é€šä¿¡ä¸è®¡ç®—é‡å 
class OverlappedOptimizer:
    def step(self):
        # å¯åŠ¨å¼‚æ­¥ all-reduce
        handles = []
        for param in model.parameters():
            handle = dist.all_reduce(param.grad, async_op=True)
            handles.append(handle)
        
        # åŒæ—¶è¿›è¡Œå…¶ä»–è®¡ç®—
        self.update_metrics()
        self.log_progress()
        
        # ç­‰å¾…é€šä¿¡å®Œæˆ
        for handle in handles:
            handle.wait()
        
        # åº”ç”¨æ¢¯åº¦
        super().step()

# 6. NCCL ä¼˜åŒ–
os.environ['NCCL_TREE_THRESHOLD'] = '0'
os.environ['NCCL_ALGO'] = 'Ring,Tree'
os.environ['NCCL_CROSS_NIC'] = '1'
os.environ['NCCL_NET_GDR_LEVEL'] = '5'
```

é¢„æœŸæ•ˆæœï¼šä¼˜åŒ–å 32 å¡æ•ˆç‡å¯æå‡åˆ° 75-80%ã€‚
</details>

**ç»ƒä¹  12.8ï¼šç”Ÿäº§ç¯å¢ƒæ•…éšœæ¢å¤**

è®¾è®¡ä¸€ä¸ªå®Œæ•´çš„æ•…éšœæ¢å¤ç³»ç»Ÿï¼Œèƒ½å¤Ÿå¤„ç†èŠ‚ç‚¹æ•…éšœã€ç½‘ç»œä¸­æ–­å’Œ GPU é”™è¯¯ï¼Œç¡®ä¿è®­ç»ƒèƒ½å¤Ÿè‡ªåŠ¨æ¢å¤ã€‚

ğŸ’¡ **æç¤º**ï¼šè€ƒè™‘æ£€æŸ¥ç‚¹ã€å¥åº·æ£€æŸ¥ã€è‡ªåŠ¨é‡å¯å’Œå¼¹æ€§è®­ç»ƒã€‚

<details>
<summary>ğŸ“ å‚è€ƒç­”æ¡ˆ</summary>

```python
import os
import time
import signal
import subprocess
from typing import Optional, Dict, Any
from dataclasses import dataclass
from datetime import datetime, timedelta

@dataclass
class TrainingState:
    """è®­ç»ƒçŠ¶æ€"""
    epoch: int
    step: int
    best_loss: float
    checkpoint_path: str
    failure_count: int = 0
    last_failure_time: Optional[datetime] = None

class ResilientTrainer:
    """å¼¹æ€§è®­ç»ƒå™¨ - è‡ªåŠ¨æ•…éšœæ¢å¤"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.state = self._load_state()
        self.health_checker = HealthChecker()
        self.checkpoint_manager = CheckpointManager(config['checkpoint_dir'])
        self.max_failures = config.get('max_failures', 3)
        self.failure_window = timedelta(hours=1)
        
        # æ³¨å†Œä¿¡å·å¤„ç†
        self._register_signal_handlers()
    
    def _register_signal_handlers(self):
        """æ³¨å†Œä¼˜é›…é€€å‡ºçš„ä¿¡å·å¤„ç†"""
        def graceful_exit(signum, frame):
            print(f"Received signal {signum}, saving checkpoint...")
            self.save_checkpoint(emergency=True)
            sys.exit(0)
        
        signal.signal(signal.SIGTERM, graceful_exit)
        signal.signal(signal.SIGINT, graceful_exit)
    
    def train(self):
        """ä¸»è®­ç»ƒå¾ªç¯ - å¸¦æ•…éšœæ¢å¤"""
        while self.state.epoch < self.config['max_epochs']:
            try:
                # å¥åº·æ£€æŸ¥
                if not self.health_checker.check_all():
                    self._handle_unhealthy_state()
                    continue
                
                # è®­ç»ƒä¸€ä¸ª epoch
                self._train_epoch()
                
                # å®šæœŸä¿å­˜æ£€æŸ¥ç‚¹
                if self.state.step % self.config['checkpoint_interval'] == 0:
                    self.save_checkpoint()
                
                # é‡ç½®æ•…éšœè®¡æ•°ï¼ˆæˆåŠŸå®Œæˆ epochï¼‰
                self.state.failure_count = 0
                
            except Exception as e:
                self._handle_training_failure(e)
    
    def _train_epoch(self):
        """è®­ç»ƒä¸€ä¸ª epoch"""
        for batch in self.dataloader:
            # æ•…éšœæ³¨å…¥æµ‹è¯•ï¼ˆå¯é€‰ï¼‰
            if self.config.get('fault_injection'):
                self._inject_random_fault()
            
            # è®­ç»ƒæ­¥éª¤
            loss = self.train_step(batch)
            
            # å¼‚å¸¸æ£€æµ‹
            if self._detect_anomaly(loss):
                raise ValueError(f"Anomaly detected: loss={loss}")
            
            self.state.step += 1
    
    def _handle_training_failure(self, error: Exception):
        """å¤„ç†è®­ç»ƒæ•…éšœ"""
        print(f"Training failed: {error}")
        
        # æ›´æ–°æ•…éšœç»Ÿè®¡
        current_time = datetime.now()
        self.state.failure_count += 1
        
        # æ£€æŸ¥æ˜¯å¦è¶…è¿‡æœ€å¤§æ•…éšœæ¬¡æ•°
        if self.state.last_failure_time:
            time_since_last = current_time - self.state.last_failure_time
            if time_since_last < self.failure_window:
                if self.state.failure_count >= self.max_failures:
                    self._escalate_failure("Too many failures in short time")
                    return
        
        self.state.last_failure_time = current_time
        
        # å°è¯•æ¢å¤
        self._attempt_recovery(error)
    
    def _attempt_recovery(self, error: Exception):
        """å°è¯•ä»æ•…éšœä¸­æ¢å¤"""
        recovery_strategies = [
            self._recover_from_oom,
            self._recover_from_nccl_error,
            self._recover_from_checkpoint,
            self._restart_workers
        ]
        
        for strategy in recovery_strategies:
            try:
                if strategy(error):
                    print(f"Recovery successful using {strategy.__name__}")
                    return
            except Exception as e:
                print(f"Recovery strategy {strategy.__name__} failed: {e}")
        
        # æ‰€æœ‰ç­–ç•¥éƒ½å¤±è´¥
        self._escalate_failure("All recovery strategies failed")
    
    def _recover_from_oom(self, error: Exception) -> bool:
        """ä» OOM é”™è¯¯æ¢å¤"""
        if "out of memory" not in str(error).lower():
            return False
        
        print("Attempting OOM recovery...")
        
        # 1. æ¸…ç†ç¼“å­˜
        torch.cuda.empty_cache()
        
        # 2. å‡å°æ‰¹å¤§å°
        self.config['batch_size'] = max(1, self.config['batch_size'] // 2)
        print(f"Reduced batch size to {self.config['batch_size']}")
        
        # 3. å¯ç”¨æ›´æ¿€è¿›çš„å†…å­˜ä¼˜åŒ–
        self.config['gradient_checkpointing'] = True
        self.config['cpu_offload'] = True
        
        # 4. é‡æ–°åˆå§‹åŒ–æ¨¡å‹
        self._reinitialize_model()
        
        return True
    
    def _recover_from_nccl_error(self, error: Exception) -> bool:
        """ä» NCCL é”™è¯¯æ¢å¤"""
        if "nccl" not in str(error).lower():
            return False
        
        print("Attempting NCCL recovery...")
        
        # 1. é”€æ¯è¿›ç¨‹ç»„
        if dist.is_initialized():
            dist.destroy_process_group()
        
        # 2. ç­‰å¾…æ‰€æœ‰è¿›ç¨‹
        time.sleep(10)
        
        # 3. é‡æ–°åˆå§‹åŒ–
        self._init_distributed()
        
        # 4. ä»æ£€æŸ¥ç‚¹æ¢å¤
        self.load_checkpoint()
        
        return True
    
    def _restart_workers(self, error: Exception) -> bool:
        """é‡å¯å·¥ä½œè¿›ç¨‹"""
        print("Restarting all workers...")
        
        # ä¿å­˜å½“å‰çŠ¶æ€
        self.save_checkpoint(emergency=True)
        
        # æ„å»ºé‡å¯å‘½ä»¤
        restart_cmd = [
            "python", "-m", "torch.distributed.launch",
            "--nproc_per_node", str(self.config['gpus_per_node']),
            "--nnodes", str(self.config['num_nodes']),
            "--node_rank", str(self.config['node_rank']),
            "--master_addr", self.config['master_addr'],
            "--master_port", self.config['master_port'],
            "train.py",
            "--resume", self.state.checkpoint_path
        ]
        
        # æ‰§è¡Œé‡å¯
        subprocess.run(restart_cmd)
        return True
    
    def save_checkpoint(self, emergency: bool = False):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        checkpoint = {
            'model_state': self.model.state_dict(),
            'optimizer_state': self.optimizer.state_dict(),
            'training_state': self.state,
            'config': self.config,
            'timestamp': datetime.now().isoformat(),
            'emergency': emergency
        }
        
        # ä¿å­˜å¤šä¸ªå‰¯æœ¬é˜²æ­¢æŸå
        paths = self.checkpoint_manager.save(checkpoint, emergency)
        self.state.checkpoint_path = paths[0]
        
        # å¼‚æ­¥ä¸Šä¼ åˆ°äº‘å­˜å‚¨
        if self.config.get('cloud_backup'):
            self._async_cloud_backup(paths[0])
    
    def _escalate_failure(self, reason: str):
        """å‡çº§æ•…éšœå¤„ç†"""
        print(f"CRITICAL: {reason}")
        
        # 1. å‘é€å‘Šè­¦
        self._send_alert(reason)
        
        # 2. ä¿å­˜è°ƒè¯•ä¿¡æ¯
        self._save_debug_info()
        
        # 3. ä¼˜é›…é€€å‡º
        sys.exit(1)

class HealthChecker:
    """å¥åº·æ£€æŸ¥å™¨"""
    
    def check_all(self) -> bool:
        """æ‰§è¡Œæ‰€æœ‰å¥åº·æ£€æŸ¥"""
        checks = [
            self.check_gpu_health(),
            self.check_network_health(),
            self.check_memory_health(),
            self.check_disk_space()
        ]
        return all(checks)
    
    def check_gpu_health(self) -> bool:
        """æ£€æŸ¥ GPU å¥åº·çŠ¶æ€"""
        try:
            # æ£€æŸ¥ GPU æ˜¯å¦å¯ç”¨
            for i in range(torch.cuda.device_count()):
                torch.cuda.set_device(i)
                torch.cuda.synchronize()
                
                # æ£€æŸ¥æ¸©åº¦
                temp = self._get_gpu_temperature(i)
                if temp > 85:
                    print(f"WARNING: GPU {i} temperature {temp}Â°C")
                    return False
                
                # æ£€æŸ¥ ECC é”™è¯¯
                ecc_errors = self._get_ecc_errors(i)
                if ecc_errors > 0:
                    print(f"WARNING: GPU {i} has {ecc_errors} ECC errors")
                    return False
            
            return True
        except Exception as e:
            print(f"GPU health check failed: {e}")
            return False
    
    def check_network_health(self) -> bool:
        """æ£€æŸ¥ç½‘ç»œå¥åº·çŠ¶æ€"""
        if not dist.is_initialized():
            return True
        
        try:
            # ç®€å•çš„ all-reduce æµ‹è¯•
            test_tensor = torch.ones(1).cuda()
            dist.all_reduce(test_tensor)
            return test_tensor.item() == dist.get_world_size()
        except Exception as e:
            print(f"Network health check failed: {e}")
            return False

# ä½¿ç”¨ç¤ºä¾‹
config = {
    'max_epochs': 100,
    'checkpoint_dir': './checkpoints',
    'checkpoint_interval': 1000,
    'max_failures': 3,
    'batch_size': 32,
    'gpus_per_node': 8,
    'num_nodes': 2,
    'node_rank': 0,
    'master_addr': '192.168.1.1',
    'master_port': '29500',
    'cloud_backup': True
}

trainer = ResilientTrainer(config)
trainer.train()
```

è¿™ä¸ªç³»ç»Ÿæä¾›äº†å®Œæ•´çš„æ•…éšœæ¢å¤èƒ½åŠ›ï¼ŒåŒ…æ‹¬è‡ªåŠ¨é‡è¯•ã€é™çº§ç­–ç•¥ã€å¥åº·æ£€æŸ¥å’Œäº‘å¤‡ä»½ï¼Œèƒ½å¤Ÿå¤„ç†ç”Ÿäº§ç¯å¢ƒä¸­çš„å„ç§æ•…éšœåœºæ™¯ã€‚
</details>

## å¸¸è§é™·é˜±ä¸é”™è¯¯ (Gotchas)

### 1. NCCL ç‰ˆæœ¬ä¸åŒ¹é…
**é™·é˜±**ï¼šä¸åŒèŠ‚ç‚¹çš„ NCCL ç‰ˆæœ¬ä¸ä¸€è‡´å¯¼è‡´é€šä¿¡å¤±è´¥ã€‚
**è§£å†³**ï¼šç»Ÿä¸€æ‰€æœ‰èŠ‚ç‚¹çš„ PyTorch å’Œ NCCL ç‰ˆæœ¬ã€‚

### 2. Hanging Without Error
**é™·é˜±**ï¼šè®­ç»ƒæŒ‚èµ·ä½†æ²¡æœ‰ä»»ä½•é”™è¯¯è¾“å‡ºã€‚
**è§£å†³**ï¼šå¯ç”¨ NCCL_DEBUG=INFO å’Œè®¾ç½®åˆç†çš„è¶…æ—¶æ—¶é—´ã€‚

### 3. éšå¼åŒæ­¥ç‚¹
**é™·é˜±**ï¼šprintã€æ—¥å¿—ç­‰æ“ä½œå¯èƒ½å¼•å…¥éšå¼åŒæ­¥ã€‚
**è§£å†³**ï¼šåªåœ¨ rank 0 è¿›è¡Œ I/O æ“ä½œï¼Œæˆ–ä½¿ç”¨å¼‚æ­¥ I/Oã€‚

### 4. GPU äº²å’Œæ€§è®¾ç½®é”™è¯¯
**é™·é˜±**ï¼šCUDA_VISIBLE_DEVICES è®¾ç½®ä¸å½“å¯¼è‡´è¿›ç¨‹çœ‹åˆ°é”™è¯¯çš„ GPUã€‚
**è§£å†³**ï¼šä½¿ç”¨ torchrun æˆ–æ­£ç¡®è®¾ç½®æ¯ä¸ªè¿›ç¨‹çš„ GPU æ˜ å°„ã€‚

### 5. æ··åˆç²¾åº¦ä¸å…¼å®¹
**é™·é˜±**ï¼šä¸åŒ GPU æ”¯æŒçš„ç²¾åº¦ä¸åŒï¼ˆFP16 vs BF16ï¼‰ã€‚
**è§£å†³**ï¼šæ£€æµ‹ç¡¬ä»¶èƒ½åŠ›ï¼Œé™çº§åˆ°æ‰€æœ‰ GPU éƒ½æ”¯æŒçš„ç²¾åº¦ã€‚

### 6. Checkpoint è…è´¥
**é™·é˜±**ï¼šä¿å­˜ checkpoint æ—¶è¿›ç¨‹è¢«ä¸­æ–­å¯¼è‡´æ–‡ä»¶æŸåã€‚
**è§£å†³**ï¼šå…ˆä¿å­˜åˆ°ä¸´æ—¶æ–‡ä»¶ï¼ŒæˆåŠŸåå†é‡å‘½åã€‚

## æœ€ä½³å®è·µæ£€æŸ¥æ¸…å•

### å¯åŠ¨å‰æ£€æŸ¥
- [ ] æ‰€æœ‰èŠ‚ç‚¹çš„ç¯å¢ƒä¸€è‡´ï¼ˆPythonã€PyTorchã€CUDA ç‰ˆæœ¬ï¼‰
- [ ] ç½‘ç»œè¿é€šæ€§æµ‹è¯•é€šè¿‡
- [ ] GPU å¥åº·æ£€æŸ¥é€šè¿‡ï¼ˆæ¸©åº¦ã€ECC é”™è¯¯ï¼‰
- [ ] ç£ç›˜ç©ºé—´å……è¶³ï¼ˆcheckpoint éœ€è¦å¤§é‡ç©ºé—´ï¼‰
- [ ] NCCL ç¯å¢ƒå˜é‡æ­£ç¡®è®¾ç½®

### è®­ç»ƒä¸­ç›‘æ§
- [ ] GPU åˆ©ç”¨ç‡ > 85%
- [ ] ç½‘ç»œå¸¦å®½åˆ©ç”¨åˆç†
- [ ] æ— è¿›ç¨‹æ˜æ˜¾è½åï¼ˆé€šè¿‡ progress barï¼‰
- [ ] å†…å­˜ä½¿ç”¨ç¨³å®šï¼ˆæ— æ³„æ¼ï¼‰
- [ ] Loss æ›²çº¿æ­£å¸¸ï¼ˆæ—  NaNã€æ— å¼‚å¸¸è·³å˜ï¼‰

### æ•…éšœæ¢å¤å‡†å¤‡
- [ ] Checkpoint å®šæœŸä¿å­˜ï¼ˆè‡³å°‘æ¯å°æ—¶ï¼‰
- [ ] æœ‰å¤šä¸ª checkpoint å‰¯æœ¬
- [ ] æ•…éšœæ¢å¤è„šæœ¬å·²æµ‹è¯•
- [ ] ç›‘æ§å‘Šè­¦å·²é…ç½®
- [ ] æœ‰å›æ»šè®¡åˆ’

### æ€§èƒ½ä¼˜åŒ–
- [ ] é€šä¿¡ä¸è®¡ç®—é‡å 
- [ ] æ¢¯åº¦ç´¯ç§¯åˆç†è®¾ç½®
- [ ] æ•°æ®åŠ è½½ä¸æ˜¯ç“¶é¢ˆ
- [ ] ä½¿ç”¨äº†åˆé€‚çš„ NCCL ç®—æ³•
- [ ] æ··åˆç²¾åº¦è®­ç»ƒå·²å¯ç”¨

### è°ƒè¯•å·¥å…·
- [ ] NCCL æ—¥å¿—å·²å¯ç”¨ï¼ˆé—®é¢˜æ’æŸ¥æ—¶ï¼‰
- [ ] è¿›ç¨‹ç›‘æ§è„šæœ¬è¿è¡Œä¸­
- [ ] æ€§èƒ½ profiling å·¥å…·å°±ç»ª
- [ ] æœ‰ core dump ç”Ÿæˆé…ç½®