# ç¬¬ 5 ç« ï¼šRLHF åŸºç¡€ä¸å®ç°

ç»è¿‡ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰åçš„è§†è§‰è¯­è¨€æ¨¡å‹è™½ç„¶èƒ½å¤Ÿç†è§£å’Œå“åº”å¤šæ¨¡æ€æŒ‡ä»¤ï¼Œä½†å…¶è¾“å‡ºå¾€å¾€æ— æ³•å®Œå…¨ç¬¦åˆäººç±»çš„åå¥½å’Œä»·å€¼è§‚ã€‚æ¨¡å‹å¯èƒ½ä¼šäº§ç”Ÿäº‹å®æ€§é”™è¯¯ã€ç”Ÿæˆæœ‰å®³å†…å®¹ï¼Œæˆ–åœ¨æè¿°å›¾åƒæ—¶å‡ºç°å¹»è§‰ã€‚åŸºäºäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼‰æä¾›äº†ä¸€ç§ç›´æ¥ä¼˜åŒ–äººç±»åå¥½çš„è®­ç»ƒèŒƒå¼ï¼Œä½¿æ¨¡å‹çš„è¾“å‡ºæ›´åŠ å‡†ç¡®ã€æœ‰ç”¨ä¸”å®‰å…¨ã€‚æœ¬ç« å°†æ·±å…¥æ¢è®¨å¦‚ä½•åœ¨ VLM ä¸­å®æ–½ RLHFï¼ŒåŒ…æ‹¬å¥–åŠ±æ¨¡å‹è®­ç»ƒã€PPO ç®—æ³•åº”ç”¨ä»¥åŠè®­ç»ƒç¨³å®šæ€§ä¿éšœç­‰å…³é”®æŠ€æœ¯ã€‚

## å­¦ä¹ ç›®æ ‡

å®Œæˆæœ¬ç« å­¦ä¹ åï¼Œæ‚¨å°†èƒ½å¤Ÿï¼š

- **ç†è§£ RLHF çš„æ ¸å¿ƒåŸç†**ï¼šæŒæ¡ä»åå¥½æ•°æ®åˆ°ç­–ç•¥ä¼˜åŒ–çš„å®Œæ•´æµç¨‹
- **æ„å»ºå¤šæ¨¡æ€å¥–åŠ±æ¨¡å‹**ï¼šè®¾è®¡é€‚åˆ VLM ç‰¹ç‚¹çš„å¥–åŠ±å‡½æ•°å’Œæ¨¡å‹æ¶æ„
- **å®æ–½ PPO è®­ç»ƒ**ï¼šæ­£ç¡®é…ç½®å’Œè°ƒè¯• PPO ç®—æ³•çš„å…³é”®è¶…å‚æ•°
- **å¤„ç† VLM ç‰¹æœ‰æŒ‘æˆ˜**ï¼šè§£å†³è§†è§‰å¹»è§‰ã€å¤šæ¨¡æ€å¯¹é½ç­‰ç‹¬ç‰¹é—®é¢˜
- **ä¿è¯è®­ç»ƒç¨³å®šæ€§**ï¼šè¯Šæ–­å’Œä¿®å¤å¥–åŠ±å´©æºƒã€KL æ•£åº¦çˆ†ç‚¸ç­‰å¸¸è§é—®é¢˜
- **ä¼˜åŒ–è®­ç»ƒæ•ˆç‡**ï¼šé€šè¿‡åˆç†çš„æ¶æ„å’Œç®—æ³•é€‰æ‹©æå‡ 3-5 å€è®­ç»ƒé€Ÿåº¦

## 5.1 RLHF æ¦‚è¿°ä¸åŠ¨æœº

### ä¸ºä»€ä¹ˆ VLM éœ€è¦ RLHF

ç›‘ç£å¾®è°ƒè™½ç„¶è®©æ¨¡å‹å­¦ä¼šäº†éµå¾ªæŒ‡ä»¤çš„åŸºæœ¬èƒ½åŠ›ï¼Œä½†å­˜åœ¨å‡ ä¸ªæ ¹æœ¬æ€§é™åˆ¶ï¼š

1. **å¹»è§‰é—®é¢˜ä¸¥é‡**ï¼šVLM ç»å¸¸æè¿°å›¾åƒä¸­ä¸å­˜åœ¨çš„ç‰©ä½“æˆ–ç»†èŠ‚ï¼Œè¿™ç§è§†è§‰å¹»è§‰æ¯”çº¯æ–‡æœ¬ LLM çš„äº‹å®æ€§é”™è¯¯æ›´éš¾é€šè¿‡ SFT è§£å†³ã€‚

2. **åå¥½å¯¹é½å›°éš¾**ï¼šäººç±»å¯¹å›¾åƒæè¿°çš„åå¥½æ˜¯ä¸»è§‚ä¸”ä¸Šä¸‹æ–‡ç›¸å…³çš„ã€‚åŒä¸€å¼ å›¾ç‰‡ï¼Œç”¨æˆ·å¯èƒ½æœŸæœ›ç®€æ´æ¦‚æ‹¬æˆ–è¯¦ç»†åˆ†æï¼ŒSFT æ•°æ®éš¾ä»¥è¦†ç›–æ‰€æœ‰åå¥½æ¨¡å¼ã€‚

3. **å®‰å…¨æ€§æŒ‘æˆ˜**ï¼šVLM éœ€è¦åŒæ—¶å¤„ç†è§†è§‰å’Œæ–‡æœ¬ä¸¤ä¸ªæ¨¡æ€çš„å®‰å…¨é—®é¢˜ï¼ŒåŒ…æ‹¬è¯†åˆ«å¹¶æ‹’ç»å¤„ç†æœ‰å®³å›¾åƒå†…å®¹ã€‚

4. **è¯„ä¼°æŒ‡æ ‡å¤±é…**ï¼šä¼ ç»Ÿçš„ BLEUã€CIDEr ç­‰æŒ‡æ ‡ä¸äººç±»åˆ¤æ–­ç›¸å…³æ€§è¾ƒä½ï¼Œç›´æ¥ä¼˜åŒ–è¿™äº›æŒ‡æ ‡åè€Œå¯èƒ½é™ä½å®é™…ä½“éªŒã€‚

RLHF é€šè¿‡å¼•å…¥äººç±»åé¦ˆä½œä¸ºä¼˜åŒ–ä¿¡å·ï¼Œç›´æ¥å¯¹é½æ¨¡å‹è¾“å‡ºä¸äººç±»åå¥½ï¼Œä»æ ¹æœ¬ä¸Šè§£å†³è¿™äº›é—®é¢˜ã€‚

### RLHF çš„æ ¸å¿ƒæµç¨‹

VLM çš„ RLHF è®­ç»ƒé€šå¸¸åŒ…å«ä¸‰ä¸ªé˜¶æ®µï¼š

```
é˜¶æ®µ 1: ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰
â”œâ”€â”€ è¾“å…¥: (å›¾åƒ, æŒ‡ä»¤) å¯¹
â”œâ”€â”€ è¾“å‡º: åˆå§‹ç­–ç•¥æ¨¡å‹ Ï€_SFT
â””â”€â”€ ç›®æ ‡: åŸºç¡€æŒ‡ä»¤éµå¾ªèƒ½åŠ›

é˜¶æ®µ 2: å¥–åŠ±æ¨¡å‹è®­ç»ƒï¼ˆRMï¼‰
â”œâ”€â”€ è¾“å…¥: äººç±»åå¥½æ•°æ® {(x, y_win, y_lose)}
â”œâ”€â”€ è¾“å‡º: å¥–åŠ±æ¨¡å‹ R(x, y)
â””â”€â”€ ç›®æ ‡: å­¦ä¹ äººç±»åå¥½å‡½æ•°

é˜¶æ®µ 3: å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–ï¼ˆPPOï¼‰
â”œâ”€â”€ è¾“å…¥: ç­–ç•¥æ¨¡å‹ Ï€ + å¥–åŠ±æ¨¡å‹ R
â”œâ”€â”€ è¾“å‡º: ä¼˜åŒ–åçš„ç­–ç•¥ Ï€_RLHF
â””â”€â”€ ç›®æ ‡: æœ€å¤§åŒ–æœŸæœ›å¥–åŠ±åŒæ—¶æ§åˆ¶ KL æ•£åº¦
```

### VLM ä¸­ RLHF çš„ç‹¬ç‰¹æŒ‘æˆ˜

ç›¸æ¯”çº¯æ–‡æœ¬ LLMï¼ŒVLM çš„ RLHF é¢ä¸´é¢å¤–çš„æŠ€æœ¯æŒ‘æˆ˜ï¼š

**1. å¤šæ¨¡æ€å¥–åŠ±å»ºæ¨¡å¤æ‚æ€§**

å¥–åŠ±æ¨¡å‹éœ€è¦åŒæ—¶ç†è§£è§†è§‰å’Œè¯­è¨€çš„å¯¹é½å…³ç³»ã€‚ä¸€ä¸ªé«˜è´¨é‡çš„å›ç­”ä¸ä»…è¦è¯­è¨€æµç•…ï¼Œæ›´è¦å‡†ç¡®åæ˜ å›¾åƒå†…å®¹ã€‚è¿™è¦æ±‚å¥–åŠ±æ¨¡å‹å…·å¤‡ï¼š

- ç»†ç²’åº¦çš„è§†è§‰ç†è§£èƒ½åŠ›ï¼ˆç‰©ä½“ã€å±æ€§ã€å…³ç³»ï¼‰
- è·¨æ¨¡æ€ä¸€è‡´æ€§åˆ¤æ–­ï¼ˆæ–‡æœ¬æ˜¯å¦å‡†ç¡®æè¿°å›¾åƒï¼‰
- ä¸Šä¸‹æ–‡ç›¸å…³çš„è´¨é‡è¯„ä¼°ï¼ˆä¸åŒä»»åŠ¡çš„è¯„åˆ¤æ ‡å‡†ä¸åŒï¼‰

**2. è®¡ç®—å’Œå†…å­˜å¼€é”€æ¿€å¢**

```
å†…å­˜å ç”¨å¯¹æ¯”ï¼ˆä»¥ 13B æ¨¡å‹ä¸ºä¾‹ï¼‰ï¼š
çº¯æ–‡æœ¬ LLM RLHF:
- Actor æ¨¡å‹: 26GB (bf16)
- Critic æ¨¡å‹: 26GB
- Reference æ¨¡å‹: 26GB (å†»ç»“)
- æ€»è®¡: ~78GB

VLM RLHF:
- Actor æ¨¡å‹: 26GB + Vision Encoder 4GB = 30GB
- Critic æ¨¡å‹: 30GB
- Reference æ¨¡å‹: 30GB (å†»ç»“)
- å›¾åƒç¼“å­˜: 10-20GB (å–å†³äºæ‰¹æ¬¡å¤§å°)
- æ€»è®¡: ~100GB+
```

**3. è®­ç»ƒä¸ç¨³å®šæ€§åŠ å‰§**

è§†è§‰ç‰¹å¾çš„é«˜ç»´åº¦å’Œå¤šæ ·æ€§ä½¿å¾—å¥–åŠ±ä¿¡å·çš„æ–¹å·®æ›´å¤§ï¼Œå®¹æ˜“å¯¼è‡´ï¼š

- å¥–åŠ±å´©æºƒï¼šæ¨¡å‹æ‰¾åˆ°è§†è§‰ç‰¹å¾çš„æ·å¾„ï¼Œäº§ç”Ÿé«˜å¥–åŠ±ä½†æ— æ„ä¹‰çš„è¾“å‡º
- KL æ•£åº¦çˆ†ç‚¸ï¼šè§†è§‰æ¡ä»¶ä¸‹çš„ç­–ç•¥åˆ†å¸ƒæ›´å®¹æ˜“åç¦»å‚è€ƒåˆ†å¸ƒ
- æ¢¯åº¦çˆ†ç‚¸ï¼šå¤šæ¨¡æ€äº¤äº’å±‚çš„æ¢¯åº¦ä¸ç¨³å®š

### ä¸çº¯æ–‡æœ¬ RLHF çš„å…³é”®å·®å¼‚

| ç»´åº¦ | çº¯æ–‡æœ¬ LLM | VLM | å½±å“ |
|------|------------|-----|------|
| **è¾“å…¥å¤æ‚åº¦** | æ–‡æœ¬åºåˆ— | æ–‡æœ¬+å›¾åƒ | éœ€è¦æ›´å¤§æ‰¹æ¬¡ç¼“å­˜ |
| **å¥–åŠ±ä¿¡å·** | åŸºäºæ–‡æœ¬è´¨é‡ | éœ€è€ƒè™‘è·¨æ¨¡æ€å¯¹é½ | å¥–åŠ±æ¨¡å‹è®¾è®¡æ›´å¤æ‚ |
| **å¹»è§‰ç±»å‹** | äº‹å®æ€§é”™è¯¯ | è§†è§‰å¹»è§‰+äº‹å®é”™è¯¯ | éœ€è¦ä¸“é—¨çš„å¹»è§‰æƒ©ç½š |
| **è®¡ç®—å¼€é”€** | åŸºå‡† | 1.5-2å€ | è®­ç»ƒæˆæœ¬æ˜¾è‘—å¢åŠ  |
| **æ•°æ®æ ‡æ³¨** | æ–‡æœ¬åå¥½ | éœ€è¦ç†è§£å›¾åƒå†…å®¹ | æ ‡æ³¨æˆæœ¬å’Œéš¾åº¦æ›´é«˜ |

### æŠ€æœ¯è·¯çº¿é€‰æ‹©

å®è·µä¸­æœ‰ä¸‰ç§ä¸»è¦çš„ RLHF å®ç°è·¯çº¿ï¼š

**1. å…¨æ¨¡å‹ RLHF**
- ä¼˜ç‚¹ï¼šæ•ˆæœæœ€å¥½ï¼Œå¯ä»¥å……åˆ†ä¼˜åŒ–å¤šæ¨¡æ€äº¤äº’
- ç¼ºç‚¹ï¼šè®¡ç®—å¼€é”€å·¨å¤§ï¼Œéœ€è¦ 8Ã—A100 ä»¥ä¸Šèµ„æº
- é€‚ç”¨ï¼šèµ„æºå……è¶³çš„ç ”ç©¶å›¢é˜Ÿ

**2. LoRA-based RLHF**
- ä¼˜ç‚¹ï¼šæ˜¾å­˜éœ€æ±‚é™ä½ 50%ï¼Œè®­ç»ƒç¨³å®š
- ç¼ºç‚¹ï¼šæ•ˆæœç•¥æœ‰ä¸‹é™ï¼ˆ5-10%ï¼‰
- é€‚ç”¨ï¼šå¤§å¤šæ•°å®è·µåœºæ™¯

**3. ä»…è¯­è¨€æ¨¡å‹ RLHF**
- ä¼˜ç‚¹ï¼šå¤ç”¨æ–‡æœ¬ RLHF åŸºç¡€è®¾æ–½ï¼Œå®ç°ç®€å•
- ç¼ºç‚¹ï¼šæ— æ³•ä¼˜åŒ–è§†è§‰ç¼–ç å™¨ï¼Œæ”¹è¿›æœ‰é™
- é€‚ç”¨ï¼šå¿«é€ŸåŸå‹éªŒè¯

## 5.2 å¥–åŠ±æ¨¡å‹çš„æ„å»ºä¸è®­ç»ƒ

å¥–åŠ±æ¨¡å‹æ˜¯ RLHF çš„æ ¸å¿ƒç»„ä»¶ï¼Œå®ƒå°†äººç±»åå¥½è½¬åŒ–ä¸ºå¯ä¼˜åŒ–çš„æ•°å€¼ä¿¡å·ã€‚åœ¨ VLM åœºæ™¯ä¸‹ï¼Œå¥–åŠ±æ¨¡å‹éœ€è¦å‡†ç¡®è¯„ä¼°å¤šæ¨¡æ€è¾“å‡ºçš„è´¨é‡ï¼Œè¿™æ¯”çº¯æ–‡æœ¬åœºæ™¯å¤æ‚å¾—å¤šã€‚

### åå¥½æ•°æ®æ”¶é›†ç­–ç•¥

é«˜è´¨é‡çš„åå¥½æ•°æ®æ˜¯è®­ç»ƒä¼˜ç§€å¥–åŠ±æ¨¡å‹çš„åŸºç¡€ã€‚VLM çš„åå¥½æ•°æ®æ”¶é›†éœ€è¦ç‰¹åˆ«æ³¨æ„ä»¥ä¸‹å‡ ç‚¹ï¼š

**1. æ•°æ®æ”¶é›†æµç¨‹è®¾è®¡**

```
æ ‡å‡†æ”¶é›†æµç¨‹ï¼š
1. é‡‡æ ·é˜¶æ®µ
   â”œâ”€â”€ è¾“å…¥: (å›¾åƒ, æŒ‡ä»¤) å¯¹
   â”œâ”€â”€ ç”Ÿæˆ: ä½¿ç”¨ä¸åŒæ¸©åº¦/ç­–ç•¥ç”Ÿæˆ K ä¸ªå›ç­”ï¼ˆK=4-7ï¼‰
   â””â”€â”€ å»é‡: ç§»é™¤ç›¸ä¼¼åº¦ > 0.9 çš„å›ç­”

2. æ ‡æ³¨é˜¶æ®µ
   â”œâ”€â”€ å±•ç¤º: å‘æ ‡æ³¨è€…å±•ç¤ºå›¾åƒã€æŒ‡ä»¤å’Œå€™é€‰å›ç­”
   â”œâ”€â”€ æ’åº: æ ‡æ³¨è€…å¯¹å›ç­”è¿›è¡Œå…¨æ’åºæˆ–æˆå¯¹æ¯”è¾ƒ
   â””â”€â”€ è´¨æ£€: è®¡ç®—æ ‡æ³¨è€…é—´ä¸€è‡´æ€§ï¼ˆKappa > 0.6ï¼‰

3. æ•°æ®æ„é€ 
   â”œâ”€â”€ æˆå¯¹æ¯”è¾ƒ: ä»æ’åºä¸­æå– (chosen, rejected) å¯¹
   â”œâ”€â”€ æƒé‡åˆ†é…: æ ¹æ®æ’åå·®è·è®¾ç½®æ ·æœ¬æƒé‡
   â””â”€â”€ å¹³è¡¡å¤„ç†: ç¡®ä¿æ­£è´Ÿæ ·æœ¬æ¯”ä¾‹åˆç†ï¼ˆ1:1 åˆ° 1:3ï¼‰
```

**2. å¤šæ ·æ€§ä¿è¯ç­–ç•¥**

åå¥½æ•°æ®éœ€è¦è¦†ç›–å¤šç§å¤±è´¥æ¨¡å¼ï¼Œé¿å…å¥–åŠ±æ¨¡å‹è¿‡æ‹Ÿåˆï¼š

- **ä»»åŠ¡å¤šæ ·æ€§**ï¼šåŒ…å«å›¾åƒæè¿°ã€è§†è§‰é—®ç­”ã€æ¨ç†ç­‰å¤šç§ä»»åŠ¡
- **é”™è¯¯ç±»å‹è¦†ç›–**ï¼š
  - å¹»è§‰é”™è¯¯ï¼ˆæè¿°ä¸å­˜åœ¨çš„ç‰©ä½“ï¼‰
  - å±æ€§é”™è¯¯ï¼ˆé¢œè‰²ã€æ•°é‡ã€ä½ç½®é”™è¯¯ï¼‰
  - å…³ç³»é”™è¯¯ï¼ˆç‰©ä½“é—´å…³ç³»æè¿°é”™è¯¯ï¼‰
  - é€»è¾‘é”™è¯¯ï¼ˆæ¨ç†è¿‡ç¨‹æœ‰è¯¯ï¼‰
- **éš¾åº¦æ¢¯åº¦**ï¼šä»æ˜æ˜¾é”™è¯¯åˆ°ç»†å¾®å·®å¼‚çš„æ ·æœ¬éƒ½è¦åŒ…å«

**3. æ ‡æ³¨è´¨é‡æ§åˆ¶**

VLM åå¥½æ ‡æ³¨çš„æŒ‘æˆ˜åœ¨äºæ ‡æ³¨è€…éœ€è¦åŒæ—¶ç†è§£å›¾åƒå’Œæ–‡æœ¬ï¼š

```python
# æ ‡æ³¨æŒ‡å—ç¤ºä¾‹
æ ‡æ³¨åŸåˆ™ä¼˜å…ˆçº§ï¼š
1. äº‹å®å‡†ç¡®æ€§ (40%)ï¼šæè¿°æ˜¯å¦ä¸å›¾åƒå†…å®¹ä¸€è‡´
2. å®Œæ•´æ€§ (25%)ï¼šæ˜¯å¦å›ç­”äº†ç”¨æˆ·çš„é—®é¢˜
3. ç›¸å…³æ€§ (20%)ï¼šæ˜¯å¦èšç„¦äºé—®é¢˜ç›¸å…³å†…å®¹
4. æµç•…æ€§ (15%)ï¼šè¯­è¨€è¡¨è¾¾æ˜¯å¦è‡ªç„¶

ç‰¹æ®Šæƒ…å†µå¤„ç†ï¼š
- å½“ä¸¤ä¸ªå›ç­”éƒ½åŒ…å«é”™è¯¯æ—¶ï¼Œé€‰æ‹©é”™è¯¯è¾ƒå°‘çš„
- å½“äº‹å®éƒ½æ­£ç¡®æ—¶ï¼Œä¼˜å…ˆé€‰æ‹©ä¿¡æ¯é‡å¤§çš„
- å¯¹äºä¸»è§‚é—®é¢˜ï¼Œè€ƒè™‘å›ç­”çš„åˆç†æ€§å’Œè®ºè¯è´¨é‡
```

### å¤šæ¨¡æ€å¥–åŠ±æ¨¡å‹æ¶æ„

å¥–åŠ±æ¨¡å‹çš„æ¶æ„è®¾è®¡ç›´æ¥å½±å“å…¶åˆ¤åˆ«èƒ½åŠ›å’Œè®­ç»ƒæ•ˆç‡ï¼š

**1. åŸºç¡€æ¶æ„é€‰æ‹©**

```
æ–¹æ¡ˆ A: ç‹¬ç«‹å¥–åŠ±å¤´
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Vision â”‚â”€â”€â”€â”€â–¶â”‚         â”‚
â”‚ Encoder â”‚     â”‚  Fusion â”‚â”€â”€â”€â”€â–¶ [Language Model] â”€â”€â”€â”€â–¶ [Reward Head]
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚  Layer  â”‚                               (å•ä¸ªæ ‡é‡)
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
ä¼˜ç‚¹ï¼šå‚æ•°æ•ˆç‡é«˜ï¼Œå¯å¤ç”¨ SFT æ¨¡å‹
ç¼ºç‚¹ï¼šè¡¨è¾¾èƒ½åŠ›å—é™

æ–¹æ¡ˆ B: åºåˆ—çº§å¥–åŠ±å»ºæ¨¡
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Vision â”‚â”€â”€â”€â”€â–¶â”‚         â”‚
â”‚ Encoder â”‚     â”‚  Fusion â”‚â”€â”€â”€â”€â–¶ [Language Model] â”€â”€â”€â”€â–¶ [Token Rewards]
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚  Layer  â”‚                               (åºåˆ—é•¿åº¦)
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                    â†“
                                                          [Aggregation]
ä¼˜ç‚¹ï¼šå¯ä»¥ç»†ç²’åº¦å»ºæ¨¡ï¼Œè¯†åˆ«å…·ä½“é”™è¯¯ä½ç½®
ç¼ºç‚¹ï¼šè®­ç»ƒå¤æ‚ï¼Œéœ€è¦æ›´å¤šæ ‡æ³¨
```

**2. å…³é”®è®¾è®¡å†³ç­–**

- **å‚æ•°å…±äº«ç­–ç•¥**ï¼šå¥–åŠ±æ¨¡å‹é€šå¸¸ä» SFT æ¨¡å‹åˆå§‹åŒ–ï¼Œå¯ä»¥é€‰æ‹©ï¼š
  - å…¨å‚æ•°å¾®è°ƒï¼šæ•ˆæœæœ€å¥½ä½†å¼€é”€å¤§
  - å†»ç»“è§†è§‰ç¼–ç å™¨ï¼šå¹³è¡¡æ•ˆæœå’Œæ•ˆç‡
  - LoRA å¾®è°ƒï¼šæ˜¾å­˜å‹å¥½ï¼Œé€‚åˆèµ„æºå—é™åœºæ™¯

- **æ± åŒ–ç­–ç•¥**ï¼šå¦‚ä½•ä»åºåˆ—è¡¨ç¤ºå¾—åˆ°æ ‡é‡å¥–åŠ±
  - æœ€åä¸€ä¸ª tokenï¼šç®€å•ä½†å¯èƒ½ä¸¢å¤±ä¿¡æ¯
  - åŠ æƒå¹³å‡ï¼šè€ƒè™‘æ‰€æœ‰ token ä½†éœ€è¦è®¾è®¡æƒé‡
  - æ³¨æ„åŠ›æ± åŒ–ï¼šå­¦ä¹ æƒé‡ï¼Œæ›´çµæ´»

- **å½’ä¸€åŒ–æ–¹æ¡ˆ**ï¼šä¿è¯å¥–åŠ±å€¼çš„ç¨³å®šæ€§
  - Batch å½’ä¸€åŒ–ï¼šè®­ç»ƒæ—¶ç¨³å®šä½†æ¨ç†æ—¶éœ€è¦ç»Ÿè®¡é‡
  - Layer å½’ä¸€åŒ–ï¼šæ›´ç¨³å®šï¼Œæ¨èä½¿ç”¨
  - æ ‡å‡†åŒ–åˆ° [-1, 1]ï¼šä¾¿äºåç»­ PPO è®­ç»ƒ

### è®­ç»ƒæŠ€å·§ä¸é¿å‘æŒ‡å—

**1. æŸå¤±å‡½æ•°è®¾è®¡**

æ ‡å‡†çš„ Bradley-Terry æ¨¡å‹ï¼š

$$\mathcal{L}_{BT} = -\mathbb{E}_{(x,y_w,y_l)} \left[ \log \sigma(r(x,y_w) - r(x,y_l)) \right]$$

å®è·µæ”¹è¿›ï¼š

$$\mathcal{L}_{total} = \mathcal{L}_{BT} + \lambda_1 \mathcal{L}_{margin} + \lambda_2 \mathcal{L}_{reg}$$

å…¶ä¸­ï¼š
- $\mathcal{L}_{margin}$ï¼šç¡®ä¿å¥½åå›ç­”çš„å¥–åŠ±å·®è·è¶³å¤Ÿå¤§
- $\mathcal{L}_{reg}$ï¼šé˜²æ­¢å¥–åŠ±å€¼è¿‡å¤§ï¼Œé€šå¸¸ä½¿ç”¨ L2 æ­£åˆ™

```python
# è¾¹é™…æŸå¤±å®ç°
margin_loss = F.relu(margin - (r_chosen - r_rejected))
# margin é€šå¸¸è®¾ä¸º 0.5-1.0
```

**2. è®­ç»ƒç¨³å®šæ€§æŠ€å·§**

- **æ¢¯åº¦è£å‰ª**ï¼šå¿…é¡»ä½¿ç”¨ï¼Œclip_norm è®¾ä¸º 1.0
- **å­¦ä¹ ç‡é¢„çƒ­**ï¼šå‰ 10% æ­¥æ•°çº¿æ€§é¢„çƒ­
- **æ—©åœç­–ç•¥**ï¼šéªŒè¯é›†å‡†ç¡®ç‡ä¸å†æå‡æ—¶åœæ­¢
- **æ•°æ®å¢å¼º**ï¼šå¯¹åŒä¸€å›¾åƒä½¿ç”¨ä¸åŒ crop/augmentation

**3. å¸¸è§é—®é¢˜è¯Šæ–­**

| é—®é¢˜ | ç°è±¡ | è§£å†³æ–¹æ¡ˆ |
|------|------|----------|
| å¥–åŠ±å´©æºƒ | æ‰€æœ‰è¾“å‡ºå¥–åŠ±è¶‹äºç›¸åŒ | å¢åŠ  margin lossï¼Œæ£€æŸ¥æ•°æ®è´¨é‡ |
| è¿‡æ‹Ÿåˆ | è®­ç»ƒé›†å‡†ç¡®ç‡ > 90%ï¼ŒéªŒè¯é›† < 70% | å¢åŠ  dropoutï¼Œå‡å°å­¦ä¹ ç‡ï¼Œæ•°æ®å¢å¼º |
| å¥–åŠ±åˆ†å¸ƒåæ–œ | å¥–åŠ±å€¼é›†ä¸­åœ¨æç«¯å€¼ | è°ƒæ•´å½’ä¸€åŒ–ç­–ç•¥ï¼Œä½¿ç”¨ tanh æ¿€æ´» |
| è§†è§‰åè§ | åªçœ‹å›¾åƒå¿½ç•¥æ–‡æœ¬ | å¢åŠ çº¯æ–‡æœ¬è´Ÿæ ·æœ¬ï¼Œå¹³è¡¡æ¨¡æ€æƒé‡ |

**4. å¹»è§‰æƒ©ç½šæœºåˆ¶**

ä¸“é—¨é’ˆå¯¹è§†è§‰å¹»è§‰è®¾è®¡çš„å¥–åŠ±è°ƒæ•´ï¼š

```python
# å¹»è§‰æ£€æµ‹ä¸æƒ©ç½š
def hallucination_penalty(image_features, text_output):
    # 1. æå–æ–‡æœ¬ä¸­æåˆ°çš„ç‰©ä½“
    mentioned_objects = extract_objects(text_output)
    
    # 2. ä½¿ç”¨ç›®æ ‡æ£€æµ‹æ¨¡å‹éªŒè¯
    detected_objects = object_detector(image_features)
    
    # 3. è®¡ç®—æƒ©ç½š
    false_mentions = mentioned_objects - detected_objects
    penalty = -alpha * len(false_mentions)
    
    return penalty

# é›†æˆåˆ°æ€»å¥–åŠ±ä¸­
final_reward = base_reward + hallucination_penalty
```

## 5.3 PPO ç®—æ³•è¯¦è§£

Proximal Policy Optimization (PPO) æ˜¯ RLHF ä¸­æœ€å¸¸ç”¨çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•ã€‚å®ƒé€šè¿‡é™åˆ¶ç­–ç•¥æ›´æ–°å¹…åº¦æ¥ä¿è¯è®­ç»ƒç¨³å®šæ€§ï¼Œç‰¹åˆ«é€‚åˆå¤§è§„æ¨¡è¯­è¨€æ¨¡å‹çš„ä¼˜åŒ–ã€‚

### PPO æ ¸å¿ƒåŸç†å›é¡¾

PPO çš„æ ¸å¿ƒæ€æƒ³æ˜¯åœ¨æœ€å¤§åŒ–æœŸæœ›å¥–åŠ±çš„åŒæ—¶ï¼Œé™åˆ¶æ–°ç­–ç•¥ä¸æ—§ç­–ç•¥çš„å·®å¼‚ï¼š

**ç›®æ ‡å‡½æ•°**ï¼š

$$\mathcal{L}^{PPO}(\theta) = \mathbb{E}_t \left[ \min \left( r_t(\theta) \hat{A}_t, \text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon) \hat{A}_t \right) \right]$$

å…¶ä¸­ï¼š
- $r_t(\theta) = \frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)}$ æ˜¯é‡è¦æ€§é‡‡æ ·æ¯”ç‡
- $\hat{A}_t$ æ˜¯ä¼˜åŠ¿å‡½æ•°ä¼°è®¡
- $\epsilon$ æ˜¯è£å‰ªå‚æ•°ï¼ˆé€šå¸¸ 0.1-0.2ï¼‰

**VLM åœºæ™¯ä¸‹çš„çŠ¶æ€-åŠ¨ä½œå®šä¹‰**ï¼š

```
çŠ¶æ€ s_t = (å›¾åƒ I, æŒ‡ä»¤ q, å·²ç”Ÿæˆæ–‡æœ¬ y_{<t})
åŠ¨ä½œ a_t = ä¸‹ä¸€ä¸ª token y_t
å¥–åŠ± r_t = {
    0,                  if t < T (ä¸­é—´æ­¥éª¤)
    R(I, q, y_{1:T}),  if t = T (åºåˆ—ç»“æŸ)
}
```

### VLM ç‰¹å®šçš„ PPO æ”¹è¿›

æ ‡å‡† PPO åœ¨ VLM ä¸Šç›´æ¥åº”ç”¨ä¼šé‡åˆ°ç‰¹æ®ŠæŒ‘æˆ˜ï¼Œéœ€è¦é’ˆå¯¹æ€§æ”¹è¿›ï¼š

**1. å¤šæ¨¡æ€ä»·å€¼å‡½æ•°è®¾è®¡**

ä»·å€¼å‡½æ•°éœ€è¦å‡†ç¡®ä¼°è®¡å¤šæ¨¡æ€çŠ¶æ€çš„æœªæ¥å›æŠ¥ï¼š

```python
class MultiModalValueHead(nn.Module):
    def __init__(self, hidden_dim, vision_dim):
        super().__init__()
        # è§†è§‰ç‰¹å¾æŠ•å½±
        self.vision_proj = nn.Linear(vision_dim, hidden_dim)
        # è·¨æ¨¡æ€æ³¨æ„åŠ›
        self.cross_attention = CrossAttention(hidden_dim)
        # ä»·å€¼é¢„æµ‹å¤´
        self.value_head = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.Tanh(),
            nn.Linear(hidden_dim // 2, 1)
        )
    
    def forward(self, text_hidden, vision_features):
        # èåˆè§†è§‰ä¿¡æ¯
        vision_proj = self.vision_proj(vision_features)
        fused = self.cross_attention(text_hidden, vision_proj)
        # é¢„æµ‹ä»·å€¼
        value = self.value_head(fused.mean(dim=1))
        return value
```

**2. ä¼˜åŠ¿å‡½æ•°ä¼°è®¡æ”¹è¿›**

GAE (Generalized Advantage Estimation) åœ¨ VLM ä¸­éœ€è¦ç‰¹åˆ«å¤„ç†ï¼š

$$\hat{A}_t = \sum_{l=0}^{\infty} (\gamma \lambda)^l \delta_{t+l}$$

å…¶ä¸­ $\delta_t = r_t + \gamma V(s_{t+1}) - V(s_t)$

VLM ç‰¹æ®Šå¤„ç†ï¼š
- **ç¨€ç–å¥–åŠ±é—®é¢˜**ï¼šåªåœ¨åºåˆ—æœ«å°¾æœ‰å¥–åŠ±ï¼Œä¸­é—´æ­¥éª¤ä½¿ç”¨ shaping reward
- **è§†è§‰æ¡ä»¶æŠ˜æ‰£**ï¼šæ ¹æ®å›¾åƒå¤æ‚åº¦åŠ¨æ€è°ƒæ•´ $\gamma$

```python
def compute_advantages_vlm(rewards, values, vision_complexity):
    # åŠ¨æ€æŠ˜æ‰£å› å­
    gamma = 0.99 - 0.05 * vision_complexity  # å¤æ‚å›¾åƒä½¿ç”¨æ›´å°æŠ˜æ‰£
    
    advantages = []
    gae = 0
    for t in reversed(range(len(rewards))):
        if t == len(rewards) - 1:
            next_value = 0
        else:
            next_value = values[t + 1]
        
        delta = rewards[t] + gamma * next_value - values[t]
        gae = delta + gamma * lambda_ * gae
        advantages.insert(0, gae)
    
    return advantages
```

**3. æ‰¹æ¬¡æ„é€ ç­–ç•¥**

VLM çš„æ‰¹æ¬¡æ„é€ éœ€è¦å¹³è¡¡è§†è§‰å¤šæ ·æ€§å’Œè®¡ç®—æ•ˆç‡ï¼š

```
æ‰¹æ¬¡ç»„ç»‡åŸåˆ™ï¼š
1. å›¾åƒå»é‡ï¼šåŒä¸€æ‰¹æ¬¡é¿å…é‡å¤å›¾åƒï¼ˆèŠ‚çœè§†è§‰ç¼–ç ï¼‰
2. é•¿åº¦æ’åºï¼šç›¸ä¼¼é•¿åº¦çš„åºåˆ—æ”¾åœ¨ä¸€èµ·ï¼ˆå‡å°‘ paddingï¼‰
3. éš¾åº¦æ··åˆï¼šç®€å•å’Œå›°éš¾æ ·æœ¬æ··åˆï¼ˆç¨³å®šè®­ç»ƒï¼‰
4. æ¨¡æ€å¹³è¡¡ï¼šç¡®ä¿çº¯æ–‡æœ¬å’Œå¤šæ¨¡æ€æ ·æœ¬éƒ½æœ‰
```

### KL æ•£åº¦çº¦æŸçš„é‡è¦æ€§

KL æ•£åº¦çº¦æŸæ˜¯é˜²æ­¢ç­–ç•¥å´©æºƒçš„å…³é”®æœºåˆ¶ï¼Œåœ¨ VLM ä¸­å°¤å…¶é‡è¦ï¼š

**1. KL æƒ©ç½šé¡¹è®¾è®¡**

$$\mathcal{L}_{total} = \mathcal{L}^{PPO} - \beta \cdot \text{KL}[\pi_\theta || \pi_{ref}]$$

å…¶ä¸­ $\beta$ éœ€è¦è‡ªé€‚åº”è°ƒæ•´ï¼š

```python
class AdaptiveKLController:
    def __init__(self, init_kl_coef=0.1, target_kl=6.0):
        self.kl_coef = init_kl_coef
        self.target_kl = target_kl
        
    def update(self, current_kl):
        if current_kl > 1.5 * self.target_kl:
            self.kl_coef *= 1.5  # å¢å¤§æƒ©ç½š
        elif current_kl < 0.5 * self.target_kl:
            self.kl_coef *= 0.75  # å‡å°æƒ©ç½š
        
        # è£å‰ªèŒƒå›´
        self.kl_coef = np.clip(self.kl_coef, 0.001, 10.0)
        return self.kl_coef
```

**2. è§†è§‰æ¡ä»¶ä¸‹çš„ KL è®¡ç®—**

VLM çš„ KL æ•£åº¦éœ€è¦è€ƒè™‘è§†è§‰æ¡ä»¶ï¼š

$$\text{KL}_{vlm} = \mathbb{E}_{(I,q)} \left[ \text{KL}[\pi_\theta(\cdot|I,q) || \pi_{ref}(\cdot|I,q)] \right]$$

å®è·µæŠ€å·§ï¼š
- å¯¹è§†è§‰ token å’Œæ–‡æœ¬ token ä½¿ç”¨ä¸åŒçš„ KL æƒé‡
- å›¾åƒå¤æ‚åº¦é«˜æ—¶å…è®¸æ›´å¤§çš„ KL æ•£åº¦
- ç›‘æ§æ¯ä¸ªæ¨¡æ€çš„ KL è´¡çŒ®ï¼Œé˜²æ­¢å•ä¸€æ¨¡æ€ä¸»å¯¼

**3. KL çˆ†ç‚¸çš„é¢„é˜²ä¸å¤„ç†**

```python
def compute_kl_with_clipping(logits_new, logits_ref, attention_mask):
    # è®¡ç®— log æ¦‚ç‡
    log_probs_new = F.log_softmax(logits_new, dim=-1)
    log_probs_ref = F.log_softmax(logits_ref, dim=-1)
    
    # KL æ•£åº¦
    kl = (log_probs_ref.exp() * (log_probs_ref - log_probs_new)).sum(-1)
    
    # è£å‰ªå¼‚å¸¸å€¼
    kl = torch.clamp(kl, min=0, max=100)
    
    # åº”ç”¨ mask å¹¶å¹³å‡
    kl = (kl * attention_mask).sum() / attention_mask.sum()
    
    return kl
```

ç›‘æ§æŒ‡æ ‡ï¼š
- KL æ•£åº¦å‡å€¼å’Œæ–¹å·®
- æœ€å¤§å•æ­¥ KL
- è§†è§‰/æ–‡æœ¬ KL æ¯”ç‡

å½“ KL > 10 æ—¶çš„ç´§æ€¥å¤„ç†ï¼š
1. ç«‹å³é™ä½å­¦ä¹ ç‡è‡³ 1/10
2. å¢å¤§ KL æƒ©ç½šç³»æ•° $\beta$
3. å›æ»šåˆ°ä¸Šä¸€ä¸ªæ£€æŸ¥ç‚¹
4. æ£€æŸ¥æ˜¯å¦æœ‰æ•°æ®åˆ†å¸ƒåç§»

## 5.4 VLM ä¸­çš„ RLHF å®è·µ

å°† RLHF ç†è®ºåº”ç”¨åˆ°å®é™… VLM è®­ç»ƒä¸­éœ€è¦ç²¾å¿ƒè®¾è®¡æµç¨‹å’Œå‚æ•°ã€‚æœ¬èŠ‚æä¾›ç»è¿‡éªŒè¯çš„å®è·µæ–¹æ¡ˆã€‚

### è®­ç»ƒæµç¨‹è®¾è®¡

**å®Œæ•´è®­ç»ƒ Pipeline**ï¼š

```
Phase 1: å‡†å¤‡é˜¶æ®µï¼ˆ1-2 å¤©ï¼‰
â”œâ”€â”€ SFT æ¨¡å‹éªŒè¯
â”‚   â”œâ”€â”€ ç¡®ä¿ SFT æ¨¡å‹æ”¶æ•›
â”‚   â”œâ”€â”€ éªŒè¯ç”Ÿæˆè´¨é‡åŸºçº¿
â”‚   â””â”€â”€ å†»ç»“ SFT æƒé‡ä½œä¸ºå‚è€ƒæ¨¡å‹
â”œâ”€â”€ å¥–åŠ±æ¨¡å‹è®­ç»ƒ
â”‚   â”œâ”€â”€ æ”¶é›†åå¥½æ•°æ®ï¼ˆ10k-50k å¯¹ï¼‰
â”‚   â”œâ”€â”€ è®­ç»ƒå¥–åŠ±æ¨¡å‹ï¼ˆå‡†ç¡®ç‡ > 65%ï¼‰
â”‚   â””â”€â”€ éªŒè¯å¥–åŠ±åˆ†å¸ƒåˆç†æ€§
â””â”€â”€ ç¯å¢ƒé…ç½®
    â”œâ”€â”€ è®¾ç½®å¤š GPU å¹¶è¡Œ
    â”œâ”€â”€ é…ç½®æ¢¯åº¦ç´¯ç§¯
    â””â”€â”€ å‡†å¤‡ç›‘æ§å·¥å…·

Phase 2: PPO è®­ç»ƒï¼ˆ3-5 å¤©ï¼‰
â”œâ”€â”€ é¢„çƒ­é˜¶æ®µï¼ˆ10% stepsï¼‰
â”‚   â”œâ”€â”€ å°å­¦ä¹ ç‡ï¼ˆ1e-7ï¼‰
â”‚   â”œâ”€â”€ å¤§ KL ç³»æ•°ï¼ˆ0.5ï¼‰
â”‚   â””â”€â”€ ç›‘æ§ç¨³å®šæ€§
â”œâ”€â”€ ä¸»è®­ç»ƒé˜¶æ®µï¼ˆ80% stepsï¼‰
â”‚   â”œâ”€â”€ æ­£å¸¸å­¦ä¹ ç‡ï¼ˆ1e-6ï¼‰
â”‚   â”œâ”€â”€ è‡ªé€‚åº” KL ç³»æ•°
â”‚   â””â”€â”€ å®šæœŸè¯„ä¼°
â””â”€â”€ æ”¶å°¾é˜¶æ®µï¼ˆ10% stepsï¼‰
    â”œâ”€â”€ å­¦ä¹ ç‡è¡°å‡
    â”œâ”€â”€ å¢å¤§ KL çº¦æŸ
    â””â”€â”€ é€‰æ‹©æœ€ä½³æ£€æŸ¥ç‚¹

Phase 3: åå¤„ç†ï¼ˆ1 å¤©ï¼‰
â”œâ”€â”€ æ¨¡å‹è¯„ä¼°
â”œâ”€â”€ æ¶ˆèå®éªŒ
â””â”€â”€ éƒ¨ç½²å‡†å¤‡
```

**æ•°æ®æµè®¾è®¡**ï¼š

```python
class RLHFDataPipeline:
    def __init__(self, batch_size=32, buffer_size=1000):
        self.batch_size = batch_size
        self.buffer = []
        self.buffer_size = buffer_size
        
    def generate_batch(self, model, prompts):
        """ç”Ÿæˆé˜¶æ®µï¼šæ”¶é›†æ¨¡å‹è¾“å‡º"""
        with torch.no_grad():
            outputs = []
            for prompt in prompts:
                # å¤šæ ·æ€§é‡‡æ ·
                for temp in [0.7, 0.9, 1.1]:
                    output = model.generate(
                        prompt, 
                        temperature=temp,
                        do_sample=True,
                        max_length=512
                    )
                    outputs.append({
                        'prompt': prompt,
                        'response': output,
                        'temperature': temp
                    })
        return outputs
    
    def score_batch(self, reward_model, experiences):
        """è¯„åˆ†é˜¶æ®µï¼šè®¡ç®—å¥–åŠ±"""
        rewards = []
        for exp in experiences:
            reward = reward_model(exp['prompt'], exp['response'])
            # æ·»åŠ é•¿åº¦æƒ©ç½š
            length_penalty = -0.01 * len(exp['response'])
            exp['reward'] = reward + length_penalty
            rewards.append(exp)
        return rewards
    
    def update_model(self, model, experiences):
        """æ›´æ–°é˜¶æ®µï¼šPPO ä¼˜åŒ–"""
        # è®¡ç®—ä¼˜åŠ¿
        advantages = self.compute_advantages(experiences)
        
        # å¤šè½®æ›´æ–°
        for _ in range(4):  # PPO epochs
            for batch in self.get_batches(experiences):
                loss = self.ppo_loss(model, batch, advantages)
                loss.backward()
                # æ¢¯åº¦è£å‰ª
                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
                optimizer.step()
```

### è¶…å‚æ•°é€‰æ‹©ç­–ç•¥

VLM RLHF çš„è¶…å‚æ•°é€‰æ‹©éœ€è¦è€ƒè™‘æ¨¡å‹è§„æ¨¡ã€æ•°æ®ç‰¹ç‚¹å’Œè®¡ç®—èµ„æºï¼š

**æ ¸å¿ƒè¶…å‚æ•°æ¨èå€¼**ï¼š

| å‚æ•° | 7B æ¨¡å‹ | 13B æ¨¡å‹ | 34B+ æ¨¡å‹ | è¯´æ˜ |
|------|---------|----------|-----------|------|
| **å­¦ä¹ ç‡** | 5e-7 | 1e-6 | 2e-6 | Actor æ¨¡å‹å­¦ä¹ ç‡ |
| **Critic å­¦ä¹ ç‡** | 1e-6 | 2e-6 | 5e-6 | é€šå¸¸æ˜¯ Actor çš„ 2-5 å€ |
| **æ‰¹æ¬¡å¤§å°** | 32 | 64 | 128 | æ¯ä¸ª GPU çš„æ‰¹æ¬¡ |
| **PPO epochs** | 4 | 4 | 2 | æ¯æ‰¹æ•°æ®çš„æ›´æ–°è½®æ•° |
| **è£å‰ªå‚æ•° Îµ** | 0.2 | 0.2 | 0.1 | å¤§æ¨¡å‹ç”¨æ›´å°å€¼ |
| **GAE Î»** | 0.95 | 0.95 | 0.97 | ä¼˜åŠ¿ä¼°è®¡å¹³æ»‘åº¦ |
| **æŠ˜æ‰£å› å­ Î³** | 0.99 | 0.99 | 0.995 | æœªæ¥å¥–åŠ±æŠ˜æ‰£ |
| **KL ç›®æ ‡** | 6.0 | 6.0 | 3.0 | ç›®æ ‡ KL æ•£åº¦ |
| **åˆå§‹ KL ç³»æ•°** | 0.1 | 0.2 | 0.5 | KL æƒ©ç½šåˆå§‹å€¼ |

**å­¦ä¹ ç‡è°ƒåº¦ç­–ç•¥**ï¼š

```python
def get_lr_scheduler(optimizer, num_training_steps):
    # ä½™å¼¦é€€ç« + é¢„çƒ­
    def lr_lambda(step):
        # 10% é¢„çƒ­
        warmup_steps = int(0.1 * num_training_steps)
        if step < warmup_steps:
            return float(step) / float(max(1, warmup_steps))
        
        # ä½™å¼¦è¡°å‡
        progress = float(step - warmup_steps) / float(max(1, num_training_steps - warmup_steps))
        return 0.5 * (1.0 + math.cos(math.pi * progress))
    
    return LambdaLR(optimizer, lr_lambda)
```

### å¤šæ¨¡æ€ç‰¹æœ‰é—®é¢˜å¤„ç†

**1. è§†è§‰ç¼–ç å™¨çš„å¤„ç†ç­–ç•¥**

```python
class VisionEncoderStrategy:
    """è§†è§‰ç¼–ç å™¨åœ¨ RLHF ä¸­çš„å¤„ç†ç­–ç•¥"""
    
    @staticmethod
    def frozen_strategy():
        """ç­–ç•¥ 1ï¼šå®Œå…¨å†»ç»“"""
        # ä¼˜ç‚¹ï¼šç¨³å®šã€çœæ˜¾å­˜
        # ç¼ºç‚¹ï¼šæ— æ³•ä¼˜åŒ–è§†è§‰è¡¨ç¤º
        return {'vision_encoder': False, 'projection': True}
    
    @staticmethod
    def staged_unfreezing():
        """ç­–ç•¥ 2ï¼šé˜¶æ®µæ€§è§£å†»"""
        stages = [
            (0, 0.3, {'vision_encoder': False, 'projection': True}),    # å‰ 30%ï¼šå†»ç»“
            (0.3, 0.7, {'vision_encoder': 'last_layer', 'projection': True}),  # ä¸­ 40%ï¼šè§£å†»æœ€åå±‚
            (0.7, 1.0, {'vision_encoder': True, 'projection': True})     # å 30%ï¼šå…¨éƒ¨è§£å†»
        ]
        return stages
    
    @staticmethod
    def lora_strategy():
        """ç­–ç•¥ 3ï¼šLoRA å¾®è°ƒ"""
        # åœ¨è§†è§‰ç¼–ç å™¨ä¸­æ’å…¥ LoRA
        return {
            'vision_lora_rank': 16,
            'vision_lora_alpha': 32,
            'vision_lora_dropout': 0.1
        }
```

**2. å¤šæ¨¡æ€å¥–åŠ±å¯¹é½**

ç¡®ä¿æ–‡æœ¬å’Œè§†è§‰è´¡çŒ®å¹³è¡¡çš„å¥–åŠ±ï¼š

```python
def compute_multimodal_reward(text_reward, vision_reward, alignment_score):
    """
    ç»„åˆå¤šä¸ªå¥–åŠ±ä¿¡å·
    text_reward: æ–‡æœ¬è´¨é‡è¯„åˆ†
    vision_reward: è§†è§‰ç›¸å…³æ€§è¯„åˆ†
    alignment_score: è·¨æ¨¡æ€å¯¹é½è¯„åˆ†
    """
    # è‡ªé€‚åº”æƒé‡
    if alignment_score < 0.5:
        # å¯¹é½å·®æ—¶ï¼Œæ›´é‡è§†å¯¹é½
        weights = {'text': 0.2, 'vision': 0.2, 'alignment': 0.6}
    else:
        # å¯¹é½å¥½æ—¶ï¼Œå¹³è¡¡å„é¡¹
        weights = {'text': 0.4, 'vision': 0.3, 'alignment': 0.3}
    
    final_reward = (
        weights['text'] * text_reward +
        weights['vision'] * vision_reward +
        weights['alignment'] * alignment_score
    )
    
    return final_reward
```

**3. å¹»è§‰æŠ‘åˆ¶æœºåˆ¶**

```python
class HallucinationSupressor:
    def __init__(self, detection_model, penalty_weight=0.5):
        self.detector = detection_model
        self.penalty_weight = penalty_weight
        
    def compute_penalty(self, image, generated_text):
        # æ£€æµ‹å¹»è§‰
        hallucinations = self.detector.detect(image, generated_text)
        
        # åˆ†çº§æƒ©ç½š
        penalties = {
            'object_hallucination': -2.0,  # ç‰©ä½“å¹»è§‰æœ€ä¸¥é‡
            'attribute_error': -1.0,        # å±æ€§é”™è¯¯æ¬¡ä¹‹
            'relation_error': -0.5          # å…³ç³»é”™è¯¯è¾ƒè½»
        }
        
        total_penalty = 0
        for h_type, h_count in hallucinations.items():
            total_penalty += penalties.get(h_type, 0) * h_count
            
        return self.penalty_weight * total_penalty
```

## 5.5 è®­ç»ƒç¨³å®šæ€§ä¸è°ƒè¯•

RLHF è®­ç»ƒçš„ä¸ç¨³å®šæ€§æ˜¯å®è·µä¸­çš„ä¸»è¦æŒ‘æˆ˜ã€‚æœ¬èŠ‚æä¾›ç³»ç»Ÿçš„è¯Šæ–­å’Œè§£å†³æ–¹æ¡ˆã€‚

### å¸¸è§ä¸ç¨³å®šç°è±¡è¯Šæ–­

**1. å¥–åŠ±å´©æºƒæ¨¡å¼è¯†åˆ«**

```
ç—‡çŠ¶è§‚å¯Ÿè¡¨ï¼š
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ç°è±¡            â”‚ å¯èƒ½åŸå›           â”‚ è¯Šæ–­æ–¹æ³•         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ å¥–åŠ±æŒç»­ä¸Šå‡    â”‚ å¥–åŠ±é»‘å®¢          â”‚ æ£€æŸ¥ç”Ÿæˆæ–‡æœ¬è´¨é‡ â”‚
â”‚ å¥–åŠ±çªç„¶ä¸‹é™    â”‚ ç­–ç•¥å´©æºƒ          â”‚ æŸ¥çœ‹ KL æ•£åº¦     â”‚
â”‚ å¥–åŠ±éœ‡è¡        â”‚ å­¦ä¹ ç‡è¿‡å¤§        â”‚ æ¢¯åº¦èŒƒæ•°ç›‘æ§     â”‚
â”‚ å¥–åŠ±åœæ»        â”‚ å±€éƒ¨æœ€ä¼˜/è¿‡æ‹Ÿåˆ   â”‚ éªŒè¯é›†è¡¨ç°       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**2. å¿«é€Ÿè¯Šæ–­è„šæœ¬**

```python
class RLHFDiagnostics:
    def __init__(self, threshold_config):
        self.thresholds = threshold_config
        self.history = defaultdict(list)
        
    def diagnose(self, metrics):
        """å®æ—¶è¯Šæ–­è®­ç»ƒçŠ¶æ€"""
        issues = []
        
        # æ£€æŸ¥å¥–åŠ±å¼‚å¸¸
        if metrics['reward'] > self.thresholds['max_reward']:
            issues.append(('CRITICAL', 'Reward hacking detected'))
        
        # æ£€æŸ¥ KL æ•£åº¦
        if metrics['kl_div'] > self.thresholds['max_kl']:
            issues.append(('WARNING', f"KL divergence too high: {metrics['kl_div']:.2f}"))
        
        # æ£€æŸ¥æ¢¯åº¦
        if metrics['grad_norm'] > self.thresholds['max_grad']:
            issues.append(('WARNING', 'Gradient explosion risk'))
        
        # æ£€æŸ¥ç”Ÿæˆé•¿åº¦
        avg_length = np.mean(metrics['response_lengths'])
        if avg_length < 20 or avg_length > 500:
            issues.append(('INFO', f'Abnormal response length: {avg_length:.0f}'))
        
        # è¶‹åŠ¿åˆ†æ
        self.history['reward'].append(metrics['reward'])
        if len(self.history['reward']) > 100:
            recent_std = np.std(self.history['reward'][-100:])
            if recent_std > self.thresholds['reward_std']:
                issues.append(('WARNING', 'Reward instability detected'))
        
        return issues
```

### å¥–åŠ±é»‘å®¢ï¼ˆReward Hackingï¼‰é˜²èŒƒ

å¥–åŠ±é»‘å®¢æ˜¯æ¨¡å‹æ‰¾åˆ°æ¬ºéª—å¥–åŠ±æ¨¡å‹çš„æ·å¾„ï¼Œäº§ç”Ÿé«˜å¥–åŠ±ä½†æ— æ„ä¹‰çš„è¾“å‡ºã€‚

**1. å…¸å‹å¥–åŠ±é»‘å®¢æ¨¡å¼**

```
VLM å¸¸è§å¥–åŠ±é»‘å®¢è¡Œä¸ºï¼š
1. é‡å¤æè¿°ï¼šä¸æ–­é‡å¤å›¾åƒä¸­çš„æ˜¾è‘—ç‰©ä½“
2. æ¨¡æ¿åŒ–å›ç­”ï¼šä½¿ç”¨å›ºå®šå¥å¼è·å¾—ç¨³å®šå¥–åŠ±
3. è¿‡åº¦è¯¦ç»†ï¼šç”Ÿæˆå†—é•¿ä½†ä¿¡æ¯é‡ä½çš„æè¿°
4. å…³é”®è¯å †ç Œï¼šå †ç§¯å¥–åŠ±æ¨¡å‹åå¥½çš„è¯æ±‡
5. å¿½ç•¥æŒ‡ä»¤ï¼šåªæè¿°å›¾åƒï¼Œä¸å›ç­”é—®é¢˜
```

**2. é˜²èŒƒæœºåˆ¶å®ç°**

```python
class RewardHackingDefense:
    def __init__(self):
        self.detectors = {
            'repetition': self.detect_repetition,
            'template': self.detect_template,
            'keyword_stuffing': self.detect_keyword_stuffing,
            'length_gaming': self.detect_length_gaming
        }
        
    def detect_repetition(self, text):
        """æ£€æµ‹é‡å¤æ¨¡å¼"""
        sentences = text.split('.')
        if len(sentences) < 3:
            return 0
        
        # è®¡ç®—å¥å­ç›¸ä¼¼åº¦
        similarities = []
        for i in range(len(sentences)-1):
            sim = self.sentence_similarity(sentences[i], sentences[i+1])
            similarities.append(sim)
        
        # é«˜ç›¸ä¼¼åº¦è¡¨ç¤ºé‡å¤
        return max(similarities) if similarities else 0
    
    def detect_template(self, responses):
        """æ£€æµ‹æ¨¡æ¿åŒ–å›ç­”"""
        # æå–ç»“æ„ç‰¹å¾
        structures = [self.extract_structure(r) for r in responses]
        
        # è®¡ç®—ç»“æ„å¤šæ ·æ€§
        unique_structures = len(set(structures))
        diversity_score = unique_structures / len(structures)
        
        return 1 - diversity_score  # ä½å¤šæ ·æ€§ = é«˜æ¨¡æ¿åŒ–
    
    def apply_defense(self, reward, text, detection_scores):
        """åº”ç”¨é˜²å¾¡æœºåˆ¶è°ƒæ•´å¥–åŠ±"""
        penalty = 0
        
        for detector_name, score in detection_scores.items():
            if score > 0.7:  # é«˜ç½®ä¿¡åº¦æ£€æµ‹
                penalty += self.penalty_weights[detector_name] * score
        
        # åº”ç”¨æƒ©ç½š
        adjusted_reward = reward - penalty
        
        # ç¡®ä¿ä¸ä¼šè¿‡åº¦æƒ©ç½š
        return max(adjusted_reward, reward * 0.3)
```

**3. å¤šæ ·æ€§å¥–åŠ±æœºåˆ¶**

```python
def diversity_bonus(current_response, previous_responses, alpha=0.1):
    """
    é¼“åŠ±å¤šæ ·æ€§çš„å¥–åŠ±è°ƒæ•´
    """
    if not previous_responses:
        return 0
    
    # è®¡ç®—ä¸å†å²å›ç­”çš„æœ€å°è·ç¦»
    min_distance = float('inf')
    for prev in previous_responses[-10:]:  # åªçœ‹æœ€è¿‘10ä¸ª
        distance = edit_distance(current_response, prev) / max(len(current_response), len(prev))
        min_distance = min(min_distance, distance)
    
    # è·ç¦»è¶Šå¤§ï¼Œå¥–åŠ±è¶Šé«˜
    bonus = alpha * min_distance
    return bonus
```

### ç›‘æ§æŒ‡æ ‡è®¾è®¡

å…¨é¢çš„ç›‘æ§æ˜¯ä¿è¯è®­ç»ƒç¨³å®šçš„å…³é”®ï¼š

**1. æ ¸å¿ƒç›‘æ§æŒ‡æ ‡**

```python
class RLHFMonitor:
    def __init__(self):
        self.metrics = {
            # å¥–åŠ±ç›¸å…³
            'reward_mean': [],
            'reward_std': [],
            'reward_max': [],
            'reward_min': [],
            
            # KL æ•£åº¦
            'kl_div_mean': [],
            'kl_div_max': [],
            'kl_per_token': [],
            
            # ç”Ÿæˆè´¨é‡
            'response_length': [],
            'unique_tokens_ratio': [],
            'perplexity': [],
            
            # è®­ç»ƒç¨³å®šæ€§
            'grad_norm': [],
            'value_loss': [],
            'policy_loss': [],
            'entropy': [],
            
            # è§†è§‰ç‰¹å®š
            'vision_attention_entropy': [],
            'cross_modal_alignment': []
        }
    
    def log_step(self, batch_metrics):
        """è®°å½•æ¯æ­¥æŒ‡æ ‡"""
        for key, value in batch_metrics.items():
            if key in self.metrics:
                self.metrics[key].append(value)
    
    def get_dashboard_data(self):
        """ç”Ÿæˆç›‘æ§é¢æ¿æ•°æ®"""
        dashboard = {}
        
        # è®¡ç®—ç§»åŠ¨å¹³å‡
        for key, values in self.metrics.items():
            if len(values) > 0:
                dashboard[f'{key}_ma10'] = np.mean(values[-10:])
                dashboard[f'{key}_ma100'] = np.mean(values[-100:])
        
        # è®¡ç®—è¶‹åŠ¿
        if len(self.metrics['reward_mean']) > 100:
            recent = np.mean(self.metrics['reward_mean'][-50:])
            past = np.mean(self.metrics['reward_mean'][-100:-50])
            dashboard['reward_trend'] = (recent - past) / abs(past)
        
        return dashboard
```

**2. å®æ—¶å‘Šè­¦ç³»ç»Ÿ**

```python
class AlertSystem:
    def __init__(self):
        self.alert_rules = [
            ('kl_div_mean > 10', 'CRITICAL', 'KL divergence explosion'),
            ('reward_std > 2', 'WARNING', 'Reward instability'),
            ('grad_norm > 100', 'CRITICAL', 'Gradient explosion'),
            ('response_length < 10', 'WARNING', 'Degenerate responses'),
            ('entropy < 0.1', 'WARNING', 'Low generation diversity')
        ]
    
    def check_alerts(self, metrics):
        alerts = []
        for rule, level, message in self.alert_rules:
            if self.evaluate_rule(rule, metrics):
                alerts.append({
                    'level': level,
                    'message': message,
                    'metrics': metrics,
                    'timestamp': time.time()
                })
        return alerts
```

## Case Study: LLaVA-RLHF çš„äººç±»åå¥½å¯¹é½å®è·µ

LLaVA-RLHF æ˜¯é¦–ä¸ªæˆåŠŸå°† RLHF åº”ç”¨äºå¼€æº VLM çš„å·¥ä½œï¼Œå…¶æ–¹æ³•è®ºå€¼å¾—æ·±å…¥åˆ†æã€‚

### æ•°æ®æ”¶é›†ä¸æ ‡æ³¨æµç¨‹

LLaVA-RLHF æ„å»ºäº†åŒ…å« 10k æ¯”è¾ƒå¯¹çš„é«˜è´¨é‡åå¥½æ•°æ®é›†ï¼š

**1. æ•°æ®æºé€‰æ‹©**
- COCO æ•°æ®é›†ï¼šæ—¥å¸¸åœºæ™¯å›¾åƒ
- Visual Genomeï¼šå¤æ‚åœºæ™¯å’Œå…³ç³»
- A-OKVQAï¼šéœ€è¦æ¨ç†çš„è§†è§‰é—®ç­”

**2. å›ç­”ç”Ÿæˆç­–ç•¥**
```
å¯¹æ¯ä¸ª (å›¾åƒ, é—®é¢˜) å¯¹ï¼š
1. ä½¿ç”¨ 4 ä¸ªä¸åŒæ¨¡å‹ç”Ÿæˆå›ç­”ï¼š
   - LLaVA-13B (base)
   - LLaVA-13B-v1.5
   - GPT-4V (ä½œä¸ºé«˜è´¨é‡å‚è€ƒ)
   - äººå·¥ç¼–å†™ (ground truth)

2. æ¸©åº¦é‡‡æ ·å¢åŠ å¤šæ ·æ€§ï¼š
   - T=0.7, 0.9, 1.1 å„ç”Ÿæˆä¸€æ¬¡
   - æ€»è®¡ 12 ä¸ªå€™é€‰å›ç­”

3. å»é‡å’Œè¿‡æ»¤ï¼š
   - ç§»é™¤å®Œå…¨ç›¸åŒçš„å›ç­”
   - è¿‡æ»¤é•¿åº¦ < 10 æˆ– > 500 çš„å›ç­”
   - ä¿ç•™ 4-6 ä¸ªæœ€å¤šæ ·çš„å›ç­”
```

**3. æ ‡æ³¨åè®®**
```
æ ‡æ³¨è€…æŒ‡å—ï¼š
ä¼˜å…ˆçº§ 1: äº‹å®å‡†ç¡®æ€§ï¼ˆæ˜¯å¦æ­£ç¡®æè¿°å›¾åƒï¼‰
ä¼˜å…ˆçº§ 2: ç›¸å…³æ€§ï¼ˆæ˜¯å¦å›ç­”äº†é—®é¢˜ï¼‰
ä¼˜å…ˆçº§ 3: æœ‰ç”¨æ€§ï¼ˆä¿¡æ¯é‡å’Œæ´å¯Ÿæ·±åº¦ï¼‰
ä¼˜å…ˆçº§ 4: è¡¨è¾¾è´¨é‡ï¼ˆæ¸…æ™°åº¦å’Œè¿è´¯æ€§ï¼‰

æ ‡æ³¨æ¥å£ï¼š
- å¹¶æ’æ˜¾ç¤ºå›¾åƒå’Œé—®é¢˜
- éšæœºé¡ºåºå±•ç¤ºå€™é€‰å›ç­”
- æ”¯æŒæ‹–æ‹½æ’åºæˆ–æˆå¯¹æ¯”è¾ƒ
- è¦æ±‚æ ‡æ³¨è€…è§£é‡Šæ’åºç†ç”±
```

### ä¸‰é˜¶æ®µè®­ç»ƒç­–ç•¥

**é˜¶æ®µ 1ï¼šè§†è§‰æŒ‡ä»¤å¾®è°ƒï¼ˆVisual Instruction Tuningï¼‰**
```
ç›®æ ‡ï¼šå»ºç«‹åŸºç¡€çš„å¤šæ¨¡æ€ç†è§£èƒ½åŠ›
æ•°æ®ï¼š595K æŒ‡ä»¤è·Ÿéšæ ·æœ¬
é…ç½®ï¼š
- å­¦ä¹ ç‡: 2e-5 (ç¬¬ä¸€è½®), 2e-6 (ç¬¬äºŒè½®)
- æ‰¹æ¬¡å¤§å°: 128
- è®­ç»ƒè½®æ•°: 1 epoch
- è§†è§‰ç¼–ç å™¨: å†»ç»“ CLIP ViT-L/14

å…³é”®æŠ€å·§ï¼š
- ä¸¤é˜¶æ®µè®­ç»ƒï¼šå…ˆè®­ç»ƒæŠ•å½±å±‚ï¼Œå†å¾®è°ƒ LLM
- æ•°æ®é…æ¯”ï¼š80% å¤šæ¨¡æ€ï¼Œ20% çº¯æ–‡æœ¬ï¼ˆä¿æŒè¯­è¨€èƒ½åŠ›ï¼‰
```

**é˜¶æ®µ 2ï¼šå¥–åŠ±æ¨¡å‹è®­ç»ƒ**
```
æ¶æ„ï¼šåŸºäº LLaVA-13B + çº¿æ€§å¥–åŠ±å¤´
æ•°æ®ï¼š10K äººç±»åå¥½å¯¹
é…ç½®ï¼š
- å­¦ä¹ ç‡: 1e-6
- æ‰¹æ¬¡å¤§å°: 64
- è®­ç»ƒæ­¥æ•°: 3 epochs
- æŸå¤±å‡½æ•°: Bradley-Terry + Margin Loss

æ€§èƒ½æŒ‡æ ‡ï¼š
- æˆå¯¹å‡†ç¡®ç‡: 67.3%
- ä¸äººç±»ä¸€è‡´æ€§: Kappa = 0.62
- éªŒè¯é›†æ³›åŒ–: 65.1%
```

**é˜¶æ®µ 3ï¼šPPO å¼ºåŒ–å­¦ä¹ **
```
é…ç½®ï¼š
- Actor å­¦ä¹ ç‡: 5e-7
- Critic å­¦ä¹ ç‡: 1e-6
- KL ç³»æ•°: åˆå§‹ 0.1ï¼Œè‡ªé€‚åº”è°ƒæ•´
- æ‰¹æ¬¡å¤§å°: 32
- PPO epochs: 4
- è®­ç»ƒæ­¥æ•°: 50K

è®­ç»ƒæŠ€å·§ï¼š
1. é¢„çƒ­é˜¶æ®µï¼ˆå‰ 10%ï¼‰ï¼š
   - å°å­¦ä¹ ç‡é˜²æ­¢å´©æºƒ
   - å¤§ KL æƒ©ç½šä¿æŒç¨³å®š

2. ä¸»è®­ç»ƒé˜¶æ®µï¼š
   - æ¯ 1000 æ­¥è¯„ä¼°éªŒè¯é›†
   - åŠ¨æ€è°ƒæ•´ KL ç³»æ•°
   - ç›‘æ§å¥–åŠ±é»‘å®¢

3. æ”¶å°¾é˜¶æ®µï¼ˆå 10%ï¼‰ï¼š
   - çº¿æ€§è¡°å‡å­¦ä¹ ç‡
   - é€‰æ‹©æœ€ä½³æ£€æŸ¥ç‚¹ï¼ˆéæœ€åä¸€ä¸ªï¼‰
```

### æ•ˆæœè¯„ä¼°ä¸åˆ†æ

**1. å®šé‡è¯„ä¼°ç»“æœ**

| æŒ‡æ ‡ | LLaVA-13B | LLaVA-RLHF | æå‡ |
|------|-----------|------------|------|
| **MMBench** | 67.7 | 71.3 | +3.6 |
| **å¹»è§‰ç‡** | 31.2% | 18.7% | -12.5% |
| **äººç±»åå¥½èƒœç‡** | - | 62.3% | - |
| **å¹³å‡å›ç­”é•¿åº¦** | 89 tokens | 126 tokens | +41% |

**2. å®šæ€§æ”¹è¿›åˆ†æ**

```
æ”¹è¿› 1ï¼šå¹»è§‰æ˜¾è‘—å‡å°‘
Before: "å›¾ä¸­æœ‰ä¸€åªçŒ«åœ¨æ¡Œå­ä¸Šï¼Œæ—è¾¹æœ‰ä¸€ä¸ªçº¢è‰²çš„çƒã€‚"
       ï¼ˆå®é™…å›¾ä¸­æ— çƒï¼‰
After:  "å›¾ä¸­æœ‰ä¸€åªç°è‰²çš„çŒ«èººåœ¨æœ¨æ¡Œä¸Šã€‚"

æ”¹è¿› 2ï¼šç»†èŠ‚æè¿°æ›´å‡†ç¡®
Before: "è¿™æ˜¯ä¸€ä¸ªæˆ¿é—´ã€‚"
After:  "è¿™æ˜¯ä¸€ä¸ªç°ä»£é£æ ¼çš„å®¢å…ï¼Œæœ‰ç±³è‰²æ²™å‘ã€ç»ç’ƒèŒ¶å‡ å’Œå¤§çª—æˆ·ã€‚"

æ”¹è¿› 3ï¼šæ¨ç†èƒ½åŠ›å¢å¼º
Question: "ä¸ºä»€ä¹ˆè¿™ä¸ªäººæˆ´ç€å®‰å…¨å¸½ï¼Ÿ"
Before: "å› ä¸ºä»–åœ¨å·¥ä½œã€‚"
After:  "è¿™ä¸ªäººæˆ´ç€å®‰å…¨å¸½æ˜¯å› ä¸ºä»–åœ¨å»ºç­‘å·¥åœ°å·¥ä½œï¼Œè¿™æ˜¯å®‰å…¨è§„å®šè¦æ±‚çš„é˜²æŠ¤è£…å¤‡ã€‚"
```

**3. å¤±è´¥æ¨¡å¼åˆ†æ**

å°½ç®¡å–å¾—æ”¹è¿›ï¼Œä»å­˜åœ¨ä¸€äº›é—®é¢˜ï¼š
- è¿‡åº¦ä¿å®ˆï¼šæœ‰æ—¶æ‹’ç»å›ç­”å®é™…å¯ä»¥å›ç­”çš„é—®é¢˜
- é•¿åº¦åè§ï¼šå€¾å‘äºç”Ÿæˆæ›´é•¿çš„å›ç­”ï¼Œå³ä½¿ç®€çŸ­å›ç­”æ›´åˆé€‚
- æ¨¡æ€ä¸å¹³è¡¡ï¼šæŸäº›æƒ…å†µä¸‹è¿‡åº¦ä¾èµ–è¯­è¨€æ¨¡å‹å…ˆéªŒï¼Œå¿½è§†è§†è§‰ä¿¡æ¯

## é«˜çº§è¯é¢˜

### å¤šæ¨¡æ€å¥–åŠ±å»ºæ¨¡çš„æŒ‘æˆ˜

å¤šæ¨¡æ€å¥–åŠ±å»ºæ¨¡é¢ä¸´ç‹¬ç‰¹çš„æŠ€æœ¯æŒ‘æˆ˜ï¼Œéœ€è¦åˆ›æ–°çš„è§£å†³æ–¹æ¡ˆï¼š

**1. è·¨æ¨¡æ€ä¸€è‡´æ€§å»ºæ¨¡**

ä¼ ç»Ÿçš„å¥–åŠ±æ¨¡å‹ä¸»è¦å…³æ³¨æ–‡æœ¬è´¨é‡ï¼Œä½† VLM éœ€è¦åŒæ—¶è¯„ä¼°è·¨æ¨¡æ€ä¸€è‡´æ€§ï¼š

```python
class CrossModalConsistencyReward:
    """è¯„ä¼°æ–‡æœ¬ä¸å›¾åƒçš„ä¸€è‡´æ€§"""
    
    def compute_consistency(self, image_features, text_embeddings):
        # æ–¹æ³• 1ï¼šå¯¹æ¯”å­¦ä¹ ç›¸ä¼¼åº¦
        similarity = F.cosine_similarity(
            self.image_proj(image_features),
            self.text_proj(text_embeddings)
        )
        
        # æ–¹æ³• 2ï¼šç»†ç²’åº¦å¯¹é½
        # æ£€æµ‹å›¾åƒä¸­çš„ç‰©ä½“
        detected_objects = self.object_detector(image_features)
        # æå–æ–‡æœ¬ä¸­æåˆ°çš„å®ä½“
        mentioned_entities = self.entity_extractor(text_embeddings)
        # è®¡ç®—é‡å åº¦
        overlap = len(detected_objects & mentioned_entities) / len(mentioned_entities)
        
        # ç»„åˆä¸¤ç§ä¿¡å·
        consistency_score = 0.6 * similarity + 0.4 * overlap
        return consistency_score
```

**2. å¤šç²’åº¦å¥–åŠ±è®¾è®¡**

ä¸åŒä»»åŠ¡éœ€è¦ä¸åŒç²’åº¦çš„å¥–åŠ±ä¿¡å·ï¼š

- **Token çº§å¥–åŠ±**ï¼šè¯†åˆ«å…·ä½“çš„å¹»è§‰ä½ç½®
- **å¥å­çº§å¥–åŠ±**ï¼šè¯„ä¼°é€»è¾‘è¿è´¯æ€§
- **æ®µè½çº§å¥–åŠ±**ï¼šæ•´ä½“è´¨é‡è¯„ä¼°

**3. ç»„åˆå¥–åŠ±å‡½æ•°çš„ä¼˜åŒ–**

$$R_{total} = \alpha R_{accuracy} + \beta R_{relevance} + \gamma R_{safety} + \delta R_{diversity}$$

æŒ‘æˆ˜åœ¨äºå¦‚ä½•è‡ªåŠ¨å­¦ä¹ æƒé‡ $\alpha, \beta, \gamma, \delta$ï¼š

```python
class AdaptiveRewardWeighting:
    def __init__(self):
        self.weights = nn.Parameter(torch.ones(4) / 4)
        
    def forward(self, rewards_dict):
        # å½’ä¸€åŒ–æƒé‡
        normalized_weights = F.softmax(self.weights, dim=0)
        
        # è®¡ç®—åŠ æƒå¥–åŠ±
        total_reward = sum(
            w * r for w, r in zip(normalized_weights, rewards_dict.values())
        )
        
        # æ·»åŠ ç†µæ­£åˆ™åŒ–ï¼Œé˜²æ­¢æƒé‡é€€åŒ–
        entropy = -(normalized_weights * normalized_weights.log()).sum()
        total_reward += 0.01 * entropy
        
        return total_reward
```

### å¹»è§‰æƒ©ç½šæœºåˆ¶è®¾è®¡

è§†è§‰å¹»è§‰æ˜¯ VLM çš„ä¸»è¦é—®é¢˜ï¼Œéœ€è¦ä¸“é—¨çš„æ£€æµ‹å’Œæƒ©ç½šæœºåˆ¶ï¼š

**1. å¹»è§‰ç±»å‹åˆ†ç±»**

```
è§†è§‰å¹»è§‰åˆ†ç±»ä½“ç³»ï¼š
â”œâ”€â”€ å¯¹è±¡å¹»è§‰ï¼ˆObject Hallucinationï¼‰
â”‚   â”œâ”€â”€ å­˜åœ¨æ€§é”™è¯¯ï¼šæè¿°ä¸å­˜åœ¨çš„ç‰©ä½“
â”‚   â””â”€â”€ æ•°é‡é”™è¯¯ï¼šç‰©ä½“æ•°é‡æè¿°é”™è¯¯
â”œâ”€â”€ å±æ€§å¹»è§‰ï¼ˆAttribute Hallucinationï¼‰
â”‚   â”œâ”€â”€ é¢œè‰²é”™è¯¯
â”‚   â”œâ”€â”€ å¤§å°é”™è¯¯
â”‚   â””â”€â”€ æè´¨é”™è¯¯
â”œâ”€â”€ å…³ç³»å¹»è§‰ï¼ˆRelation Hallucinationï¼‰
â”‚   â”œâ”€â”€ ç©ºé—´å…³ç³»é”™è¯¯
â”‚   â””â”€â”€ åŠ¨ä½œå…³ç³»é”™è¯¯
â””â”€â”€ çŸ¥è¯†å¹»è§‰ï¼ˆKnowledge Hallucinationï¼‰
    â””â”€â”€ é”™è¯¯çš„èƒŒæ™¯çŸ¥è¯†æ¨æ–­
```

**2. åˆ†çº§æƒ©ç½šç­–ç•¥**

```python
class HallucinationPenaltySchedule:
    def __init__(self):
        # ä¸åŒç±»å‹å¹»è§‰çš„åŸºç¡€æƒ©ç½š
        self.base_penalties = {
            'object_existence': -2.0,    # æœ€ä¸¥é‡
            'object_count': -1.5,
            'attribute': -1.0,
            'relation': -0.8,
            'knowledge': -0.5            # ç›¸å¯¹è¾ƒè½»
        }
        
    def compute_penalty(self, hallucination_report, training_step):
        # éšè®­ç»ƒè¿›ç¨‹å¢å¼ºæƒ©ç½š
        severity_multiplier = min(2.0, 1.0 + training_step / 10000)
        
        total_penalty = 0
        for h_type, count in hallucination_report.items():
            base = self.base_penalties.get(h_type, -0.5)
            # å¤šä¸ªå¹»è§‰çš„è¶…çº¿æ€§æƒ©ç½š
            penalty = base * (count ** 1.2) * severity_multiplier
            total_penalty += penalty
            
        return total_penalty
```

**3. ä¸»åŠ¨å¹»è§‰é¢„é˜²**

```python
class HallucinationPrevention:
    def __init__(self, vision_grounder):
        self.grounder = vision_grounder
        
    def guided_generation(self, model, image, partial_text):
        """å¼•å¯¼ç”Ÿæˆä»¥å‡å°‘å¹»è§‰"""
        # 1. æå–å·²ç”Ÿæˆæ–‡æœ¬ä¸­çš„å®ä½“
        entities = self.extract_entities(partial_text)
        
        # 2. è§†è§‰æ¥åœ°éªŒè¯
        grounded = self.grounder.verify(image, entities)
        
        # 3. è°ƒæ•´ç”Ÿæˆæ¦‚ç‡
        if not grounded:
            # é™ä½ç»§ç»­æè¿°è¯¥å®ä½“çš„æ¦‚ç‡
            mask = self.create_entity_mask(entities[-1])
            # åº”ç”¨åˆ° logits
            logits = model.get_logits()
            logits[mask] -= 5.0  # å¼ºæƒ©ç½š
            
        return logits
```

### Constitutional AI åœ¨ VLM ä¸­çš„åº”ç”¨

Constitutional AI (CAI) é€šè¿‡è‡ªæˆ‘æ‰¹è¯„å’Œä¿®æ­£æ¥æå‡æ¨¡å‹å®‰å…¨æ€§å’Œæœ‰ç”¨æ€§ï¼š

**1. VLM çš„ Constitutional åŸåˆ™**

```python
VLM_CONSTITUTION = [
    # å‡†ç¡®æ€§åŸåˆ™
    "åªæè¿°å›¾åƒä¸­å®é™…å¯è§çš„å†…å®¹",
    "ä¸å¯¹å›¾åƒå†…å®¹è¿›è¡Œæœªç»è¯å®çš„æ¨æµ‹",
    "æ‰¿è®¤è§†è§‰ä¿¡æ¯çš„å±€é™æ€§",
    
    # å®‰å…¨æ€§åŸåˆ™
    "ä¸ç”Ÿæˆå¯èƒ½é€ æˆä¼¤å®³çš„å†…å®¹",
    "å°Šé‡å›¾åƒä¸­äººç‰©çš„éšç§",
    "é¿å…å¼ºåŒ–åè§å’Œåˆ»æ¿å°è±¡",
    
    # æœ‰ç”¨æ€§åŸåˆ™
    "æä¾›ä¿¡æ¯ä¸°å¯Œä¸”ç›¸å…³çš„å›ç­”",
    "æ ¹æ®ç”¨æˆ·éœ€æ±‚è°ƒæ•´è¯¦ç»†ç¨‹åº¦",
    "æ‰¿è®¤ä¸ç¡®å®šæ€§è€ŒéçŒœæµ‹"
]
```

**2. è‡ªæˆ‘æ‰¹è¯„ä¸ä¿®æ­£æµç¨‹**

```python
class ConstitutionalVLM:
    def __init__(self, base_model, constitution):
        self.model = base_model
        self.constitution = constitution
        
    def generate_with_critique(self, image, prompt):
        # æ­¥éª¤ 1ï¼šåˆå§‹ç”Ÿæˆ
        initial_response = self.model.generate(image, prompt)
        
        # æ­¥éª¤ 2ï¼šè‡ªæˆ‘æ‰¹è¯„
        critique_prompt = f"""
        è¯·è¯„ä¼°ä»¥ä¸‹å›ç­”æ˜¯å¦è¿åäº†è¿™äº›åŸåˆ™ï¼š
        {self.constitution}
        
        å›ç­”ï¼š{initial_response}
        """
        critique = self.model.generate(image, critique_prompt)
        
        # æ­¥éª¤ 3ï¼šä¿®æ­£
        if "è¿å" in critique:
            revision_prompt = f"""
            åŸºäºä»¥ä¸‹æ‰¹è¯„ï¼Œä¿®æ­£å›ç­”ï¼š
            æ‰¹è¯„ï¼š{critique}
            åŸå›ç­”ï¼š{initial_response}
            """
            revised_response = self.model.generate(image, revision_prompt)
            return revised_response
        
        return initial_response
```

**3. Constitutional RLHF**

å°† CAI åŸåˆ™é›†æˆåˆ° RLHF è®­ç»ƒä¸­ï¼š

```python
def constitutional_reward(response, image, constitution):
    """åŸºäº constitution çš„å¥–åŠ±å‡½æ•°"""
    rewards = []
    
    for principle in constitution:
        # è¯„ä¼°æ˜¯å¦éµå®ˆåŸåˆ™
        adherence = evaluate_adherence(response, image, principle)
        rewards.append(adherence)
    
    # åŠ æƒå¹³å‡ï¼Œå…³é”®åŸåˆ™æƒé‡æ›´é«˜
    weights = [2.0 if "å®‰å…¨" in p else 1.0 for p in constitution]
    final_reward = np.average(rewards, weights=weights)
    
    return final_reward
```

## æœ¬ç« å°ç»“

æœ¬ç« æ·±å…¥æ¢è®¨äº† VLM çš„ RLHF è®­ç»ƒï¼Œä»ç†è®ºåŸºç¡€åˆ°å®è·µç»†èŠ‚ã€‚å…³é”®è¦ç‚¹åŒ…æ‹¬ï¼š

**æ ¸å¿ƒæ¦‚å¿µ**ï¼š
- RLHF é€šè¿‡äººç±»åé¦ˆç›´æ¥ä¼˜åŒ–æ¨¡å‹è¾“å‡ºï¼Œè§£å†³ SFT æ— æ³•å¤„ç†çš„åå¥½å¯¹é½é—®é¢˜
- VLM çš„ RLHF æ¯”çº¯æ–‡æœ¬æ›´å¤æ‚ï¼Œéœ€è¦å¤„ç†è·¨æ¨¡æ€å¯¹é½å’Œè§†è§‰å¹»è§‰
- PPO ç®—æ³•éœ€è¦é’ˆå¯¹ VLM ç‰¹ç‚¹è¿›è¡Œæ”¹è¿›ï¼Œç‰¹åˆ«æ˜¯ä»·å€¼å‡½æ•°å’Œ KL çº¦æŸè®¾è®¡

**å…³é”®æŠ€æœ¯**ï¼š
- å¥–åŠ±æ¨¡å‹éœ€è¦åŒæ—¶è¯„ä¼°æ–‡æœ¬è´¨é‡å’Œè§†è§‰ä¸€è‡´æ€§
- å¤šé˜¶æ®µè®­ç»ƒç­–ç•¥ï¼šSFT â†’ å¥–åŠ±æ¨¡å‹ â†’ PPO ä¼˜åŒ–
- ç¨³å®šæ€§ä¿éšœæœºåˆ¶ï¼šKL æ•£åº¦æ§åˆ¶ã€å¥–åŠ±é»‘å®¢é˜²èŒƒã€å®æ—¶ç›‘æ§

**å®è·µç»éªŒ**ï¼š
- è®¡ç®—èµ„æºéœ€æ±‚æ¯”çº¯æ–‡æœ¬ RLHF é«˜ 1.5-2 å€
- å¹»è§‰ç‡å¯é™ä½ 40-60%ï¼Œä½†éœ€è¦ä¸“é—¨çš„æ£€æµ‹å’Œæƒ©ç½šæœºåˆ¶
- Constitutional AI å¯ä»¥è¿›ä¸€æ­¥æå‡å®‰å…¨æ€§å’Œæœ‰ç”¨æ€§

## ç»ƒä¹ é¢˜

### åŸºç¡€é¢˜ï¼ˆç†è§£æ¦‚å¿µï¼‰

**ç»ƒä¹  5.1**ï¼šè§£é‡Šä¸ºä»€ä¹ˆ VLM çš„ RLHF æ¯”çº¯æ–‡æœ¬ LLM æ›´å…·æŒ‘æˆ˜æ€§ï¼Ÿåˆ—ä¸¾è‡³å°‘ä¸‰ä¸ªç‹¬ç‰¹æŒ‘æˆ˜ã€‚

ğŸ’¡ **æç¤º**ï¼šè€ƒè™‘è¾“å…¥æ¨¡æ€ã€å¥–åŠ±å»ºæ¨¡ã€è®¡ç®—èµ„æºç­‰æ–¹é¢ã€‚

<details>
<summary>å‚è€ƒç­”æ¡ˆ</summary>

VLM RLHF çš„ç‹¬ç‰¹æŒ‘æˆ˜åŒ…æ‹¬ï¼š
1. **å¤šæ¨¡æ€å¥–åŠ±å»ºæ¨¡**ï¼šéœ€è¦åŒæ—¶è¯„ä¼°æ–‡æœ¬è´¨é‡å’Œè§†è§‰ä¸€è‡´æ€§ï¼Œå¥–åŠ±å‡½æ•°è®¾è®¡æ›´å¤æ‚
2. **è®¡ç®—å¼€é”€æ¿€å¢**ï¼šè§†è§‰ç¼–ç å™¨å¢åŠ æ˜¾å­˜å ç”¨ï¼Œå›¾åƒç¼“å­˜éœ€è¦é¢å¤– 10-20GB
3. **è§†è§‰å¹»è§‰é—®é¢˜**ï¼šæ¯”çº¯æ–‡æœ¬çš„äº‹å®æ€§é”™è¯¯æ›´éš¾æ£€æµ‹å’Œçº æ­£
4. **è®­ç»ƒä¸ç¨³å®šæ€§**ï¼šè§†è§‰ç‰¹å¾çš„é«˜ç»´åº¦å¯¼è‡´å¥–åŠ±ä¿¡å·æ–¹å·®æ›´å¤§
5. **æ•°æ®æ ‡æ³¨æˆæœ¬**ï¼šæ ‡æ³¨è€…éœ€è¦åŒæ—¶ç†è§£å›¾åƒå’Œæ–‡æœ¬ï¼Œè¦æ±‚æ›´é«˜

</details>

**ç»ƒä¹  5.2**ï¼šPPO ç®—æ³•ä¸­çš„è£å‰ªå‚æ•° $\epsilon$ èµ·ä»€ä¹ˆä½œç”¨ï¼Ÿåœ¨ VLM åœºæ™¯ä¸‹åº”è¯¥å¦‚ä½•è®¾ç½®ï¼Ÿ

ğŸ’¡ **æç¤º**ï¼šè€ƒè™‘ç­–ç•¥æ›´æ–°çš„ç¨³å®šæ€§å’Œæ¨¡å‹è§„æ¨¡çš„å…³ç³»ã€‚

<details>
<summary>å‚è€ƒç­”æ¡ˆ</summary>

è£å‰ªå‚æ•° $\epsilon$ é™åˆ¶é‡è¦æ€§é‡‡æ ·æ¯”ç‡ $r_t(\theta)$ çš„èŒƒå›´ä¸º $[1-\epsilon, 1+\epsilon]$ï¼Œé˜²æ­¢ç­–ç•¥æ›´æ–°è¿‡å¤§å¯¼è‡´è®­ç»ƒå´©æºƒã€‚

åœ¨ VLM åœºæ™¯ä¸‹çš„è®¾ç½®åŸåˆ™ï¼š
- å°æ¨¡å‹ï¼ˆ7Bï¼‰ï¼š$\epsilon = 0.2$ï¼Œå…è®¸è¾ƒå¤§æ›´æ–°
- ä¸­ç­‰æ¨¡å‹ï¼ˆ13Bï¼‰ï¼š$\epsilon = 0.2$ï¼Œæ ‡å‡†è®¾ç½®
- å¤§æ¨¡å‹ï¼ˆ34B+ï¼‰ï¼š$\epsilon = 0.1$ï¼Œæ›´ä¿å®ˆçš„æ›´æ–°

VLM ç”±äºå¤šæ¨¡æ€è¾“å…¥çš„å¤æ‚æ€§ï¼Œå»ºè®®ä½¿ç”¨æ¯”çº¯æ–‡æœ¬ç•¥å°çš„ $\epsilon$ å€¼ä»¥ä¿è¯ç¨³å®šæ€§ã€‚

</details>

**ç»ƒä¹  5.3**ï¼šæè¿° Bradley-Terry æ¨¡å‹åœ¨å¥–åŠ±æ¨¡å‹è®­ç»ƒä¸­çš„ä½œç”¨ï¼Œå¹¶å†™å‡ºæŸå¤±å‡½æ•°ã€‚

ğŸ’¡ **æç¤º**ï¼šè¿™æ˜¯ä¸€ä¸ªç»å…¸çš„æˆå¯¹æ¯”è¾ƒæ¨¡å‹ã€‚

<details>
<summary>å‚è€ƒç­”æ¡ˆ</summary>

Bradley-Terry æ¨¡å‹å°†äººç±»åå¥½å»ºæ¨¡ä¸ºæˆå¯¹æ¯”è¾ƒçš„æ¦‚ç‡ï¼š

$$P(y_w \succ y_l | x) = \frac{\exp(r(x, y_w))}{\exp(r(x, y_w)) + \exp(r(x, y_l))} = \sigma(r(x, y_w) - r(x, y_l))$$

æŸå¤±å‡½æ•°ï¼š
$$\mathcal{L}_{BT} = -\mathbb{E}_{(x,y_w,y_l)} [\log \sigma(r(x, y_w) - r(x, y_l))]$$

å…¶ä¸­ $y_w$ æ˜¯åå¥½çš„å›ç­”ï¼Œ$y_l$ æ˜¯ä¸åå¥½çš„å›ç­”ï¼Œ$\sigma$ æ˜¯ sigmoid å‡½æ•°ã€‚

è¯¥æ¨¡å‹å‡è®¾åå¥½æ¦‚ç‡ä¸å¥–åŠ±å·®å€¼çš„ sigmoid æˆæ­£æ¯”ï¼Œè‡ªç„¶åœ°å°†åå¥½å­¦ä¹ è½¬åŒ–ä¸ºäºŒåˆ†ç±»é—®é¢˜ã€‚

</details>

### æŒ‘æˆ˜é¢˜ï¼ˆæ·±å…¥æ€è€ƒï¼‰

**ç»ƒä¹  5.4**ï¼šè®¾è®¡ä¸€ä¸ªæ£€æµ‹å’Œé‡åŒ–è§†è§‰å¹»è§‰çš„è¯„ä¼°æ¡†æ¶ï¼ŒåŒ…æ‹¬æŒ‡æ ‡å®šä¹‰å’Œè®¡ç®—æ–¹æ³•ã€‚

ğŸ’¡ **æç¤º**ï¼šè€ƒè™‘ä¸åŒç±»å‹çš„å¹»è§‰ï¼ˆç‰©ä½“ã€å±æ€§ã€å…³ç³»ï¼‰å’Œè‡ªåŠ¨åŒ–è¯„ä¼°çš„å¯è¡Œæ€§ã€‚

<details>
<summary>å‚è€ƒç­”æ¡ˆ</summary>

è§†è§‰å¹»è§‰è¯„ä¼°æ¡†æ¶ï¼š

1. **å¹»è§‰ç‡æŒ‡æ ‡**ï¼š
   - ç‰©ä½“å¹»è§‰ç‡ = é”™è¯¯æåŠçš„ç‰©ä½“æ•° / æ€»æåŠç‰©ä½“æ•°
   - å±æ€§é”™è¯¯ç‡ = é”™è¯¯å±æ€§æè¿°æ•° / æ€»å±æ€§æè¿°æ•°
   - å…³ç³»é”™è¯¯ç‡ = é”™è¯¯å…³ç³»æè¿°æ•° / æ€»å…³ç³»æè¿°æ•°

2. **è‡ªåŠ¨æ£€æµ‹æµç¨‹**ï¼š
```python
def evaluate_hallucination(image, generated_text):
    # æ­¥éª¤1ï¼šä½¿ç”¨ç›®æ ‡æ£€æµ‹æ¨¡å‹è·å– ground truth
    gt_objects = detect_objects(image)
    gt_attributes = extract_attributes(image)
    
    # æ­¥éª¤2ï¼šä»ç”Ÿæˆæ–‡æœ¬æå–å£°æ˜
    claimed_objects = extract_entities(generated_text)
    claimed_attributes = extract_attributes_from_text(generated_text)
    
    # æ­¥éª¤3ï¼šè®¡ç®—å„ç±»å¹»è§‰
    object_hallucination = len(claimed_objects - gt_objects) / len(claimed_objects)
    attribute_errors = compute_attribute_mismatch(claimed_attributes, gt_attributes)
    
    # æ­¥éª¤4ï¼šåŠ æƒç»¼åˆå¾—åˆ†
    hallucination_score = 0.5 * object_hallucination + 0.3 * attribute_errors + 0.2 * relation_errors
    
    return {
        'overall_score': hallucination_score,
        'object_rate': object_hallucination,
        'attribute_rate': attribute_errors,
        'details': detailed_report
    }
```

3. **äººå·¥éªŒè¯é‡‡æ ·**ï¼š
   - éšæœºæŠ½å– 5% æ ·æœ¬äººå·¥éªŒè¯
   - è®¡ç®—è‡ªåŠ¨è¯„ä¼°ä¸äººå·¥è¯„ä¼°çš„ä¸€è‡´æ€§
   - è¿­ä»£æ”¹è¿›æ£€æµ‹æ¨¡å‹

</details>

**ç»ƒä¹  5.5**ï¼šå¦‚æœåœ¨ PPO è®­ç»ƒè¿‡ç¨‹ä¸­å‘ç° KL æ•£åº¦æŒç»­å¢å¤§è¶…è¿‡ç›®æ ‡å€¼ 10 å€ï¼Œåº”è¯¥å¦‚ä½•è¯Šæ–­å’Œè§£å†³ï¼Ÿ

ğŸ’¡ **æç¤º**ï¼šè¿™é€šå¸¸æ„å‘³ç€ç­–ç•¥åç¦»å‚è€ƒæ¨¡å‹å¤ªè¿œï¼Œéœ€è¦å¤šæ–¹é¢è°ƒæ•´ã€‚

<details>
<summary>å‚è€ƒç­”æ¡ˆ</summary>

è¯Šæ–­å’Œè§£å†³æ­¥éª¤ï¼š

1. **ç«‹å³åº”æ€¥æªæ–½**ï¼š
   - åœæ­¢è®­ç»ƒï¼Œé˜²æ­¢ç­–ç•¥å®Œå…¨å´©æºƒ
   - é™ä½å­¦ä¹ ç‡è‡³å½“å‰çš„ 1/10
   - å¢å¤§ KL æƒ©ç½šç³»æ•° $\beta$ è‡³ 2-5 å€

2. **æ ¹å› åˆ†æ**ï¼š
```python
# æ£€æŸ¥å„ä¸ªç»„ä»¶çš„ KL è´¡çŒ®
def diagnose_kl_explosion():
    # åˆ†ææ–‡æœ¬å’Œè§†è§‰éƒ¨åˆ†çš„ KL
    text_kl = compute_kl(text_logits_new, text_logits_ref)
    vision_kl = compute_kl(vision_features_new, vision_features_ref)
    
    # æ£€æŸ¥æ˜¯å¦æœ‰ç‰¹å®š token å¯¼è‡´ KL çˆ†ç‚¸
    per_token_kl = compute_per_token_kl()
    problematic_tokens = tokens[per_token_kl > 50]
    
    # æ£€æŸ¥å¥–åŠ±åˆ†å¸ƒ
    reward_stats = analyze_reward_distribution()
    if reward_stats['std'] > 5:
        print("å¥–åŠ±æ–¹å·®è¿‡å¤§å¯¼è‡´ç­–ç•¥ä¸ç¨³å®š")
    
    return diagnostic_report
```

3. **ä¿®å¤ç­–ç•¥**ï¼š
   - å›æ»šåˆ°æœ€è¿‘çš„ç¨³å®šæ£€æŸ¥ç‚¹
   - ä½¿ç”¨æ›´ä¿å®ˆçš„ PPO è£å‰ªå‚æ•°ï¼ˆå‡å° $\epsilon$ï¼‰
   - å¢åŠ å‚è€ƒæ¨¡å‹çš„æƒé‡ï¼ˆæ··åˆå½“å‰ç­–ç•¥å’Œå‚è€ƒç­–ç•¥ï¼‰
   - æ£€æŸ¥æ˜¯å¦æœ‰æ•°æ®åˆ†å¸ƒåç§»

4. **é¢„é˜²æªæ–½**ï¼š
   - è®¾ç½® KL æ•£åº¦çš„ç¡¬ä¸Šé™ï¼Œè¶…è¿‡æ—¶è‡ªåŠ¨åœæ­¢
   - ä½¿ç”¨è‡ªé€‚åº” KL æ§åˆ¶å™¨åŠ¨æ€è°ƒæ•´ $\beta$
   - å¢åŠ ç›‘æ§é¢‘ç‡ï¼ŒåŠæ—©å‘ç°å¼‚å¸¸

</details>

**ç»ƒä¹  5.6**ï¼šè®¾è®¡ä¸€ä¸ªå¤šç›®æ ‡ RLHF ç³»ç»Ÿï¼ŒåŒæ—¶ä¼˜åŒ–å‡†ç¡®æ€§ã€å®‰å…¨æ€§å’Œå¤šæ ·æ€§ï¼Œå¦‚ä½•å¤„ç†ç›®æ ‡é—´çš„å†²çªï¼Ÿ

ğŸ’¡ **æç¤º**ï¼šè€ƒè™‘ Pareto æœ€ä¼˜å’ŒåŠ¨æ€æƒé‡è°ƒæ•´ã€‚

<details>
<summary>å‚è€ƒç­”æ¡ˆ</summary>

å¤šç›®æ ‡ RLHF ç³»ç»Ÿè®¾è®¡ï¼š

1. **ç›®æ ‡å®šä¹‰ä¸åº¦é‡**ï¼š
   - å‡†ç¡®æ€§ï¼šè§†è§‰ä¸€è‡´æ€§å¾—åˆ† + äº‹å®æ­£ç¡®æ€§
   - å®‰å…¨æ€§ï¼šæœ‰å®³å†…å®¹æ£€æµ‹å¾—åˆ†çš„è´Ÿå€¼
   - å¤šæ ·æ€§ï¼šç”Ÿæˆå†…å®¹çš„ç†µ + è¯æ±‡ä¸°å¯Œåº¦

2. **å†²çªå¤„ç†æœºåˆ¶**ï¼š
```python
class MultiObjectiveRLHF:
    def __init__(self):
        self.objectives = ['accuracy', 'safety', 'diversity']
        self.weights = nn.Parameter(torch.ones(3) / 3)
        
    def compute_pareto_reward(self, rewards_dict):
        # æ£€æµ‹ Pareto æ”¯é…å…³ç³»
        is_pareto_optimal = self.check_pareto_dominance(rewards_dict)
        
        if is_pareto_optimal:
            # Pareto æœ€ä¼˜è§£ï¼Œä½¿ç”¨å½“å‰æƒé‡
            return self.weighted_sum(rewards_dict, self.weights)
        else:
            # é Pareto æœ€ä¼˜ï¼Œæƒ©ç½šä¸»å¯¼çš„ç›®æ ‡
            dominated_objective = self.find_dominated_objective(rewards_dict)
            penalty_weights = self.weights.clone()
            penalty_weights[dominated_objective] *= 2.0
            return self.weighted_sum(rewards_dict, penalty_weights)
    
    def adaptive_weight_update(self, performance_history):
        # åŸºäºå†å²æ€§èƒ½åŠ¨æ€è°ƒæ•´æƒé‡
        for obj in self.objectives:
            if performance_history[obj][-10:].mean() < threshold[obj]:
                # è¯¥ç›®æ ‡è¡¨ç°ä¸ä½³ï¼Œå¢åŠ æƒé‡
                self.weights[obj] *= 1.1
        
        # é‡æ–°å½’ä¸€åŒ–
        self.weights = F.softmax(self.weights, dim=0)
```

3. **çº¦æŸä¼˜åŒ–æ–¹æ³•**ï¼š
   - å°†å®‰å…¨æ€§ä½œä¸ºç¡¬çº¦æŸï¼š$R_{total} = R_{accuracy} + R_{diversity}$, s.t. $R_{safety} > \tau$
   - ä½¿ç”¨æ‹‰æ ¼æœ—æ—¥ä¹˜å­æ³•å¤„ç†çº¦æŸ

4. **å®è·µå»ºè®®**ï¼š
   - åˆæœŸä¾§é‡å®‰å…¨æ€§ï¼ˆæƒé‡ 0.5ï¼‰ï¼Œç¡®ä¿åŸºç¡€å®‰å…¨
   - ä¸­æœŸå¹³è¡¡ä¸‰ä¸ªç›®æ ‡ï¼ˆå„ 0.33ï¼‰
   - åæœŸæ ¹æ®åº”ç”¨åœºæ™¯å¾®è°ƒæƒé‡

</details>

**ç»ƒä¹  5.7**ï¼šæ¯”è¾ƒ RLHF å’Œ DPOï¼ˆDirect Preference Optimizationï¼‰åœ¨ VLM ä¸Šçš„ä¼˜ç¼ºç‚¹ï¼Œä»€ä¹ˆæƒ…å†µä¸‹é€‰æ‹©å“ªç§æ–¹æ³•ï¼Ÿ

ğŸ’¡ **æç¤º**ï¼šDPO ç›´æ¥ä¼˜åŒ–åå¥½ï¼Œæ— éœ€è®­ç»ƒå¥–åŠ±æ¨¡å‹å’Œ PPOã€‚

<details>
<summary>å‚è€ƒç­”æ¡ˆ</summary>

RLHF vs DPO æ¯”è¾ƒï¼š

**RLHF ä¼˜åŠ¿**ï¼š
- çµæ´»æ€§é«˜ï¼šå¯ä»¥ç»„åˆå¤šä¸ªå¥–åŠ±ä¿¡å·
- åœ¨çº¿å­¦ä¹ ï¼šå¯ä»¥æŒç»­ä»æ–°åé¦ˆä¸­å­¦ä¹ 
- æ¢ç´¢èƒ½åŠ›ï¼šPPO çš„éšæœºç­–ç•¥æœ‰åŠ©äºæ¢ç´¢

**RLHF åŠ£åŠ¿**ï¼š
- è®­ç»ƒå¤æ‚ï¼šéœ€è¦å¥–åŠ±æ¨¡å‹ + PPO ä¸¤é˜¶æ®µ
- èµ„æºæ¶ˆè€—å¤§ï¼šéœ€è¦ 4 ä¸ªæ¨¡å‹åŒæ—¶åœ¨æ˜¾å­˜ä¸­
- ä¸ç¨³å®šï¼šå®¹æ˜“å‡ºç°å¥–åŠ±é»‘å®¢ã€KL çˆ†ç‚¸

**DPO ä¼˜åŠ¿**ï¼š
- ç®€å•ç›´æ¥ï¼šä¸€æ­¥ä¼˜åŒ–ï¼Œæ— éœ€å¥–åŠ±æ¨¡å‹
- ç¨³å®šæ€§å¥½ï¼šä¸å­˜åœ¨å¥–åŠ±é»‘å®¢é—®é¢˜
- èµ„æºå‹å¥½ï¼šåªéœ€è¦ 2 ä¸ªæ¨¡å‹ï¼ˆç­–ç•¥+å‚è€ƒï¼‰

**DPO åŠ£åŠ¿**ï¼š
- ç¦»çº¿å­¦ä¹ ï¼šä¾èµ–é¢„å…ˆæ”¶é›†çš„åå¥½æ•°æ®
- è¡¨è¾¾èƒ½åŠ›å—é™ï¼šéš¾ä»¥ç»„åˆå¤æ‚çš„å¥–åŠ±ä¿¡å·
- æ¢ç´¢ä¸è¶³ï¼šå€¾å‘äºä¿å®ˆç­–ç•¥

**é€‰æ‹©å»ºè®®**ï¼š

é€‰æ‹© RLHF å½“ï¼š
- æœ‰å……è¶³çš„è®¡ç®—èµ„æºï¼ˆ8Ã—A100 ä»¥ä¸Šï¼‰
- éœ€è¦åœ¨çº¿å­¦ä¹ å’ŒæŒç»­æ”¹è¿›
- å¥–åŠ±å‡½æ•°å¤æ‚ï¼Œéœ€è¦ç»„åˆå¤šä¸ªä¿¡å·
- è¿½æ±‚æœ€ä½³æ•ˆæœ

é€‰æ‹© DPO å½“ï¼š
- èµ„æºå—é™ï¼ˆ4Ã—A100 ä»¥ä¸‹ï¼‰
- æœ‰é«˜è´¨é‡çš„ç¦»çº¿åå¥½æ•°æ®
- è¿½æ±‚è®­ç»ƒç¨³å®šæ€§å’Œå¯é‡å¤æ€§
- å¿«é€ŸåŸå‹å’Œè¿­ä»£

æ··åˆæ–¹æ¡ˆï¼š
- å…ˆç”¨ DPO å¿«é€Ÿè·å¾—åŸºçº¿æ¨¡å‹
- å†ç”¨ RLHF ç²¾ç»†è°ƒä¼˜å…³é”®æŒ‡æ ‡

</details>

## å¸¸è§é™·é˜±ä¸é”™è¯¯

### 1. å¥–åŠ±æ¨¡å‹è¿‡æ‹Ÿåˆ
**ç—‡çŠ¶**ï¼šè®­ç»ƒé›†å‡†ç¡®ç‡ > 90%ï¼ŒéªŒè¯é›† < 60%

**åŸå› **ï¼šåå¥½æ•°æ®é‡å¤ªå°‘æˆ–å¤šæ ·æ€§ä¸è¶³

**è§£å†³æ–¹æ¡ˆ**ï¼š
- å¢åŠ æ•°æ®å¢å¼ºï¼ˆå›¾åƒå˜æ¢ã€æ–‡æœ¬æ”¹å†™ï¼‰
- ä½¿ç”¨ dropoutï¼ˆ0.1-0.2ï¼‰å’Œ L2 æ­£åˆ™åŒ–
- æ—©åœç­–ç•¥ï¼Œä¸è¦è®­ç»ƒå¤ªå¤š epoch

### 2. KL æ•£åº¦çˆ†ç‚¸
**ç—‡çŠ¶**ï¼šKL > 50ï¼Œç”Ÿæˆæ–‡æœ¬è´¨é‡æ€¥å‰§ä¸‹é™

**åŸå› **ï¼šå­¦ä¹ ç‡å¤ªå¤§æˆ–å¥–åŠ±ä¿¡å·ä¸ç¨³å®š

**è§£å†³æ–¹æ¡ˆ**ï¼š
- ç«‹å³é™ä½å­¦ä¹ ç‡
- å¢å¤§ KL æƒ©ç½šç³»æ•°
- æ£€æŸ¥å¥–åŠ±æ¨¡å‹æ˜¯å¦æ­£å¸¸

### 3. å¥–åŠ±é»‘å®¢
**ç—‡çŠ¶**ï¼šå¥–åŠ±æŒç»­ä¸Šå‡ä½†ç”Ÿæˆè´¨é‡ä¸‹é™

**åŸå› **ï¼šæ¨¡å‹æ‰¾åˆ°æ¬ºéª—å¥–åŠ±æ¨¡å‹çš„æ·å¾„

**è§£å†³æ–¹æ¡ˆ**ï¼š
- æ·»åŠ å¤šæ ·æ€§å¥–åŠ±
- å®šæœŸæ›´æ–°å¥–åŠ±æ¨¡å‹
- äººå·¥å®¡æŸ¥é«˜å¥–åŠ±æ ·æœ¬

### 4. è§†è§‰-æ–‡æœ¬ä¸å¹³è¡¡
**ç—‡çŠ¶**ï¼šæ¨¡å‹å¿½è§†å›¾åƒæˆ–è¿‡åº¦ä¾èµ–å›¾åƒ

**åŸå› **ï¼šæ¨¡æ€æƒé‡è®¾ç½®ä¸å½“

**è§£å†³æ–¹æ¡ˆ**ï¼š
- åˆ†åˆ«ç›‘æ§å„æ¨¡æ€çš„æ¢¯åº¦èŒƒæ•°
- ä½¿ç”¨æ¢¯åº¦è£å‰ªå¹³è¡¡æ›´æ–°
- è°ƒæ•´æŸå¤±å‡½æ•°ä¸­çš„æ¨¡æ€æƒé‡

### 5. è®­ç»ƒæ•ˆç‡ä½ä¸‹
**ç—‡çŠ¶**ï¼šGPU åˆ©ç”¨ç‡ < 70%ï¼Œè®­ç»ƒé€Ÿåº¦æ…¢

**åŸå› **ï¼šæ•°æ®åŠ è½½ç“¶é¢ˆæˆ–æ‰¹æ¬¡ç»„ç»‡ä¸å½“

**è§£å†³æ–¹æ¡ˆ**ï¼š
- é¢„å…ˆç¼“å­˜è§†è§‰ç‰¹å¾
- ä¼˜åŒ–æ‰¹æ¬¡ç»„ç»‡ï¼ˆç›¸ä¼¼é•¿åº¦åˆ†ç»„ï¼‰
- ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯å‡å°‘é€šä¿¡å¼€é”€

## æœ€ä½³å®è·µæ£€æŸ¥æ¸…å•

### è®­ç»ƒå‰å‡†å¤‡
- [ ] SFT æ¨¡å‹å·²å……åˆ†æ”¶æ•›ï¼ˆéªŒè¯é›†æŸå¤±ç¨³å®šï¼‰
- [ ] åå¥½æ•°æ®è´¨é‡æ£€æŸ¥ï¼ˆæ ‡æ³¨ä¸€è‡´æ€§ > 0.6ï¼‰
- [ ] å¥–åŠ±æ¨¡å‹éªŒè¯é›†å‡†ç¡®ç‡ > 65%
- [ ] è®¡ç®—èµ„æºå……è¶³ï¼ˆè‡³å°‘ 4Ã—A100 æˆ–ç­‰æ•ˆï¼‰
- [ ] è®¾ç½®å®Œæ•´çš„ç›‘æ§æŒ‡æ ‡ä½“ç³»
- [ ] å‡†å¤‡å›æ»šæœºåˆ¶å’Œæ£€æŸ¥ç‚¹ç­–ç•¥

### è®­ç»ƒä¸­ç›‘æ§
- [ ] KL æ•£åº¦ä¿æŒåœ¨ç›®æ ‡èŒƒå›´ï¼ˆ3-10ï¼‰
- [ ] å¥–åŠ±åˆ†å¸ƒæ­£å¸¸ï¼ˆæ— å¼‚å¸¸å³°å€¼ï¼‰
- [ ] ç”Ÿæˆé•¿åº¦åˆç†ï¼ˆ20-200 tokensï¼‰
- [ ] æ¢¯åº¦èŒƒæ•°ç¨³å®šï¼ˆ< 10ï¼‰
- [ ] GPU åˆ©ç”¨ç‡ > 80%
- [ ] å®šæœŸäººå·¥è¯„ä¼°ç”Ÿæˆè´¨é‡

### è¶…å‚æ•°é…ç½®
- [ ] å­¦ä¹ ç‡ï¼šActor < Criticï¼ˆé€šå¸¸ 1:2 åˆ° 1:5ï¼‰
- [ ] PPO epochsï¼š4ï¼ˆå°æ¨¡å‹ï¼‰æˆ– 2ï¼ˆå¤§æ¨¡å‹ï¼‰
- [ ] è£å‰ªå‚æ•°ï¼š0.1-0.2
- [ ] KL ç³»æ•°ï¼šåˆå§‹ 0.1-0.2ï¼Œè‡ªé€‚åº”è°ƒæ•´
- [ ] æ‰¹æ¬¡å¤§å°ï¼šå°½å¯èƒ½å¤§ï¼ˆå—æ˜¾å­˜é™åˆ¶ï¼‰
- [ ] æ¢¯åº¦ç´¯ç§¯ï¼š4-8 æ­¥ï¼ˆå¹³è¡¡æ•ˆç‡å’Œç¨³å®šæ€§ï¼‰

### è´¨é‡ä¿è¯
- [ ] å®æ–½å¥–åŠ±é»‘å®¢æ£€æµ‹æœºåˆ¶
- [ ] æ·»åŠ å¹»è§‰æƒ©ç½š
- [ ] ä¿æŒè®­ç»ƒæ•°æ®å¤šæ ·æ€§
- [ ] å®šæœŸæ›´æ–°éªŒè¯é›†
- [ ] å¯¹æ¯”å¤šä¸ªæ£€æŸ¥ç‚¹é€‰æ‹©æœ€ä½³
- [ ] è¿›è¡Œæ¶ˆèå®éªŒéªŒè¯å„ç»„ä»¶è´¡çŒ®

### éƒ¨ç½²å‡†å¤‡
- [ ] æ¨¡å‹é‡åŒ–æµ‹è¯•ï¼ˆç²¾åº¦æŸå¤± < 2%ï¼‰
- [ ] æ¨ç†é€Ÿåº¦ä¼˜åŒ–ï¼ˆæ‰¹å¤„ç†ã€ç¼“å­˜ï¼‰
- [ ] å®‰å…¨è¿‡æ»¤å™¨é›†æˆ
- [ ] A/B æµ‹è¯•æ¡†æ¶å‡†å¤‡
- [ ] å›æ»šæ–¹æ¡ˆåˆ¶å®š
- [ ] ç›‘æ§å’Œå‘Šè­¦ç³»ç»Ÿé…ç½®