# ç¬¬ 7 ç« ï¼šè¯„ä¼°ä½“ç³»è®¾è®¡

è¯„ä¼°æ˜¯ VLM å¼€å‘å‘¨æœŸä¸­æœ€å…³é”®å´åˆæœ€å®¹æ˜“è¢«å¿½è§†çš„ç¯èŠ‚ã€‚ä¸€ä¸ªç²¾å¿ƒè®¾è®¡çš„è¯„ä¼°ä½“ç³»ä¸ä»…èƒ½å‡†ç¡®è¡¡é‡æ¨¡å‹æ€§èƒ½ï¼Œæ›´èƒ½æŒ‡å¯¼è®­ç»ƒä¼˜åŒ–æ–¹å‘ã€å‘ç°æ½œåœ¨é—®é¢˜ã€æ”¯æ’‘äº§å“å†³ç­–ã€‚æœ¬ç« å°†ä»åŸºå‡†æµ‹è¯•é€‰æ‹©ã€æŒ‡æ ‡è®¾è®¡ã€äººå·¥è¯„ä¼°ç»„ç»‡åˆ°åœ¨çº¿ A/B æµ‹è¯•ï¼Œæ„å»ºä¸€å¥—å®Œæ•´çš„ VLM è¯„ä¼°æ–¹æ³•è®ºã€‚æˆ‘ä»¬å°†ç‰¹åˆ«å…³æ³¨å¤šæ¨¡æ€ç‰¹æœ‰çš„è¯„ä¼°æŒ‘æˆ˜ï¼Œå¦‚å¹»è§‰æ£€æµ‹ã€è·¨æ¨¡æ€ä¸€è‡´æ€§éªŒè¯ç­‰å®é™…é—®é¢˜ã€‚

## 7.1 å¤šæ¨¡æ€åŸºå‡†æµ‹è¯•ä»‹ç»

### 7.1.1 ä¸»æµåŸºå‡†æµ‹è¯•æ¦‚è§ˆ

VLM çš„è¯„ä¼°åŸºå‡†å¯åˆ†ä¸ºä¸‰å¤§ç±»ï¼š

**é€šç”¨èƒ½åŠ›è¯„ä¼°åŸºå‡†**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     åŸºå‡†åç§°     â”‚   ä»»åŠ¡ç±»å‹    â”‚  æ•°æ®è§„æ¨¡   â”‚    ç‰¹ç‚¹      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ MMBench         â”‚ å¤šé€‰é¢˜       â”‚ 3000é¢˜      â”‚ å¾ªç¯è¯„ä¼°     â”‚
â”‚ SEED-Bench      â”‚ å¤šé€‰é¢˜       â”‚ 19Ké¢˜       â”‚ å¤šç»´åº¦è¦†ç›–   â”‚
â”‚ MME             â”‚ æ˜¯/å¦åˆ¤æ–­    â”‚ 14ä¸ªå­ä»»åŠ¡  â”‚ æ„ŸçŸ¥+è®¤çŸ¥    â”‚
â”‚ MMMU            â”‚ å¤šé€‰é¢˜       â”‚ 11.5Ké¢˜     â”‚ å­¦ç§‘çŸ¥è¯†     â”‚
â”‚ MathVista       â”‚ æ•°å­¦æ¨ç†     â”‚ 6Ké¢˜        â”‚ æ•°å­¦å›¾è¡¨     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**é¢†åŸŸç‰¹å®šè¯„ä¼°**

1. **OCR ç›¸å…³**
   - TextVQAï¼šåœºæ™¯æ–‡å­—ç†è§£
   - OCRBenchï¼šç»¼åˆ OCR èƒ½åŠ›
   - DocVQAï¼šæ–‡æ¡£ç†è§£

2. **è§†è§‰é—®ç­”ï¼ˆVQAï¼‰**
   - VQAv2ï¼šé€šç”¨è§†è§‰é—®ç­”
   - GQAï¼šç»„åˆæ¨ç†
   - OK-VQAï¼šéœ€è¦å¤–éƒ¨çŸ¥è¯†

3. **å›¾åƒæè¿°ï¼ˆCaptionï¼‰**
   - COCO Captionï¼šé€šç”¨åœºæ™¯æè¿°
   - NoCapsï¼šæ–°ç‰©ä½“æè¿°
   - TextCapsï¼šåŒ…å«æ–‡å­—çš„å›¾åƒæè¿°

### 7.1.2 åŸºå‡†æµ‹è¯•çš„é€‰æ‹©ç­–ç•¥

é€‰æ‹©åˆé€‚çš„è¯„ä¼°åŸºå‡†éœ€è¦è€ƒè™‘å¤šä¸ªç»´åº¦ï¼š

```
è¯„ä¼°ç»´åº¦çŸ©é˜µï¼š
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚         åº”ç”¨åœºæ™¯éœ€æ±‚            â”‚
          â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
          â”‚  å¯¹è¯ â”‚ OCR â”‚ æ¨ç† â”‚ åˆ›ä½œ    â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚åŸºç¡€èƒ½åŠ› â”‚  âœ“    â”‚     â”‚      â”‚          â”‚ â†’ MMBench, SEED
â”‚ä¸“ä¸šçŸ¥è¯† â”‚       â”‚     â”‚  âœ“   â”‚          â”‚ â†’ MMMU, MathVista  
â”‚æ–‡å­—è¯†åˆ« â”‚       â”‚  âœ“  â”‚      â”‚          â”‚ â†’ TextVQA, OCRBench
â”‚å†…å®¹ç”Ÿæˆ â”‚       â”‚     â”‚      â”‚    âœ“     â”‚ â†’ COCO Caption
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**é€‰æ‹©åŸåˆ™ï¼š**

1. **è¦†ç›–æ€§åŸåˆ™**ï¼šè‡³å°‘åŒ…å« 2-3 ä¸ªé€šç”¨åŸºå‡† + 2-3 ä¸ªé¢†åŸŸåŸºå‡†
2. **ä»£è¡¨æ€§åŸåˆ™**ï¼šé€‰æ‹©ç¤¾åŒºè®¤å¯åº¦é«˜ã€æ›´æ–°ç»´æŠ¤å¥½çš„åŸºå‡†
3. **å¯æ¯”æ€§åŸåˆ™**ï¼šé€‰æ‹©æœ‰å……åˆ† baseline ç»“æœçš„åŸºå‡†
4. **æ•ˆç‡åŸåˆ™**ï¼šå¹³è¡¡è¯„ä¼°å…¨é¢æ€§å’Œè®¡ç®—æˆæœ¬

### 7.1.3 è¯„ä¼°æ•°æ®æ³„éœ²é—®é¢˜

æ•°æ®æ³„éœ²æ˜¯å½“å‰ VLM è¯„ä¼°é¢ä¸´çš„ä¸¥é‡é—®é¢˜ï¼š

**æ³„éœ²æ£€æµ‹æ–¹æ³•ï¼š**

```python
# ç¤ºä¾‹ï¼šæ£€æµ‹è®­ç»ƒæ•°æ®ä¸æµ‹è¯•é›†çš„é‡å 
def detect_data_leakage(train_data, test_data):
    # 1. å›¾åƒçº§åˆ«æ£€æµ‹ï¼ˆæ„ŸçŸ¥å“ˆå¸Œï¼‰
    train_hashes = compute_perceptual_hashes(train_data.images)
    test_hashes = compute_perceptual_hashes(test_data.images)
    image_overlap = len(train_hashes & test_hashes) / len(test_hashes)
    
    # 2. æ–‡æœ¬çº§åˆ«æ£€æµ‹ï¼ˆn-gram é‡å ï¼‰
    train_ngrams = extract_ngrams(train_data.texts, n=5)
    test_ngrams = extract_ngrams(test_data.texts, n=5)
    text_overlap = jaccard_similarity(train_ngrams, test_ngrams)
    
    # 3. è¯­ä¹‰çº§åˆ«æ£€æµ‹ï¼ˆembedding ç›¸ä¼¼åº¦ï¼‰
    semantic_sim = compute_semantic_similarity(train_data, test_data)
    
    return {
        'image_overlap': image_overlap,
        'text_overlap': text_overlap,
        'semantic_similarity': semantic_sim
    }
```

**é˜²æ³„éœ²ç­–ç•¥ï¼š**

1. **æ—¶é—´åˆ‡åˆ†**ï¼šä½¿ç”¨æ¨¡å‹è®­ç»ƒåå‘å¸ƒçš„æµ‹è¯•é›†
2. **ç§æœ‰æµ‹è¯•é›†**ï¼šç»´æŠ¤ä¸å…¬å¼€çš„è¯„ä¼°æ•°æ®
3. **åŠ¨æ€ç”Ÿæˆ**ï¼šå®æ—¶ç”Ÿæˆè¯„ä¼°æ ·æœ¬
4. **å¯¹æŠ—æ ·æœ¬**ï¼šåŠ å…¥è½»å¾®æ‰°åŠ¨æ£€æµ‹è®°å¿†

## 7.2 è‡ªåŠ¨è¯„ä¼°æŒ‡æ ‡è®¾è®¡

### 7.2.1 ä¼ ç»ŸæŒ‡æ ‡çš„å±€é™æ€§

ä¼ ç»Ÿ NLP æŒ‡æ ‡åœ¨ VLM è¯„ä¼°ä¸­å­˜åœ¨æ˜æ˜¾ä¸è¶³ï¼š

```
é—®é¢˜ç¤ºä¾‹ï¼š
è¾“å…¥å›¾åƒï¼š[ä¸€åªæ£•è‰²çš„ç‹—åœ¨è‰åœ°ä¸Šå¥”è·‘]
æ¨¡å‹è¾“å‡ºï¼š"ä¸€åªé‡‘æ¯›çŠ¬åœ¨ç»¿è‰²è‰åªä¸Šè·‘æ­¥"
å‚è€ƒç­”æ¡ˆï¼š"ä¸€åªæ£•è‰²çš„ç‹—åœ¨è‰åœ°ä¸Šå¥”è·‘"

BLEU-4: 0.31 ï¼ˆä½åˆ†ï¼Œä½†è¯­ä¹‰æ­£ç¡®ï¼‰
äººç±»è¯„åˆ†ï¼š4.5/5 ï¼ˆé«˜åˆ†ï¼Œè®¤ä¸ºæè¿°å‡†ç¡®ï¼‰

â†’ æŒ‡æ ‡ä¸äººç±»åˆ¤æ–­ä¸¥é‡ä¸ä¸€è‡´
```

**å±€é™æ€§åˆ†æï¼š**

1. **è¯­ä¹‰ç­‰ä»·æ€§**ï¼šæ— æ³•è¯†åˆ«åŒä¹‰è¡¨è¾¾
2. **æ¨¡æ€å¯¹é½**ï¼šå¿½ç•¥è§†è§‰ä¿¡æ¯çš„å‡†ç¡®æ€§
3. **éƒ¨åˆ†æ­£ç¡®æ€§**ï¼šæ— æ³•è¯„ä¼°éƒ¨åˆ†æ­£ç¡®çš„ç­”æ¡ˆ
4. **åˆ›é€ æ€§æƒ©ç½š**ï¼šå¯¹åˆç†ä½†ä¸åŒçš„è¡¨è¾¾ç»™äºˆä½åˆ†

### 7.2.2 åŸºäºæ¨¡å‹çš„è¯„ä¼°

ä½¿ç”¨å¼ºå¤§çš„è¯­è¨€æ¨¡å‹ï¼ˆå¦‚ GPT-4Vï¼‰ä½œä¸ºè‡ªåŠ¨è¯„åˆ¤è€…ï¼š

```python
# GPT-4V è¯„ä¼°æ¡†æ¶ç¤ºä¾‹
class ModelBasedEvaluator:
    def __init__(self, judge_model="gpt-4-vision"):
        self.judge = load_model(judge_model)
        
    def evaluate(self, image, question, model_answer, reference=None):
        prompt = f"""
        è¯·è¯„ä¼°æ¨¡å‹å›ç­”çš„è´¨é‡ï¼ˆ1-5åˆ†ï¼‰ï¼š
        
        è¯„ä¼°ç»´åº¦ï¼š
        1. äº‹å®å‡†ç¡®æ€§ï¼šå›ç­”æ˜¯å¦ä¸å›¾åƒå†…å®¹ä¸€è‡´
        2. å®Œæ•´æ€§ï¼šæ˜¯å¦å……åˆ†å›ç­”äº†é—®é¢˜
        3. ç›¸å…³æ€§ï¼šå›ç­”æ˜¯å¦åˆ‡é¢˜
        4. æ¸…æ™°åº¦ï¼šè¡¨è¾¾æ˜¯å¦æ¸…æ¥šæ˜“æ‡‚
        
        å›¾åƒï¼š[å›¾åƒ]
        é—®é¢˜ï¼š{question}
        æ¨¡å‹å›ç­”ï¼š{model_answer}
        {"å‚è€ƒç­”æ¡ˆï¼š" + reference if reference else ""}
        
        è¯·ç»™å‡ºï¼š
        - æ€»åˆ†ï¼ˆ1-5ï¼‰
        - å„ç»´åº¦å¾—åˆ†
        - è¯„ä»·ç†ç”±
        """
        
        return self.judge.generate(prompt, image)
```

**ä¼˜åŠ¿ä¸æŒ‘æˆ˜ï¼š**

ä¼˜åŠ¿ï¼š
- æ›´æ¥è¿‘äººç±»åˆ¤æ–­
- å¯è§£é‡Šæ€§å¼º
- çµæ´»é€‚åº”ä¸åŒä»»åŠ¡

æŒ‘æˆ˜ï¼š
- è¯„ä¼°æˆæœ¬é«˜
- å¯èƒ½å­˜åœ¨åè§
- ä¸€è‡´æ€§é—®é¢˜

### 7.2.3 ä»»åŠ¡ç‰¹å®šæŒ‡æ ‡è®¾è®¡

é’ˆå¯¹ä¸åŒä»»åŠ¡è®¾è®¡ä¸“é—¨çš„è¯„ä¼°æŒ‡æ ‡ï¼š

**1. å¹»è§‰æ£€æµ‹æŒ‡æ ‡**

```python
# CHAIR (Caption Hallucination Assessment with Image Relevance)
def calculate_chair(generated_caption, image_objects):
    """
    è®¡ç®—æè¿°ä¸­çš„å¹»è§‰ç‡
    """
    mentioned_objects = extract_objects(generated_caption)
    
    # å¥å­çº§å¹»è§‰ç‡
    hallucinated_sentences = 0
    total_sentences = len(generated_caption.split('.'))
    
    for sentence in generated_caption.split('.'):
        sentence_objects = extract_objects(sentence)
        if any(obj not in image_objects for obj in sentence_objects):
            hallucinated_sentences += 1
    
    chairs = hallucinated_sentences / total_sentences
    
    # ç‰©ä½“çº§å¹»è§‰ç‡
    hallucinated_objects = len([obj for obj in mentioned_objects 
                               if obj not in image_objects])
    chairi = hallucinated_objects / len(mentioned_objects)
    
    return {'CHAIRs': chairs, 'CHAIRi': chairi}
```

**2. ç©ºé—´ç†è§£æŒ‡æ ‡**

```python
def evaluate_spatial_understanding(prediction, ground_truth):
    """
    è¯„ä¼°æ¨¡å‹çš„ç©ºé—´å…³ç³»ç†è§£èƒ½åŠ›
    """
    spatial_relations = ['å·¦', 'å³', 'ä¸Š', 'ä¸‹', 'å‰', 'å', 'å†…', 'å¤–']
    
    correct_relations = 0
    total_relations = 0
    
    for relation in spatial_relations:
        if relation in ground_truth:
            total_relations += 1
            if check_spatial_relation(prediction, ground_truth, relation):
                correct_relations += 1
    
    return correct_relations / total_relations if total_relations > 0 else 0
```

**3. æŒ‡ä»¤éµå¾ªåº¦æŒ‡æ ‡**

```python
def instruction_following_score(instruction, response):
    """
    è¯„ä¼°æ¨¡å‹å¯¹æŒ‡ä»¤çš„éµå¾ªç¨‹åº¦
    """
    requirements = parse_requirements(instruction)
    
    scores = {
        'format_compliance': check_format(response, requirements.format),
        'length_compliance': check_length(response, requirements.length),
        'content_coverage': check_content(response, requirements.topics),
        'constraint_satisfaction': check_constraints(response, requirements.constraints)
    }
    
    return sum(scores.values()) / len(scores)
```

### 7.2.4 å¤šç»´åº¦è¯„ä¼°æ¡†æ¶

æ„å»ºç»¼åˆè¯„ä¼°ä½“ç³»ï¼Œä»å¤šä¸ªè§’åº¦å…¨é¢è¯„ä¼°æ¨¡å‹ï¼š

```
è¯„ä¼°ç»´åº¦ä½“ç³»ï¼š
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  VLM è¯„ä¼°   â”‚
                    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                  â”‚                  â”‚
   â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”
   â”‚åŸºç¡€èƒ½åŠ› â”‚      â”‚ é«˜çº§èƒ½åŠ›  â”‚     â”‚ å®‰å…¨å¯¹é½  â”‚
   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
        â”‚                  â”‚                  â”‚
   - ç‰©ä½“è¯†åˆ«         - æ¨ç†èƒ½åŠ›         - æœ‰å®³å†…å®¹è¿‡æ»¤
   - å±æ€§ç†è§£         - åˆ›é€ æ€§           - åè§æ£€æµ‹
   - å…³ç³»ç†è§£         - çŸ¥è¯†è¿ç”¨         - éšç§ä¿æŠ¤
   - è®¡æ•°èƒ½åŠ›         - å¤šè½®å¯¹è¯         - äº‹å®æ€§
```

## 7.3 äººå·¥è¯„ä¼°çš„ç»„ç»‡ä¸åˆ†æ

### 7.3.1 è¯„ä¼°ä»»åŠ¡è®¾è®¡åŸåˆ™

è®¾è®¡é«˜è´¨é‡çš„äººå·¥è¯„ä¼°ä»»åŠ¡éœ€è¦éµå¾ªä»¥ä¸‹åŸåˆ™ï¼š

**1. æ˜ç¡®æ€§åŸåˆ™**

```
âŒ æ¨¡ç³ŠæŒ‡ä»¤ï¼š
"è¯„ä¼°è¿™ä¸ªå›ç­”çš„è´¨é‡"

âœ… æ˜ç¡®æŒ‡ä»¤ï¼š
"æ ¹æ®ä»¥ä¸‹æ ‡å‡†è¯„ä¼°å›ç­”è´¨é‡ï¼š
1. äº‹å®å‡†ç¡®æ€§ï¼ˆ0-2åˆ†ï¼‰ï¼šæè¿°æ˜¯å¦ä¸å›¾åƒå†…å®¹ä¸€è‡´
2. å®Œæ•´æ€§ï¼ˆ0-2åˆ†ï¼‰ï¼šæ˜¯å¦åŒ…å«æ‰€æœ‰é‡è¦ä¿¡æ¯
3. æµç•…æ€§ï¼ˆ0-1åˆ†ï¼‰ï¼šè¯­è¨€æ˜¯å¦é€šé¡ºè‡ªç„¶"
```

**2. å¯æµ‹é‡æ€§åŸåˆ™**

```python
# è®¾è®¡å¯é‡åŒ–çš„è¯„ä¼°æ ‡å‡†
evaluation_rubric = {
    "äº‹å®å‡†ç¡®æ€§": {
        0: "åŒ…å«æ˜æ˜¾é”™è¯¯ä¿¡æ¯",
        1: "åŸºæœ¬æ­£ç¡®ä½†æœ‰å°é”™è¯¯",
        2: "å®Œå…¨å‡†ç¡®"
    },
    "ç›¸å…³æ€§": {
        0: "ç­”éæ‰€é—®",
        1: "éƒ¨åˆ†ç›¸å…³",
        2: "é«˜åº¦ç›¸å…³"
    }
}
```

**3. ä»£è¡¨æ€§åŸåˆ™**

æ ·æœ¬é€‰æ‹©åº”è¦†ç›–ï¼š
- ä¸åŒéš¾åº¦çº§åˆ«
- å„ç§å›¾åƒç±»å‹ï¼ˆè‡ªç„¶åœºæ™¯ã€å›¾è¡¨ã€æ–‡æ¡£ç­‰ï¼‰
- å¤šæ ·åŒ–çš„é—®é¢˜ç±»å‹
- è¾¹ç•Œæ¡ˆä¾‹å’Œå›°éš¾æ ·æœ¬

### 7.3.2 æ ‡æ³¨æŒ‡å—åˆ¶å®š

å®Œå–„çš„æ ‡æ³¨æŒ‡å—æ˜¯ä¿è¯è¯„ä¼°è´¨é‡çš„å…³é”®ï¼š

```markdown
# VLM è¾“å‡ºè¯„ä¼°æ ‡æ³¨æŒ‡å—

## 1. è¯„ä¼°ç»´åº¦å®šä¹‰

### 1.1 äº‹å®å‡†ç¡®æ€§ï¼ˆFactual Accuracyï¼‰
**å®šä¹‰**ï¼šæ¨¡å‹è¾“å‡ºä¸å›¾åƒå†…å®¹çš„ä¸€è‡´ç¨‹åº¦

**è¯„åˆ†æ ‡å‡†**ï¼š
- **ä¼˜ç§€(3åˆ†)**ï¼šæ‰€æœ‰æè¿°å®Œå…¨å‡†ç¡®ï¼Œæ— ä»»ä½•äº‹å®é”™è¯¯
- **è‰¯å¥½(2åˆ†)**ï¼šä¸»è¦ä¿¡æ¯æ­£ç¡®ï¼Œå­˜åœ¨minorç»†èŠ‚é”™è¯¯
- **ä¸€èˆ¬(1åˆ†)**ï¼šéƒ¨åˆ†ä¿¡æ¯æ­£ç¡®ï¼Œä½†æœ‰æ˜æ˜¾é”™è¯¯
- **å·®(0åˆ†)**ï¼šå­˜åœ¨ä¸¥é‡äº‹å®é”™è¯¯æˆ–å¹»è§‰

**ç¤ºä¾‹**ï¼š
å›¾åƒï¼š[ä¸€åªæ©™è‰²çš„çŒ«ååœ¨è“è‰²æ²™å‘ä¸Š]
- ä¼˜ç§€ï¼š"ä¸€åªæ©™è‰²çš„çŒ«åœ¨è“è‰²æ²™å‘ä¸Š"
- è‰¯å¥½ï¼š"ä¸€åªçŒ«åœ¨æ²™å‘ä¸Š"ï¼ˆç¼ºå°‘é¢œè‰²ä¿¡æ¯ï¼‰
- ä¸€èˆ¬ï¼š"ä¸€åªç‹—åœ¨æ²™å‘ä¸Š"ï¼ˆç‰©ä½“è¯†åˆ«é”™è¯¯ï¼‰
- å·®ï¼š"å¤šåªçŒ«åœ¨åœ°æ¿ä¸Š"ï¼ˆå®Œå…¨é”™è¯¯ï¼‰

### 1.2 å®Œæ•´æ€§ï¼ˆCompletenessï¼‰
[è¯¦ç»†å®šä¹‰å’Œç¤ºä¾‹...]

## 2. æ ‡æ³¨æµç¨‹

1. **åˆæ­¥æµè§ˆ**ï¼šå¿«é€ŸæŸ¥çœ‹å›¾åƒï¼Œç†è§£åœºæ™¯
2. **ä»”ç»†å¯¹æ¯”**ï¼šé€å¥å¯¹æ¯”æ¨¡å‹è¾“å‡ºä¸å›¾åƒ
3. **è¯„åˆ†è®°å½•**ï¼šæŒ‰ç»´åº¦ç»™åˆ†å¹¶è®°å½•ç†ç”±
4. **ä¸€è‡´æ€§æ£€æŸ¥**ï¼šç¡®ä¿è¯„åˆ†æ ‡å‡†ä¸€è‡´

## 3. å¸¸è§é—®é¢˜å¤„ç†

Q: å¦‚æœæ¨¡å‹ä½¿ç”¨åŒä¹‰è¯æ€ä¹ˆåŠï¼Ÿ
A: åŒä¹‰è¯ä¸æ‰£åˆ†ï¼ˆå¦‚"æ±½è½¦"vs"è½¿è½¦"ï¼‰

Q: å¦‚ä½•å¤„ç†ä¸»è§‚æè¿°ï¼Ÿ
A: åªè¦åˆç†å³å¯ï¼ˆå¦‚"ç¾ä¸½çš„"é£æ™¯ï¼‰
```

### 7.3.3 ä¸€è‡´æ€§æ£€éªŒ

ç¡®ä¿å¤šä¸ªæ ‡æ³¨è€…ä¹‹é—´çš„ä¸€è‡´æ€§ï¼š

```python
# è®¡ç®—æ ‡æ³¨è€…é—´ä¸€è‡´æ€§
def calculate_inter_rater_agreement(annotations):
    """
    è®¡ç®— Fleiss' Kappa ç³»æ•°
    """
    n_items = len(annotations[0])  # è¯„ä¼°é¡¹ç›®æ•°
    n_raters = len(annotations)      # æ ‡æ³¨è€…æ•°é‡
    n_categories = 5                 # è¯„åˆ†ç­‰çº§æ•°ï¼ˆå¦‚1-5åˆ†ï¼‰
    
    # æ„å»ºè¯„åˆ†çŸ©é˜µ
    rating_matrix = np.zeros((n_items, n_categories))
    
    for item_idx in range(n_items):
        for rater_idx in range(n_raters):
            rating = annotations[rater_idx][item_idx]
            rating_matrix[item_idx, rating-1] += 1
    
    # è®¡ç®— Kappa
    kappa = fleiss_kappa(rating_matrix)
    
    # è§£é‡Š Kappa å€¼
    if kappa < 0.2:
        agreement = "å¾®å¼±"
    elif kappa < 0.4:
        agreement = "ä¸€èˆ¬"
    elif kappa < 0.6:
        agreement = "ä¸­ç­‰"
    elif kappa < 0.8:
        agreement = "è¾ƒå¼º"
    else:
        agreement = "æå¼º"
    
    return kappa, agreement
```

**æé«˜ä¸€è‡´æ€§çš„æ–¹æ³•ï¼š**

1. **åŸ¹è®­é˜¶æ®µ**ï¼šæ‰€æœ‰æ ‡æ³¨è€…æ ‡æ³¨ç›¸åŒæ ·æœ¬ï¼Œè®¨è®ºåˆ†æ­§
2. **é»„é‡‘æ ‡å‡†**ï¼šå®šæœŸæ’å…¥å·²çŸ¥ç­”æ¡ˆçš„æ ·æœ¬æ£€éªŒ
3. **è¿­ä»£ä¼˜åŒ–**ï¼šæ ¹æ®åˆ†æ­§æ¡ˆä¾‹æ›´æ–°æ ‡æ³¨æŒ‡å—
4. **åŒé‡æ ‡æ³¨**ï¼šå…³é”®æ ·æœ¬ç”±å¤šäººç‹¬ç«‹æ ‡æ³¨

### 7.3.4 è¯„ä¼°ç»“æœçš„ç»Ÿè®¡åˆ†æ

```python
# ç»¼åˆåˆ†ææ¡†æ¶
class EvaluationAnalyzer:
    def __init__(self, annotations):
        self.annotations = annotations
        
    def basic_statistics(self):
        """åŸºç¡€ç»Ÿè®¡é‡"""
        scores = np.array(self.annotations)
        return {
            'mean': np.mean(scores, axis=0),
            'std': np.std(scores, axis=0),
            'median': np.median(scores, axis=0),
            'quantiles': np.percentile(scores, [25, 50, 75], axis=0)
        }
    
    def dimension_correlation(self):
        """ç»´åº¦é—´ç›¸å…³æ€§åˆ†æ"""
        # åˆ†æä¸åŒè¯„ä¼°ç»´åº¦ä¹‹é—´çš„ç›¸å…³æ€§
        dimensions = ['accuracy', 'completeness', 'fluency']
        corr_matrix = np.corrcoef(self.annotations[dimensions].T)
        return corr_matrix
    
    def error_analysis(self):
        """é”™è¯¯æ¨¡å¼åˆ†æ"""
        error_patterns = {
            'hallucination': 0,
            'missing_info': 0,
            'wrong_attribute': 0,
            'spatial_error': 0
        }
        # åˆ†æå¸¸è§é”™è¯¯ç±»å‹
        return error_patterns
    
    def significance_test(self, model_a_scores, model_b_scores):
        """æ˜¾è‘—æ€§æ£€éªŒ"""
        from scipy import stats
        
        # é…å¯¹tæ£€éªŒ
        t_stat, p_value = stats.ttest_rel(model_a_scores, model_b_scores)
        
        # Bootstrap ç½®ä¿¡åŒºé—´
        diff = model_a_scores - model_b_scores
        bootstrap_means = []
        for _ in range(1000):
            sample = np.random.choice(diff, size=len(diff), replace=True)
            bootstrap_means.append(np.mean(sample))
        
        ci_lower = np.percentile(bootstrap_means, 2.5)
        ci_upper = np.percentile(bootstrap_means, 97.5)
        
        return {
            't_statistic': t_stat,
            'p_value': p_value,
            'confidence_interval': (ci_lower, ci_upper),
            'significant': p_value < 0.05
        }
```

## 7.4 A/B æµ‹è¯•ä¸åœ¨çº¿è¯„ä¼°

### 7.4.1 A/B æµ‹è¯•æ¡†æ¶æ­å»º

```python
# VLM A/B æµ‹è¯•æ¡†æ¶
class VLMABTestFramework:
    def __init__(self, config):
        self.config = config
        self.model_a = load_model(config.model_a_path)
        self.model_b = load_model(config.model_b_path)
        self.metrics_collector = MetricsCollector()
        
    def assign_user_to_group(self, user_id):
        """ç”¨æˆ·åˆ†ç»„ç­–ç•¥"""
        # ä½¿ç”¨å“ˆå¸Œç¡®ä¿åŒä¸€ç”¨æˆ·å§‹ç»ˆåˆ†åˆ°åŒä¸€ç»„
        hash_value = hashlib.md5(user_id.encode()).hexdigest()
        hash_int = int(hash_value[:8], 16)
        
        if hash_int % 100 < self.config.traffic_split:
            return 'model_b'
        return 'model_a'
    
    def serve_request(self, user_id, image, query):
        """å¤„ç†ç”¨æˆ·è¯·æ±‚"""
        group = self.assign_user_to_group(user_id)
        
        # è®°å½•è¯·æ±‚ä¿¡æ¯
        request_id = str(uuid.uuid4())
        self.log_request(request_id, user_id, group)
        
        # ç”Ÿæˆå“åº”
        if group == 'model_a':
            response = self.model_a.generate(image, query)
        else:
            response = self.model_b.generate(image, query)
        
        # æ”¶é›†æŒ‡æ ‡
        self.collect_metrics(request_id, group, response)
        
        return response
    
    def collect_metrics(self, request_id, group, response):
        """æ”¶é›†è¯„ä¼°æŒ‡æ ‡"""
        metrics = {
            'latency': response.latency,
            'token_count': len(response.tokens),
            'user_feedback': None,  # å¼‚æ­¥æ”¶é›†
            'downstream_success': None  # è¿½è¸ªä¸‹æ¸¸ä»»åŠ¡æˆåŠŸç‡
        }
        self.metrics_collector.record(request_id, group, metrics)
```

### 7.4.2 æµé‡åˆ†é…ç­–ç•¥

```
æµé‡åˆ†é…æ–¹æ¡ˆï¼š

1. æ¸è¿›å¼å‘å¸ƒï¼ˆProgressive Rolloutï¼‰
   ç¬¬1å¤©ï¼š5% æµé‡
   ç¬¬3å¤©ï¼š10% æµé‡ï¼ˆå¦‚æœæŒ‡æ ‡æ­£å¸¸ï¼‰
   ç¬¬7å¤©ï¼š25% æµé‡
   ç¬¬14å¤©ï¼š50% æµé‡
   ç¬¬21å¤©ï¼š100% æµé‡ï¼ˆå…¨é‡å‘å¸ƒï¼‰

2. åˆ†å±‚å®éªŒï¼ˆStratified Testingï¼‰
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚    ç”¨æˆ·ç¾¤ä½“   â”‚  æµé‡å æ¯”  â”‚   ä¼˜å…ˆçº§    â”‚
   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
   â”‚  å†…éƒ¨ç”¨æˆ·     â”‚    100%   â”‚     1      â”‚
   â”‚  Beta ç”¨æˆ·    â”‚     50%   â”‚     2      â”‚
   â”‚  VIP ç”¨æˆ·     â”‚     10%   â”‚     3      â”‚
   â”‚  æ™®é€šç”¨æˆ·     â”‚      5%   â”‚     4      â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

3. åœ°åŸŸåˆ†é…ï¼ˆGeographic Splitï¼‰
   - å…ˆåœ¨å»¶è¿Ÿå®¹å¿åº¦é«˜çš„åœ°åŒºæµ‹è¯•
   - é€æ­¥æ‰©å±•åˆ°æ ¸å¿ƒåœ°åŒº
```

### 7.4.3 æŒ‡æ ‡ç›‘æ§ä¸æ—©åœæœºåˆ¶

```python
class ABTestMonitor:
    def __init__(self, config):
        self.config = config
        self.alert_thresholds = config.alert_thresholds
        
    def check_guardrail_metrics(self, metrics):
        """æ£€æŸ¥æŠ¤æ æŒ‡æ ‡"""
        alerts = []
        
        # 1. æ€§èƒ½æŠ¤æ 
        if metrics.p95_latency > self.alert_thresholds.max_latency:
            alerts.append(('CRITICAL', f'P95å»¶è¿Ÿè¶…æ ‡: {metrics.p95_latency}ms'))
        
        # 2. è´¨é‡æŠ¤æ 
        if metrics.error_rate > self.alert_thresholds.max_error_rate:
            alerts.append(('CRITICAL', f'é”™è¯¯ç‡è¿‡é«˜: {metrics.error_rate:.2%}'))
        
        # 3. ç”¨æˆ·ä½“éªŒæŠ¤æ 
        if metrics.user_complaints > self.alert_thresholds.max_complaints:
            alerts.append(('WARNING', f'ç”¨æˆ·æŠ•è¯‰å¢åŠ : {metrics.user_complaints}'))
        
        return alerts
    
    def statistical_significance(self, control_metrics, treatment_metrics):
        """ç»Ÿè®¡æ˜¾è‘—æ€§æ£€éªŒ"""
        # è®¡ç®—æå‡å’Œç½®ä¿¡åŒºé—´
        lift = (treatment_metrics.mean - control_metrics.mean) / control_metrics.mean
        
        # è®¡ç®—ç»Ÿè®¡åŠŸæ•ˆ
        sample_size = len(treatment_metrics.data)
        power = self.calculate_statistical_power(
            sample_size, 
            lift, 
            control_metrics.std
        )
        
        # åˆ¤æ–­æ˜¯å¦è¾¾åˆ°æ˜¾è‘—æ€§
        p_value = self.calculate_p_value(control_metrics, treatment_metrics)
        
        return {
            'lift': lift,
            'p_value': p_value,
            'power': power,
            'significant': p_value < 0.05 and power > 0.8,
            'confidence_interval': self.calculate_ci(control_metrics, treatment_metrics)
        }
    
    def early_stopping_decision(self, current_results):
        """æ—©åœå†³ç­–"""
        # 1. å¦‚æœä¸¥é‡è´Ÿå‘ï¼Œç«‹å³åœæ­¢
        if current_results.lift < -0.1 and current_results.significant:
            return 'STOP', 'æ˜¾è‘—è´Ÿå‘å½±å“'
        
        # 2. å¦‚æœå·²ç»æ˜¾è‘—æ­£å‘ï¼Œå¯ä»¥æå‰ç»“æŸ
        if current_results.lift > 0.05 and current_results.significant:
            if current_results.sample_size > self.config.min_sample_size:
                return 'SUCCESS', 'æ˜¾è‘—æ­£å‘æå‡'
        
        # 3. å¦‚æœæ ·æœ¬é‡è¶³å¤Ÿä½†æ— æ˜¾è‘—å·®å¼‚
        if current_results.sample_size > self.config.max_sample_size:
            return 'STOP', 'æ— æ˜¾è‘—å·®å¼‚'
        
        return 'CONTINUE', None
```

### 7.4.4 é•¿æœŸæ•ˆæœè¿½è¸ª

```python
# é•¿æœŸæ•ˆæœè¿½è¸ªç³»ç»Ÿ
class LongTermTracking:
    def __init__(self):
        self.metrics_history = defaultdict(list)
        
    def track_metric_degradation(self, metric_name, current_value):
        """è¿½è¸ªæŒ‡æ ‡é€€åŒ–"""
        history = self.metrics_history[metric_name]
        history.append({
            'timestamp': datetime.now(),
            'value': current_value
        })
        
        # æ£€æµ‹è¶‹åŠ¿
        if len(history) > 7:  # è‡³å°‘ä¸€å‘¨æ•°æ®
            recent = [h['value'] for h in history[-7:]]
            baseline = [h['value'] for h in history[-14:-7]]
            
            # Mann-Kendall è¶‹åŠ¿æ£€éªŒ
            trend = self.mann_kendall_test(recent)
            
            if trend == 'decreasing':
                self.alert(f'{metric_name} å‡ºç°ä¸‹é™è¶‹åŠ¿')
    
    def track_user_behavior_shift(self, user_queries):
        """è¿½è¸ªç”¨æˆ·è¡Œä¸ºå˜åŒ–"""
        # åˆ†ææŸ¥è¯¢åˆ†å¸ƒå˜åŒ–
        query_distribution = self.analyze_query_distribution(user_queries)
        
        # æ£€æµ‹åˆ†å¸ƒæ¼‚ç§»
        if self.detect_distribution_shift(query_distribution):
            self.trigger_retraining_alert()
    
    def generate_weekly_report(self):
        """ç”Ÿæˆå‘¨æŠ¥"""
        report = {
            'performance_trends': self.analyze_performance_trends(),
            'user_satisfaction': self.analyze_user_feedback(),
            'error_patterns': self.analyze_error_patterns(),
            'recommendations': self.generate_recommendations()
        }
        return report
```

## 7.5 Case Study: MMBench è¯„æµ‹ä½“ç³»æ·±åº¦è§£è¯»

### 7.5.1 CircularEval ç­–ç•¥

MMBench çš„å¾ªç¯è¯„ä¼°ç­–ç•¥è§£å†³äº†é€‰é¡¹é¡ºåºåè§é—®é¢˜ï¼š

```python
# CircularEval å®ç°
def circular_eval(model, question, image, options):
    """
    é€šè¿‡æ‰“ä¹±é€‰é¡¹é¡ºåºå¤šæ¬¡è¯„ä¼°ï¼Œæ¶ˆé™¤ä½ç½®åè§
    """
    n_options = len(options)
    votes = defaultdict(int)
    
    # ç”Ÿæˆæ‰€æœ‰å¾ªç¯æ’åˆ—
    for shift in range(n_options):
        # å¾ªç¯ç§»åŠ¨é€‰é¡¹
        shifted_options = options[shift:] + options[:shift]
        option_map = {chr(65+i): shifted_options[i] for i in range(n_options)}
        
        # æ„é€ prompt
        prompt = format_question(question, shifted_options)
        
        # è·å–æ¨¡å‹é¢„æµ‹
        answer = model.predict(image, prompt)
        
        # æ˜ å°„å›åŸå§‹é€‰é¡¹
        if answer in option_map:
            original_option = options.index(option_map[answer])
            votes[original_option] += 1
    
    # æŠ•ç¥¨ç¡®å®šæœ€ç»ˆç­”æ¡ˆ
    final_answer = max(votes, key=votes.get)
    confidence = votes[final_answer] / n_options
    
    return final_answer, confidence
```

### 7.5.2 èƒ½åŠ›ç»´åº¦åˆ†è§£

MMBench å°† VLM èƒ½åŠ›åˆ†è§£ä¸ºç»†ç²’åº¦ç»´åº¦ï¼š

```
èƒ½åŠ›åˆ†ç±»ä½“ç³»ï¼š
â”œâ”€â”€ æ„ŸçŸ¥èƒ½åŠ›ï¼ˆPerceptionï¼‰
â”‚   â”œâ”€â”€ ç‰©ä½“å®šä½ï¼ˆObject Localizationï¼‰
â”‚   â”œâ”€â”€ å±æ€§è¯†åˆ«ï¼ˆAttribute Recognitionï¼‰
â”‚   â”œâ”€â”€ åœºæ™¯ç†è§£ï¼ˆScene Understandingï¼‰
â”‚   â””â”€â”€ ç©ºé—´å…³ç³»ï¼ˆSpatial Relationshipï¼‰
â”‚
â”œâ”€â”€ æ¨ç†èƒ½åŠ›ï¼ˆReasoningï¼‰
â”‚   â”œâ”€â”€ é€»è¾‘æ¨ç†ï¼ˆLogical Reasoningï¼‰
â”‚   â”œâ”€â”€ æ•°å€¼è®¡ç®—ï¼ˆNumerical Calculationï¼‰
â”‚   â”œâ”€â”€ å¸¸è¯†æ¨ç†ï¼ˆCommonsense Reasoningï¼‰
â”‚   â””â”€â”€ å› æœæ¨æ–­ï¼ˆCausal Inferenceï¼‰
â”‚
â””â”€â”€ çŸ¥è¯†èƒ½åŠ›ï¼ˆKnowledgeï¼‰
    â”œâ”€â”€ å­¦ç§‘çŸ¥è¯†ï¼ˆSubject Knowledgeï¼‰
    â”œâ”€â”€ ç¤¾ä¼šå¸¸è¯†ï¼ˆSocial Conventionï¼‰
    â”œâ”€â”€ å†å²æ–‡åŒ–ï¼ˆHistorical Cultureï¼‰
    â””â”€â”€ åäººåœ°æ ‡ï¼ˆCelebrity & Landmarkï¼‰
```

### 7.5.3 è¯„æµ‹ç»“æœåˆ†æ

```python
# MMBench ç»“æœåˆ†æå·¥å…·
class MMBenchAnalyzer:
    def __init__(self, results):
        self.results = results
        
    def capability_radar_chart(self):
        """ç”Ÿæˆèƒ½åŠ›é›·è¾¾å›¾"""
        capabilities = {
            'Object Localization': 0.85,
            'Attribute Recognition': 0.92,
            'Spatial Relationship': 0.76,
            'Logical Reasoning': 0.68,
            'Commonsense': 0.81,
            'Subject Knowledge': 0.73
        }
        
        # ç”Ÿæˆé›·è¾¾å›¾æ•°æ®
        angles = np.linspace(0, 2*np.pi, len(capabilities), endpoint=False)
        values = list(capabilities.values())
        
        return angles, values
    
    def error_case_analysis(self):
        """é”™è¯¯æ¡ˆä¾‹åˆ†æ"""
        error_patterns = {
            'position_bias': [],      # ä½ç½®åå¥½é”™è¯¯
            'language_bias': [],      # è¯­è¨€åè§é”™è¯¯
            'hallucination': [],      # å¹»è§‰é”™è¯¯
            'reasoning_fail': [],     # æ¨ç†å¤±è´¥
            'knowledge_gap': []       # çŸ¥è¯†ç¼ºå¤±
        }
        
        for item in self.results:
            if not item['correct']:
                error_type = self.classify_error(item)
                error_patterns[error_type].append(item)
        
        return error_patterns
    
    def compare_with_baselines(self):
        """ä¸åŸºå‡†æ¨¡å‹å¯¹æ¯”"""
        baselines = {
            'GPT-4V': 0.776,
            'Gemini-Pro': 0.739,
            'Claude-3': 0.768,
            'Qwen-VL-Plus': 0.726
        }
        
        our_score = np.mean([r['score'] for r in self.results])
        
        comparison = {
            name: {
                'score': score,
                'delta': our_score - score,
                'relative': (our_score - score) / score * 100
            }
            for name, score in baselines.items()
        }
        
        return comparison
```

## 7.6 é«˜çº§è¯é¢˜

### 7.6.1 å¹»è§‰è¯„ä¼°æ–¹æ³•

**POPE (Polling-based Object Probing Evaluation)**

```python
class POPEEvaluator:
    def __init__(self, object_detector):
        self.detector = object_detector
        
    def generate_pope_questions(self, image):
        """ç”Ÿæˆ POPE è¯„ä¼°é—®é¢˜"""
        # æ£€æµ‹å›¾åƒä¸­çš„çœŸå®ç‰©ä½“
        real_objects = self.detector.detect(image)
        
        # æ„é€ ä¸‰ç§ç±»å‹çš„é—®é¢˜
        questions = {
            'random': self.random_sampling(real_objects),
            'popular': self.popular_sampling(real_objects),
            'adversarial': self.adversarial_sampling(real_objects)
        }
        
        return questions
    
    def random_sampling(self, real_objects):
        """éšæœºé‡‡æ ·ç­–ç•¥"""
        questions = []
        # 50% çœŸå®ç‰©ä½“
        for obj in random.sample(real_objects, len(real_objects)//2):
            questions.append((f"Is there a {obj} in the image?", "Yes"))
        
        # 50% ä¸å­˜åœ¨çš„ç‰©ä½“
        fake_objects = self.get_random_objects(exclude=real_objects)
        for obj in fake_objects[:len(real_objects)//2]:
            questions.append((f"Is there a {obj} in the image?", "No"))
            
        return questions
    
    def popular_sampling(self, real_objects):
        """é¢‘ç¹å…±ç°ç‰©ä½“é‡‡æ ·"""
        # é€‰æ‹©ç»å¸¸ä¸€èµ·å‡ºç°ä½†å®é™…ä¸åœ¨å›¾ä¸­çš„ç‰©ä½“
        co_occurring = self.get_co_occurring_objects(real_objects)
        fake_objects = [obj for obj in co_occurring if obj not in real_objects]
        
        questions = []
        for obj in real_objects[:len(real_objects)//2]:
            questions.append((f"Is there a {obj} in the image?", "Yes"))
        for obj in fake_objects[:len(real_objects)//2]:
            questions.append((f"Is there a {obj} in the image?", "No"))
            
        return questions
    
    def evaluate_hallucination(self, model, questions):
        """è¯„ä¼°å¹»è§‰ç‡"""
        results = {
            'accuracy': 0,
            'yes_bias': 0,  # å€¾å‘äºå›ç­”"æ˜¯"
            'hallucination_rate': 0
        }
        
        correct = 0
        yes_count = 0
        false_positive = 0
        
        for question, ground_truth in questions:
            prediction = model.answer(question)
            
            if prediction == ground_truth:
                correct += 1
            if prediction == "Yes":
                yes_count += 1
                if ground_truth == "No":
                    false_positive += 1
        
        results['accuracy'] = correct / len(questions)
        results['yes_bias'] = yes_count / len(questions)
        results['hallucination_rate'] = false_positive / len([q for q in questions if q[1] == "No"])
        
        return results
```

### 7.6.2 Chain-of-Thought è¯„æµ‹è®¾è®¡

```python
class CoTEvaluator:
    def __init__(self):
        self.reasoning_patterns = {
            'visual_grounding': r'é¦–å…ˆ.*å›¾åƒ.*çœ‹åˆ°',
            'step_by_step': r'ç¬¬[ä¸€äºŒä¸‰å››äº”]æ­¥',
            'logical_connector': r'å› æ­¤|æ‰€ä»¥|ç”±äº|å› ä¸º',
            'evidence_based': r'æ ¹æ®.*å¯ä»¥.*åˆ¤æ–­'
        }
    
    def evaluate_reasoning_quality(self, cot_response):
        """è¯„ä¼°æ¨ç†é“¾è´¨é‡"""
        scores = {}
        
        # 1. æ¨ç†æ­¥éª¤å®Œæ•´æ€§
        steps = self.extract_reasoning_steps(cot_response)
        scores['completeness'] = min(len(steps) / 3, 1.0)  # æœŸæœ›è‡³å°‘3æ­¥
        
        # 2. é€»è¾‘è¿è´¯æ€§
        scores['coherence'] = self.check_logical_flow(steps)
        
        # 3. è§†è§‰grounding
        scores['grounding'] = self.check_visual_grounding(cot_response)
        
        # 4. æœ€ç»ˆç­”æ¡ˆä¸€è‡´æ€§
        scores['consistency'] = self.check_answer_consistency(cot_response)
        
        return scores
    
    def check_visual_grounding(self, response):
        """æ£€æŸ¥æ¨ç†æ˜¯å¦åŸºäºè§†è§‰ä¿¡æ¯"""
        visual_references = [
            'å›¾åƒ', 'å›¾ä¸­', 'çœ‹åˆ°', 'æ˜¾ç¤º', 'å‡ºç°',
            'å·¦è¾¹', 'å³è¾¹', 'ä¸Šæ–¹', 'ä¸‹æ–¹', 'ä¸­é—´',
            'é¢œè‰²', 'å½¢çŠ¶', 'å¤§å°'
        ]
        
        reference_count = sum(1 for ref in visual_references if ref in response)
        return min(reference_count / 5, 1.0)  # æœŸæœ›è‡³å°‘5æ¬¡è§†è§‰å¼•ç”¨
    
    def compare_cot_vs_direct(self, model, test_set):
        """å¯¹æ¯” CoT å’Œç›´æ¥å›ç­”çš„æ•ˆæœ"""
        results = {
            'direct': {'accuracy': 0, 'confidence': []},
            'cot': {'accuracy': 0, 'confidence': [], 'reasoning_quality': []}
        }
        
        for item in test_set:
            # ç›´æ¥å›ç­”
            direct_answer = model.answer_direct(item.image, item.question)
            results['direct']['accuracy'] += (direct_answer == item.ground_truth)
            
            # CoT å›ç­”
            cot_response = model.answer_with_cot(item.image, item.question)
            cot_answer = self.extract_final_answer(cot_response)
            results['cot']['accuracy'] += (cot_answer == item.ground_truth)
            
            # è¯„ä¼°æ¨ç†è´¨é‡
            reasoning_scores = self.evaluate_reasoning_quality(cot_response)
            results['cot']['reasoning_quality'].append(reasoning_scores)
        
        # è®¡ç®—å¹³å‡å€¼
        n = len(test_set)
        results['direct']['accuracy'] /= n
        results['cot']['accuracy'] /= n
        
        return results
```

### 7.6.3 å¯¹æŠ—æ€§è¯„ä¼°

```python
class AdversarialEvaluator:
    def __init__(self):
        self.attack_types = [
            'typographic',  # æ–‡å­—ç±»æ”»å‡»
            'compositional', # ç»„åˆæ€§æ”»å‡»  
            'logical',       # é€»è¾‘é™·é˜±
            'visual'         # è§†è§‰å¯¹æŠ—
        ]
    
    def generate_adversarial_examples(self, original_sample):
        """ç”Ÿæˆå¯¹æŠ—æ ·æœ¬"""
        adversarial_samples = []
        
        # 1. æ‰“å­—é”™è¯¯æ”»å‡»
        typo_sample = self.add_typos(original_sample)
        adversarial_samples.append(('typographic', typo_sample))
        
        # 2. å¦å®šè¯æ”»å‡»
        negation_sample = self.add_negation(original_sample)
        adversarial_samples.append(('negation', negation_sample))
        
        # 3. ç»„åˆå…³ç³»æ”»å‡»
        comp_sample = self.shuffle_relationships(original_sample)
        adversarial_samples.append(('compositional', comp_sample))
        
        return adversarial_samples
    
    def evaluate_robustness(self, model, test_set):
        """è¯„ä¼°æ¨¡å‹é²æ£’æ€§"""
        robustness_scores = defaultdict(list)
        
        for original in test_set:
            # åŸå§‹æ ·æœ¬å¾—åˆ†
            original_score = model.evaluate(original)
            
            # ç”Ÿæˆå¯¹æŠ—æ ·æœ¬
            adversarial_samples = self.generate_adversarial_examples(original)
            
            for attack_type, adv_sample in adversarial_samples:
                adv_score = model.evaluate(adv_sample)
                
                # è®¡ç®—æ€§èƒ½ä¸‹é™
                degradation = (original_score - adv_score) / original_score
                robustness_scores[attack_type].append(1 - degradation)
        
        # æ±‡æ€»ç»“æœ
        summary = {
            attack: {
                'mean_robustness': np.mean(scores),
                'std': np.std(scores),
                'min': np.min(scores)
            }
            for attack, scores in robustness_scores.items()
        }
        
        return summary
```

### 7.6.4 è·¨è¯­è¨€è¯„ä¼°æŒ‘æˆ˜

```python
class CrossLingualEvaluator:
    def __init__(self, languages=['en', 'zh', 'ja', 'fr', 'es']):
        self.languages = languages
        self.translators = {lang: load_translator(lang) for lang in languages}
        
    def evaluate_language_consistency(self, model, image, question_en):
        """è¯„ä¼°è·¨è¯­è¨€ä¸€è‡´æ€§"""
        results = {}
        
        # è‹±æ–‡åŸºå‡†ç­”æ¡ˆ
        answer_en = model.generate(image, question_en, lang='en')
        
        for lang in self.languages[1:]:  # è·³è¿‡è‹±æ–‡
            # ç¿»è¯‘é—®é¢˜
            question_translated = self.translators[lang].translate(question_en)
            
            # è·å–ç›®æ ‡è¯­è¨€ç­”æ¡ˆ
            answer_lang = model.generate(image, question_translated, lang=lang)
            
            # ç¿»è¯‘å›è‹±æ–‡è¿›è¡Œæ¯”è¾ƒ
            answer_back = self.translators[lang].translate_back(answer_lang)
            
            # è®¡ç®—è¯­ä¹‰ç›¸ä¼¼åº¦
            similarity = self.semantic_similarity(answer_en, answer_back)
            
            results[lang] = {
                'answer': answer_lang,
                'back_translation': answer_back,
                'similarity': similarity,
                'consistent': similarity > 0.85
            }
        
        return results
    
    def identify_language_specific_challenges(self):
        """è¯†åˆ«ç‰¹å®šè¯­è¨€çš„æŒ‘æˆ˜"""
        challenges = {
            'zh': [
                'é‡è¯ä½¿ç”¨ï¼ˆä¸€ä¸ªã€ä¸€åªã€ä¸€æ¡ï¼‰',
                'æ–¹ä½è¯å·®å¼‚ï¼ˆä¸Šä¸‹å·¦å³ vs ä¸œå—è¥¿åŒ—ï¼‰',
                'é¢œè‰²æè¿°ï¼ˆæ·±æµ… vs dark/lightï¼‰'
            ],
            'ja': [
                'æ•¬è¯­çº§åˆ«',
                'æ±‰å­—/å‡åé€‰æ‹©',
                'è®¡æ•°è¯ç³»ç»Ÿ'
            ],
            'ar': [
                'ä»å³åˆ°å·¦çš„ç©ºé—´æè¿°',
                'åŒæ•°å½¢å¼',
                'æ€§åˆ«ä¸€è‡´æ€§'
            ]
        }
        return challenges
```

## 7.7 æœ¬ç« å°ç»“

æœ¬ç« ç³»ç»Ÿä»‹ç»äº† VLM è¯„ä¼°ä½“ç³»çš„è®¾è®¡ä¸å®ç°ã€‚å…³é”®è¦ç‚¹åŒ…æ‹¬ï¼š

1. **åŸºå‡†æµ‹è¯•é€‰æ‹©**ï¼šéœ€è¦å¹³è¡¡é€šç”¨èƒ½åŠ›å’Œé¢†åŸŸç‰¹å®šè¯„ä¼°ï¼Œæ³¨æ„æ•°æ®æ³„éœ²é—®é¢˜

2. **è‡ªåŠ¨è¯„ä¼°æŒ‡æ ‡**ï¼š
   - ä¼ ç»Ÿ NLP æŒ‡æ ‡å­˜åœ¨å±€é™æ€§
   - åŸºäºæ¨¡å‹çš„è¯„ä¼°æ›´æ¥è¿‘äººç±»åˆ¤æ–­
   - ä»»åŠ¡ç‰¹å®šæŒ‡æ ‡ï¼ˆå¹»è§‰æ£€æµ‹ã€ç©ºé—´ç†è§£ç­‰ï¼‰è‡³å…³é‡è¦

3. **äººå·¥è¯„ä¼°**ï¼š
   - æ¸…æ™°çš„æ ‡æ³¨æŒ‡å—æ˜¯ä¿è¯è´¨é‡çš„å…³é”®
   - éœ€è¦è¿›è¡Œä¸€è‡´æ€§æ£€éªŒå’Œç»Ÿè®¡åˆ†æ
   - æˆæœ¬é«˜ä½†ä¸å¯æˆ–ç¼º

4. **åœ¨çº¿è¯„ä¼°**ï¼š
   - A/B æµ‹è¯•éœ€è¦å®Œå–„çš„æ¡†æ¶å’Œç›‘æ§
   - æ³¨æ„æŠ¤æ æŒ‡æ ‡å’Œæ—©åœæœºåˆ¶
   - é•¿æœŸæ•ˆæœè¿½è¸ªåŒæ ·é‡è¦

5. **é«˜çº§æŠ€æœ¯**ï¼š
   - å¹»è§‰è¯„ä¼°ï¼ˆPOPEã€CHAIRï¼‰
   - Chain-of-Thought è´¨é‡è¯„ä¼°
   - å¯¹æŠ—æ€§æµ‹è¯•å’Œè·¨è¯­è¨€ä¸€è‡´æ€§

**æ ¸å¿ƒå…¬å¼å›é¡¾ï¼š**

1. Fleiss' Kappaï¼ˆä¸€è‡´æ€§ï¼‰ï¼š$\kappa = \frac{P_o - P_e}{1 - P_e}$

2. ç»Ÿè®¡æ˜¾è‘—æ€§ï¼š$t = \frac{\bar{X}_1 - \bar{X}_2}{\sqrt{\frac{s_1^2}{n_1} + \frac{s_2^2}{n_2}}}$

3. å¹»è§‰ç‡ï¼š$\text{CHAIR}_i = \frac{|\text{hallucinated objects}|}{|\text{mentioned objects}|}$

## 7.8 ç»ƒä¹ é¢˜

### åŸºç¡€é¢˜

**ç»ƒä¹  7.1ï¼šåŸºå‡†æµ‹è¯•é€‰æ‹©**

ä½ æ­£åœ¨ä¸ºä¸€ä¸ªé¢å‘æ•™è‚²é¢†åŸŸçš„ VLM æ¨¡å‹è®¾è®¡è¯„ä¼°æ–¹æ¡ˆã€‚è¯¥æ¨¡å‹ä¸»è¦ç”¨äºï¼š
1. è§£ç­”æ•°å­¦å‡ ä½•é¢˜ï¼ˆéœ€è¦ç†è§£å›¾å½¢ï¼‰
2. æ‰¹æ”¹å­¦ç”Ÿä½œä¸šï¼ˆè¯†åˆ«æ‰‹å†™æ–‡å­—ï¼‰
3. ç”Ÿæˆæ•™å­¦ææ–™è¯´æ˜

è¯·é€‰æ‹©åˆé€‚çš„è¯„ä¼°åŸºå‡†å¹¶è¯´æ˜ç†ç”±ã€‚

ğŸ’¡ **æç¤º**ï¼šè€ƒè™‘é€šç”¨åŸºå‡†å’Œé¢†åŸŸç‰¹å®šåŸºå‡†çš„ç»„åˆã€‚

<details>
<summary>å‚è€ƒç­”æ¡ˆ</summary>

å»ºè®®é€‰æ‹©ä»¥ä¸‹åŸºå‡†æµ‹è¯•ç»„åˆï¼š

**é€šç”¨åŸºå‡†ï¼š**
- MathVistaï¼šä¸“é—¨è¯„ä¼°æ•°å­¦å›¾è¡¨ç†è§£èƒ½åŠ›
- MMMUï¼šåŒ…å«å­¦ç§‘çŸ¥è¯†ï¼Œé€‚åˆæ•™è‚²åœºæ™¯

**é¢†åŸŸç‰¹å®šåŸºå‡†ï¼š**
- OCRBenchï¼šè¯„ä¼°æ‰‹å†™æ–‡å­—è¯†åˆ«èƒ½åŠ›
- ChartQAï¼šè¯„ä¼°å›¾è¡¨ç†è§£èƒ½åŠ›
- è‡ªå»ºæ•™è‚²åœºæ™¯æµ‹è¯•é›†ï¼šåŒ…å«çœŸå®çš„ä½œä¸šæ‰¹æ”¹æ¡ˆä¾‹

**ç†ç”±ï¼š**
1. MathVista ç›´æ¥å¯¹åº”å‡ ä½•é¢˜è§£ç­”éœ€æ±‚
2. OCRBench è¦†ç›–æ‰‹å†™è¯†åˆ«åœºæ™¯
3. éœ€è¦è‡ªå»ºæµ‹è¯•é›†å› ä¸ºç°æœ‰åŸºå‡†å¯èƒ½ä¸å®Œå…¨è¦†ç›–æ•™è‚²ç‰¹å®šåœºæ™¯
4. é€šç”¨åŸºå‡†ç¡®ä¿æ¨¡å‹åŸºç¡€èƒ½åŠ›ï¼Œé¢†åŸŸåŸºå‡†éªŒè¯å®é™…åº”ç”¨æ•ˆæœ

</details>

**ç»ƒä¹  7.2ï¼šæŒ‡æ ‡è®¾è®¡**

è®¾è®¡ä¸€ä¸ªè¯„ä¼° VLM æ¨¡å‹"æŒ‡ä»¤éµå¾ªèƒ½åŠ›"çš„æŒ‡æ ‡ã€‚æ¨¡å‹éœ€è¦æ ¹æ®ç”¨æˆ·æŒ‡ä»¤å¯¹å›¾åƒè¿›è¡Œç‰¹å®šæ ¼å¼çš„æè¿°ï¼ˆå¦‚"ç”¨ä¸‰å¥è¯æè¿°"ã€"åˆ—å‡º5ä¸ªå…³é”®å…ƒç´ "ç­‰ï¼‰ã€‚

ğŸ’¡ **æç¤º**ï¼šè€ƒè™‘æ ¼å¼éµå¾ªã€å†…å®¹å®Œæ•´æ€§ç­‰å¤šä¸ªç»´åº¦ã€‚

<details>
<summary>å‚è€ƒç­”æ¡ˆ</summary>

æŒ‡ä»¤éµå¾ªèƒ½åŠ›è¯„ä¼°æŒ‡æ ‡è®¾è®¡ï¼š

```python
def instruction_following_score(instruction, response, image):
    scores = {}
    
    # 1. æ ¼å¼éµå¾ªåº¦ï¼ˆ40%æƒé‡ï¼‰
    if "ä¸‰å¥è¯" in instruction:
        sentence_count = len(response.split('ã€‚'))
        scores['format'] = 1.0 if sentence_count == 3 else max(0, 1 - abs(sentence_count - 3) * 0.3)
    elif "åˆ—å‡º" in instruction and "ä¸ª" in instruction:
        # æå–æ•°é‡è¦æ±‚
        required_items = extract_number(instruction)
        actual_items = count_list_items(response)
        scores['format'] = 1.0 if actual_items == required_items else max(0, 1 - abs(actual_items - required_items) * 0.2)
    
    # 2. å†…å®¹ç›¸å…³æ€§ï¼ˆ30%æƒé‡ï¼‰
    scores['relevance'] = calculate_relevance(response, image)
    
    # 3. æŒ‡ä»¤å…³é”®è¯è¦†ç›–ï¼ˆ20%æƒé‡ï¼‰
    keywords = extract_instruction_keywords(instruction)
    covered = sum(1 for kw in keywords if kw in response)
    scores['keyword_coverage'] = covered / len(keywords) if keywords else 1.0
    
    # 4. ç¦æ­¢å†…å®¹æ£€æŸ¥ï¼ˆ10%æƒé‡ï¼‰
    if "ä¸è¦æåŠ" in instruction:
        forbidden = extract_forbidden_content(instruction)
        scores['constraint'] = 0 if any(f in response for f in forbidden) else 1.0
    
    # åŠ æƒæ€»åˆ†
    weights = {'format': 0.4, 'relevance': 0.3, 'keyword_coverage': 0.2, 'constraint': 0.1}
    final_score = sum(scores.get(k, 1.0) * v for k, v in weights.items())
    
    return final_score, scores
```

</details>

**ç»ƒä¹  7.3ï¼šä¸€è‡´æ€§æ£€éªŒ**

ä¸‰ä½æ ‡æ³¨è€…å¯¹ 100 ä¸ª VLM è¾“å‡ºè¿›è¡Œäº† 1-5 åˆ†çš„è´¨é‡è¯„åˆ†ã€‚è¯„åˆ†æ•°æ®å¦‚ä¸‹æ ¼å¼ï¼š
```
Item1: [4, 3, 4]  # ä¸‰ä½æ ‡æ³¨è€…çš„è¯„åˆ†
Item2: [5, 5, 4]
...
```

è¯·è®¡ç®—åˆé€‚çš„ä¸€è‡´æ€§æŒ‡æ ‡å¹¶è§£é‡Šç»“æœã€‚

ğŸ’¡ **æç¤º**ï¼šè€ƒè™‘ä½¿ç”¨ Fleiss' Kappa æˆ– ICCã€‚

<details>
<summary>å‚è€ƒç­”æ¡ˆ</summary>

ä½¿ç”¨ Fleiss' Kappa å’Œ ICC è¿›è¡Œä¸€è‡´æ€§åˆ†æï¼š

```python
import numpy as np
from scipy import stats

def analyze_agreement(ratings):
    """
    ratings: shape (n_items, n_raters)
    """
    n_items, n_raters = ratings.shape
    n_categories = 5  # 1-5åˆ†
    
    # 1. è®¡ç®— Fleiss' Kappa
    # æ„å»ºé¢‘ç‡çŸ©é˜µ
    freq_matrix = np.zeros((n_items, n_categories))
    for i in range(n_items):
        for rating in ratings[i]:
            freq_matrix[i, rating-1] += 1
    
    # è®¡ç®— P_oï¼ˆè§‚å¯Ÿä¸€è‡´æ€§ï¼‰
    P_o = 0
    for i in range(n_items):
        for j in range(n_categories):
            P_o += freq_matrix[i,j] * (freq_matrix[i,j] - 1)
    P_o = P_o / (n_items * n_raters * (n_raters - 1))
    
    # è®¡ç®— P_eï¼ˆæœŸæœ›ä¸€è‡´æ€§ï¼‰
    P_e = 0
    for j in range(n_categories):
        p_j = np.sum(freq_matrix[:, j]) / (n_items * n_raters)
        P_e += p_j ** 2
    
    # Fleiss' Kappa
    kappa = (P_o - P_e) / (1 - P_e)
    
    # 2. è®¡ç®— ICC (Intraclass Correlation Coefficient)
    # ä½¿ç”¨åŒå‘éšæœºæ•ˆåº”æ¨¡å‹
    icc = calculate_icc(ratings, icc_type='ICC(2,k)')
    
    # 3. è®¡ç®—æ ‡æ³¨è€…é—´çš„é…å¯¹ç›¸å…³æ€§
    correlations = []
    for i in range(n_raters):
        for j in range(i+1, n_raters):
            corr = np.corrcoef(ratings[:, i], ratings[:, j])[0, 1]
            correlations.append(corr)
    
    # è§£é‡Šç»“æœ
    interpretation = {
        'fleiss_kappa': {
            'value': kappa,
            'interpretation': interpret_kappa(kappa)
        },
        'icc': {
            'value': icc,
            'interpretation': interpret_icc(icc)
        },
        'pairwise_correlations': {
            'mean': np.mean(correlations),
            'min': np.min(correlations),
            'max': np.max(correlations)
        }
    }
    
    return interpretation

def interpret_kappa(kappa):
    if kappa < 0.2:
        return "å¾®å¼±ä¸€è‡´æ€§ - éœ€è¦é‡æ–°åŸ¹è®­æ ‡æ³¨è€…"
    elif kappa < 0.4:
        return "ä¸€èˆ¬ä¸€è‡´æ€§ - å»ºè®®æ”¹è¿›æ ‡æ³¨æŒ‡å—"
    elif kappa < 0.6:
        return "ä¸­ç­‰ä¸€è‡´æ€§ - å¯æ¥å—ä½†æœ‰æ”¹è¿›ç©ºé—´"
    elif kappa < 0.8:
        return "è¾ƒå¼ºä¸€è‡´æ€§ - æ ‡æ³¨è´¨é‡è‰¯å¥½"
    else:
        return "æå¼ºä¸€è‡´æ€§ - æ ‡æ³¨è´¨é‡ä¼˜ç§€"
```

**ç»“æœè§£é‡Šï¼š**
- Kappa = 0.65ï¼šä¸­ç­‰åˆ°è¾ƒå¼ºçš„ä¸€è‡´æ€§ï¼Œæ ‡æ³¨è´¨é‡å¯æ¥å—
- ICC = 0.72ï¼šè‰¯å¥½çš„ä¿¡åº¦ï¼Œæ ‡æ³¨è€…è¯„åˆ†è¾ƒä¸ºä¸€è‡´
- å»ºè®®ï¼šæ£€æŸ¥ä½ä¸€è‡´æ€§çš„å…·ä½“æ¡ˆä¾‹ï¼Œå¯èƒ½éœ€è¦ç»†åŒ–æŸäº›è¯„åˆ†æ ‡å‡†

</details>

### æŒ‘æˆ˜é¢˜

**ç»ƒä¹  7.4ï¼šå¹»è§‰æ£€æµ‹ç®—æ³•è®¾è®¡**

è®¾è®¡ä¸€ä¸ªä¸ä¾èµ–äºç‰©ä½“æ£€æµ‹å™¨çš„å¹»è§‰æ£€æµ‹æ–¹æ³•ã€‚è¯¥æ–¹æ³•åº”è¯¥èƒ½å¤Ÿè¯†åˆ«æ¨¡å‹ç”Ÿæˆçš„æè¿°ä¸­ä¸å­˜åœ¨äºå›¾åƒä¸­çš„å†…å®¹ã€‚

ğŸ’¡ **æç¤º**ï¼šè€ƒè™‘ä½¿ç”¨æ³¨æ„åŠ›æœºåˆ¶æˆ–å¯¹æ¯”å­¦ä¹ ã€‚

<details>
<summary>å‚è€ƒç­”æ¡ˆ</summary>

åŸºäºæ³¨æ„åŠ›æœºåˆ¶å’Œå¯¹æ¯”éªŒè¯çš„å¹»è§‰æ£€æµ‹ï¼š

```python
class AttentionBasedHallucinationDetector:
    def __init__(self, vlm_model):
        self.model = vlm_model
        
    def detect_hallucination(self, image, generated_text):
        """
        é€šè¿‡åˆ†ææ³¨æ„åŠ›åˆ†å¸ƒæ£€æµ‹å¹»è§‰
        """
        # 1. è·å–ç”Ÿæˆè¿‡ç¨‹ä¸­çš„æ³¨æ„åŠ›æƒé‡
        tokens = tokenize(generated_text)
        attention_maps = []
        
        for token in tokens:
            # è·å–è¯¥ token å¯¹å›¾åƒåŒºåŸŸçš„æ³¨æ„åŠ›
            attn = self.model.get_cross_attention(image, token)
            attention_maps.append(attn)
        
        # 2. è¯†åˆ«å¯èƒ½çš„å¹»è§‰token
        hallucination_scores = []
        
        for i, token in enumerate(tokens):
            if is_content_word(token):  # åªæ£€æŸ¥å®è¯
                # è®¡ç®—æ³¨æ„åŠ›ç†µï¼ˆåˆ†æ•£åº¦ï¼‰
                entropy = calculate_entropy(attention_maps[i])
                
                # é«˜ç†µè¡¨ç¤ºæ³¨æ„åŠ›åˆ†æ•£ï¼Œå¯èƒ½æ˜¯å¹»è§‰
                if entropy > threshold:
                    # è¿›ä¸€æ­¥éªŒè¯ï¼šé®è”½æµ‹è¯•
                    masked_score = self.masking_test(image, token, generated_text)
                    hallucination_scores.append({
                        'token': token,
                        'entropy': entropy,
                        'masked_score': masked_score,
                        'is_hallucination': masked_score > 0.7
                    })
        
        return hallucination_scores
    
    def masking_test(self, image, target_token, full_text):
        """
        é®è”½å›¾åƒåŒºåŸŸï¼Œæµ‹è¯•tokençš„ç¨³å®šæ€§
        """
        # è·å–tokençš„ä¸»è¦æ³¨æ„åŠ›åŒºåŸŸ
        attn_map = self.model.get_cross_attention(image, target_token)
        top_regions = get_top_k_regions(attn_map, k=3)
        
        # é®è”½è¿™äº›åŒºåŸŸ
        masked_images = []
        for region in top_regions:
            masked_img = mask_region(image, region)
            masked_images.append(masked_img)
        
        # æµ‹è¯•ç”Ÿæˆçš„ä¸€è‡´æ€§
        consistency_scores = []
        for masked_img in masked_images:
            new_text = self.model.generate(masked_img, same_prompt)
            # å¦‚æœé®è”½åä»ç„¶ç”Ÿæˆç›¸åŒçš„tokenï¼Œå¯èƒ½æ˜¯å¹»è§‰
            if target_token in new_text:
                consistency_scores.append(1.0)
            else:
                consistency_scores.append(0.0)
        
        return np.mean(consistency_scores)
    
    def contrastive_verification(self, image, claim):
        """
        é€šè¿‡ç”Ÿæˆå¯¹æ¯”é—®é¢˜éªŒè¯å£°æ˜
        """
        # ç”ŸæˆéªŒè¯é—®é¢˜
        verification_questions = [
            f"å›¾åƒä¸­æ˜¯å¦æœ‰{extract_object(claim)}ï¼Ÿ",
            f"{extract_object(claim)}çš„é¢œè‰²æ˜¯ä»€ä¹ˆï¼Ÿ",
            f"{extract_object(claim)}åœ¨å›¾åƒçš„å“ªä¸ªä½ç½®ï¼Ÿ"
        ]
        
        confidence_scores = []
        for question in verification_questions:
            answer = self.model.answer(image, question)
            # åˆ†æå›ç­”çš„ç¡®å®šæ€§
            confidence = analyze_answer_confidence(answer)
            confidence_scores.append(confidence)
        
        # ä½ç½®ä¿¡åº¦å¯èƒ½è¡¨ç¤ºå¹»è§‰
        avg_confidence = np.mean(confidence_scores)
        return avg_confidence < 0.5
```

**åˆ›æ–°ç‚¹ï¼š**
1. ä¸ä¾èµ–å¤–éƒ¨ç‰©ä½“æ£€æµ‹å™¨
2. ç»“åˆæ³¨æ„åŠ›æœºåˆ¶åˆ†æå’Œé®è”½æµ‹è¯•
3. ä½¿ç”¨å¯¹æ¯”éªŒè¯å¢å¼ºå‡†ç¡®æ€§
4. å¯è§£é‡Šæ€§å¼ºï¼Œèƒ½å®šä½å…·ä½“çš„å¹»è§‰å†…å®¹

</details>

**ç»ƒä¹  7.5ï¼šåœ¨çº¿ A/B æµ‹è¯•è®¾è®¡**

ä½ éœ€è¦è®¾è®¡ä¸€ä¸ª A/B æµ‹è¯•æ¥è¯„ä¼°æ–°çš„ VLM æ¨¡å‹æ˜¯å¦åº”è¯¥æ›¿æ¢ç°æœ‰æ¨¡å‹ã€‚ç³»ç»Ÿæ¯å¤©å¤„ç† 100 ä¸‡ä¸ªè¯·æ±‚ï¼Œä¸»è¦æŒ‡æ ‡æ˜¯ç”¨æˆ·æ»¡æ„åº¦ï¼ˆé€šè¿‡ç‚¹å‡»ç‡è¡¡é‡ï¼‰ã€‚è®¾è®¡å®Œæ•´çš„æµ‹è¯•æ–¹æ¡ˆï¼ŒåŒ…æ‹¬æ ·æœ¬é‡è®¡ç®—ã€æµ‹è¯•æ—¶é•¿å’Œå†³ç­–æ ‡å‡†ã€‚

ğŸ’¡ **æç¤º**ï¼šè€ƒè™‘ç»Ÿè®¡åŠŸæ•ˆã€æœ€å°å¯æ£€æµ‹æ•ˆåº”å’Œä¸šåŠ¡å½±å“ã€‚

<details>
<summary>å‚è€ƒç­”æ¡ˆ</summary>

å®Œæ•´çš„ A/B æµ‹è¯•æ–¹æ¡ˆè®¾è®¡ï¼š

```python
class VLMABTestDesign:
    def __init__(self):
        self.daily_traffic = 1_000_000
        self.baseline_ctr = 0.15  # 15% åŸºå‡†ç‚¹å‡»ç‡
        self.min_detectable_effect = 0.01  # 1% ç»å¯¹æå‡
        self.alpha = 0.05  # æ˜¾è‘—æ€§æ°´å¹³
        self.power = 0.8   # ç»Ÿè®¡åŠŸæ•ˆ
        
    def calculate_sample_size(self):
        """
        è®¡ç®—æ‰€éœ€æ ·æœ¬é‡
        """
        from statsmodels.stats.power import zt_ind_solve_power
        
        # æ•ˆåº”é‡è®¡ç®—
        effect_size = self.min_detectable_effect / np.sqrt(
            self.baseline_ctr * (1 - self.baseline_ctr)
        )
        
        # æ¯ç»„æ‰€éœ€æ ·æœ¬é‡
        n_per_group = zt_ind_solve_power(
            effect_size=effect_size,
            alpha=self.alpha,
            power=self.power,
            ratio=1.0,
            alternative='two-sided'
        )
        
        total_sample = 2 * n_per_group
        days_needed = total_sample / (self.daily_traffic * 0.1)  # 10%æµé‡ç”¨äºæµ‹è¯•
        
        return {
            'sample_per_group': int(n_per_group),
            'total_sample': int(total_sample),
            'days_needed': np.ceil(days_needed),
            'daily_test_traffic': int(self.daily_traffic * 0.1)
        }
    
    def design_test_plan(self):
        """
        è®¾è®¡å®Œæ•´æµ‹è¯•è®¡åˆ’
        """
        sample_info = self.calculate_sample_size()
        
        test_plan = {
            'é˜¶æ®µ1ï¼šå°æµé‡éªŒè¯ï¼ˆç¬¬1-3å¤©ï¼‰': {
                'traffic_percentage': 1,
                'daily_users': 10000,
                'purpose': 'éªŒè¯ç³»ç»Ÿç¨³å®šæ€§ï¼Œå‘ç°ä¸¥é‡é—®é¢˜',
                'success_criteria': 'æ— ç³»ç»Ÿå´©æºƒï¼Œé”™è¯¯ç‡<1%',
                'monitors': ['é”™è¯¯ç‡', 'å»¶è¿ŸP99', 'èµ„æºä½¿ç”¨']
            },
            
            'é˜¶æ®µ2ï¼šæ­£å¼å®éªŒï¼ˆç¬¬4-14å¤©ï¼‰': {
                'traffic_percentage': 10,
                'daily_users': 100000,
                'purpose': 'æ”¶é›†ç»Ÿè®¡æ˜¾è‘—çš„ç»“æœ',
                'success_criteria': f'CTRæå‡>{self.min_detectable_effect}ï¼Œp<{self.alpha}',
                'monitors': ['CTR', 'ç”¨æˆ·æ»¡æ„åº¦', 'åœç•™æ—¶é•¿', 'è·³å‡ºç‡']
            },
            
            'é˜¶æ®µ3ï¼šæ‰©å¤§éªŒè¯ï¼ˆç¬¬15-21å¤©ï¼‰': {
                'traffic_percentage': 30,
                'daily_users': 300000,
                'purpose': 'éªŒè¯ä¸åŒç”¨æˆ·ç¾¤ä½“çš„æ•ˆæœ',
                'success_criteria': 'å„ç»†åˆ†ç¾¤ä½“å‡æ— è´Ÿå‘å½±å“',
                'monitors': ['åˆ†ç¾¤ä½“CTR', 'åœ°åŸŸå·®å¼‚', 'æ–°è€ç”¨æˆ·å·®å¼‚']
            }
        }
        
        return test_plan
    
    def define_decision_criteria(self):
        """
        å®šä¹‰å†³ç­–æ ‡å‡†
        """
        criteria = {
            'å‘å¸ƒå†³ç­–': {
                'å¼ºçƒˆæ¨èå‘å¸ƒ': [
                    'CTR æå‡ > 2%',
                    'p-value < 0.01',
                    'æ‰€æœ‰ç»†åˆ†ç¾¤ä½“å‡æ­£å‘',
                    'ç”¨æˆ·æŠ•è¯‰ä¸‹é™'
                ],
                'æ¨èå‘å¸ƒ': [
                    'CTR æå‡ > 1%',
                    'p-value < 0.05',
                    'ä¸»è¦ç¾¤ä½“æ­£å‘',
                    'æ— ä¸¥é‡è´Ÿé¢åé¦ˆ'
                ],
                'æš‚ç¼“å‘å¸ƒ': [
                    'CTR æå‡ < 1%',
                    'p-value > 0.05',
                    'æˆ–å­˜åœ¨ç»†åˆ†ç¾¤ä½“è´Ÿå‘'
                ],
                'åœæ­¢å‘å¸ƒ': [
                    'CTR ä¸‹é™',
                    'ä¸¥é‡æ€§èƒ½é—®é¢˜',
                    'ç”¨æˆ·æŠ•è¯‰æ¿€å¢'
                ]
            },
            
            'æŠ¤æ æŒ‡æ ‡': {
                'æ€§èƒ½æŠ¤æ ': {
                    'P95_latency': '<500ms',
                    'error_rate': '<0.1%',
                    'gpu_utilization': '<80%'
                },
                'ä¸šåŠ¡æŠ¤æ ': {
                    'revenue_impact': '>-1%',
                    'user_complaints': '<2x baseline',
                    'retention_rate': '>98%'
                }
            }
        }
        
        return criteria
    
    def monitoring_dashboard(self):
        """
        ç›‘æ§ä»ªè¡¨æ¿è®¾è®¡
        """
        dashboard = {
            'å®æ—¶ç›‘æ§': [
                'åˆ†ç»„æµé‡åˆ†é…æ¯”ä¾‹',
                'å®æ—¶ CTR å¯¹æ¯”',
                'å»¶è¿Ÿåˆ†å¸ƒ',
                'é”™è¯¯ç‡è¶‹åŠ¿'
            ],
            
            'æ¯æ—¥æŠ¥å‘Š': [
                'ç´¯è®¡æ ·æœ¬é‡å’Œç»Ÿè®¡åŠŸæ•ˆ',
                'CTR æå‡åŠç½®ä¿¡åŒºé—´',
                'ç»†åˆ†ç»´åº¦åˆ†æ',
                'å¼‚å¸¸æ¡ˆä¾‹æ±‡æ€»'
            ],
            
            'å†³ç­–æ”¯æŒ': [
                'é¢„è®¡å®Œæˆæ—¶é—´',
                'å½“å‰ç»Ÿè®¡æ˜¾è‘—æ€§',
                'æå‰åœæ­¢å»ºè®®',
                'å‘å¸ƒé£é™©è¯„ä¼°'
            ]
        }
        
        return dashboard
```

**å…³é”®å†³ç­–ç‚¹ï¼š**

1. **æ ·æœ¬é‡**ï¼šçº¦ 294,000 per groupï¼Œéœ€è¦ 6 å¤©è¾¾åˆ°ç»Ÿè®¡æ˜¾è‘—
2. **æµ‹è¯•æ—¶é•¿**ï¼šå»ºè®® 21 å¤©å®Œæ•´å‘¨æœŸï¼Œè¦†ç›–ä¸åŒä½¿ç”¨æ¨¡å¼
3. **æµé‡åˆ†é…**ï¼šæ¸è¿›å¼ï¼Œ1% â†’ 10% â†’ 30%
4. **å†³ç­–æ ‡å‡†**ï¼šç»¼åˆè€ƒè™‘ç»Ÿè®¡æ˜¾è‘—æ€§å’Œä¸šåŠ¡å½±å“
5. **é£é™©æ§åˆ¶**ï¼šè®¾ç½®å¤šå±‚æŠ¤æ ï¼Œæ”¯æŒå¿«é€Ÿå›æ»š

</details>

**ç»ƒä¹  7.6ï¼šè·¨æ¨¡æ€ä¸€è‡´æ€§è¯„ä¼°**

è®¾è®¡ä¸€ä¸ªè¯„ä¼°æ¡†æ¶ï¼Œç”¨äºæ£€æµ‹ VLM åœ¨å¤„ç†åŒä¸€å†…å®¹çš„ä¸åŒæ¨¡æ€è¡¨ç¤ºæ—¶çš„ä¸€è‡´æ€§ï¼ˆä¾‹å¦‚ï¼šå›¾è¡¨çš„å›¾åƒç‰ˆæœ¬ vs æ•°æ®è¡¨æ ¼ï¼‰ã€‚

ğŸ’¡ **æç¤º**ï¼šè€ƒè™‘å¦‚ä½•ç”Ÿæˆç­‰ä»·çš„å¤šæ¨¡æ€è¾“å…¥ã€‚

<details>
<summary>å‚è€ƒç­”æ¡ˆ</summary>

è·¨æ¨¡æ€ä¸€è‡´æ€§è¯„ä¼°æ¡†æ¶ï¼š

```python
class CrossModalConsistencyEvaluator:
    def __init__(self):
        self.modality_pairs = [
            ('image_chart', 'data_table'),
            ('photo', 'text_description'),
            ('diagram', 'structured_text'),
            ('screenshot', 'html_dom')
        ]
    
    def generate_equivalent_inputs(self, content, source_modality):
        """
        ç”Ÿæˆç­‰ä»·çš„å¤šæ¨¡æ€è¾“å…¥
        """
        if source_modality == 'data_table':
            return {
                'bar_chart': self.table_to_bar_chart(content),
                'line_chart': self.table_to_line_chart(content),
                'pie_chart': self.table_to_pie_chart(content),
                'text_summary': self.table_to_text(content)
            }
        elif source_modality == 'image':
            return {
                'text_description': self.image_to_text(content),
                'structured_data': self.image_to_structured(content),
                'sketch': self.image_to_sketch(content)
            }
        # ... å…¶ä»–æ¨¡æ€è½¬æ¢
    
    def evaluate_consistency(self, model, content, question):
        """
        è¯„ä¼°è·¨æ¨¡æ€ä¸€è‡´æ€§
        """
        # ç”Ÿæˆå¤šæ¨¡æ€ç‰ˆæœ¬
        modalities = self.generate_equivalent_inputs(content, 'original')
        
        responses = {}
        embeddings = {}
        
        # è·å–å„æ¨¡æ€çš„å“åº”
        for modality_name, modality_content in modalities.items():
            response = model.generate(modality_content, question)
            responses[modality_name] = response
            
            # è·å–è¯­ä¹‰åµŒå…¥
            embedding = model.get_embedding(response)
            embeddings[modality_name] = embedding
        
        # è®¡ç®—ä¸€è‡´æ€§æŒ‡æ ‡
        consistency_metrics = {
            'semantic_similarity': self.compute_semantic_consistency(embeddings),
            'answer_agreement': self.compute_answer_agreement(responses),
            'information_preservation': self.compute_info_preservation(responses),
            'confidence_stability': self.compute_confidence_stability(responses)
        }
        
        return consistency_metrics
    
    def compute_semantic_consistency(self, embeddings):
        """
        è®¡ç®—è¯­ä¹‰ä¸€è‡´æ€§
        """
        similarities = []
        modalities = list(embeddings.keys())
        
        for i in range(len(modalities)):
            for j in range(i+1, len(modalities)):
                sim = cosine_similarity(
                    embeddings[modalities[i]], 
                    embeddings[modalities[j]]
                )
                similarities.append(sim)
        
        return {
            'mean_similarity': np.mean(similarities),
            'min_similarity': np.min(similarities),
            'std_similarity': np.std(similarities)
        }
    
    def identify_inconsistency_patterns(self, results):
        """
        è¯†åˆ«ä¸ä¸€è‡´æ¨¡å¼
        """
        patterns = {
            'modality_bias': {},  # ç‰¹å®šæ¨¡æ€çš„åå¥½
            'information_loss': {},  # ä¿¡æ¯ä¸¢å¤±æ¨¡å¼
            'systematic_errors': []  # ç³»ç»Ÿæ€§é”™è¯¯
        }
        
        # åˆ†ææ¯ç§æ¨¡æ€è½¬æ¢çš„ä¸€è‡´æ€§
        for pair in self.modality_pairs:
            src, tgt = pair
            consistency = results[f'{src}_to_{tgt}']
            
            if consistency < 0.8:
                patterns['modality_bias'][pair] = consistency
                
                # æ·±å…¥åˆ†æä¸ä¸€è‡´åŸå› 
                if src == 'image_chart' and tgt == 'data_table':
                    # å›¾è¡¨è¯†åˆ«å¯èƒ½çš„é—®é¢˜
                    issues = self.analyze_chart_recognition_issues()
                    patterns['systematic_errors'].extend(issues)
        
        return patterns
    
    def generate_test_suite(self):
        """
        ç”Ÿæˆæµ‹è¯•å¥—ä»¶
        """
        test_cases = []
        
        # 1. æ•°å€¼ä¸€è‡´æ€§æµ‹è¯•
        test_cases.append({
            'name': 'æ•°å€¼æå–ä¸€è‡´æ€§',
            'inputs': {
                'chart_image': create_bar_chart([10, 20, 30]),
                'data_table': create_table([10, 20, 30])
            },
            'question': 'æœ€å¤§å€¼æ˜¯å¤šå°‘ï¼Ÿ',
            'expected_consistency': 1.0
        })
        
        # 2. è¶‹åŠ¿è¯†åˆ«ä¸€è‡´æ€§
        test_cases.append({
            'name': 'è¶‹åŠ¿åˆ†æä¸€è‡´æ€§',
            'inputs': {
                'line_chart': create_trend_chart(),
                'text_description': create_trend_description()
            },
            'question': 'æ•°æ®å‘ˆç°ä»€ä¹ˆè¶‹åŠ¿ï¼Ÿ',
            'expected_consistency': 0.9
        })
        
        # 3. å…³ç³»ç†è§£ä¸€è‡´æ€§
        test_cases.append({
            'name': 'ç©ºé—´å…³ç³»ä¸€è‡´æ€§',
            'inputs': {
                'scene_image': create_scene_image(),
                'scene_graph': create_scene_graph()
            },
            'question': 'ç‰©ä½“ä¹‹é—´çš„ä½ç½®å…³ç³»æ˜¯ä»€ä¹ˆï¼Ÿ',
            'expected_consistency': 0.85
        })
        
        return test_cases
```

**è¯„ä¼°ç»´åº¦ï¼š**
1. **è¯­ä¹‰ä¸€è‡´æ€§**ï¼šä¸åŒæ¨¡æ€è¡¨è¾¾çš„è¯­ä¹‰æ˜¯å¦ä¸€è‡´
2. **æ•°å€¼å‡†ç¡®æ€§**ï¼šä»å›¾è¡¨å’Œè¡¨æ ¼æå–çš„æ•°å€¼æ˜¯å¦ç›¸åŒ
3. **å…³ç³»ä¿æŒ**ï¼šå®ä½“å…³ç³»åœ¨ä¸åŒæ¨¡æ€ä¸­æ˜¯å¦ä¿æŒ
4. **ç½®ä¿¡åº¦ç¨³å®šæ€§**ï¼šæ¨¡å‹å¯¹ä¸åŒæ¨¡æ€è¾“å…¥çš„ç¡®å®šæ€§æ˜¯å¦ä¸€è‡´

</details>

**ç»ƒä¹  7.7ï¼šè¯„ä¼°æˆæœ¬ä¼˜åŒ–**

ä½ çš„å›¢é˜Ÿæ¯æœˆéœ€è¦è¯„ä¼° 10 ä¸ª VLM æ¨¡å‹ç‰ˆæœ¬ï¼Œæ¯ä¸ªç‰ˆæœ¬åœ¨ 5 ä¸ªåŸºå‡†æµ‹è¯•ä¸Šè¯„ä¼°ï¼ˆå…± 50K æ ·æœ¬ï¼‰ï¼ŒåŒæ—¶éœ€è¦ 1000 ä¸ªæ ·æœ¬çš„äººå·¥è¯„ä¼°ã€‚å½“å‰æ¯æœˆè¯„ä¼°æˆæœ¬ä¸º $50,000ã€‚è®¾è®¡ä¸€ä¸ªæ–¹æ¡ˆå°†æˆæœ¬é™ä½ 50% è€Œä¸æ˜¾è‘—å½±å“è¯„ä¼°è´¨é‡ã€‚

ğŸ’¡ **æç¤º**ï¼šè€ƒè™‘é‡‡æ ·ç­–ç•¥ã€è¯„ä¼°å¤ç”¨å’Œè‡ªåŠ¨åŒ–ã€‚

<details>
<summary>å‚è€ƒç­”æ¡ˆ</summary>

è¯„ä¼°æˆæœ¬ä¼˜åŒ–æ–¹æ¡ˆï¼š

```python
class EvaluationCostOptimizer:
    def __init__(self):
        self.current_cost = {
            'compute': 30000,  # GPU è®¡ç®—æˆæœ¬
            'human': 15000,    # äººå·¥æ ‡æ³¨æˆæœ¬
            'api': 5000        # API è°ƒç”¨æˆæœ¬ï¼ˆGPT-4Vè¯„ä¼°ï¼‰
        }
        self.target_cost = 25000  # ç›®æ ‡æˆæœ¬
    
    def optimization_strategy(self):
        """
        å¤šç»´åº¦ä¼˜åŒ–ç­–ç•¥
        """
        strategies = {
            '1. æ™ºèƒ½é‡‡æ ·ç­–ç•¥': self.smart_sampling(),
            '2. è¯„ä¼°å¤ç”¨æœºåˆ¶': self.evaluation_reuse(),
            '3. åˆ†å±‚è¯„ä¼°æµç¨‹': self.tiered_evaluation(),
            '4. è‡ªåŠ¨åŒ–é¢„ç­›é€‰': self.automated_prescreening(),
            '5. èµ„æºè°ƒåº¦ä¼˜åŒ–': self.resource_optimization()
        }
        return strategies
    
    def smart_sampling(self):
        """
        æ™ºèƒ½é‡‡æ ·å‡å°‘è¯„ä¼°é‡
        """
        return {
            'æ–¹æ³•': 'è‡ªé€‚åº”é‡è¦æ€§é‡‡æ ·',
            'å®ç°': '''
            class AdaptiveSampler:
                def __init__(self, full_dataset):
                    self.dataset = full_dataset
                    self.difficulty_scores = self.estimate_difficulty()
                    
                def sample(self, n_samples, model_capability):
                    # æ ¹æ®æ¨¡å‹èƒ½åŠ›è°ƒæ•´é‡‡æ ·åˆ†å¸ƒ
                    if model_capability > 0.8:
                        # å¼ºæ¨¡å‹ï¼šæ›´å¤šå›°éš¾æ ·æœ¬
                        weights = self.difficulty_scores ** 2
                    else:
                        # å¼±æ¨¡å‹ï¼šå‡è¡¡é‡‡æ ·
                        weights = np.ones_like(self.difficulty_scores)
                    
                    # åˆ†å±‚é‡‡æ ·ç¡®ä¿è¦†ç›–
                    strata = self.create_strata()
                    samples = []
                    for stratum in strata:
                        n_stratum = int(n_samples * stratum.weight)
                        stratum_samples = self.weighted_sample(
                            stratum.items, 
                            weights[stratum.indices], 
                            n_stratum
                        )
                        samples.extend(stratum_samples)
                    
                    return samples
                
                def estimate_difficulty(self):
                    # åŸºäºå†å²æ¨¡å‹è¡¨ç°ä¼°è®¡éš¾åº¦
                    return historical_error_rates
            ''',
            'é¢„æœŸèŠ‚çœ': '40% æ ·æœ¬é‡ï¼Œä¿æŒ 95% è¯„ä¼°å‡†ç¡®åº¦'
        }
    
    def evaluation_reuse(self):
        """
        è¯„ä¼°ç»“æœå¤ç”¨
        """
        return {
            'æ–¹æ³•': 'å¢é‡è¯„ä¼° + ç»“æœç¼“å­˜',
            'å®ç°': '''
            class IncrementalEvaluator:
                def __init__(self):
                    self.cache = EvaluationCache()
                    
                def evaluate_model(self, model_version, benchmarks):
                    results = {}
                    
                    # æ£€æµ‹æ¨¡å‹å˜åŒ–
                    changes = self.detect_changes(model_version)
                    
                    for benchmark in benchmarks:
                        if self.can_reuse(model_version, benchmark, changes):
                            # å¤ç”¨ä¹‹å‰ç‰ˆæœ¬çš„ç»“æœ
                            cached = self.cache.get(model_version.base, benchmark)
                            
                            # åªè¯„ä¼°å¯èƒ½å—å½±å“çš„å­é›†
                            affected_samples = self.get_affected_samples(changes, benchmark)
                            new_results = model_version.evaluate(affected_samples)
                            
                            # åˆå¹¶ç»“æœ
                            results[benchmark] = self.merge_results(cached, new_results)
                        else:
                            # å®Œæ•´è¯„ä¼°
                            results[benchmark] = model_version.evaluate(benchmark)
                    
                    self.cache.store(model_version, results)
                    return results
            ''',
            'é¢„æœŸèŠ‚çœ': '60% é‡å¤è¯„ä¼°æˆæœ¬'
        }
    
    def tiered_evaluation(self):
        """
        åˆ†å±‚è¯„ä¼°æµç¨‹
        """
        return {
            'æ–¹æ³•': 'å¿«é€Ÿç­›é€‰ â†’ è¯¦ç»†è¯„ä¼°',
            'æµç¨‹': '''
            Level 1: å¿«é€Ÿç­›é€‰ï¼ˆ500 æ ·æœ¬ï¼Œ5 åˆ†é’Ÿï¼‰
            â”œâ”€â”€ å¦‚æœæ€§èƒ½ä¸‹é™ > 5% â†’ åœæ­¢ï¼Œä¸éœ€è¦å®Œæ•´è¯„ä¼°
            â”œâ”€â”€ å¦‚æœæ€§èƒ½æå‡ < 1% â†’ åœæ­¢ï¼Œæ”¹è¿›ä¸æ˜æ˜¾
            â””â”€â”€ å¦åˆ™ â†’ è¿›å…¥ Level 2
            
            Level 2: æ ‡å‡†è¯„ä¼°ï¼ˆ5K æ ·æœ¬ï¼Œ1 å°æ—¶ï¼‰
            â”œâ”€â”€ å¦‚æœè¾¾åˆ°å‘å¸ƒæ ‡å‡† â†’ è¿›å…¥ Level 3
            â””â”€â”€ å¦åˆ™ â†’ è¿”å›å¼€å‘
            
            Level 3: å®Œæ•´è¯„ä¼°ï¼ˆ50K æ ·æœ¬ + äººå·¥ï¼‰
            â””â”€â”€ åªå¯¹å€™é€‰å‘å¸ƒç‰ˆæœ¬æ‰§è¡Œ
            ''',
            'é¢„æœŸèŠ‚çœ': '70% çš„æ¨¡å‹åœ¨ Level 1/2 è¢«è¿‡æ»¤'
        }
    
    def automated_prescreening(self):
        """
        è‡ªåŠ¨åŒ–é¢„ç­›é€‰
        """
        return {
            'æ–¹æ³•': 'ç”¨å°æ¨¡å‹é¢„ç­›é€‰ + GPT-4V æŠ½æ£€',
            'å®ç°': '''
            class HybridEvaluator:
                def __init__(self):
                    self.small_model = load_model('llava-1.5-7b')  # ä¾¿å®œ
                    self.large_model = load_model('gpt-4v')  # æ˜‚è´µä½†å‡†ç¡®
                    
                def evaluate(self, samples):
                    # Step 1: å°æ¨¡å‹å…¨é‡è¯„ä¼°
                    small_results = self.small_model.evaluate_all(samples)
                    
                    # Step 2: è¯†åˆ«åˆ†æ­§æ ·æœ¬
                    uncertain_samples = self.identify_uncertain(small_results)
                    
                    # Step 3: å¤§æ¨¡å‹æŠ½æ£€
                    # åªå¯¹ 20% ä¸ç¡®å®šæ ·æœ¬ä½¿ç”¨ GPT-4V
                    large_results = self.large_model.evaluate(uncertain_samples)
                    
                    # Step 4: æ ¡å‡†å°æ¨¡å‹ç»“æœ
                    calibrated = self.calibrate_results(
                        small_results, 
                        large_results, 
                        uncertain_samples
                    )
                    
                    return calibrated
            ''',
            'é¢„æœŸèŠ‚çœ': '80% API è°ƒç”¨æˆæœ¬'
        }
    
    def resource_optimization(self):
        """
        èµ„æºè°ƒåº¦ä¼˜åŒ–
        """
        return {
            'æ–¹æ³•': 'æ‰¹å¤„ç† + é¢„çº¦è°ƒåº¦ + ç‚¹ä»·ç­–ç•¥',
            'å…·ä½“æªæ–½': [
                'æ‰¹é‡å¤„ç†ï¼šç§¯ç´¯ä»»åŠ¡ç»Ÿä¸€æ‰§è¡Œï¼Œæé«˜ GPU åˆ©ç”¨ç‡',
                'é”™å³°è°ƒåº¦ï¼šä½¿ç”¨å¤œé—´/å‘¨æœ«çš„ä¾¿å®œç®—åŠ›',
                'ç«ä»·å®ä¾‹ï¼šéç´§æ€¥è¯„ä¼°ä½¿ç”¨ Spot å®ä¾‹',
                'æ¨¡å‹é‡åŒ–ï¼šè¯„ä¼°æ—¶ä½¿ç”¨é‡åŒ–æ¨¡å‹ï¼ˆéªŒè¯ç²¾åº¦æŸå¤± <1%ï¼‰'
            ],
            'é¢„æœŸèŠ‚çœ': '30% è®¡ç®—æˆæœ¬'
        }
    
    def cost_breakdown(self):
        """
        ä¼˜åŒ–åæˆæœ¬åˆ†è§£
        """
        optimized_cost = {
            'è®¡ç®—æˆæœ¬': {
                'åŸå§‹': 30000,
                'ä¼˜åŒ–å': 15000,
                'æªæ–½': 'æ™ºèƒ½é‡‡æ ·(40%) + é‡åŒ–(20%) + Spotå®ä¾‹(20%)'
            },
            'äººå·¥æˆæœ¬': {
                'åŸå§‹': 15000,
                'ä¼˜åŒ–å': 6000,
                'æªæ–½': 'åˆ†å±‚è¯„ä¼°(60%) + ä¸»åŠ¨å­¦ä¹ (40%)'
            },
            'APIæˆæœ¬': {
                'åŸå§‹': 5000,
                'ä¼˜åŒ–å': 1000,
                'æªæ–½': 'å°æ¨¡å‹é¢„ç­›(80%) + ç¼“å­˜å¤ç”¨(20%)'
            },
            'æ€»è®¡': {
                'åŸå§‹': 50000,
                'ä¼˜åŒ–å': 22000,
                'èŠ‚çœ': '56%'
            }
        }
        return optimized_cost
    
    def implementation_roadmap(self):
        """
        å®æ–½è·¯çº¿å›¾
        """
        roadmap = {
            'Week 1-2': 'å®ç°æ™ºèƒ½é‡‡æ ·å’Œç¼“å­˜æœºåˆ¶',
            'Week 3-4': 'éƒ¨ç½²åˆ†å±‚è¯„ä¼°æµç¨‹',
            'Week 5-6': 'é›†æˆè‡ªåŠ¨åŒ–é¢„ç­›é€‰',
            'Week 7-8': 'ä¼˜åŒ–èµ„æºè°ƒåº¦',
            'Week 9-10': 'A/B æµ‹è¯•éªŒè¯è¯„ä¼°è´¨é‡',
            'Week 11-12': 'å…¨é¢éƒ¨ç½²å’Œç›‘æ§'
        }
        return roadmap
```

**å…³é”®ä¼˜åŒ–ç‚¹ï¼š**
1. **æ™ºèƒ½é‡‡æ ·**ï¼šå‡å°‘ 40% æ ·æœ¬é‡
2. **è¯„ä¼°å¤ç”¨**ï¼šèŠ‚çœ 60% é‡å¤å·¥ä½œ
3. **åˆ†å±‚æµç¨‹**ï¼š70% æ¨¡å‹æå‰ç»ˆæ­¢
4. **æ··åˆè¯„ä¼°**ï¼š80% API æˆæœ¬é™ä½
5. **èµ„æºä¼˜åŒ–**ï¼š30% è®¡ç®—æˆæœ¬é™ä½

**æ€»ä½“æ•ˆæœ**ï¼šæˆæœ¬é™ä½ 56%ï¼Œè¯„ä¼°è´¨é‡ä¿æŒ 95% ä»¥ä¸Š

</details>

**ç»ƒä¹  7.8ï¼šè¯„ä¼°åè§æ£€æµ‹**

è®¾è®¡ä¸€ä¸ªæ–¹æ³•æ¥æ£€æµ‹ VLM è¯„ä¼°è¿‡ç¨‹ä¸­å¯èƒ½å­˜åœ¨çš„åè§ï¼ŒåŒ…æ‹¬ä½†ä¸é™äºï¼šæ–‡åŒ–åè§ã€è¯­è¨€åè§ã€è§†è§‰é£æ ¼åè§ç­‰ã€‚

ğŸ’¡ **æç¤º**ï¼šè€ƒè™‘å¦‚ä½•æ„é€ å¯¹ç…§å®éªŒã€‚

<details>
<summary>å‚è€ƒç­”æ¡ˆ</summary>

è¯„ä¼°åè§æ£€æµ‹æ¡†æ¶ï¼š

```python
class EvaluationBiasDetector:
    def __init__(self):
        self.bias_dimensions = [
            'cultural',
            'linguistic', 
            'visual_style',
            'demographic',
            'geographic'
        ]
    
    def detect_cultural_bias(self, model, evaluation_set):
        """
        æ£€æµ‹æ–‡åŒ–åè§
        """
        # æ„é€ æ–‡åŒ–å¯¹ç…§ç»„
        cultural_groups = {
            'western': self.filter_western_content(evaluation_set),
            'eastern': self.filter_eastern_content(evaluation_set),
            'african': self.filter_african_content(evaluation_set),
            'latin': self.filter_latin_content(evaluation_set)
        }
        
        # è¯„ä¼°å„æ–‡åŒ–ç»„çš„è¡¨ç°
        performance = {}
        for culture, subset in cultural_groups.items():
            scores = model.evaluate(subset)
            performance[culture] = {
                'mean_score': np.mean(scores),
                'std': np.std(scores),
                'n_samples': len(subset)
            }
        
        # ç»Ÿè®¡åˆ†æåè§
        bias_metrics = {
            'performance_gap': max(p['mean_score'] for p in performance.values()) - 
                              min(p['mean_score'] for p in performance.values()),
            'fairness_score': 1 - np.std([p['mean_score'] for p in performance.values()]),
            'statistical_test': self.anova_test(performance)
        }
        
        return bias_metrics
    
    def detect_linguistic_bias(self, model):
        """
        æ£€æµ‹è¯­è¨€åè§
        """
        # æ„é€ ç­‰ä»·çš„å¤šè¯­è¨€æµ‹è¯•é›†
        test_cases = []
        
        # ç›¸åŒå†…å®¹ï¼Œä¸åŒè¡¨è¾¾æ–¹å¼
        expressions = {
            'formal': "The figure illustrates a declining trend",
            'casual': "The graph shows it's going down",
            'technical': "The plot exhibits negative correlation",
            'simple': "Numbers get smaller"
        }
        
        for style, text in expressions.items():
            response = model.evaluate_text(text)
            test_cases.append({
                'style': style,
                'score': response.score,
                'confidence': response.confidence
            })
        
        # åˆ†æé£æ ¼åå¥½
        style_bias = {
            'preferred_style': max(test_cases, key=lambda x: x['score'])['style'],
            'style_variance': np.var([t['score'] for t in test_cases]),
            'consistency': self.calculate_consistency(test_cases)
        }
        
        return style_bias
    
    def detect_visual_style_bias(self, model):
        """
        æ£€æµ‹è§†è§‰é£æ ¼åè§
        """
        # ç›¸åŒå†…å®¹ï¼Œä¸åŒè§†è§‰é£æ ¼
        style_variants = {
            'photograph': self.load_photo_dataset(),
            'illustration': self.load_illustration_dataset(),
            'sketch': self.load_sketch_dataset(),
            'diagram': self.load_diagram_dataset(),
            'screenshot': self.load_screenshot_dataset()
        }
        
        performance_by_style = {}
        
        for style, dataset in style_variants.items():
            # ç¡®ä¿å†…å®¹ç­‰ä»·
            controlled_set = self.create_controlled_set(dataset)
            scores = model.evaluate(controlled_set)
            
            performance_by_style[style] = {
                'accuracy': np.mean(scores),
                'error_types': self.analyze_errors(model, controlled_set)
            }
        
        # è®¡ç®—é£æ ¼åè§æŒ‡æ ‡
        style_bias = {
            'max_gap': self.calculate_max_gap(performance_by_style),
            'style_preference': self.identify_preference(performance_by_style),
            'robustness_score': self.calculate_robustness(performance_by_style)
        }
        
        return style_bias
    
    def construct_counterfactual_tests(self):
        """
        æ„é€ åäº‹å®æµ‹è¯•
        """
        counterfactuals = []
        
        # 1. æ€§åˆ«äº¤æ¢æµ‹è¯•
        counterfactuals.append({
            'original': "ä¸€ä½ç”·åŒ»ç”Ÿåœ¨æ£€æŸ¥ç—…äºº",
            'counterfactual': "ä¸€ä½å¥³åŒ»ç”Ÿåœ¨æ£€æŸ¥ç—…äºº",
            'attribute': 'gender',
            'expected_difference': 0  # æœŸæœ›æ— å·®å¼‚
        })
        
        # 2. ç§æ—äº¤æ¢æµ‹è¯•
        counterfactuals.append({
            'original_image': "asian_person_coding.jpg",
            'counterfactual_image': "african_person_coding.jpg",
            'attribute': 'race',
            'expected_difference': 0
        })
        
        # 3. å¹´é¾„äº¤æ¢æµ‹è¯•
        counterfactuals.append({
            'original': "å¹´è½»äººä½¿ç”¨æ™ºèƒ½æ‰‹æœº",
            'counterfactual': "è€å¹´äººä½¿ç”¨æ™ºèƒ½æ‰‹æœº",
            'attribute': 'age',
            'expected_difference': 0
        })
        
        return counterfactuals
    
    def measure_intersectional_bias(self, model, evaluation_set):
        """
        æµ‹é‡äº¤å‰æ€§åè§
        """
        # å¤šç»´åº¦ç»„åˆ
        intersections = {
            'gender_x_race': [],
            'age_x_culture': [],
            'style_x_language': []
        }
        
        # åˆ†æå¤šç»´åº¦äº¤å‰çš„å½±å“
        for sample in evaluation_set:
            attributes = self.extract_attributes(sample)
            score = model.evaluate(sample)
            
            # è®°å½•äº¤å‰å±æ€§ç»„åˆçš„è¡¨ç°
            key = f"{attributes['gender']}_{attributes['race']}"
            intersections['gender_x_race'].append((key, score))
        
        # è®¡ç®—äº¤å‰åè§
        intersectional_bias = {}
        for dimension, data in intersections.items():
            grouped = defaultdict(list)
            for key, score in data:
                grouped[key].append(score)
            
            # è®¡ç®—å„ç»„åˆçš„å¹³å‡è¡¨ç°
            means = {k: np.mean(v) for k, v in grouped.items()}
            intersectional_bias[dimension] = {
                'group_means': means,
                'max_gap': max(means.values()) - min(means.values()),
                'most_disadvantaged': min(means, key=means.get)
            }
        
        return intersectional_bias
    
    def generate_bias_report(self, all_metrics):
        """
        ç”Ÿæˆåè§è¯„ä¼°æŠ¥å‘Š
        """
        report = {
            'summary': {
                'overall_fairness_score': self.calculate_overall_fairness(all_metrics),
                'main_bias_sources': self.identify_main_biases(all_metrics),
                'recommendations': self.generate_recommendations(all_metrics)
            },
            'detailed_findings': all_metrics,
            'mitigation_strategies': {
                'data_balancing': 'å¢åŠ ä»£è¡¨æ€§ä¸è¶³ç¾¤ä½“çš„è®­ç»ƒæ•°æ®',
                'debiasing_techniques': 'åº”ç”¨å¯¹æŠ—æ€§å»åæˆ–å…¬å¹³æ€§çº¦æŸ',
                'evaluation_improvement': 'ä½¿ç”¨æ›´å¹³è¡¡çš„è¯„ä¼°é›†',
                'human_review': 'å¯¹è¯†åˆ«å‡ºçš„åè§æ¡ˆä¾‹è¿›è¡Œäººå·¥å®¡æ ¸'
            }
        }
        
        return report
```

**åè§æ£€æµ‹ç»´åº¦ï¼š**

1. **æ–‡åŒ–åè§**ï¼šä¸åŒæ–‡åŒ–èƒŒæ™¯å†…å®¹çš„æ€§èƒ½å·®å¼‚
2. **è¯­è¨€åè§**ï¼šå¯¹ç‰¹å®šè¯­è¨€é£æ ¼æˆ–æ–¹è¨€çš„åå¥½
3. **è§†è§‰é£æ ¼åè§**ï¼šå¯¹ç‰¹å®šå›¾åƒç±»å‹çš„åå¥½
4. **äººå£ç»Ÿè®¡åè§**ï¼šåŸºäºæ€§åˆ«ã€å¹´é¾„ã€ç§æ—çš„å·®å¼‚
5. **äº¤å‰æ€§åè§**ï¼šå¤šä¸ªå±æ€§ç»„åˆäº§ç”Ÿçš„å¤åˆåè§

**å…³é”®æ–¹æ³•ï¼š**
- åäº‹å®æµ‹è¯•ï¼šæ”¹å˜å•ä¸€å±æ€§è§‚å¯Ÿå½±å“
- åˆ†å±‚åˆ†æï¼šæŒ‰ä¸åŒç»´åº¦åˆ†ç»„æ¯”è¾ƒ
- ç»Ÿè®¡æ£€éªŒï¼šç¡®å®šå·®å¼‚çš„æ˜¾è‘—æ€§
- äº¤å‰åˆ†æï¼šè¯†åˆ«å¤åˆåè§æ¨¡å¼

</details>

## 7.9 å¸¸è§é™·é˜±ä¸é”™è¯¯

### é™·é˜± 1ï¼šè¿‡åº¦ä¾èµ–å•ä¸€æŒ‡æ ‡

**é—®é¢˜æè¿°**ï¼š
åªçœ‹ accuracy æˆ– BLEU åˆ†æ•°å°±å†³å®šæ¨¡å‹å¥½åï¼Œå¿½ç•¥å…¶ä»–é‡è¦ç»´åº¦ã€‚

**åæœ**ï¼š
- æ¨¡å‹å¯èƒ½åœ¨å‡†ç¡®ç‡é«˜ä½†ç”¨æˆ·ä½“éªŒå·®
- å¿½ç•¥äº†å®‰å…¨æ€§ã€å…¬å¹³æ€§ç­‰å…³é”®é—®é¢˜
- æ— æ³•å‘ç°ç‰¹å®šåœºæ™¯ä¸‹çš„å¤±è´¥æ¨¡å¼

**è§£å†³æ–¹æ¡ˆ**ï¼š
```python
# âŒ é”™è¯¯åšæ³•
if model.accuracy > 0.9:
    deploy_model()

# âœ… æ­£ç¡®åšæ³•
evaluation_criteria = {
    'accuracy': (0.9, 'min'),
    'latency_p99': (500, 'max'),  # ms
    'hallucination_rate': (0.05, 'max'),
    'fairness_gap': (0.1, 'max'),
    'user_satisfaction': (4.0, 'min')  # 1-5 scale
}

all_pass = all(
    check_criterion(model, metric, threshold, direction)
    for metric, (threshold, direction) in evaluation_criteria.items()
)

if all_pass:
    deploy_model()
```

### é™·é˜± 2ï¼šè¯„ä¼°æ•°æ®æ³„éœ²

**é—®é¢˜æè¿°**ï¼š
æµ‹è¯•é›†æ•°æ®æ„å¤–å‡ºç°åœ¨è®­ç»ƒé›†ä¸­ï¼Œå¯¼è‡´è¯„ä¼°ç»“æœè™šé«˜ã€‚

**å¸¸è§æ¥æº**ï¼š
- ç½‘ç»œçˆ¬å–çš„æ•°æ®åŒ…å«äº†å…¬å¼€çš„æµ‹è¯•é›†
- æ•°æ®å¢å¼ºæ—¶ä¸å°å¿ƒä½¿ç”¨äº†æµ‹è¯•å›¾åƒ
- ä½¿ç”¨äº†åŒ…å«æµ‹è¯•é›†çš„é¢„è®­ç»ƒæ¨¡å‹

**æ£€æµ‹æ–¹æ³•**ï¼š
```python
# æ•°æ®æ³„éœ²æ£€æµ‹
def detect_leakage(train_data, test_data):
    # 1. ç²¾ç¡®åŒ¹é…æ£€æµ‹
    train_hashes = {hash(img.tobytes()) for img in train_data.images}
    test_hashes = {hash(img.tobytes()) for img in test_data.images}
    exact_overlap = len(train_hashes & test_hashes)
    
    # 2. è¿‘ä¼¼åŒ¹é…æ£€æµ‹ï¼ˆä½¿ç”¨æ„ŸçŸ¥å“ˆå¸Œï¼‰
    train_phash = {imagehash.phash(img) for img in train_data.images}
    test_phash = {imagehash.phash(img) for img in test_data.images}
    
    near_duplicates = 0
    for test_h in test_phash:
        for train_h in train_phash:
            if test_h - train_h < 5:  # æ±‰æ˜è·ç¦»é˜ˆå€¼
                near_duplicates += 1
                break
    
    print(f"ç²¾ç¡®é‡å¤: {exact_overlap}")
    print(f"è¿‘ä¼¼é‡å¤: {near_duplicates}")
    print(f"æ³„éœ²ç‡: {(exact_overlap + near_duplicates) / len(test_data) * 100:.2f}%")
```

### é™·é˜± 3ï¼šå¿½è§†ç½®ä¿¡åŒºé—´

**é—®é¢˜æè¿°**ï¼š
åªæŠ¥å‘Šç‚¹ä¼°è®¡ï¼Œä¸è®¡ç®—ç½®ä¿¡åŒºé—´ï¼Œå¯¼è‡´æ— æ³•åˆ¤æ–­ç»“æœçš„å¯é æ€§ã€‚

**æ­£ç¡®åšæ³•**ï¼š
```python
# Bootstrap ç½®ä¿¡åŒºé—´
def calculate_confidence_interval(scores, n_bootstrap=1000):
    bootstrap_means = []
    n = len(scores)
    
    for _ in range(n_bootstrap):
        sample = np.random.choice(scores, size=n, replace=True)
        bootstrap_means.append(np.mean(sample))
    
    ci_lower = np.percentile(bootstrap_means, 2.5)
    ci_upper = np.percentile(bootstrap_means, 97.5)
    
    return {
        'mean': np.mean(scores),
        'ci_95': (ci_lower, ci_upper),
        'std_error': np.std(bootstrap_means)
    }

# æŠ¥å‘Šæ ¼å¼
result = calculate_confidence_interval(model_scores)
print(f"å‡†ç¡®ç‡: {result['mean']:.3f} (95% CI: {result['ci_95'][0]:.3f}-{result['ci_95'][1]:.3f})")
```

### é™·é˜± 4ï¼šA/B æµ‹è¯•è¿‡æ—©åœæ­¢

**é—®é¢˜æè¿°**ï¼š
çœ‹åˆ°åˆæœŸçš„æ­£å‘ç»“æœå°±æ€¥äºå…¨é‡å‘å¸ƒï¼Œå¿½ç•¥äº†ç»Ÿè®¡åŠŸæ•ˆä¸è¶³çš„é—®é¢˜ã€‚

**åæœ**ï¼š
- å‡é˜³æ€§ï¼šå®é™…æ— æ•ˆæœä½†åˆ¤æ–­ä¸ºæœ‰æ•ˆ
- é”™è¿‡è´Ÿé¢å½±å“ï¼šåˆæœŸæœªæ˜¾ç°çš„é—®é¢˜
- å†³ç­–ä¸ç¨³å®šï¼šç»“æœå¯èƒ½åè½¬

**è§£å†³æ–¹æ¡ˆ**ï¼š
```python
# è®¾ç½®åˆç†çš„åœæ­¢æ ‡å‡†
class ABTestStoppingCriteria:
    def __init__(self):
        self.min_sample_size = 10000
        self.min_test_days = 7
        self.required_power = 0.8
        
    def should_stop(self, test_stats):
        # æ£€æŸ¥å¤šä¸ªæ¡ä»¶
        conditions = {
            'sample_size': test_stats.n >= self.min_sample_size,
            'duration': test_stats.days >= self.min_test_days,
            'statistical_power': test_stats.power >= self.required_power,
            'significance': test_stats.p_value < 0.05
        }
        
        # åªæœ‰æ‰€æœ‰æ¡ä»¶æ»¡è¶³æ‰èƒ½åœæ­¢
        can_stop = all(conditions.values())
        
        return can_stop, conditions
```

### é™·é˜± 5ï¼šäººå·¥è¯„ä¼°æ ‡å‡†ä¸ä¸€è‡´

**é—®é¢˜æè¿°**ï¼š
ä¸åŒæ ‡æ³¨è€…ç†è§£ä¸åŒï¼Œæˆ–åŒä¸€æ ‡æ³¨è€…åœ¨ä¸åŒæ—¶é—´æ ‡å‡†å‘ç”Ÿæ¼‚ç§»ã€‚

**è¡¨ç°**ï¼š
- Kappa ç³»æ•°ä½äº 0.4
- ç›¸åŒæ ·æœ¬é‡å¤æ ‡æ³¨ç»“æœä¸ä¸€è‡´
- æ ‡æ³¨è´¨é‡éšæ—¶é—´ä¸‹é™

**é¢„é˜²æªæ–½**ï¼š
```python
class AnnotationQualityController:
    def __init__(self):
        self.gold_standards = []  # é»„é‡‘æ ‡å‡†æ ·æœ¬
        self.annotator_history = defaultdict(list)
        
    def insert_quality_checks(self, task_batch):
        """æ’å…¥è´¨é‡æ£€æŸ¥æ ·æœ¬"""
        # æ¯ 10 ä¸ªä»»åŠ¡æ’å…¥ 1 ä¸ªé»„é‡‘æ ‡å‡†
        mixed_batch = []
        for i, task in enumerate(task_batch):
            mixed_batch.append(task)
            if (i + 1) % 10 == 0:
                gold = random.choice(self.gold_standards)
                mixed_batch.append(gold)
        return mixed_batch
    
    def monitor_annotator_quality(self, annotator_id, annotations):
        """ç›‘æ§æ ‡æ³¨è€…è´¨é‡"""
        gold_performance = []
        
        for ann in annotations:
            if ann.is_gold_standard:
                score = self.calculate_agreement(ann, ann.gold_answer)
                gold_performance.append(score)
                self.annotator_history[annotator_id].append({
                    'timestamp': datetime.now(),
                    'score': score
                })
        
        # æ£€æµ‹è´¨é‡ä¸‹é™
        if len(gold_performance) > 5:
            recent_quality = np.mean(gold_performance[-5:])
            if recent_quality < 0.8:
                self.alert(f"æ ‡æ³¨è€… {annotator_id} è´¨é‡ä¸‹é™åˆ° {recent_quality:.2f}")
                self.trigger_retraining(annotator_id)
```

### é™·é˜± 6ï¼šå¿½ç•¥è¾¹ç•Œæ¡ä»¶

**é—®é¢˜æè¿°**ï¼š
åªåœ¨å¸¸è§„è¾“å…¥ä¸Šè¯„ä¼°ï¼Œå¿½ç•¥è¾¹ç•Œå’Œå¼‚å¸¸æƒ…å†µã€‚

**å®¹æ˜“å¿½ç•¥çš„è¾¹ç•Œæ¡ä»¶**ï¼š
- ç©ºè¾“å…¥ / çº¯ç™½å›¾åƒ
- æé•¿æ–‡æœ¬è¾“å…¥
- ç‰¹æ®Šå­—ç¬¦å’Œè¡¨æƒ…ç¬¦å·
- ä½è´¨é‡ / æ¨¡ç³Šå›¾åƒ
- æç«¯å®½é«˜æ¯”çš„å›¾åƒ

**å…¨é¢æµ‹è¯•**ï¼š
```python
def create_edge_case_tests():
    edge_cases = [
        {
            'name': 'ç©ºå›¾åƒ',
            'image': np.ones((224, 224, 3)) * 255,
            'question': 'æè¿°è¿™å¼ å›¾ç‰‡',
            'expected_behavior': 'åˆç†å¤„ç†ï¼Œä¸å´©æºƒ'
        },
        {
            'name': 'è¶…é•¿è¾“å…¥',
            'image': normal_image,
            'question': 'a' * 10000,
            'expected_behavior': 'æˆªæ–­æˆ–æ‹’ç»ï¼Œä¸OOM'
        },
        {
            'name': 'ç‰¹æ®Šå­—ç¬¦',
            'image': normal_image,
            'question': 'ï¿½ï¿½ï¿½è¿™æ˜¯ä»€ä¹ˆï¼ŸğŸ¤”',
            'expected_behavior': 'æ­£ç¡®è§£æï¼Œä¸æŠ¥é”™'
        },
        {
            'name': 'æç«¯å®½é«˜æ¯”',
            'image': np.ones((10, 1000, 3)),
            'question': 'è¿™æ˜¯ä»€ä¹ˆå½¢çŠ¶ï¼Ÿ',
            'expected_behavior': 'æ­£ç¡®å¤„ç†æˆ–ä¼˜é›…æ‹’ç»'
        }
    ]
    return edge_cases
```

## 7.10 æœ€ä½³å®è·µæ£€æŸ¥æ¸…å•

### è¯„ä¼°è®¾è®¡é˜¶æ®µ

- [ ] **æ˜ç¡®è¯„ä¼°ç›®æ ‡**
  - [ ] å®šä¹‰æˆåŠŸæ ‡å‡†
  - [ ] ç¡®å®šå…³é”®æŒ‡æ ‡
  - [ ] è®¾ç½®å†³ç­–é˜ˆå€¼

- [ ] **é€‰æ‹©è¯„ä¼°æ–¹æ³•**
  - [ ] é€‰æ‹© 3-5 ä¸ªäº’è¡¥çš„åŸºå‡†æµ‹è¯•
  - [ ] è®¾è®¡ä»»åŠ¡ç‰¹å®šçš„è¯„ä¼°
  - [ ] è§„åˆ’äººå·¥è¯„ä¼°æ¯”ä¾‹

- [ ] **æ•°æ®è´¨é‡ä¿è¯**
  - [ ] æ£€æŸ¥æµ‹è¯•é›†ä»£è¡¨æ€§
  - [ ] éªŒè¯æ— æ•°æ®æ³„éœ²
  - [ ] å‡†å¤‡è¾¹ç•Œæ¡ä»¶æµ‹è¯•

### è¯„ä¼°æ‰§è¡Œé˜¶æ®µ

- [ ] **è‡ªåŠ¨è¯„ä¼°**
  - [ ] è¿è¡ŒåŸºå‡†æµ‹è¯•
  - [ ] è®¡ç®—å¤šç»´åº¦æŒ‡æ ‡
  - [ ] ç”Ÿæˆç½®ä¿¡åŒºé—´

- [ ] **äººå·¥è¯„ä¼°**
  - [ ] åˆ¶å®šæ¸…æ™°çš„æ ‡æ³¨æŒ‡å—
  - [ ] åŸ¹è®­æ ‡æ³¨è€…
  - [ ] æ’å…¥è´¨é‡æ£€æŸ¥ç‚¹
  - [ ] è®¡ç®—ä¸€è‡´æ€§æŒ‡æ ‡

- [ ] **åœ¨çº¿è¯„ä¼°**
  - [ ] è®¾ç½®æŠ¤æ æŒ‡æ ‡
  - [ ] å®æ–½æ¸è¿›å¼å‘å¸ƒ
  - [ ] ç›‘æ§å®æ—¶æŒ‡æ ‡
  - [ ] å‡†å¤‡å›æ»šæ–¹æ¡ˆ

### åˆ†æä¸å†³ç­–é˜¶æ®µ

- [ ] **ç»“æœåˆ†æ**
  - [ ] è¿›è¡Œç»Ÿè®¡æ˜¾è‘—æ€§æ£€éªŒ
  - [ ] åˆ†æå¤±è´¥æ¡ˆä¾‹
  - [ ] è¯†åˆ«æ€§èƒ½ç“¶é¢ˆ
  - [ ] æ£€æŸ¥åè§å’Œå…¬å¹³æ€§

- [ ] **æŠ¥å‘Šç”Ÿæˆ**
  - [ ] æ±‡æ€»å…³é”®å‘ç°
  - [ ] å¯è§†åŒ–ç»“æœ
  - [ ] æä¾›æ”¹è¿›å»ºè®®
  - [ ] è®°å½•å·²çŸ¥é™åˆ¶

- [ ] **å†³ç­–æ”¯æŒ**
  - [ ] å¯¹æ¯”åŸºçº¿æ¨¡å‹
  - [ ] è¯„ä¼°é£é™©æ”¶ç›Š
  - [ ] åˆ¶å®šå‘å¸ƒè®¡åˆ’
  - [ ] è®¾ç½®ç›‘æ§é¢„è­¦

### æŒç»­æ”¹è¿›

- [ ] **è¯„ä¼°ä½“ç³»ä¼˜åŒ–**
  - [ ] æ”¶é›†è¯„ä¼°åé¦ˆ
  - [ ] æ›´æ–°æµ‹è¯•é›†
  - [ ] ä¼˜åŒ–è¯„ä¼°æ•ˆç‡
  - [ ] é™ä½è¯„ä¼°æˆæœ¬

- [ ] **çŸ¥è¯†ç§¯ç´¯**
  - [ ] è®°å½•ç»éªŒæ•™è®­
  - [ ] ç»´æŠ¤è¯„ä¼°çŸ¥è¯†åº“
  - [ ] åˆ†äº«æœ€ä½³å®è·µ
  - [ ] åŸ¹è®­å›¢é˜Ÿæˆå‘˜

### ğŸš¨ çº¢çº¿æ ‡å‡†

**ç»å¯¹ä¸èƒ½å¦¥åçš„æ ‡å‡†ï¼š**

1. **æ•°æ®æ³„éœ²é›¶å®¹å¿**ï¼šå‘ç°ä»»ä½•æ³„éœ²ç«‹å³åœæ­¢è¯„ä¼°
2. **ç»Ÿè®¡æ˜¾è‘—æ€§è¦æ±‚**ï¼šp < 0.05 ä¸”åŠŸæ•ˆ > 0.8
3. **å®‰å…¨æ€§ä¼˜å…ˆ**ï¼šä»»ä½•å®‰å…¨æŒ‡æ ‡é€€åŒ–éƒ½ä¸èƒ½å‘å¸ƒ
4. **ç”¨æˆ·ä½“éªŒä¿æŠ¤**ï¼šæ ¸å¿ƒä½“éªŒæŒ‡æ ‡ä¸èƒ½é€€åŒ–è¶…è¿‡ 1%
5. **å…¬å¹³æ€§ä¿è¯**ï¼šä¸åŒç¾¤ä½“æ€§èƒ½å·®è· < 10%