# ç¬¬ 9 ç« ï¼šCUDA OOM è°ƒè¯•å®Œå…¨æŒ‡å—

åœ¨ VLM è®­ç»ƒè¿‡ç¨‹ä¸­ï¼ŒCUDA Out of Memory (OOM) é”™è¯¯å¯èƒ½æ˜¯æœ€å¸¸è§ä¹Ÿæœ€ä»¤äººå¤´ç–¼çš„é—®é¢˜ã€‚å½“ä½ èŠ±è´¹æ•°å°æ—¶å‡†å¤‡æ•°æ®ã€é…ç½®ç¯å¢ƒï¼Œæ»¡æ€€æœŸå¾…åœ°å¯åŠ¨è®­ç»ƒï¼Œå´åœ¨ç¬¬ä¸€ä¸ª batch å°±é­é‡ OOM å´©æºƒæ—¶ï¼Œé‚£ç§æŒ«è´¥æ„Ÿç›¸ä¿¡æ¯ä¸ª AI å·¥ç¨‹å¸ˆéƒ½æ·±æœ‰ä½“ä¼šã€‚æœ¬ç« å°†ç³»ç»Ÿä»‹ç» VLM è®­ç»ƒä¸­çš„å†…å­˜ç®¡ç†ï¼Œå¸®åŠ©ä½ å¿«é€Ÿè¯Šæ–­å’Œè§£å†³ OOM é—®é¢˜ï¼Œè®©è®­ç»ƒè¿‡ç¨‹æ›´åŠ é¡ºç•…ã€‚

## å­¦ä¹ ç›®æ ‡

å®Œæˆæœ¬ç« å­¦ä¹ åï¼Œä½ å°†èƒ½å¤Ÿï¼š

- **30 ç§’å†…å®šä½** OOM çš„å…·ä½“åŸå› ï¼ˆæ¨¡å‹ã€æ¢¯åº¦ã€æ¿€æ´»å€¼è¿˜æ˜¯ä¼˜åŒ–å™¨ï¼‰
- **æŒæ¡ 5 ç§ç´§æ€¥å¤„ç†æ–¹æ¡ˆ**ï¼Œè®©è®­ç»ƒç«‹å³æ¢å¤è¿è¡Œ
- **ç²¾ç¡®è®¡ç®—**ä»»æ„ VLM é…ç½®çš„å†…å­˜éœ€æ±‚ï¼Œé¿å…ç›²ç›®è¯•é”™
- **è¯†åˆ«å¹¶è§„é¿** VLM ç‰¹æœ‰çš„ 4 ç±»å†…å­˜é™·é˜±
- **å»ºç«‹ç³»ç»Ÿçš„å†…å­˜ä¼˜åŒ–æµç¨‹**ï¼Œå°†æ˜¾å­˜åˆ©ç”¨ç‡æå‡è‡³ 95% ä»¥ä¸Š

## 9.1 å¿«é€Ÿè¯Šæ–­å†…å­˜å ç”¨

å½“é­é‡ OOM æ—¶ï¼Œé¦–è¦ä»»åŠ¡æ˜¯å¿«é€Ÿå®šä½å†…å­˜ç“¶é¢ˆã€‚VLM è®­ç»ƒçš„å†…å­˜å ç”¨ä¸»è¦åˆ†ä¸ºå››ä¸ªéƒ¨åˆ†ï¼šæ¨¡å‹å‚æ•°ã€æ¢¯åº¦ã€æ¿€æ´»å€¼å’Œä¼˜åŒ–å™¨çŠ¶æ€ã€‚è®©æˆ‘ä»¬é€ä¸€åˆ†æã€‚

### 9.1.1 æ¨¡å‹å‚æ•°å†…å­˜è®¡ç®—

VLM çš„å‚æ•°å†…å­˜åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦ç»„ä»¶ï¼š

```
æ€»å‚æ•°å†…å­˜ = è§†è§‰ç¼–ç å™¨ + è¯­è¨€æ¨¡å‹ + è¿æ¥å±‚
```

**å¿«é€Ÿä¼°ç®—å…¬å¼**ï¼ˆä»¥ FP16 ä¸ºä¾‹ï¼‰ï¼š

$$M_{params} = 2 \times (N_{vision} + N_{language} + N_{connector}) \text{ bytes}$$

å…¶ä¸­ï¼š
- $N_{vision}$ï¼šè§†è§‰ç¼–ç å™¨å‚æ•°é‡ï¼ˆå¦‚ ViT-L/14 çº¦ 304Mï¼‰
- $N_{language}$ï¼šè¯­è¨€æ¨¡å‹å‚æ•°é‡ï¼ˆå¦‚ Vicuna-7B çº¦ 7Bï¼‰
- $N_{connector}$ï¼šè¿æ¥å±‚å‚æ•°é‡ï¼ˆé€šå¸¸ < 100Mï¼‰

**å®ä¾‹è®¡ç®—**ï¼šLLaVA-1.5-7B
```
è§†è§‰ç¼–ç å™¨ (CLIP-ViT-L/14): 304M Ã— 2 bytes = 608 MB
è¯­è¨€æ¨¡å‹ (Vicuna-7B): 7B Ã— 2 bytes = 14 GB
MLP è¿æ¥å±‚: 20M Ã— 2 bytes = 40 MB
æ€»è®¡: çº¦ 14.6 GB
```

### 9.1.2 æ¢¯åº¦å†…å­˜è®¡ç®—

è®­ç»ƒæ—¶æ¯ä¸ªå‚æ•°éƒ½éœ€è¦å­˜å‚¨æ¢¯åº¦ï¼Œå†…å­˜å ç”¨ä¸å‚æ•°ç›¸åŒï¼š

$$M_{gradients} = M_{params}$$

ä½†æ³¨æ„ï¼Œå¦‚æœå†»ç»“éƒ¨åˆ†æ¨¡å—ï¼ˆå¦‚è§†è§‰ç¼–ç å™¨ï¼‰ï¼Œè¯¥éƒ¨åˆ†ä¸äº§ç”Ÿæ¢¯åº¦ï¼š

```
å¯è®­ç»ƒå‚æ•°æ¢¯åº¦ = æ€»å‚æ•° - å†»ç»“å‚æ•°
```

**ä¼˜åŒ–æŠ€å·§**ï¼šåˆ†é˜¶æ®µè§£å†»
- Stage 1: åªè®­ç»ƒè¿æ¥å±‚ï¼ˆæ¢¯åº¦å†…å­˜ < 100MBï¼‰
- Stage 2: è§£å†»è¯­è¨€æ¨¡å‹ï¼ˆæ¢¯åº¦å†…å­˜çº¦ 14GBï¼‰
- Stage 3: å…¨éƒ¨è§£å†»ï¼ˆæ¢¯åº¦å†…å­˜çº¦ 14.6GBï¼‰

### 9.1.3 æ¿€æ´»å€¼å†…å­˜åˆ†æ

æ¿€æ´»å€¼ï¼ˆä¸­é—´å¼ é‡ï¼‰æ˜¯ OOM çš„ä¸»è¦å…ƒå‡¶ï¼Œå…¶å¤§å°ä¸ batch sizeã€åºåˆ—é•¿åº¦æˆæ­£æ¯”ï¼š

$$M_{activation} = O(B \times L \times H \times N_{layers})$$

å…¶ä¸­ï¼š
- $B$ï¼šbatch size
- $L$ï¼šåºåˆ—é•¿åº¦
- $H$ï¼šéšè—ç»´åº¦
- $N_{layers}$ï¼šå±‚æ•°

**VLM æ¿€æ´»å€¼ç‰¹ç‚¹**ï¼š

1. **è§†è§‰ tokens çˆ†ç‚¸**ï¼š
   - å•å¼ å›¾åƒäº§ç”Ÿå¤§é‡ tokensï¼ˆå¦‚ 576 ä¸ª for ViT-L/14ï¼‰
   - å¤šå›¾åœºæ™¯ä¸‹æ¿€æ´»å€¼æ€¥å‰§å¢é•¿

2. **æ³¨æ„åŠ›çŸ©é˜µ**ï¼š
   $$M_{attention} = B \times N_{heads} \times L^2 \times 4 \text{ bytes}$$
   
   å½“ $L = 2048$ æ—¶ï¼Œå•ä¸ªæ³¨æ„åŠ›å±‚å°±éœ€è¦ $B \times 32 \times 4M \times 4 = 512B$ MBï¼

### 9.1.4 ä¼˜åŒ–å™¨çŠ¶æ€å†…å­˜

ä¸åŒä¼˜åŒ–å™¨çš„å†…å­˜å ç”¨å·®å¼‚å·¨å¤§ï¼š

| ä¼˜åŒ–å™¨ | çŠ¶æ€å†…å­˜ | è®¡ç®—å…¬å¼ |
|--------|----------|----------|
| SGD | 0ï¼ˆæ— åŠ¨é‡ï¼‰æˆ– $M_{params}$ï¼ˆæœ‰åŠ¨é‡ï¼‰ | $M_{optimizer} = M_{params}$ |
| Adam | $2 \times M_{params}$ | ä¸€é˜¶ã€äºŒé˜¶åŠ¨é‡å„å ä¸€ä»½ |
| AdamW | $2 \times M_{params}$ | åŒ Adam |
| Adafactor | $M_{params} / N$ | åˆ†è§£äºŒé˜¶åŠ¨é‡ï¼ŒèŠ‚çœå†…å­˜ |

**ç¤ºä¾‹**ï¼š7B æ¨¡å‹ä½¿ç”¨ Adam
```
ä¼˜åŒ–å™¨çŠ¶æ€ = 14 GB Ã— 2 = 28 GB
æ€»å†…å­˜éœ€æ±‚ = 14.6 (å‚æ•°) + 14.6 (æ¢¯åº¦) + 28 (ä¼˜åŒ–å™¨) + æ¿€æ´»å€¼
           > 57.2 GB + æ¿€æ´»å€¼
```

è¿™å°±æ˜¯ä¸ºä»€ä¹ˆå•å¡ V100 (32GB) éš¾ä»¥è®­ç»ƒ 7B æ¨¡å‹ï¼

### 9.1.5 å†…å­˜å ç”¨å¿«é€Ÿè¯Šæ–­æµç¨‹

```python
import torch

def diagnose_memory():
    # 1. æ£€æŸ¥å½“å‰å†…å­˜ä½¿ç”¨
    allocated = torch.cuda.memory_allocated() / 1024**3
    reserved = torch.cuda.memory_reserved() / 1024**3
    
    print(f"å·²åˆ†é…: {allocated:.2f} GB")
    print(f"å·²é¢„ç•™: {reserved:.2f} GB")
    
    # 2. æ‰“å°è¯¦ç»†å†…å­˜å¿«ç…§
    print(torch.cuda.memory_summary())
    
    # 3. å®šä½å¤§å¼ é‡
    for obj in gc.get_objects():
        if torch.is_tensor(obj) and obj.is_cuda:
            print(f"{obj.shape}, {obj.dtype}, {obj.element_size() * obj.nelement() / 1024**2:.2f} MB")
```

**30 ç§’è¯Šæ–­æ¸…å•**ï¼š
1. è¿è¡Œ `nvidia-smi` æŸ¥çœ‹æ€»ä½“å ç”¨
2. è°ƒç”¨ `diagnose_memory()` å®šä½å¤§å¼ é‡
3. æ£€æŸ¥ batch size å’Œåºåˆ—é•¿åº¦
4. ç¡®è®¤ä¼˜åŒ–å™¨ç±»å‹
5. éªŒè¯æ˜¯å¦å¼€å¯æ··åˆç²¾åº¦

## 9.2 ç´§æ€¥å¤„ç†æ–¹æ¡ˆ

å½“ OOM å‘ç”Ÿæ—¶ï¼Œä»¥ä¸‹æ–¹æ¡ˆå¯ä»¥å¿«é€Ÿæ¢å¤è®­ç»ƒï¼ŒæŒ‰ä¼˜å…ˆçº§æ’åºï¼š

### 9.2.1 Gradient Checkpointingï¼ˆæ¢¯åº¦æ£€æŸ¥ç‚¹ï¼‰

æœ€æœ‰æ•ˆçš„å†…å­˜ä¼˜åŒ–æŠ€æœ¯ï¼Œç”¨è®¡ç®—æ¢å†…å­˜ï¼š

```python
# å¼€å¯ gradient checkpointing
model.gradient_checkpointing_enable()

# å¯¹äº VLMï¼Œå¯ä»¥é€‰æ‹©æ€§å¼€å¯
vision_encoder.gradient_checkpointing_enable()  # è§†è§‰ç¼–ç å™¨
language_model.gradient_checkpointing_enable()   # è¯­è¨€æ¨¡å‹
```

**å†…å­˜èŠ‚çœ**ï¼šæ¿€æ´»å€¼ä» $O(N_{layers})$ é™è‡³ $O(\sqrt{N_{layers}})$

**æ€§èƒ½å½±å“**ï¼šè®­ç»ƒé€Ÿåº¦é™ä½ 15-30%

**æœ€ä½³å®è·µ**ï¼š
- ä¼˜å…ˆåœ¨è¯­è¨€æ¨¡å‹ä¸Šå¼€å¯ï¼ˆå±‚æ•°å¤šï¼Œæ•ˆæœæ˜æ˜¾ï¼‰
- è§†è§‰ç¼–ç å™¨å¯é€‰ï¼ˆå±‚æ•°å°‘ï¼Œæ”¶ç›Šæœ‰é™ï¼‰
- ç»“åˆ FlashAttention ä½¿ç”¨æ•ˆæœæ›´ä½³

### 9.2.2 Batch Size åŠ¨æ€è°ƒæ•´

æ™ºèƒ½è°ƒæ•´ batch sizeï¼Œæœ€å¤§åŒ–æ˜¾å­˜åˆ©ç”¨ï¼š

```python
def find_optimal_batch_size(model, initial_bs=32):
    batch_size = initial_bs
    
    while batch_size > 0:
        try:
            # å°è¯•å‰å‘ä¼ æ’­
            dummy_batch = create_dummy_batch(batch_size)
            loss = model(dummy_batch)
            loss.backward()
            
            print(f"æœ€ä½³ batch size: {batch_size}")
            return batch_size
            
        except RuntimeError as e:
            if "out of memory" in str(e):
                # æ¸…ç†ç¼“å­˜
                torch.cuda.empty_cache()
                # å‡åŠé‡è¯•
                batch_size = batch_size // 2
            else:
                raise e
    
    return 1  # æœ€å° batch size
```

**æ¢¯åº¦ç´¯ç§¯è¡¥å¿**ï¼š
```python
# ç›®æ ‡ï¼šç­‰æ•ˆ batch size = 32
actual_batch_size = 4  # å—é™äºæ˜¾å­˜
accumulation_steps = 32 // 4  # ç´¯ç§¯ 8 æ­¥

for step, batch in enumerate(dataloader):
    loss = model(batch) / accumulation_steps
    loss.backward()
    
    if (step + 1) % accumulation_steps == 0:
        optimizer.step()
        optimizer.zero_grad()
```

### 9.2.3 æ··åˆç²¾åº¦è®­ç»ƒä¼˜åŒ–

FP16/BF16 è®­ç»ƒå¯èŠ‚çœ 50% å†…å­˜ï¼š

```python
from torch.cuda.amp import autocast, GradScaler

scaler = GradScaler()

with autocast(dtype=torch.float16):
    outputs = model(inputs)
    loss = criterion(outputs, targets)

# ç¼©æ”¾æ¢¯åº¦é˜²æ­¢ä¸‹æº¢
scaler.scale(loss).backward()
scaler.step(optimizer)
scaler.update()
```

**VLM ç‰¹æ®Šè€ƒè™‘**ï¼š
- è§†è§‰ç¼–ç å™¨å»ºè®®ä¿æŒ FP32ï¼ˆæ•°å€¼ç¨³å®šæ€§ï¼‰
- è¯­è¨€æ¨¡å‹å¯ä»¥å®‰å…¨ä½¿ç”¨ FP16
- æ³¨æ„åŠ›å±‚ä½¿ç”¨ BF16 æ›´ç¨³å®š

### 9.2.4 CPU Offloading

å°†éƒ¨åˆ†æ•°æ®è½¬ç§»åˆ° CPU å†…å­˜ï¼š

```python
# DeepSpeed ZeRO-Offload é…ç½®
ds_config = {
    "zero_optimization": {
        "stage": 2,
        "offload_optimizer": {
            "device": "cpu",
            "pin_memory": True
        },
        "offload_param": {
            "device": "cpu",
            "pin_memory": True
        }
    }
}
```

**æƒè¡¡**ï¼š
- ä¼˜ç‚¹ï¼šå¯è®­ç»ƒè¶…å¤§æ¨¡å‹ï¼ˆå¦‚ 65Bï¼‰
- ç¼ºç‚¹ï¼šè®­ç»ƒé€Ÿåº¦é™ä½ 2-3 å€
- é€‚ç”¨ï¼šå•å¡è®­ç»ƒå¤§æ¨¡å‹çš„æ— å¥ˆé€‰æ‹©

### 9.2.5 æ¨¡å‹å¹¶è¡Œç­–ç•¥

å½“å•å¡æ— æ³•å®¹çº³æ—¶ï¼Œè€ƒè™‘æ¨¡å‹å¹¶è¡Œï¼š

```python
# Pipeline å¹¶è¡Œç¤ºä¾‹
from torch.distributed.pipeline.sync import Pipe

# å°†æ¨¡å‹åˆ†å‰²ä¸ºä¸¤éƒ¨åˆ†
model = nn.Sequential(
    vision_encoder,    # GPU 0
    language_model     # GPU 1
)

# åˆ›å»º pipeline
model = Pipe(model, balance=[1, 1], devices=[0, 1])
```

**VLM å¹¶è¡Œå»ºè®®**ï¼š
- è§†è§‰ç¼–ç å™¨å’Œè¯­è¨€æ¨¡å‹å¤©ç„¶åˆ†ç¦»ï¼Œé€‚åˆ pipeline
- Tensor å¹¶è¡Œé€‚åˆå•ä¸ª Transformer å±‚
- ä¼˜å…ˆä½¿ç”¨æ•°æ®å¹¶è¡Œï¼Œæ€§èƒ½æœ€ä½³

## 9.3 å†…å­˜åˆ†æå·¥å…·ä½¿ç”¨

æŒæ¡å†…å­˜åˆ†æå·¥å…·æ˜¯è§£å†³ OOM é—®é¢˜çš„å…³é”®ã€‚æœ¬èŠ‚ä»‹ç» 4 ä¸ªå¿…å¤‡å·¥å…·åŠå…¶é«˜çº§ç”¨æ³•ã€‚

### 9.3.1 torch.cuda.memory_summary() æ·±åº¦è§£æ

PyTorch å†…ç½®çš„æœ€å¼ºå¤§å†…å­˜åˆ†æå·¥å…·ï¼š

```python
def analyze_memory_detailed():
    # è·å–å®Œæ•´å†…å­˜æŠ¥å‘Š
    summary = torch.cuda.memory_summary(device=0, abbreviated=False)
    print(summary)
    
    # å…³é”®æŒ‡æ ‡è§£è¯»
    stats = torch.cuda.memory_stats()
    
    print("\n=== å†…å­˜ä½¿ç”¨ç»†åˆ† ===")
    print(f"å½“å‰åˆ†é…: {stats['allocated_bytes.all.current'] / 1024**3:.2f} GB")
    print(f"å³°å€¼åˆ†é…: {stats['allocated_bytes.all.peak'] / 1024**3:.2f} GB")
    print(f"é¢„ç•™å†…å­˜: {stats['reserved_bytes.all.current'] / 1024**3:.2f} GB")
    print(f"æ´»è·ƒå†…å­˜å—: {stats['active_bytes.all.current'] / 1024**3:.2f} GB")
    
    # å†…å­˜ç¢ç‰‡åˆ†æ
    fragmentation = 1 - (stats['allocated_bytes.all.current'] / 
                        stats['reserved_bytes.all.current'])
    print(f"å†…å­˜ç¢ç‰‡ç‡: {fragmentation * 100:.1f}%")
    
    # OOM æ¬¡æ•°
    print(f"OOM é‡è¯•æ¬¡æ•°: {stats['num_ooms']}")
```

**å…³é”®æŒ‡æ ‡è§£è¯»**ï¼š
- **Allocated vs Reserved**ï¼šReserved æ˜¯ PyTorch å‘ CUDA ç”³è¯·çš„æ€»å†…å­˜ï¼ŒAllocated æ˜¯å®é™…ä½¿ç”¨çš„
- **ç¢ç‰‡ç‡ > 20%**ï¼šéœ€è¦è°ƒç”¨ `torch.cuda.empty_cache()` æ•´ç†å†…å­˜
- **num_ooms > 0**ï¼šè¯´æ˜å‘ç”Ÿè¿‡ OOM å¹¶è‡ªåŠ¨é‡è¯•

### 9.3.2 nvidia-smi é«˜çº§ç”¨æ³•

ä¸åªæ˜¯çœ‹æ˜¾å­˜å ç”¨ï¼Œæ›´å¤šé«˜çº§åŠŸèƒ½ï¼š

```bash
# 1. æŒç»­ç›‘æ§ï¼ˆæ¯ 0.1 ç§’åˆ·æ–°ï¼‰
nvidia-smi -l 0.1

# 2. åªæ˜¾ç¤ºå†…å­˜ä¿¡æ¯
nvidia-smi --query-gpu=memory.used,memory.free,memory.total \
           --format=csv,noheader,nounits -l 1

# 3. ç›‘æ§ç‰¹å®šè¿›ç¨‹
nvidia-smi pmon -i 0

# 4. å¯¼å‡ºè¯¦ç»†æ—¥å¿—ç”¨äºåˆ†æ
nvidia-smi --query-gpu=timestamp,name,memory.used,memory.free,utilization.gpu \
           --format=csv -l 1 > gpu_log.csv
```

**Python é›†æˆç›‘æ§**ï¼š
```python
import subprocess
import pandas as pd

def monitor_gpu_memory():
    result = subprocess.run([
        'nvidia-smi', 
        '--query-gpu=memory.used,memory.free,memory.total',
        '--format=csv,noheader,nounits'
    ], capture_output=True, text=True)
    
    lines = result.stdout.strip().split('\n')
    for i, line in enumerate(lines):
        used, free, total = map(int, line.split(', '))
        usage_percent = (used / total) * 100
        print(f"GPU {i}: {used}/{total} MB ({usage_percent:.1f}%)")
        
        if usage_percent > 90:
            print(f"âš ï¸  GPU {i} å†…å­˜ä½¿ç”¨è¶…è¿‡ 90%ï¼")
```

### 9.3.3 Memory Profiler å®æˆ˜

ä½¿ç”¨ PyTorch Profiler è¿½è¸ªå†…å­˜åˆ†é…ï¼š

```python
from torch.profiler import profile, ProfilerActivity, record_function

def profile_memory_usage(model, dataloader):
    with profile(
        activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],
        profile_memory=True,
        record_shapes=True,
        with_stack=True
    ) as prof:
        
        for i, batch in enumerate(dataloader):
            if i >= 3:  # åªåˆ†æå‰ 3 ä¸ª batch
                break
                
            with record_function("forward"):
                outputs = model(batch)
                
            with record_function("loss"):
                loss = compute_loss(outputs, batch['labels'])
                
            with record_function("backward"):
                loss.backward()
                
            with record_function("optimizer"):
                optimizer.step()
                optimizer.zero_grad()
    
    # è¾“å‡ºåˆ†æç»“æœ
    print(prof.key_averages().table(sort_by="cuda_memory_usage", row_limit=10))
    
    # ç”Ÿæˆ Chrome è¿½è¸ªæ–‡ä»¶
    prof.export_chrome_trace("memory_trace.json")
    
    # æ‰¾å‡ºå†…å­˜çƒ­ç‚¹
    for evt in prof.key_averages():
        if evt.cuda_memory_usage > 100 * 1024 * 1024:  # > 100MB
            print(f"å†…å­˜çƒ­ç‚¹: {evt.key}, ä½¿ç”¨: {evt.cuda_memory_usage / 1024**2:.1f} MB")
```

**åˆ†ææŠ€å·§**ï¼š
1. ç”¨ Chrome æµè§ˆå™¨æ‰“å¼€ `chrome://tracing`ï¼ŒåŠ è½½ json æ–‡ä»¶
2. æŸ¥çœ‹å†…å­˜åˆ†é…æ—¶é—´çº¿ï¼Œå®šä½å³°å€¼
3. è¯†åˆ«å†…å­˜æ³„æ¼ï¼ˆæŒç»­å¢é•¿çš„æ›²çº¿ï¼‰

### 9.3.4 è‡ªå®šä¹‰å†…å­˜ç›‘æ§

æ„å»ºå®æ—¶å†…å­˜ç›‘æ§ç³»ç»Ÿï¼š

```python
import threading
import time
import matplotlib.pyplot as plt
from collections import deque

class MemoryMonitor:
    def __init__(self, interval=1.0, max_history=100):
        self.interval = interval
        self.max_history = max_history
        self.memory_history = deque(maxlen=max_history)
        self.time_history = deque(maxlen=max_history)
        self.running = False
        self.thread = None
        
    def start(self):
        self.running = True
        self.thread = threading.Thread(target=self._monitor_loop)
        self.thread.start()
        
    def stop(self):
        self.running = False
        if self.thread:
            self.thread.join()
            
    def _monitor_loop(self):
        start_time = time.time()
        while self.running:
            # è®°å½•å†…å­˜ä½¿ç”¨
            allocated = torch.cuda.memory_allocated() / 1024**3
            self.memory_history.append(allocated)
            self.time_history.append(time.time() - start_time)
            
            # æ£€æµ‹å¼‚å¸¸
            if allocated > 0.9 * torch.cuda.get_device_properties(0).total_memory / 1024**3:
                print(f"âš ï¸  å†…å­˜å‘Šè­¦: {allocated:.2f} GB")
                self._dump_tensors()
                
            time.sleep(self.interval)
            
    def _dump_tensors(self):
        """è¾“å‡ºå ç”¨å†…å­˜æœ€å¤§çš„å¼ é‡"""
        tensors = []
        for obj in gc.get_objects():
            if torch.is_tensor(obj) and obj.is_cuda:
                tensors.append((
                    obj.numel() * obj.element_size(),
                    str(obj.shape),
                    str(obj.dtype)
                ))
        
        tensors.sort(reverse=True)
        print("\n=== Top 5 å†…å­˜å ç”¨å¼ é‡ ===")
        for size, shape, dtype in tensors[:5]:
            print(f"{size / 1024**2:.1f} MB: {shape} ({dtype})")
            
    def plot(self):
        plt.figure(figsize=(10, 4))
        plt.plot(self.time_history, self.memory_history)
        plt.xlabel('æ—¶é—´ (ç§’)')
        plt.ylabel('æ˜¾å­˜ä½¿ç”¨ (GB)')
        plt.title('è®­ç»ƒè¿‡ç¨‹æ˜¾å­˜ç›‘æ§')
        plt.grid(True)
        plt.show()

# ä½¿ç”¨ç¤ºä¾‹
monitor = MemoryMonitor(interval=0.5)
monitor.start()

# è®­ç»ƒä»£ç 
train_model()

monitor.stop()
monitor.plot()
```

### 9.3.5 å†…å­˜æ³„æ¼æ£€æµ‹

VLM è®­ç»ƒä¸­å¸¸è§çš„å†…å­˜æ³„æ¼æ¨¡å¼ï¼š

```python
def detect_memory_leak(model, dataloader, num_iterations=50):
    """æ£€æµ‹è®­ç»ƒè¿‡ç¨‹ä¸­çš„å†…å­˜æ³„æ¼"""
    
    memory_usage = []
    
    for i, batch in enumerate(dataloader):
        if i >= num_iterations:
            break
            
        # è®­ç»ƒæ­¥éª¤
        outputs = model(batch)
        loss = compute_loss(outputs, batch['labels'])
        loss.backward()
        optimizer.step()
        optimizer.zero_grad()
        
        # è®°å½•å†…å­˜
        torch.cuda.synchronize()
        memory_usage.append(torch.cuda.memory_allocated())
        
        # æ¯ 10 æ­¥æ£€æŸ¥ä¸€æ¬¡
        if i > 0 and i % 10 == 0:
            # è®¡ç®—å†…å­˜å¢é•¿ç‡
            recent_memory = memory_usage[-10:]
            growth_rate = (recent_memory[-1] - recent_memory[0]) / recent_memory[0]
            
            if growth_rate > 0.05:  # å¢é•¿è¶…è¿‡ 5%
                print(f"âš ï¸  å¯èƒ½å­˜åœ¨å†…å­˜æ³„æ¼ï¼æ­¥éª¤ {i}, å¢é•¿ç‡: {growth_rate:.2%}")
                
                # å°è¯•å®šä½æ³„æ¼æº
                for name, param in model.named_parameters():
                    if param.grad is not None and param.grad.data_ptr() != 0:
                        # æ£€æŸ¥æ¢¯åº¦æ˜¯å¦å¼‚å¸¸ç´¯ç§¯
                        if hasattr(param, '_grad_accumulation_count'):
                            if param._grad_accumulation_count > 1:
                                print(f"  æ¢¯åº¦ç´¯ç§¯å¼‚å¸¸: {name}")
    
    return memory_usage

# å¸¸è§æ³„æ¼åŸå› åŠè§£å†³æ–¹æ¡ˆ
"""
1. ä¿å­˜äº†è®¡ç®—å›¾ï¼šä½¿ç”¨ loss.item() è€Œä¸æ˜¯ loss
2. åˆ—è¡¨ç´¯ç§¯å¼ é‡ï¼šå®šæœŸæ¸…ç†æˆ–ä½¿ç”¨ .detach()
3. è‡ªå®šä¹‰ autograd å‡½æ•°ï¼šç¡®ä¿æ­£ç¡®å®ç° backward
4. hook æœªé‡Šæ”¾ï¼šè®­ç»ƒç»“æŸåè°ƒç”¨ handle.remove()
"""
```

## 9.4 VLM ç‰¹æœ‰çš„å†…å­˜é™·é˜±

VLM ç›¸æ¯”çº¯è¯­è¨€æ¨¡å‹ï¼Œæœ‰å…¶ç‹¬ç‰¹çš„å†…å­˜æŒ‘æˆ˜ã€‚æœ¬èŠ‚æ·±å…¥å‰–æ 4 ç±»å¸¸è§é™·é˜±åŠè§£å†³æ–¹æ¡ˆã€‚

### 9.4.1 è§†è§‰ç¼–ç å™¨å†…å­˜çˆ†ç‚¸

**é—®é¢˜ç°è±¡**ï¼š
- å•å¼ é«˜åˆ†è¾¨ç‡å›¾åƒå°± OOM
- å¤šå›¾è¾“å…¥æ—¶å†…å­˜æŒ‡æ•°å¢é•¿
- åŠ¨æ€åˆ†è¾¨ç‡å¯¼è‡´å†…å­˜ä¸å¯é¢„æµ‹

**æ ¹æœ¬åŸå› **ï¼š

```python
# é—®é¢˜ä»£ç ç¤ºä¾‹
def process_images(images, vision_encoder):
    # å±é™©ï¼æ‰€æœ‰å›¾åƒåŒæ—¶ç¼–ç 
    features = []
    for img in images:  # images: [B, N, C, H, W]
        feat = vision_encoder(img)  # æ¯æ¬¡éƒ½ä¿ç•™åœ¨æ˜¾å­˜ä¸­
        features.append(feat)
    return torch.stack(features)
```

**å†…å­˜è®¡ç®—**ï¼š
```
å•å¼ å›¾åƒ tokens = (H/patch_size) Ã— (W/patch_size)
ViT-L/14: 1024Ã—1024 å›¾åƒ â†’ 5184 tokensï¼
å†…å­˜ = B Ã— N_images Ã— tokens Ã— hidden_dim Ã— 4 bytes
     = 1 Ã— 4 Ã— 5184 Ã— 1024 Ã— 4 = 84.9 MBï¼ˆä»…æ¿€æ´»å€¼ï¼‰
```

**è§£å†³æ–¹æ¡ˆ**ï¼š

```python
# æ–¹æ¡ˆ 1ï¼šæ‰¹å¤„ç†ä¼˜åŒ–
def process_images_optimized(images, vision_encoder, max_batch=2):
    B, N, C, H, W = images.shape
    features = []
    
    # åˆ†æ‰¹å¤„ç†
    for i in range(0, N, max_batch):
        batch_images = images[:, i:i+max_batch]
        with torch.cuda.amp.autocast():  # ä½¿ç”¨æ··åˆç²¾åº¦
            feat = vision_encoder(batch_images)
        features.append(feat)
        
        # åŠæ—¶æ¸…ç†
        if i + max_batch < N:
            torch.cuda.empty_cache()
    
    return torch.cat(features, dim=1)

# æ–¹æ¡ˆ 2ï¼šåŠ¨æ€åˆ†è¾¨ç‡ç­–ç•¥
def adaptive_resolution(image, base_resolution=336):
    """æ ¹æ®æ˜¾å­˜åŠ¨æ€è°ƒæ•´åˆ†è¾¨ç‡"""
    available_memory = torch.cuda.mem_get_info()[0] / 1024**3  # GB
    
    if available_memory < 4:
        return F.interpolate(image, size=(base_resolution, base_resolution))
    elif available_memory < 8:
        return F.interpolate(image, size=(base_resolution*2, base_resolution*2))
    else:
        return image  # åŸå§‹åˆ†è¾¨ç‡
```

### 9.4.2 æ³¨æ„åŠ›çŸ©é˜µå†…å­˜é—®é¢˜

**é—®é¢˜ç°è±¡**ï¼š
- é•¿åºåˆ—ï¼ˆ>2048 tokensï¼‰ç›´æ¥ OOM
- å¤šæ¨¡æ€ token æ··åˆå¯¼è‡´å†…å­˜æ¿€å¢
- Cross-attention å†…å­˜å¼€é”€å·¨å¤§

**å†…å­˜åˆ†æ**ï¼š

æ ‡å‡†æ³¨æ„åŠ›å†…å­˜å¤æ‚åº¦ï¼š$O(L^2)$

```python
# æ³¨æ„åŠ›çŸ©é˜µå¤§å°è®¡ç®—
def attention_memory(seq_len, num_heads, batch_size):
    # Q @ K^T çš„å¤§å°
    memory_bytes = batch_size * num_heads * seq_len * seq_len * 4
    return memory_bytes / 1024**3  # GB

# ç¤ºä¾‹ï¼š2048 tokens, 32 heads, batch_size=1
print(f"æ³¨æ„åŠ›çŸ©é˜µ: {attention_memory(2048, 32, 1):.2f} GB")
# è¾“å‡º: 0.50 GBï¼ˆå•å±‚ï¼ï¼‰
```

**è§£å†³æ–¹æ¡ˆ**ï¼š

```python
# æ–¹æ¡ˆ 1ï¼šFlash Attention
from flash_attn import flash_attn_func

class FlashAttentionVLM(nn.Module):
    def forward(self, q, k, v):
        # Flash Attentionï¼šå†…å­˜ä» O(L^2) é™è‡³ O(L)
        return flash_attn_func(q, k, v, causal=False)

# æ–¹æ¡ˆ 2ï¼šæ»‘åŠ¨çª—å£æ³¨æ„åŠ›
def sliding_window_attention(q, k, v, window_size=512):
    """åªè®¡ç®—å±€éƒ¨çª—å£å†…çš„æ³¨æ„åŠ›"""
    B, H, L, D = q.shape
    attention_scores = []
    
    for i in range(0, L, window_size // 2):  # 50% é‡å 
        start = max(0, i - window_size // 2)
        end = min(L, i + window_size)
        
        q_window = q[:, :, start:end]
        k_window = k[:, :, start:end]
        v_window = v[:, :, start:end]
        
        scores = torch.matmul(q_window, k_window.transpose(-2, -1))
        scores = F.softmax(scores / math.sqrt(D), dim=-1)
        out = torch.matmul(scores, v_window)
        attention_scores.append(out)
    
    return combine_windows(attention_scores)

# æ–¹æ¡ˆ 3ï¼šç¨€ç–æ³¨æ„åŠ›
class SparseAttentionVLM(nn.Module):
    def __init__(self, sparsity_ratio=0.9):
        super().__init__()
        self.sparsity_ratio = sparsity_ratio
        
    def forward(self, q, k, v):
        # åªä¿ç•™ top-k æ³¨æ„åŠ›æƒé‡
        scores = torch.matmul(q, k.transpose(-2, -1))
        
        # ä¿ç•™ top 10% çš„å€¼
        k_val = int((1 - self.sparsity_ratio) * scores.shape[-1])
        topk_scores, topk_indices = torch.topk(scores, k_val, dim=-1)
        
        # åˆ›å»ºç¨€ç–çŸ©é˜µ
        sparse_scores = torch.zeros_like(scores)
        sparse_scores.scatter_(-1, topk_indices, topk_scores)
        
        attn_weights = F.softmax(sparse_scores, dim=-1)
        return torch.matmul(attn_weights, v)
```

### 9.4.3 å¤šåˆ†è¾¨ç‡å¤„ç†é™·é˜±

**é—®é¢˜ç°è±¡**ï¼š
- ä¸åŒåˆ†è¾¨ç‡å›¾åƒå¯¼è‡´å†…å­˜æ³¢åŠ¨
- åŠ¨æ€ padding é€ æˆå†…å­˜æµªè´¹
- æ‰¹å¤„ç†æ—¶æœ€å¤§åˆ†è¾¨ç‡å†³å®šæ•´ä½“å†…å­˜

**ç¤ºä¾‹é—®é¢˜**ï¼š

```python
# é—®é¢˜ä»£ç 
def batch_images_naive(image_list):
    # æ‰€æœ‰å›¾åƒ pad åˆ°æœ€å¤§å°ºå¯¸ â†’ å†…å­˜æµªè´¹ï¼
    max_h = max(img.shape[-2] for img in image_list)
    max_w = max(img.shape[-1] for img in image_list)
    
    padded_images = []
    for img in image_list:
        pad_h = max_h - img.shape[-2]
        pad_w = max_w - img.shape[-1]
        padded = F.pad(img, (0, pad_w, 0, pad_h))
        padded_images.append(padded)
    
    return torch.stack(padded_images)
```

**ä¼˜åŒ–æ–¹æ¡ˆ**ï¼š

```python
# æ–¹æ¡ˆ 1ï¼šåˆ†ç»„æ‰¹å¤„ç†
def group_by_resolution(images, num_groups=3):
    """æŒ‰åˆ†è¾¨ç‡åˆ†ç»„ï¼Œå‡å°‘ padding æµªè´¹"""
    # è®¡ç®—æ¯å¼ å›¾åƒçš„åƒç´ æ•°
    resolutions = [img.shape[-2] * img.shape[-1] for img in images]
    
    # K-means èšç±»
    groups = defaultdict(list)
    sorted_indices = np.argsort(resolutions)
    
    for i, idx in enumerate(sorted_indices):
        group_id = i * num_groups // len(sorted_indices)
        groups[group_id].append(images[idx])
    
    # æ¯ç»„å•ç‹¬å¤„ç†
    processed_groups = []
    for group_images in groups.values():
        batch = batch_images_naive(group_images)  # ç»„å†… padding
        processed_groups.append(batch)
    
    return processed_groups

# æ–¹æ¡ˆ 2ï¼šåŠ¨æ€åˆ†å—å¤„ç†
class DynamicPatchProcessor:
    def __init__(self, base_size=224, max_patches=16):
        self.base_size = base_size
        self.max_patches = max_patches
        
    def process(self, image):
        H, W = image.shape[-2:]
        
        # è®¡ç®—éœ€è¦çš„ patch æ•°é‡
        n_h = math.ceil(H / self.base_size)
        n_w = math.ceil(W / self.base_size)
        
        if n_h * n_w > self.max_patches:
            # é™é‡‡æ ·ä»¥æ»¡è¶³å†…å­˜é™åˆ¶
            scale = math.sqrt(self.max_patches / (n_h * n_w))
            new_h = int(H * scale)
            new_w = int(W * scale)
            image = F.interpolate(image, size=(new_h, new_w))
            n_h = math.ceil(new_h / self.base_size)
            n_w = math.ceil(new_w / self.base_size)
        
        # åˆ†å—å¤„ç†
        patches = []
        for i in range(n_h):
            for j in range(n_w):
                patch = image[..., 
                             i*self.base_size:(i+1)*self.base_size,
                             j*self.base_size:(j+1)*self.base_size]
                patches.append(patch)
        
        return patches, (n_h, n_w)
```

### 9.4.4 äº¤å‰æ³¨æ„åŠ›å†…å­˜ä¼˜åŒ–

**é—®é¢˜ç°è±¡**ï¼š
- è§†è§‰-è¯­è¨€äº¤å‰æ³¨æ„åŠ›å†…å­˜å¼€é”€å·¨å¤§
- å¤šå±‚äº¤å‰æ³¨æ„åŠ›ç´¯ç§¯å¯¼è‡´ OOM
- Cache æœºåˆ¶å¤±æ•ˆ

**å†…å­˜åˆ†æ**ï¼š

```python
# äº¤å‰æ³¨æ„åŠ›å†…å­˜è®¡ç®—
def cross_attention_memory(text_len, image_tokens, num_layers, hidden_dim):
    # æ¯å±‚éƒ½éœ€è¦å­˜å‚¨ K, V
    kv_memory = 2 * image_tokens * hidden_dim * 4  # bytes
    
    # æ³¨æ„åŠ›çŸ©é˜µ
    attn_memory = text_len * image_tokens * 4  # bytes
    
    total = num_layers * (kv_memory + attn_memory)
    return total / 1024**3  # GB

# ç¤ºä¾‹ï¼š1024 text tokens, 576 image tokens, 24 layers
memory = cross_attention_memory(1024, 576, 24, 4096)
print(f"äº¤å‰æ³¨æ„åŠ›å†…å­˜: {memory:.2f} GB")
```

**ä¼˜åŒ–ç­–ç•¥**ï¼š

```python
# æ–¹æ¡ˆ 1ï¼šå…±äº« KV cache
class SharedCrossAttention(nn.Module):
    def __init__(self, num_layers, hidden_dim):
        super().__init__()
        # åªåœ¨ç¬¬ä¸€å±‚è®¡ç®— image KVï¼Œåç»­å±‚å¤ç”¨
        self.image_proj_k = nn.Linear(hidden_dim, hidden_dim)
        self.image_proj_v = nn.Linear(hidden_dim, hidden_dim)
        self.layers = nn.ModuleList([
            CrossAttentionLayer(hidden_dim) for _ in range(num_layers)
        ])
        
    def forward(self, text_hidden, image_features):
        # ä¸€æ¬¡æ€§è®¡ç®—æ‰€æœ‰å±‚çš„ KV
        image_k = self.image_proj_k(image_features)
        image_v = self.image_proj_v(image_features)
        
        for layer in self.layers:
            text_hidden = layer(text_hidden, image_k, image_v)
        
        return text_hidden

# æ–¹æ¡ˆ 2ï¼šé—¨æ§äº¤å‰æ³¨æ„åŠ›
class GatedCrossAttention(nn.Module):
    """åªåœ¨å¿…è¦æ—¶è¿›è¡Œäº¤å‰æ³¨æ„åŠ›"""
    def __init__(self, hidden_dim, threshold=0.5):
        super().__init__()
        self.gate = nn.Linear(hidden_dim, 1)
        self.threshold = threshold
        self.cross_attn = CrossAttentionLayer(hidden_dim)
        
    def forward(self, text_hidden, image_features):
        # è®¡ç®—é—¨æ§å€¼
        gate_scores = torch.sigmoid(self.gate(text_hidden.mean(dim=1)))
        
        if gate_scores.mean() > self.threshold:
            # æ‰§è¡Œäº¤å‰æ³¨æ„åŠ›
            return self.cross_attn(text_hidden, image_features)
        else:
            # è·³è¿‡ï¼ŒèŠ‚çœå†…å­˜
            return text_hidden

# æ–¹æ¡ˆ 3ï¼šä½ç§©åˆ†è§£
class LowRankCrossAttention(nn.Module):
    """ä½¿ç”¨ä½ç§©åˆ†è§£å‡å°‘å‚æ•°å’Œå†…å­˜"""
    def __init__(self, hidden_dim, rank=64):
        super().__init__()
        self.rank = rank
        
        # åˆ†è§£ W_q, W_k, W_v
        self.q_down = nn.Linear(hidden_dim, rank, bias=False)
        self.q_up = nn.Linear(rank, hidden_dim, bias=False)
        
        self.kv_down = nn.Linear(hidden_dim, rank * 2, bias=False)
        self.kv_up = nn.Linear(rank * 2, hidden_dim * 2, bias=False)
        
    def forward(self, text_hidden, image_features):
        # ä½ç§©æŠ•å½±
        q = self.q_up(self.q_down(text_hidden))
        kv = self.kv_up(self.kv_down(image_features))
        k, v = kv.chunk(2, dim=-1)
        
        # æ ‡å‡†æ³¨æ„åŠ›è®¡ç®—
        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(q.size(-1))
        attn = F.softmax(scores, dim=-1)
        return torch.matmul(attn, v)
```

### 9.4.5 å†…å­˜ä¼˜åŒ–æœ€ä½³å®è·µæ±‡æ€»

```python
class MemoryOptimizedVLM:
    """é›†æˆæ‰€æœ‰å†…å­˜ä¼˜åŒ–æŠ€æœ¯çš„ VLM"""
    
    def __init__(self, config):
        self.config = config
        self.setup_memory_optimization()
        
    def setup_memory_optimization(self):
        # 1. å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹
        if self.config.gradient_checkpointing:
            self.model.gradient_checkpointing_enable()
            
        # 2. ä½¿ç”¨ Flash Attention
        if self.config.use_flash_attention:
            replace_attention_with_flash_attention(self.model)
            
        # 3. æ··åˆç²¾åº¦è®­ç»ƒ
        if self.config.mixed_precision:
            self.scaler = torch.cuda.amp.GradScaler()
            
        # 4. å†…å­˜ç›‘æ§
        self.memory_monitor = MemoryMonitor(interval=10)
        
    def train_step(self, batch):
        # åŠ¨æ€è°ƒæ•´ batch size
        if self.should_reduce_batch_size():
            batch = self.split_batch(batch)
            
        # åˆ†ç»„å¤„ç†å¤šåˆ†è¾¨ç‡å›¾åƒ
        image_groups = self.group_images_by_resolution(batch['images'])
        
        total_loss = 0
        for images in image_groups:
            with torch.cuda.amp.autocast(enabled=self.config.mixed_precision):
                # å‰å‘ä¼ æ’­
                outputs = self.model(images, batch['text'])
                loss = self.criterion(outputs, batch['labels'])
                
            # åå‘ä¼ æ’­
            if self.config.mixed_precision:
                self.scaler.scale(loss).backward()
            else:
                loss.backward()
                
            total_loss += loss.item()
            
            # åŠæ—¶æ¸…ç†
            if len(image_groups) > 1:
                torch.cuda.empty_cache()
                
        return total_loss / len(image_groups)
        
    def should_reduce_batch_size(self):
        """åŠ¨æ€æ£€æµ‹æ˜¯å¦éœ€è¦å‡å° batch size"""
        memory_usage = torch.cuda.memory_allocated() / torch.cuda.max_memory_allocated()
        return memory_usage > 0.9
```

## æœ¬ç« å°ç»“

æœ¬ç« ç³»ç»Ÿä»‹ç»äº† VLM è®­ç»ƒä¸­ CUDA OOM é—®é¢˜çš„è¯Šæ–­å’Œè§£å†³æ–¹æ³•ã€‚æˆ‘ä»¬å­¦ä¹ äº†ï¼š

**æ ¸å¿ƒæ¦‚å¿µ**ï¼š
- VLM å†…å­˜å ç”¨çš„å››å¤§ç»„æˆï¼šæ¨¡å‹å‚æ•°ã€æ¢¯åº¦ã€æ¿€æ´»å€¼ã€ä¼˜åŒ–å™¨çŠ¶æ€
- å†…å­˜è®¡ç®—å…¬å¼ï¼šç²¾ç¡®é¢„ä¼°ä»»æ„é…ç½®çš„å†…å­˜éœ€æ±‚
- VLM ç‰¹æœ‰æŒ‘æˆ˜ï¼šè§†è§‰ tokens çˆ†ç‚¸ã€æ³¨æ„åŠ›äºŒæ¬¡å¤æ‚åº¦ã€å¤šåˆ†è¾¨ç‡å¤„ç†

**å…³é”®æŠ€æœ¯**ï¼š
- **Gradient Checkpointing**ï¼šç”¨è®¡ç®—æ¢å†…å­˜ï¼Œæ¿€æ´»å€¼ä» $O(N)$ é™è‡³ $O(\sqrt{N})$
- **Flash Attention**ï¼šæ³¨æ„åŠ›å†…å­˜ä» $O(L^2)$ é™è‡³ $O(L)$
- **åŠ¨æ€æ‰¹å¤„ç†**ï¼šæ ¹æ®æ˜¾å­˜å®æ—¶è°ƒæ•´ batch size
- **æ··åˆç²¾åº¦è®­ç»ƒ**ï¼šFP16/BF16 èŠ‚çœ 50% å†…å­˜

**å®ç”¨å·¥å…·**ï¼š
- `torch.cuda.memory_summary()`ï¼šæ·±åº¦å†…å­˜åˆ†æ
- `nvidia-smi` é«˜çº§ç”¨æ³•ï¼šæŒç»­ç›‘æ§å’Œæ—¥å¿—å¯¼å‡º
- PyTorch Profilerï¼šå†…å­˜çƒ­ç‚¹å®šä½
- è‡ªå®šä¹‰ç›‘æ§ç³»ç»Ÿï¼šå®æ—¶é¢„è­¦å’Œè‡ªåŠ¨è°ƒæ•´

**VLM ä¼˜åŒ–ç­–ç•¥**ï¼š
1. è§†è§‰ç¼–ç å™¨ï¼šåˆ†æ‰¹å¤„ç†ã€åŠ¨æ€åˆ†è¾¨ç‡
2. æ³¨æ„åŠ›ä¼˜åŒ–ï¼šFlash/ç¨€ç–/æ»‘åŠ¨çª—å£æ³¨æ„åŠ›
3. å¤šåˆ†è¾¨ç‡ï¼šåˆ†ç»„æ‰¹å¤„ç†ã€åŠ¨æ€åˆ†å—
4. äº¤å‰æ³¨æ„åŠ›ï¼šKV å…±äº«ã€é—¨æ§æœºåˆ¶ã€ä½ç§©åˆ†è§£

è®°ä½ï¼š**OOM ä¸æ˜¯æ— è§£çš„**ã€‚é€šè¿‡ç³»ç»Ÿçš„åˆ†æå’Œåˆç†çš„ä¼˜åŒ–ï¼Œå³ä½¿åœ¨æœ‰é™çš„ç¡¬ä»¶ä¸Šä¹Ÿèƒ½è®­ç»ƒå¤§è§„æ¨¡ VLMã€‚å…³é”®æ˜¯ç†è§£å†…å­˜åˆ†é…æœºåˆ¶ï¼Œé€‰æ‹©åˆé€‚çš„ä¼˜åŒ–ç­–ç•¥ï¼Œå¹¶å»ºç«‹å®Œå–„çš„ç›‘æ§ä½“ç³»ã€‚

## ç»ƒä¹ é¢˜

### åŸºç¡€é¢˜

**ç»ƒä¹  9.1**ï¼šè®¡ç®— LLaVA-1.5-13B åœ¨ä»¥ä¸‹é…ç½®ä¸‹çš„æœ€å°æ˜¾å­˜éœ€æ±‚ï¼š
- è§†è§‰ç¼–ç å™¨ï¼šCLIP-ViT-L/14ï¼ˆ304M å‚æ•°ï¼‰
- è¯­è¨€æ¨¡å‹ï¼šVicuna-13B
- ä¼˜åŒ–å™¨ï¼šAdamW
- æ‰¹å¤§å°ï¼š1
- åºåˆ—é•¿åº¦ï¼š2048
- æ··åˆç²¾åº¦ï¼šFP16

ğŸ’¡ **æç¤º**ï¼šåˆ†åˆ«è®¡ç®—å‚æ•°ã€æ¢¯åº¦ã€ä¼˜åŒ–å™¨çŠ¶æ€çš„å†…å­˜ï¼Œæ¿€æ´»å€¼å¯æŒ‰ç»éªŒä¼°ç®—ä¸ºå‚æ•°çš„ 2-3 å€ã€‚

<details>
<summary>ğŸ“ å‚è€ƒç­”æ¡ˆ</summary>

å†…å­˜è®¡ç®—ï¼š
1. å‚æ•°å†…å­˜ï¼ˆFP16ï¼‰ï¼š
   - è§†è§‰ç¼–ç å™¨ï¼š304M Ã— 2 = 0.61 GB
   - è¯­è¨€æ¨¡å‹ï¼š13B Ã— 2 = 26 GB
   - è¿æ¥å±‚ï¼šçº¦ 50M Ã— 2 = 0.1 GB
   - æ€»è®¡ï¼š26.71 GB

2. æ¢¯åº¦å†…å­˜ï¼šç­‰äºå‚æ•°å†…å­˜ = 26.71 GB

3. ä¼˜åŒ–å™¨çŠ¶æ€ï¼ˆAdamWï¼‰ï¼š
   - ä¸€é˜¶åŠ¨é‡ï¼š26.71 GB
   - äºŒé˜¶åŠ¨é‡ï¼š26.71 GB
   - æ€»è®¡ï¼š53.42 GB

4. æ¿€æ´»å€¼ï¼ˆç»éªŒä¼°ç®—ï¼‰ï¼š
   - çº¦å‚æ•°çš„ 2.5 å€ = 66.78 GB

æœ€å°æ˜¾å­˜éœ€æ±‚ï¼š26.71 + 26.71 + 53.42 + 66.78 = **173.62 GB**

è¿™å°±æ˜¯ä¸ºä»€ä¹ˆéœ€è¦å¤šå¡è®­ç»ƒæˆ–ä½¿ç”¨å†…å­˜ä¼˜åŒ–æŠ€æœ¯ï¼
</details>

**ç»ƒä¹  9.2**ï¼šç»™å®šä¸€ä¸ª OOM é”™è¯¯ä¿¡æ¯ï¼Œè¯†åˆ«é—®é¢˜åŸå› å¹¶æå‡ºè§£å†³æ–¹æ¡ˆï¼š
```
RuntimeError: CUDA out of memory. Tried to allocate 2.50 GiB 
(GPU 0; 23.69 GiB total capacity; 21.45 GiB already allocated; 
1.89 GiB free; 21.50 GiB reserved in total by PyTorch)
```

ğŸ’¡ **æç¤º**ï¼šæ³¨æ„ allocated vs reserved çš„å·®å¼‚ï¼Œä»¥åŠè¯·æ±‚åˆ†é…çš„å¤§å°ã€‚

<details>
<summary>ğŸ“ å‚è€ƒç­”æ¡ˆ</summary>

é—®é¢˜åˆ†æï¼š
1. å·²åˆ†é…ï¼š21.45 GBï¼Œå·²é¢„ç•™ï¼š21.50 GB
2. ç¢ç‰‡ç‡å¾ˆä½ï¼š(21.50 - 21.45) / 21.50 = 0.23%
3. å‰©ä½™ç©ºé—´ï¼š1.89 GB < 2.50 GBï¼ˆè¯·æ±‚ï¼‰

åŸå› ï¼šå†…å­˜å·²åŸºæœ¬ç”¨å°½ï¼Œæ— æ³•æ»¡è¶³æ–°çš„å¤§å—åˆ†é…è¯·æ±‚ï¼ˆå¯èƒ½æ˜¯æ³¨æ„åŠ›çŸ©é˜µï¼‰ã€‚

è§£å†³æ–¹æ¡ˆï¼š
1. ç«‹å³æªæ–½ï¼š
   - å‡å° batch sizeï¼ˆå¦‚æœ > 1ï¼‰
   - å¯ç”¨ gradient checkpointing
   - è°ƒç”¨ torch.cuda.empty_cache()

2. ä¼˜åŒ–æªæ–½ï¼š
   - ä½¿ç”¨ Flash Attentionï¼ˆ2.50 GB æš—ç¤ºæ˜¯æ³¨æ„åŠ›çŸ©é˜µï¼‰
   - å¯ç”¨æ··åˆç²¾åº¦è®­ç»ƒ
   - è€ƒè™‘æ¨¡å‹å¹¶è¡Œæˆ– CPU offloading
</details>

**ç»ƒä¹  9.3**ï¼šç¼–å†™ä»£ç ï¼Œå®ç°ä¸€ä¸ªå‡½æ•°è‡ªåŠ¨æ‰¾åˆ°æœ€å¤§å¯ç”¨ batch sizeï¼š

ğŸ’¡ **æç¤º**ï¼šä½¿ç”¨äºŒåˆ†æœç´¢ï¼Œå¤„ç† OOM å¼‚å¸¸ã€‚

<details>
<summary>ğŸ“ å‚è€ƒç­”æ¡ˆ</summary>

```python
def find_max_batch_size(model, create_batch_fn, min_bs=1, max_bs=128):
    """äºŒåˆ†æœç´¢æ‰¾åˆ°æœ€å¤§ batch size"""
    
    def can_run(batch_size):
        try:
            batch = create_batch_fn(batch_size)
            output = model(batch)
            loss = output.loss
            loss.backward()
            
            # æ¸…ç†
            del output, loss, batch
            torch.cuda.empty_cache()
            return True
            
        except RuntimeError as e:
            if "out of memory" in str(e):
                torch.cuda.empty_cache()
                return False
            raise e
    
    # äºŒåˆ†æœç´¢
    left, right = min_bs, max_bs
    best_bs = min_bs
    
    while left <= right:
        mid = (left + right) // 2
        
        if can_run(mid):
            best_bs = mid
            left = mid + 1
        else:
            right = mid - 1
    
    # éªŒè¯æœ€ç»ˆç»“æœ
    if can_run(best_bs):
        print(f"æœ€å¤§ batch size: {best_bs}")
        
        # ç•™å‡ºå®‰å…¨è¾¹é™…
        safe_bs = int(best_bs * 0.9)
        print(f"æ¨è batch size: {safe_bs}")
        return safe_bs
    else:
        return best_bs - 1
```
</details>

### æŒ‘æˆ˜é¢˜

**ç»ƒä¹  9.4**ï¼šè®¾è®¡ä¸€ä¸ªè‡ªé€‚åº”å†…å­˜ç®¡ç†ç³»ç»Ÿï¼Œèƒ½å¤Ÿï¼š
- ç›‘æ§å†…å­˜ä½¿ç”¨è¶‹åŠ¿
- é¢„æµ‹ OOM é£é™©
- è‡ªåŠ¨è°ƒæ•´è®­ç»ƒå‚æ•°

ğŸ’¡ **æç¤º**ï¼šè€ƒè™‘ä½¿ç”¨æ»‘åŠ¨çª—å£å’Œçº¿æ€§å›å½’é¢„æµ‹å†…å­˜å¢é•¿ã€‚

<details>
<summary>ğŸ“ å‚è€ƒç­”æ¡ˆ</summary>

```python
class AdaptiveMemoryManager:
    def __init__(self, window_size=10, oom_threshold=0.85):
        self.window_size = window_size
        self.oom_threshold = oom_threshold
        self.memory_history = deque(maxlen=window_size)
        self.step_history = deque(maxlen=window_size)
        
    def update(self, step):
        # è®°å½•å½“å‰å†…å­˜
        allocated = torch.cuda.memory_allocated() / torch.cuda.max_memory_allocated()
        self.memory_history.append(allocated)
        self.step_history.append(step)
        
    def predict_oom_risk(self, future_steps=10):
        if len(self.memory_history) < 3:
            return 0.0
            
        # çº¿æ€§å›å½’é¢„æµ‹
        X = np.array(self.step_history).reshape(-1, 1)
        y = np.array(self.memory_history)
        
        from sklearn.linear_model import LinearRegression
        model = LinearRegression()
        model.fit(X, y)
        
        # é¢„æµ‹æœªæ¥å†…å­˜ä½¿ç”¨
        future_step = self.step_history[-1] + future_steps
        predicted_memory = model.predict([[future_step]])[0]
        
        # è®¡ç®— OOM é£é™©
        if predicted_memory > self.oom_threshold:
            risk = min(1.0, (predicted_memory - self.oom_threshold) / 0.15)
        else:
            risk = 0.0
            
        return risk
        
    def adjust_training_params(self, config, risk):
        """æ ¹æ®é£é™©è°ƒæ•´å‚æ•°"""
        adjustments = {}
        
        if risk > 0.8:
            # é«˜é£é™©ï¼šæ¿€è¿›è°ƒæ•´
            adjustments['batch_size'] = max(1, config.batch_size // 2)
            adjustments['gradient_checkpointing'] = True
            adjustments['accumulation_steps'] = config.accumulation_steps * 2
            
        elif risk > 0.5:
            # ä¸­é£é™©ï¼šæ¸©å’Œè°ƒæ•´
            adjustments['batch_size'] = int(config.batch_size * 0.75)
            adjustments['gradient_checkpointing'] = True
            
        elif risk > 0.3:
            # ä½é£é™©ï¼šå°å¹…ä¼˜åŒ–
            adjustments['mixed_precision'] = True
            
        return adjustments
        
    def emergency_cleanup(self):
        """ç´§æ€¥å†…å­˜æ¸…ç†"""
        # 1. æ¸…ç©ºç¼“å­˜
        torch.cuda.empty_cache()
        
        # 2. åˆ é™¤ä¸å¿…è¦çš„å¼ é‡
        import gc
        for obj in gc.get_objects():
            if torch.is_tensor(obj) and obj.is_cuda:
                if obj.grad_fn is None:  # ä¸åœ¨è®¡ç®—å›¾ä¸­
                    del obj
        
        # 3. åŒæ­¥å¹¶å†æ¬¡æ¸…ç†
        torch.cuda.synchronize()
        torch.cuda.empty_cache()
        gc.collect()
```

ä½¿ç”¨è¯¥ç³»ç»Ÿå¯ä»¥é¢„é˜² OOMï¼Œè€Œä¸æ˜¯ç­‰åˆ°å‘ç”Ÿåå†å¤„ç†ã€‚
</details>

**ç»ƒä¹  9.5**ï¼šåˆ†æå¹¶ä¼˜åŒ–ä»¥ä¸‹ VLM å‰å‘ä¼ æ’­ä»£ç çš„å†…å­˜ä½¿ç”¨ï¼š

```python
def forward(self, images, text_ids):
    # è§†è§‰ç¼–ç 
    B, N, C, H, W = images.shape
    all_features = []
    for i in range(B):
        img_features = []
        for j in range(N):
            feat = self.vision_encoder(images[i, j])
            img_features.append(feat)
        all_features.append(torch.stack(img_features))
    vision_features = torch.stack(all_features)
    
    # æ–‡æœ¬åµŒå…¥
    text_embeds = self.text_embedder(text_ids)
    
    # äº¤å‰æ³¨æ„åŠ›
    for layer in self.cross_attention_layers:
        text_embeds = layer(text_embeds, vision_features)
    
    return text_embeds
```

ğŸ’¡ **æç¤º**ï¼šè€ƒè™‘å‘é‡åŒ–ã€å†…å­˜å¤ç”¨ã€æ¢¯åº¦æ£€æŸ¥ç‚¹ã€‚

<details>
<summary>ğŸ“ å‚è€ƒç­”æ¡ˆ</summary>

ä¼˜åŒ–åçš„ä»£ç ï¼š

```python
def forward(self, images, text_ids):
    B, N, C, H, W = images.shape
    
    # ä¼˜åŒ– 1ï¼šå‘é‡åŒ–å¤„ç†ï¼Œé¿å…å¾ªç¯
    images_flat = images.view(B * N, C, H, W)
    
    # ä¼˜åŒ– 2ï¼šä½¿ç”¨ checkpoint å‡å°‘æ¿€æ´»å€¼å†…å­˜
    if self.training and self.use_checkpointing:
        vision_features = checkpoint(self.vision_encoder, images_flat)
    else:
        vision_features = self.vision_encoder(images_flat)
    
    # ä¼˜åŒ– 3ï¼šåŸåœ° reshapeï¼Œé¿å…é¢å¤–å†…å­˜åˆ†é…
    vision_features = vision_features.view(B, N, -1, vision_features.size(-1))
    
    # ä¼˜åŒ– 4ï¼šå¦‚æœ N å¾ˆå¤§ï¼Œè€ƒè™‘åˆ†å—å¤„ç†
    if N > 4:
        chunk_size = 2
        chunks = []
        for i in range(0, N, chunk_size):
            chunk = vision_features[:, i:i+chunk_size]
            chunks.append(chunk)
            
            # ä¼˜åŒ– 5ï¼šåŠæ—¶é‡Šæ”¾ä¸­é—´ç»“æœ
            if i + chunk_size < N:
                del chunk
                torch.cuda.empty_cache()
        
        vision_features = torch.cat(chunks, dim=1)
    
    # æ–‡æœ¬åµŒå…¥
    text_embeds = self.text_embedder(text_ids)
    
    # ä¼˜åŒ– 6ï¼šé‡ç”¨ vision_features çš„ KV cache
    vision_k = self.vision_proj_k(vision_features)
    vision_v = self.vision_proj_v(vision_features)
    
    for layer in self.cross_attention_layers:
        if self.training and self.use_checkpointing:
            text_embeds = checkpoint(
                layer, text_embeds, vision_k, vision_v
            )
        else:
            text_embeds = layer(text_embeds, vision_k, vision_v)
    
    return text_embeds
```

å†…å­˜èŠ‚çœï¼š
1. å‘é‡åŒ–ï¼šå‡å°‘ä¸´æ—¶å¼ é‡åˆ›å»º
2. Checkpointingï¼šæ¿€æ´»å€¼å†…å­˜é™ä½ 50-70%
3. KV å¤ç”¨ï¼šé¿å…æ¯å±‚é‡å¤è®¡ç®—
4. åˆ†å—å¤„ç†ï¼šå³°å€¼å†…å­˜é™ä½
5. åŠæ—¶æ¸…ç†ï¼šé¿å…å†…å­˜ç´¯ç§¯
</details>

**ç»ƒä¹  9.6**ï¼šè®¾è®¡å®éªŒæ¯”è¾ƒä¸åŒæ³¨æ„åŠ›å®ç°çš„å†…å­˜-é€Ÿåº¦æƒè¡¡ï¼š
- æ ‡å‡†æ³¨æ„åŠ›
- Flash Attention
- ç¨€ç–æ³¨æ„åŠ›
- æ»‘åŠ¨çª—å£æ³¨æ„åŠ›

ğŸ’¡ **æç¤º**ï¼šå›ºå®šåºåˆ—é•¿åº¦ï¼Œæµ‹é‡å†…å­˜å ç”¨å’Œæ¨ç†æ—¶é—´ã€‚

<details>
<summary>ğŸ“ å‚è€ƒç­”æ¡ˆ</summary>

```python
import time
import torch
import matplotlib.pyplot as plt

def benchmark_attention_methods():
    seq_lengths = [512, 1024, 2048, 4096]
    batch_size = 1
    hidden_dim = 1024
    num_heads = 16
    
    results = {
        'standard': {'memory': [], 'time': []},
        'flash': {'memory': [], 'time': []},
        'sparse': {'memory': [], 'time': []},
        'sliding': {'memory': [], 'time': []}
    }
    
    for seq_len in seq_lengths:
        # å‡†å¤‡è¾“å…¥
        q = torch.randn(batch_size, num_heads, seq_len, hidden_dim // num_heads).cuda()
        k = torch.randn_like(q)
        v = torch.randn_like(q)
        
        # 1. æ ‡å‡†æ³¨æ„åŠ›
        torch.cuda.reset_peak_memory_stats()
        start = time.time()
        
        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(q.size(-1))
        attn = F.softmax(scores, dim=-1)
        out = torch.matmul(attn, v)
        
        torch.cuda.synchronize()
        standard_time = time.time() - start
        standard_memory = torch.cuda.max_memory_allocated() / 1024**3
        
        results['standard']['time'].append(standard_time)
        results['standard']['memory'].append(standard_memory)
        
        # 2. Flash Attentionï¼ˆæ¨¡æ‹Ÿï¼‰
        torch.cuda.reset_peak_memory_stats()
        start = time.time()
        
        # Flash attention å†…å­˜å¤æ‚åº¦ O(seq_len) è€Œé O(seq_len^2)
        # è¿™é‡Œç®€åŒ–æ¨¡æ‹Ÿ
        chunk_size = 64
        out_flash = []
        for i in range(0, seq_len, chunk_size):
            q_chunk = q[:, :, i:i+chunk_size]
            scores_chunk = torch.matmul(q_chunk, k.transpose(-2, -1)) / math.sqrt(q.size(-1))
            attn_chunk = F.softmax(scores_chunk, dim=-1)
            out_chunk = torch.matmul(attn_chunk, v)
            out_flash.append(out_chunk)
        
        out_flash = torch.cat(out_flash, dim=2)
        
        torch.cuda.synchronize()
        flash_time = time.time() - start
        flash_memory = torch.cuda.max_memory_allocated() / 1024**3
        
        results['flash']['time'].append(flash_time)
        results['flash']['memory'].append(flash_memory)
        
        # 3. ç¨€ç–æ³¨æ„åŠ›
        torch.cuda.reset_peak_memory_stats()
        start = time.time()
        
        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(q.size(-1))
        # åªä¿ç•™ top 10%
        k_sparse = int(0.1 * scores.shape[-1])
        topk_scores, topk_indices = torch.topk(scores, k_sparse, dim=-1)
        sparse_scores = torch.zeros_like(scores)
        sparse_scores.scatter_(-1, topk_indices, topk_scores)
        attn = F.softmax(sparse_scores, dim=-1)
        out = torch.matmul(attn, v)
        
        torch.cuda.synchronize()
        sparse_time = time.time() - start
        sparse_memory = torch.cuda.max_memory_allocated() / 1024**3
        
        results['sparse']['time'].append(sparse_time)
        results['sparse']['memory'].append(sparse_memory)
        
        # 4. æ»‘åŠ¨çª—å£
        torch.cuda.reset_peak_memory_stats()
        start = time.time()
        
        window_size = min(256, seq_len)
        out_sliding = []
        
        for i in range(0, seq_len, window_size // 2):
            start_idx = max(0, i - window_size // 2)
            end_idx = min(seq_len, i + window_size)
            
            q_window = q[:, :, start_idx:end_idx]
            k_window = k[:, :, start_idx:end_idx]
            v_window = v[:, :, start_idx:end_idx]
            
            scores_window = torch.matmul(q_window, k_window.transpose(-2, -1)) / math.sqrt(q.size(-1))
            attn_window = F.softmax(scores_window, dim=-1)
            out_window = torch.matmul(attn_window, v_window)
            out_sliding.append(out_window)
        
        torch.cuda.synchronize()
        sliding_time = time.time() - start
        sliding_memory = torch.cuda.max_memory_allocated() / 1024**3
        
        results['sliding']['time'].append(sliding_time)
        results['sliding']['memory'].append(sliding_memory)
        
        # æ¸…ç†
        torch.cuda.empty_cache()
    
    # å¯è§†åŒ–ç»“æœ
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))
    
    for method in results:
        ax1.plot(seq_lengths, results[method]['memory'], marker='o', label=method)
        ax2.plot(seq_lengths, results[method]['time'], marker='s', label=method)
    
    ax1.set_xlabel('åºåˆ—é•¿åº¦')
    ax1.set_ylabel('å³°å€¼å†…å­˜ (GB)')
    ax1.set_title('å†…å­˜å ç”¨å¯¹æ¯”')
    ax1.legend()
    ax1.grid(True)
    
    ax2.set_xlabel('åºåˆ—é•¿åº¦')
    ax2.set_ylabel('æ¨ç†æ—¶é—´ (ç§’)')
    ax2.set_title('é€Ÿåº¦å¯¹æ¯”')
    ax2.legend()
    ax2.grid(True)
    
    plt.tight_layout()
    plt.show()
    
    return results

# è¿è¡ŒåŸºå‡†æµ‹è¯•
results = benchmark_attention_methods()

# åˆ†ææƒè¡¡
print("\n=== å†…å­˜-é€Ÿåº¦æƒè¡¡åˆ†æ ===")
for seq_len_idx, seq_len in enumerate([512, 1024, 2048, 4096]):
    print(f"\nåºåˆ—é•¿åº¦ {seq_len}:")
    base_memory = results['standard']['memory'][seq_len_idx]
    base_time = results['standard']['time'][seq_len_idx]
    
    for method in ['flash', 'sparse', 'sliding']:
        memory_save = (1 - results[method]['memory'][seq_len_idx] / base_memory) * 100
        speed_diff = (results[method]['time'][seq_len_idx] / base_time - 1) * 100
        
        print(f"  {method}: å†…å­˜èŠ‚çœ {memory_save:.1f}%, é€Ÿåº¦å˜åŒ– {speed_diff:+.1f}%")
```

å…¸å‹ç»“æœï¼š
- Flash Attentionï¼šå†…å­˜èŠ‚çœ 60-80%ï¼Œé€Ÿåº¦æå‡ 10-30%
- ç¨€ç–æ³¨æ„åŠ›ï¼šå†…å­˜èŠ‚çœ 30-50%ï¼Œé€Ÿåº¦ç•¥æœ‰ä¸‹é™
- æ»‘åŠ¨çª—å£ï¼šå†…å­˜èŠ‚çœ 70-90%ï¼Œé€Ÿåº¦ä¸‹é™ 20-40%

é€‰æ‹©å»ºè®®ï¼š
- é•¿åºåˆ—ï¼ˆ>2048ï¼‰ï¼šFlash Attention æœ€ä¼˜
- å†…å­˜æåº¦å—é™ï¼šæ»‘åŠ¨çª—å£
- éœ€è¦å…¨å±€ä¿¡æ¯ï¼šç¨€ç–æ³¨æ„åŠ›
</details>

**ç»ƒä¹  9.7**ï¼šå®ç°ä¸€ä¸ª VLM ä¸“ç”¨çš„å†…å­˜é¢„ç®—åˆ†é…å™¨ï¼Œç»™å®šæ€»æ˜¾å­˜é¢„ç®—ï¼Œè‡ªåŠ¨åˆ†é…ç»™ä¸åŒç»„ä»¶ã€‚

ğŸ’¡ **æç¤º**ï¼šè€ƒè™‘ç»„ä»¶ä¼˜å…ˆçº§ã€æœ€å°éœ€æ±‚ã€æ€§èƒ½å½±å“ã€‚

<details>
<summary>ğŸ“ å‚è€ƒç­”æ¡ˆ</summary>

```python
class VLMMemoryBudgetAllocator:
    def __init__(self, total_memory_gb, model_config):
        self.total_memory = total_memory_gb
        self.config = model_config
        
        # ç»„ä»¶ä¼˜å…ˆçº§ï¼ˆè¶Šå°è¶Šé‡è¦ï¼‰
        self.priorities = {
            'model_params': 1,
            'gradients': 2,
            'optimizer': 3,
            'vision_features': 4,
            'attention': 5,
            'activations': 6
        }
        
    def calculate_minimum_requirements(self):
        """è®¡ç®—å„ç»„ä»¶æœ€å°å†…å­˜éœ€æ±‚"""
        min_req = {}
        
        # æ¨¡å‹å‚æ•°ï¼ˆå¿…é¡»ï¼‰
        param_size = self.config.num_params * 2 / 1024**3  # FP16
        min_req['model_params'] = param_size
        
        # æ¢¯åº¦ï¼ˆå¦‚æœè®­ç»ƒï¼‰
        if self.config.training:
            min_req['gradients'] = param_size * self.config.trainable_ratio
        else:
            min_req['gradients'] = 0
            
        # ä¼˜åŒ–å™¨
        if self.config.optimizer == 'adam':
            min_req['optimizer'] = min_req['gradients'] * 2
        elif self.config.optimizer == 'sgd':
            min_req['optimizer'] = min_req['gradients']
        else:
            min_req['optimizer'] = 0
            
        # è§†è§‰ç‰¹å¾ï¼ˆæœ€å° batch=1ï¼‰
        vision_tokens = self.config.image_size ** 2 / self.config.patch_size ** 2
        min_req['vision_features'] = (
            vision_tokens * self.config.hidden_dim * 4 / 1024**3
        )
        
        # æ³¨æ„åŠ›ï¼ˆå¯ä»¥ç”¨ Flash Attention å‹ç¼©ï¼‰
        min_seq_len = 512  # æœ€å°åºåˆ—é•¿åº¦
        if self.config.use_flash_attention:
            min_req['attention'] = min_seq_len * self.config.hidden_dim * 4 / 1024**3
        else:
            min_req['attention'] = min_seq_len ** 2 * 4 / 1024**3
            
        # æ¿€æ´»å€¼ï¼ˆå¯ä»¥ç”¨ checkpoint å‹ç¼©ï¼‰
        if self.config.gradient_checkpointing:
            min_req['activations'] = param_size * 0.5
        else:
            min_req['activations'] = param_size * 2
            
        return min_req
        
    def allocate_budget(self):
        """åˆ†é…å†…å­˜é¢„ç®—"""
        min_requirements = self.calculate_minimum_requirements()
        total_min = sum(min_requirements.values())
        
        if total_min > self.total_memory:
            return self.emergency_allocation(min_requirements)
            
        # å‰©ä½™é¢„ç®—
        remaining = self.total_memory - total_min
        
        # åˆå§‹åˆ†é…ï¼ˆæ»¡è¶³æœ€å°éœ€æ±‚ï¼‰
        allocation = min_requirements.copy()
        
        # æŒ‰ä¼˜å…ˆçº§åˆ†é…å‰©ä½™å†…å­˜
        sorted_components = sorted(
            min_requirements.keys(), 
            key=lambda x: self.priorities[x]
        )
        
        # è®¡ç®—æƒé‡
        weights = {}
        total_weight = 0
        for comp in sorted_components:
            weight = 1.0 / self.priorities[comp]
            weights[comp] = weight
            total_weight += weight
            
        # åˆ†é…å‰©ä½™å†…å­˜
        for comp in sorted_components:
            extra = remaining * (weights[comp] / total_weight)
            allocation[comp] += extra
            
        return self.optimize_allocation(allocation)
        
    def emergency_allocation(self, min_requirements):
        """ç´§æ€¥æ¨¡å¼ï¼šå†…å­˜ä¸è¶³æ—¶çš„åˆ†é…ç­–ç•¥"""
        allocation = {}
        available = self.total_memory
        
        # 1. å¿…é¡»æ»¡è¶³æ¨¡å‹å‚æ•°
        allocation['model_params'] = min_requirements['model_params']
        available -= allocation['model_params']
        
        if available <= 0:
            raise MemoryError("æ˜¾å­˜ä¸è¶³ä»¥åŠ è½½æ¨¡å‹ï¼")
            
        # 2. å¯ç”¨æ‰€æœ‰å†…å­˜ä¼˜åŒ–
        self.config.gradient_checkpointing = True
        self.config.use_flash_attention = True
        self.config.mixed_precision = True
        
        # 3. æœ€å°åŒ–å…¶ä»–ç»„ä»¶
        if self.config.training:
            # ä½¿ç”¨ Adafactor æˆ– 8-bit Adam
            allocation['optimizer'] = min_requirements['gradients'] * 0.5
            allocation['gradients'] = min_requirements['gradients']
            available -= allocation['optimizer'] + allocation['gradients']
        else:
            allocation['optimizer'] = 0
            allocation['gradients'] = 0
            
        # 4. æé™å‹ç¼©æ¿€æ´»å€¼
        allocation['activations'] = min(available * 0.3, min_requirements['model_params'] * 0.3)
        available -= allocation['activations']
        
        # 5. åˆ†é…å‰©ä½™ç»™è§†è§‰å’Œæ³¨æ„åŠ›
        allocation['vision_features'] = available * 0.4
        allocation['attention'] = available * 0.6
        
        return allocation
        
    def optimize_allocation(self, allocation):
        """ä¼˜åŒ–åˆ†é…ä»¥æœ€å¤§åŒ–æ€§èƒ½"""
        optimized = allocation.copy()
        
        # è§„åˆ™ 1ï¼šå¦‚æœ batch size å¯ä»¥ç¿»å€ï¼Œé‡æ–°åˆ†é…
        vision_memory = allocation['vision_features']
        if vision_memory > self.config.min_vision_memory * 2:
            # å¯ä»¥æ”¯æŒ batch size = 2
            extra = vision_memory - self.config.min_vision_memory * 2
            # å°†å¤šä½™çš„åˆ†é…ç»™æ³¨æ„åŠ›
            optimized['attention'] += extra * 0.5
            optimized['activations'] += extra * 0.5
            
        # è§„åˆ™ 2ï¼šå¹³è¡¡æ¿€æ´»å€¼å’Œæ³¨æ„åŠ›
        ratio = optimized['activations'] / optimized['attention']
        if ratio > 3:
            # æ¿€æ´»å€¼è¿‡å¤šï¼Œé‡æ–°å¹³è¡¡
            diff = (optimized['activations'] - optimized['attention'] * 2) / 2
            optimized['activations'] -= diff
            optimized['attention'] += diff
            
        return optimized
        
    def get_training_config(self, allocation):
        """æ ¹æ®å†…å­˜åˆ†é…ç”Ÿæˆè®­ç»ƒé…ç½®"""
        config = {}
        
        # Batch size
        vision_batch = int(allocation['vision_features'] / self.config.min_vision_memory)
        config['batch_size'] = max(1, vision_batch)
        
        # åºåˆ—é•¿åº¦
        if self.config.use_flash_attention:
            # Flash Attention: O(L) å†…å­˜
            max_seq_len = int(allocation['attention'] * 1024**3 / 
                            (self.config.hidden_dim * 4))
        else:
            # æ ‡å‡†æ³¨æ„åŠ›: O(L^2) å†…å­˜
            max_seq_len = int(math.sqrt(allocation['attention'] * 1024**3 / 4))
        
        config['max_seq_length'] = min(max_seq_len, 4096)
        
        # æ¢¯åº¦ç´¯ç§¯
        if config['batch_size'] < self.config.target_batch_size:
            config['accumulation_steps'] = self.config.target_batch_size // config['batch_size']
        else:
            config['accumulation_steps'] = 1
            
        # å†…å­˜ä¼˜åŒ–è®¾ç½®
        config['gradient_checkpointing'] = allocation['activations'] < self.config.num_params * 1.5 / 1024**3
        config['mixed_precision'] = True
        config['use_flash_attention'] = self.config.use_flash_attention
        
        return config

# ä½¿ç”¨ç¤ºä¾‹
model_config = {
    'num_params': 7e9,  # 7B å‚æ•°
    'trainable_ratio': 1.0,  # å…¨é‡å¾®è°ƒ
    'training': True,
    'optimizer': 'adam',
    'image_size': 336,
    'patch_size': 14,
    'hidden_dim': 4096,
    'use_flash_attention': True,
    'gradient_checkpointing': False,
    'target_batch_size': 32,
    'min_vision_memory': 0.5  # GB
}

# 24GB æ˜¾å­˜ï¼ˆå¦‚ RTX 3090ï¼‰
allocator = VLMMemoryBudgetAllocator(24, model_config)
allocation = allocator.allocate_budget()
training_config = allocator.get_training_config(allocation)

print("=== å†…å­˜åˆ†é…æ–¹æ¡ˆ ===")
for component, memory in allocation.items():
    print(f"{component}: {memory:.2f} GB")
    
print("\n=== æ¨èè®­ç»ƒé…ç½® ===")
for key, value in training_config.items():
    print(f"{key}: {value}")
```

è¿™ä¸ªåˆ†é…å™¨å¯ä»¥æ ¹æ®ç¡¬ä»¶è‡ªåŠ¨ä¼˜åŒ–è®­ç»ƒé…ç½®ï¼Œé¿å…æ‰‹åŠ¨è¯•é”™ã€‚
</details>

**ç»ƒä¹  9.8**ï¼šåˆ†æçœŸå® VLM è®­ç»ƒæ—¥å¿—ï¼Œè¯Šæ–­å†…å­˜æ³„æ¼é—®é¢˜ã€‚

ç»™å®šä»¥ä¸‹è®­ç»ƒæ—¥å¿—ç‰‡æ®µï¼š
```
Step 100: Loss=2.34, Memory=18.2GB
Step 200: Loss=2.11, Memory=18.5GB  
Step 300: Loss=1.98, Memory=18.9GB
Step 400: Loss=1.87, Memory=19.4GB
Step 500: Loss=1.76, Memory=20.1GB
Step 600: Loss=1.65, Memory=20.9GB
Step 700: Loss=1.54, Memory=21.8GB
Step 800: Loss=1.43, Memory=22.9GB
Step 900: Loss=1.32, Memory=24.2GB
Step 1000: CUDA OOM
```

ğŸ’¡ **æç¤º**ï¼šè®¡ç®—å†…å­˜å¢é•¿ç‡ï¼Œåˆ†æå¯èƒ½çš„æ³„æ¼æºã€‚

<details>
<summary>ğŸ“ å‚è€ƒç­”æ¡ˆ</summary>

åˆ†æè¿‡ç¨‹ï¼š

1. **å†…å­˜å¢é•¿æ¨¡å¼**ï¼š
   - åˆå§‹ï¼š18.2 GB
   - æœ€ç»ˆï¼š24.2 GBï¼ˆOOM å‰ï¼‰
   - æ€»å¢é•¿ï¼š6.0 GB
   - å¹³å‡æ¯ 100 æ­¥å¢é•¿ï¼š0.67 GB
   - å¢é•¿ç‡ï¼šçº¿æ€§å¢é•¿ï¼Œéå¯¹æ•°å¢é•¿

2. **å¢é•¿é€Ÿåº¦åˆ†æ**ï¼š
   ```python
   steps = [100, 200, 300, 400, 500, 600, 700, 800, 900]
   memory = [18.2, 18.5, 18.9, 19.4, 20.1, 20.9, 21.8, 22.9, 24.2]
   
   growth_rates = []
   for i in range(1, len(memory)):
       rate = (memory[i] - memory[i-1]) / 100  # GB per step
       growth_rates.append(rate * 1000)  # MB per step
   
   print("æ¯ 100 æ­¥å†…å­˜å¢é•¿ï¼ˆMBï¼‰:")
   for i, rate in enumerate(growth_rates):
       print(f"Step {(i+1)*100}-{(i+2)*100}: {rate:.1f} MB")
   ```
   
   è¾“å‡ºï¼š
   ```
   Step 100-200: 3.0 MB
   Step 200-300: 4.0 MB
   Step 300-400: 5.0 MB
   Step 400-500: 7.0 MB
   Step 500-600: 8.0 MB
   Step 600-700: 9.0 MB
   Step 700-800: 11.0 MB
   Step 800-900: 13.0 MB
   ```
   
   **å‘ç°**ï¼šå¢é•¿é€Ÿåº¦åœ¨åŠ é€Ÿï¼

3. **å¯èƒ½çš„æ³„æ¼æº**ï¼š

   a) **æ¢¯åº¦ç´¯ç§¯æœªæ¸…ç†**ï¼š
   ```python
   # é”™è¯¯ä»£ç 
   total_loss = 0
   for batch in dataloader:
       loss = model(batch)
       total_loss += loss  # å±é™©ï¼ä¿ç•™è®¡ç®—å›¾
       loss.backward()
   ```
   
   b) **åˆ—è¡¨ç´¯ç§¯å¼ é‡**ï¼š
   ```python
   # é”™è¯¯ä»£ç 
   losses = []
   for step in range(1000):
       loss = train_step()
       losses.append(loss)  # åº”è¯¥ç”¨ loss.item()
   ```
   
   c) **Hook æœªé‡Šæ”¾**ï¼š
   ```python
   # é”™è¯¯ä»£ç 
   def register_hooks(model):
       for layer in model.layers:
           layer.register_forward_hook(save_activation)
   # è®­ç»ƒç»“æŸåæœªè°ƒç”¨ handle.remove()
   ```
   
   d) **åŠ¨æ€å›¾åƒå¤§å°å¯¼è‡´ç¼“å­˜ç´¯ç§¯**ï¼š
   ```python
   # VLM ç‰¹æœ‰é—®é¢˜
   image_cache = {}
   for batch in dataloader:
       h, w = batch['image'].shape[-2:]
       key = f"{h}x{w}"
       if key not in image_cache:
           image_cache[key] = process_image(batch['image'])
       # ç¼“å­˜æ— é™å¢é•¿ï¼
   ```

4. **è¯Šæ–­ä»£ç **ï¼š
   ```python
   def diagnose_memory_leak(model, dataloader):
       import gc
       import weakref
       
       # è·Ÿè¸ªå¼ é‡å¼•ç”¨
       tensors_before = set()
       for obj in gc.get_objects():
           if torch.is_tensor(obj) and obj.is_cuda:
               tensors_before.add(weakref.ref(obj))
       
       # è¿è¡Œ 10 æ­¥
       for i, batch in enumerate(dataloader):
           if i >= 10:
               break
           loss = train_step(model, batch)
           
       # æ£€æŸ¥æ–°å¢å¼ é‡
       tensors_after = set()
       leaked_tensors = []
       
       for obj in gc.get_objects():
           if torch.is_tensor(obj) and obj.is_cuda:
               ref = weakref.ref(obj)
               tensors_after.add(ref)
               
               if ref not in tensors_before:
                   # æ–°å¢å¼ é‡
                   size_mb = obj.numel() * obj.element_size() / 1024**2
                   if size_mb > 1:  # åªå…³æ³¨ > 1MB çš„å¼ é‡
                       leaked_tensors.append({
                           'shape': obj.shape,
                           'dtype': obj.dtype,
                           'size_mb': size_mb,
                           'requires_grad': obj.requires_grad,
                           'grad_fn': obj.grad_fn is not None
                       })
       
       # æŒ‰å¤§å°æ’åº
       leaked_tensors.sort(key=lambda x: x['size_mb'], reverse=True)
       
       print(f"å‘ç° {len(leaked_tensors)} ä¸ªå¯ç–‘å¼ é‡")
       for tensor_info in leaked_tensors[:5]:
           print(f"  {tensor_info}")
       
       return leaked_tensors
   ```

5. **ä¿®å¤æ–¹æ¡ˆ**ï¼š
   ```python
   # ä¿®å¤ 1ï¼šä½¿ç”¨ .item() è·å–æ ‡é‡
   total_loss += loss.item()
   
   # ä¿®å¤ 2ï¼šå®šæœŸæ¸…ç†ç¼“å­˜
   if step % 100 == 0:
       torch.cuda.empty_cache()
       
   # ä¿®å¤ 3ï¼šä½¿ç”¨ with torch.no_grad()
   with torch.no_grad():
       metrics = evaluate(model, val_loader)
       
   # ä¿®å¤ 4ï¼šé™åˆ¶ç¼“å­˜å¤§å°
   from functools import lru_cache
   
   @lru_cache(maxsize=10)
   def cached_process_image(h, w):
       return process_image_size(h, w)
   ```

**ç»“è®º**ï¼šè¯¥æ—¥å¿—æ˜¾ç¤ºå…¸å‹çš„æ¢¯åº¦/æ¿€æ´»å€¼ç´¯ç§¯æ³„æ¼ï¼Œæ¯æ­¥æ³„æ¼çº¦ 6-13 MBï¼Œéœ€è¦æ£€æŸ¥è®­ç»ƒå¾ªç¯ä¸­çš„å¼ é‡å¼•ç”¨ã€‚
</details>

## å¸¸è§é™·é˜±ä¸é”™è¯¯ (Gotchas)

### 1. æ··æ·† allocated å’Œ reserved å†…å­˜

```python
# é”™è¯¯ç†è§£
print(f"å·²ç”¨å†…å­˜: {torch.cuda.memory_reserved()}")  # é”™ï¼è¿™æ˜¯é¢„ç•™çš„

# æ­£ç¡®
print(f"å®é™…ä½¿ç”¨: {torch.cuda.memory_allocated()}")
print(f"PyTorch é¢„ç•™: {torch.cuda.memory_reserved()}")
print(f"å¯ç”¨äºåˆ†é…: {torch.cuda.memory_reserved() - torch.cuda.memory_allocated()}")
```

### 2. å¿½è§† VLM çš„äºŒæ¬¡å¤æ‚åº¦

```python
# å±é™©ï¼šæ³¨æ„åŠ›å†…å­˜æ˜¯ O(L^2)
seq_len = 4096
memory_gb = (seq_len ** 2 * 4) / 1024**3  # 64 MB ä»…å•ä¸ªå¤´ï¼

# å®‰å…¨ï¼šä½¿ç”¨ Flash Attention æˆ–åˆ†å—
```

### 3. åŠ¨æ€å›¾åƒå¤§å°çš„é™·é˜±

```python
# é—®é¢˜ï¼šbatch ä¸­ä¸€å¼ å¤§å›¾å¯¼è‡´æ•´ä½“ OOM
images = [img1_224x224, img2_224x224, img3_1024x1024]  # ç¬¬ä¸‰å¼ å¯¼è‡´ OOM

# è§£å†³ï¼šé¢„å…ˆæ’åºå’Œåˆ†ç»„
images.sort(key=lambda x: x.shape[-2] * x.shape[-1])
```

### 4. æ¢¯åº¦æ£€æŸ¥ç‚¹çš„è¯¯ç”¨

```python
# é”™è¯¯ï¼šå¯¹å°æ¨¡å‹ä½¿ç”¨åè€Œæ›´æ…¢
tiny_model.gradient_checkpointing_enable()  # 2 å±‚æ¨¡å‹ï¼Œæ”¶ç›Šä¸ºè´Ÿ

# æ­£ç¡®ï¼šåªå¯¹æ·±å±‚æ¨¡å‹ä½¿ç”¨
if model.num_layers >= 12:
    model.gradient_checkpointing_enable()
```

### 5. ä¼˜åŒ–å™¨çŠ¶æ€çš„é—å¿˜

```python
# å®¹æ˜“å¿½è§†ï¼šAdam éœ€è¦ 2 å€å‚æ•°å†…å­˜
# 7B æ¨¡å‹ + Adam = 14GB (å‚æ•°) + 14GB (æ¢¯åº¦) + 28GB (ä¼˜åŒ–å™¨) = 56GBï¼

# è€ƒè™‘ä½¿ç”¨ 8-bit Adam æˆ– Adafactor
```

### 6. CPU-GPU ä¼ è¾“ç“¶é¢ˆ

```python
# æ…¢ï¼šé¢‘ç¹çš„å°æ‰¹é‡ä¼ è¾“
for img in images:
    img = img.cuda()  # æ¯æ¬¡ä¼ è¾“å¼€é”€å¤§

# å¿«ï¼šæ‰¹é‡ä¼ è¾“
images = torch.stack(images).cuda()  # ä¸€æ¬¡ä¼ è¾“
```

### 7. å†…å­˜ç¢ç‰‡åŒ–

```python
# å¯¼è‡´ç¢ç‰‡åŒ–ï¼šé¢‘ç¹åˆ†é…ä¸åŒå¤§å°
for size in [100, 1000, 10, 10000, 1]:
    tensor = torch.randn(size).cuda()

# ç›‘æ§ç¢ç‰‡åŒ–
fragmentation = 1 - (torch.cuda.memory_allocated() / torch.cuda.memory_reserved())
if fragmentation > 0.3:
    torch.cuda.empty_cache()  # æ•´ç†å†…å­˜
```

### 8. å¤šè¿›ç¨‹è®­ç»ƒçš„å†…å­˜é‡å¤

```python
# é”™è¯¯ï¼šæ¯ä¸ªè¿›ç¨‹éƒ½åŠ è½½å®Œæ•´æ¨¡å‹
model = load_model()  # æ¯ä¸ª GPU éƒ½æœ‰å®Œæ•´å‰¯æœ¬

# æ­£ç¡®ï¼šä½¿ç”¨ DDP æˆ– FSDP
model = FSDP(model, sharding_strategy=ShardingStrategy.FULL_SHARD)
```

## æœ€ä½³å®è·µæ£€æŸ¥æ¸…å•

### è®­ç»ƒå‰æ£€æŸ¥

- [ ] **è®¡ç®—å†…å­˜éœ€æ±‚**
  - [ ] æ¨¡å‹å‚æ•°å†…å­˜
  - [ ] æ¢¯åº¦å†…å­˜ï¼ˆè€ƒè™‘å†»ç»“å±‚ï¼‰
  - [ ] ä¼˜åŒ–å™¨çŠ¶æ€å†…å­˜
  - [ ] ä¼°ç®—æ¿€æ´»å€¼å†…å­˜

- [ ] **é€‰æ‹©åˆé€‚çš„ä¼˜åŒ–å™¨**
  - [ ] å†…å­˜å……è¶³ï¼šAdamW
  - [ ] å†…å­˜ç´§å¼ ï¼šAdafactor æˆ– 8-bit Adam
  - [ ] æåº¦å—é™ï¼šSGD with momentum

- [ ] **é…ç½®å†…å­˜ä¼˜åŒ–**
  - [ ] å¯ç”¨æ··åˆç²¾åº¦ï¼ˆFP16/BF16ï¼‰
  - [ ] è€ƒè™‘ gradient checkpointing
  - [ ] è¯„ä¼° Flash Attention é€‚ç”¨æ€§

- [ ] **æ•°æ®åŠ è½½ä¼˜åŒ–**
  - [ ] è®¾ç½®åˆç†çš„ num_workers
  - [ ] ä½¿ç”¨ pin_memory=True
  - [ ] é¢„å¤„ç†å›¾åƒå°ºå¯¸

### è®­ç»ƒä¸­ç›‘æ§

- [ ] **å®æ—¶å†…å­˜ç›‘æ§**
  - [ ] æ¯ N æ­¥è®°å½•å†…å­˜ä½¿ç”¨
  - [ ] ç›‘æ§å†…å­˜å¢é•¿è¶‹åŠ¿
  - [ ] è®¾ç½® OOM é¢„è­¦é˜ˆå€¼ï¼ˆå¦‚ 90%ï¼‰

- [ ] **æ€§èƒ½æŒ‡æ ‡è·Ÿè¸ª**
  - [ ] GPU åˆ©ç”¨ç‡
  - [ ] å†…å­˜ç¢ç‰‡ç‡
  - [ ] æ•°æ®åŠ è½½æ—¶é—´å æ¯”

- [ ] **å¼‚å¸¸å¤„ç†**
  - [ ] OOM è‡ªåŠ¨æ¢å¤æœºåˆ¶
  - [ ] åŠ¨æ€ batch size è°ƒæ•´
  - [ ] Checkpoint ä¿å­˜ç­–ç•¥

### è°ƒè¯• OOM æ—¶

- [ ] **å¿«é€Ÿè¯Šæ–­**ï¼ˆ30 ç§’å†…ï¼‰
  - [ ] è¿è¡Œ nvidia-smi æŸ¥çœ‹æ€»ä½“å ç”¨
  - [ ] æ‰“å° torch.cuda.memory_summary()
  - [ ] æ£€æŸ¥ batch size å’Œåºåˆ—é•¿åº¦
  - [ ] ç¡®è®¤æ˜¯å¦å¼€å¯äº†ä¼˜åŒ–

- [ ] **æ·±åº¦åˆ†æ**ï¼ˆ5 åˆ†é’Ÿå†…ï¼‰
  - [ ] ä½¿ç”¨ Profiler å®šä½å†…å­˜çƒ­ç‚¹
  - [ ] æ£€æŸ¥æ˜¯å¦æœ‰å†…å­˜æ³„æ¼
  - [ ] åˆ†ææ³¨æ„åŠ›çŸ©é˜µå¤§å°
  - [ ] éªŒè¯å›¾åƒåˆ†è¾¨ç‡

- [ ] **ä¼˜åŒ–æªæ–½**ï¼ˆæŒ‰ä¼˜å…ˆçº§ï¼‰
  1. å‡å° batch size
  2. å¯ç”¨ gradient checkpointing
  3. ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒ
  4. å¯ç”¨ Flash Attention
  5. å†»ç»“éƒ¨åˆ†å±‚
  6. ä½¿ç”¨ CPU offloading
  7. åˆ‡æ¢åˆ°æ¨¡å‹å¹¶è¡Œ

### ä¼˜åŒ–éªŒè¯

- [ ] **å†…å­˜ä¼˜åŒ–æ•ˆæœ**
  - [ ] å³°å€¼å†…å­˜é™ä½ %
  - [ ] å¯æ”¯æŒçš„æœ€å¤§ batch size
  - [ ] è®­ç»ƒé€Ÿåº¦å˜åŒ–

- [ ] **æ¨¡å‹è´¨é‡æ£€æŸ¥**
  - [ ] æŸå¤±æ”¶æ•›æ­£å¸¸
  - [ ] éªŒè¯é›†æŒ‡æ ‡ç¨³å®š
  - [ ] æ— æ•°å€¼æº¢å‡º/ä¸‹æº¢

- [ ] **ç¨³å®šæ€§æµ‹è¯•**
  - [ ] é•¿æ—¶é—´è®­ç»ƒæ—  OOM
  - [ ] æ— å†…å­˜æ³„æ¼
  - [ ] æ¢å¤è®­ç»ƒæ­£å¸¸

é€šè¿‡ç³»ç»Ÿåœ°æ‰§è¡Œè¿™ä¸ªæ£€æŸ¥æ¸…å•ï¼Œå¯ä»¥æœ‰æ•ˆé¢„é˜²å’Œè§£å†³ VLM è®­ç»ƒä¸­çš„å†…å­˜é—®é¢˜ï¼Œç¡®ä¿è®­ç»ƒé¡ºåˆ©è¿›è¡Œã€‚
