<!DOCTYPE html>
<html lang="zh">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <base href="./">
    <title>第 10 章：训练崩溃与 NaN 问题</title>
    <link rel="stylesheet" href="assets/style.css">
    <link rel="stylesheet" href="assets/highlight.css">
    <script src="assets/script.js" defer></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>
        window.MathJax = {
            tex: {
                inlineMath: [['$', '$']],
                displayMath: [['$$', '$$']],
                processEscapes: false,
                packages: {'[+]': ['noerrors', 'ams']}
            },
            options: {
                ignoreHtmlClass: 'tex2jax_ignore',
                processHtmlClass: 'tex2jax_process'
            },
            loader: {
                load: ['[tex]/noerrors', '[tex]/ams']
            }
        };
    </script>
</head>
<body>
    <div class="container">
        <nav id="sidebar" class="sidebar">
            <div class="sidebar-header">
                <h3>目录</h3>
                <button id="sidebar-toggle" class="sidebar-toggle">
                    <span></span>
                    <span></span>
                    <span></span>
                </button>
            </div>
            <div class="sidebar-search">
                <input type="text" id="sidebar-search-input" placeholder="搜索..." autocomplete="off">
            </div>
            <div id="tree-container">
                <nav class="tree-nav" role="tree">
                    <div class="tree-item " >
                        <a href="index.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">视觉语言模型（VLM）的监督微调与强化学习实战教程</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter1.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第 1 章：VLM 架构与原理</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter2.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第 2 章：数据准备与预处理</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter3.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第 3 章：SFT 训练策略</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter4.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第 4 章：分布式训练与优化</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter5.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第 5 章：RLHF 基础与实现</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter6.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第 6 章：直接偏好优化（DPO）</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter7.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第 7 章：评估体系设计</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter8.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第 8 章：模型部署与服务化</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter9.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第 9 章：CUDA OOM 调试完全指南</span>
                        </a>
                    </div>
                
                    <div class="tree-item active" >
                        <a href="chapter10.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第 10 章：训练崩溃与 NaN 问题</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter11.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第 11 章：训练速度优化实战</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter12.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第 12 章：多机多卡调试地狱</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="CLAUDE.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">Untitled</span>
                        </a>
                    </div>
                </nav>
            </div>
        </nav>
        
        <main class="content">
            <article>
                <h1 id="10-nan">第 10 章：训练崩溃与 NaN 问题</h1>
<p>训练过程中突然出现 Loss 爆炸或 NaN，是每个 VLM 工程师的噩梦。一个原本正常运行的训练，可能在几个 step 内彻底崩溃，浪费数天的计算资源。本章将系统介绍训练不稳定的根本原因、快速诊断方法，以及经过实战检验的解决方案。我们将学习如何在 5 分钟内定位问题，掌握混合精度训练的稳定性技巧，并建立完善的容错机制。</p>
<h2 id="101-loss-5">10.1 Loss 爆炸的 5 分钟排查流程</h2>
<p>当训练 Loss 突然飙升或出现 NaN 时，时间就是金钱。以下是经过大量实践总结的快速诊断流程：</p>
<h3 id="1011-30">10.1.1 第一步：立即保存现场（30 秒）</h3>
<div class="codehilite"><pre><span></span><code><span class="c1"># 紧急保存当前状态</span>
<span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">({</span>
    <span class="s1">&#39;step&#39;</span><span class="p">:</span> <span class="n">current_step</span><span class="p">,</span>
    <span class="s1">&#39;model_state&#39;</span><span class="p">:</span> <span class="n">model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span>
    <span class="s1">&#39;optimizer_state&#39;</span><span class="p">:</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span>
    <span class="s1">&#39;loss_history&#39;</span><span class="p">:</span> <span class="n">loss_history</span><span class="p">[</span><span class="o">-</span><span class="mi">100</span><span class="p">:],</span>  <span class="c1"># 最近100个step的loss</span>
    <span class="s1">&#39;grad_norm_history&#39;</span><span class="p">:</span> <span class="n">grad_norm_history</span><span class="p">[</span><span class="o">-</span><span class="mi">100</span><span class="p">:],</span>
<span class="p">},</span> <span class="sa">f</span><span class="s1">&#39;debug_checkpoint_step_</span><span class="si">{</span><span class="n">current_step</span><span class="si">}</span><span class="s1">.pt&#39;</span><span class="p">)</span>
</code></pre></div>

<h3 id="1012-loss-1">10.1.2 第二步：检查 Loss 曲线模式（1 分钟）</h3>
<p>Loss 爆炸通常有三种模式，每种对应不同的原因：</p>
<div class="codehilite"><pre><span></span><code>模式 1: 突然跳跃
Loss: 2.1 → 2.0 → 1.9 → 8734.5 → NaN
原因: 单个异常样本或数值溢出

模式 2: 指数增长
Loss: 2.1 → 2.3 → 2.8 → 4.5 → 12.3 → 89.7 → NaN
原因: 学习率过大或梯度累积错误

模式 3: 震荡发散
Loss: 2.1 → 1.8 → 2.5 → 1.6 → 3.2 → 1.4 → 5.8 → NaN
原因: 优化器状态损坏或数值不稳定
</code></pre></div>

<h3 id="1013-2">10.1.3 第三步：定位问题层级（2 分钟）</h3>
<p>使用以下代码快速定位问题发生的层级：</p>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">check_model_health</span><span class="p">(</span><span class="n">model</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;快速检查模型各层的健康状态&quot;&quot;&quot;</span>
    <span class="n">issues</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">():</span>
        <span class="c1"># 检查参数本身</span>
        <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">isnan</span><span class="p">(</span><span class="n">param</span><span class="p">)</span><span class="o">.</span><span class="n">any</span><span class="p">():</span>
            <span class="n">issues</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;NaN in parameter: </span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">isinf</span><span class="p">(</span><span class="n">param</span><span class="p">)</span><span class="o">.</span><span class="n">any</span><span class="p">():</span>
            <span class="n">issues</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Inf in parameter: </span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

        <span class="c1"># 检查梯度</span>
        <span class="k">if</span> <span class="n">param</span><span class="o">.</span><span class="n">grad</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">isnan</span><span class="p">(</span><span class="n">param</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span><span class="o">.</span><span class="n">any</span><span class="p">():</span>
                <span class="n">issues</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;NaN in gradient: </span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">isinf</span><span class="p">(</span><span class="n">param</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span><span class="o">.</span><span class="n">any</span><span class="p">():</span>
                <span class="n">issues</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Inf in gradient: </span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

            <span class="c1"># 检查梯度范数</span>
            <span class="n">grad_norm</span> <span class="o">=</span> <span class="n">param</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">norm</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
            <span class="k">if</span> <span class="n">grad_norm</span> <span class="o">&gt;</span> <span class="mi">1000</span><span class="p">:</span>
                <span class="n">issues</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Large gradient norm (</span><span class="si">{</span><span class="n">grad_norm</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">): </span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">issues</span>
</code></pre></div>

<h3 id="1014-15">10.1.4 第四步：检查关键数值（1.5 分钟）</h3>
<p>VLM 训练中最容易出问题的数值计算：</p>
<ol>
<li><strong>注意力分数</strong>：</li>
</ol>
<div class="codehilite"><pre><span></span><code><span class="c1"># 检查注意力权重</span>
<span class="n">attention_weights</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">scores</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">d_k</span><span class="p">),</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="k">if</span> <span class="p">(</span><span class="n">attention_weights</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">any</span><span class="p">():</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;警告：出现全零注意力权重（数值下溢）&quot;</span><span class="p">)</span>
<span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">isnan</span><span class="p">(</span><span class="n">attention_weights</span><span class="p">)</span><span class="o">.</span><span class="n">any</span><span class="p">():</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;警告：注意力权重包含 NaN&quot;</span><span class="p">)</span>
</code></pre></div>

<ol start="2">
<li><strong>损失函数中的 log 操作</strong>：</li>
</ol>
<div class="codehilite"><pre><span></span><code><span class="c1"># 添加数值稳定性</span>
<span class="n">logits</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
<span class="c1"># 错误：可能导致 log(0)</span>
<span class="n">loss</span> <span class="o">=</span> <span class="o">-</span><span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">probs</span><span class="p">[</span><span class="n">target</span><span class="p">])</span>
<span class="c1"># 正确：添加 epsilon</span>
<span class="n">loss</span> <span class="o">=</span> <span class="o">-</span><span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">probs</span><span class="p">[</span><span class="n">target</span><span class="p">]</span> <span class="o">+</span> <span class="mf">1e-8</span><span class="p">)</span>
</code></pre></div>

<ol start="3">
<li><strong>LayerNorm 的除法</strong>：</li>
</ol>
<div class="codehilite"><pre><span></span><code><span class="c1"># 检查 LayerNorm 是否稳定</span>
<span class="k">def</span> <span class="nf">stable_layer_norm</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">):</span>
    <span class="n">mean</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">var</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">unbiased</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="c1"># 确保方差不为零</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">mean</span><span class="p">)</span> <span class="o">/</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">var</span> <span class="o">+</span> <span class="n">eps</span><span class="p">)</span>
</code></pre></div>

<h3 id="1015">10.1.5 紧急处理决策树</h3>
<div class="codehilite"><pre><span></span><code><span class="nx">发现</span><span class="w"> </span><span class="nx">Loss</span><span class="w"> </span><span class="nx">爆炸</span><span class="o">/</span><span class="nx">NaN</span>
<span class="err">│</span>
<span class="err">├─</span><span class="w"> </span><span class="nx">是否在前</span><span class="w"> </span><span class="mi">1000</span><span class="w"> </span><span class="nx">步内</span><span class="err">？</span>
<span class="err">│</span><span class="w">  </span><span class="err">├─</span><span class="w"> </span><span class="nx">是</span><span class="w"> </span><span class="err">→</span><span class="w"> </span><span class="nx">检查初始化和学习率预热</span>
<span class="err">│</span><span class="w">  </span><span class="err">└─</span><span class="w"> </span><span class="nx">否</span><span class="w"> </span><span class="err">→</span><span class="w"> </span><span class="nx">继续诊断</span>
<span class="err">│</span>
<span class="err">├─</span><span class="w"> </span><span class="nx">是否使用混合精度</span><span class="err">？</span>
<span class="err">│</span><span class="w">  </span><span class="err">├─</span><span class="w"> </span><span class="nx">是</span><span class="w"> </span><span class="err">→</span><span class="w"> </span><span class="nx">检查</span><span class="w"> </span><span class="nx">loss</span><span class="w"> </span><span class="nx">scaling</span><span class="w"> </span><span class="nx">和</span><span class="w"> </span><span class="nx">dtype</span><span class="w"> </span><span class="nx">转换</span>
<span class="err">│</span><span class="w">  </span><span class="err">└─</span><span class="w"> </span><span class="nx">否</span><span class="w"> </span><span class="err">→</span><span class="w"> </span><span class="nx">检查数值溢出</span>
<span class="err">│</span>
<span class="err">├─</span><span class="w"> </span><span class="nx">是否有异常大的梯度</span><span class="err">？</span>
<span class="err">│</span><span class="w">  </span><span class="err">├─</span><span class="w"> </span><span class="nx">是</span><span class="w"> </span><span class="err">→</span><span class="w"> </span><span class="nx">降低学习率或增强</span><span class="w"> </span><span class="nx">gradient</span><span class="w"> </span><span class="nx">clipping</span>
<span class="err">│</span><span class="w">  </span><span class="err">└─</span><span class="w"> </span><span class="nx">否</span><span class="w"> </span><span class="err">→</span><span class="w"> </span><span class="nx">检查数据和损失函数</span>
<span class="err">│</span>
<span class="err">└─</span><span class="w"> </span><span class="nx">是否可以从</span><span class="w"> </span><span class="nx">checkpoint</span><span class="w"> </span><span class="nx">恢复</span><span class="err">？</span>
<span class="w">   </span><span class="err">├─</span><span class="w"> </span><span class="nx">是</span><span class="w"> </span><span class="err">→</span><span class="w"> </span><span class="nx">调整超参数后恢复训练</span>
<span class="w">   </span><span class="err">└─</span><span class="w"> </span><span class="nx">否</span><span class="w"> </span><span class="err">→</span><span class="w"> </span><span class="nx">降级到更保守的配置重新开始</span>
</code></pre></div>

<h2 id="102">10.2 梯度监控与异常值定位</h2>
<h3 id="1021">10.2.1 实时梯度监控系统</h3>
<p>建立完善的梯度监控是预防训练崩溃的第一道防线：</p>
<div class="codehilite"><pre><span></span><code><span class="k">class</span> <span class="nc">GradientMonitor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;梯度监控器，实时跟踪梯度统计信息&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">logger</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">model</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">logger</span> <span class="o">=</span> <span class="n">logger</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">history</span> <span class="o">=</span> <span class="n">defaultdict</span><span class="p">(</span><span class="nb">list</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">anomaly_threshold</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s1">&#39;max_norm&#39;</span><span class="p">:</span> <span class="mf">100.0</span><span class="p">,</span>
            <span class="s1">&#39;min_norm&#39;</span><span class="p">:</span> <span class="mf">1e-8</span><span class="p">,</span>
            <span class="s1">&#39;nan_tolerance&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
        <span class="p">}</span>

    <span class="k">def</span> <span class="nf">check_gradients</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">step</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;检查当前步的梯度健康状态&quot;&quot;&quot;</span>
        <span class="n">alerts</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">param</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">():</span>
            <span class="k">if</span> <span class="n">param</span><span class="o">.</span><span class="n">grad</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">continue</span>

            <span class="n">grad</span> <span class="o">=</span> <span class="n">param</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">data</span>

            <span class="c1"># 计算统计信息</span>
            <span class="n">grad_norm</span> <span class="o">=</span> <span class="n">grad</span><span class="o">.</span><span class="n">norm</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
            <span class="n">grad_mean</span> <span class="o">=</span> <span class="n">grad</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
            <span class="n">grad_std</span> <span class="o">=</span> <span class="n">grad</span><span class="o">.</span><span class="n">std</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>

            <span class="c1"># 记录历史</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="n">name</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">({</span>
                <span class="s1">&#39;step&#39;</span><span class="p">:</span> <span class="n">step</span><span class="p">,</span>
                <span class="s1">&#39;norm&#39;</span><span class="p">:</span> <span class="n">grad_norm</span><span class="p">,</span>
                <span class="s1">&#39;mean&#39;</span><span class="p">:</span> <span class="n">grad_mean</span><span class="p">,</span>
                <span class="s1">&#39;std&#39;</span><span class="p">:</span> <span class="n">grad_std</span>
            <span class="p">})</span>

            <span class="c1"># 异常检测</span>
            <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">isnan</span><span class="p">(</span><span class="n">grad</span><span class="p">)</span><span class="o">.</span><span class="n">any</span><span class="p">():</span>
                <span class="n">alerts</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Step </span><span class="si">{</span><span class="n">step</span><span class="si">}</span><span class="s2">: NaN gradient in </span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">grad_norm</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">anomaly_threshold</span><span class="p">[</span><span class="s1">&#39;max_norm&#39;</span><span class="p">]:</span>
                <span class="n">alerts</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Step </span><span class="si">{</span><span class="n">step</span><span class="si">}</span><span class="s2">: Large gradient norm </span><span class="si">{</span><span class="n">grad_norm</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> in </span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">grad_norm</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">anomaly_threshold</span><span class="p">[</span><span class="s1">&#39;min_norm&#39;</span><span class="p">]</span> <span class="ow">and</span> <span class="n">grad_norm</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">alerts</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Step </span><span class="si">{</span><span class="n">step</span><span class="si">}</span><span class="s2">: Vanishing gradient </span><span class="si">{</span><span class="n">grad_norm</span><span class="si">:</span><span class="s2">.2e</span><span class="si">}</span><span class="s2"> in </span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">alerts</span>
</code></pre></div>

<h3 id="1022">10.2.2 梯度异常的根源分析</h3>
<p>不同层的梯度异常往往指向不同的问题：</p>
<ol>
<li>
<p><strong>视觉编码器层的梯度爆炸</strong>
   - 原因：图像预处理错误（如未归一化）
   - 解决：检查图像输入范围，确保在 [-1, 1] 或 [0, 1]</p>
</li>
<li>
<p><strong>投影层的梯度消失</strong>
   - 原因：维度不匹配或初始化不当
   - 解决：使用 Xavier 或 Kaiming 初始化</p>
</li>
<li>
<p><strong>语言模型层的梯度震荡</strong>
   - 原因：序列长度变化过大或 padding 策略不当
   - 解决：使用动态 padding 和注意力 mask</p>
</li>
</ol>
<h3 id="1023">10.2.3 高级梯度分析工具</h3>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">analyze_gradient_flow</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">input_batch</span><span class="p">,</span> <span class="n">target_batch</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;分析梯度在模型中的流动情况&quot;&quot;&quot;</span>

    <span class="n">model</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">input_batch</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">compute_loss</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target_batch</span><span class="p">)</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

    <span class="c1"># 收集每层的梯度信息</span>
    <span class="n">gradient_flow</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">():</span>
        <span class="k">if</span> <span class="n">param</span><span class="o">.</span><span class="n">grad</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">grad_data</span> <span class="o">=</span> <span class="n">param</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">data</span>
            <span class="n">gradient_flow</span><span class="o">.</span><span class="n">append</span><span class="p">({</span>
                <span class="s1">&#39;layer&#39;</span><span class="p">:</span> <span class="n">name</span><span class="p">,</span>
                <span class="s1">&#39;grad_norm&#39;</span><span class="p">:</span> <span class="n">grad_data</span><span class="o">.</span><span class="n">norm</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span>
                <span class="s1">&#39;grad_mean&#39;</span><span class="p">:</span> <span class="n">grad_data</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span>
                <span class="s1">&#39;grad_max&#39;</span><span class="p">:</span> <span class="n">grad_data</span><span class="o">.</span><span class="n">max</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span>
                <span class="s1">&#39;grad_min&#39;</span><span class="p">:</span> <span class="n">grad_data</span><span class="o">.</span><span class="n">min</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span>
                <span class="s1">&#39;percent_zeros&#39;</span><span class="p">:</span> <span class="p">(</span><span class="n">grad_data</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">*</span> <span class="mi">100</span>
            <span class="p">})</span>

    <span class="c1"># 可视化梯度流</span>
    <span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

    <span class="n">layers</span> <span class="o">=</span> <span class="p">[</span><span class="n">g</span><span class="p">[</span><span class="s1">&#39;layer&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;.&#39;</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">g</span> <span class="ow">in</span> <span class="n">gradient_flow</span><span class="p">]</span>
    <span class="n">grad_norms</span> <span class="o">=</span> <span class="p">[</span><span class="n">g</span><span class="p">[</span><span class="s1">&#39;grad_norm&#39;</span><span class="p">]</span> <span class="k">for</span> <span class="n">g</span> <span class="ow">in</span> <span class="n">gradient_flow</span><span class="p">]</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">semilogy</span><span class="p">(</span><span class="n">grad_norms</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Gradient Norm&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">layers</span><span class="p">)),</span> <span class="n">layers</span><span class="p">,</span> <span class="n">rotation</span><span class="o">=</span><span class="mi">45</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;right&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Layers&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Gradient Norm (log scale)&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Gradient Flow Through Network&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

    <span class="k">return</span> <span class="n">gradient_flow</span>
</code></pre></div>

<h2 id="103">10.3 混合精度训练的稳定性技巧</h2>
<h3 id="1031-fp16-vs-bf16">10.3.1 FP16 vs BF16 的选择</h3>
<p>混合精度训练是提升训练速度的关键，但也是稳定性问题的主要来源：</p>
<div class="codehilite"><pre><span></span><code>FP16 (半精度浮点)
├─ 优点：硬件支持广泛，速度快
├─ 缺点：数值范围小 (±65,504)，容易溢出
└─ 适用：稳定的模型，充分的 loss scaling

BF16 (Brain Float 16)
├─ 优点：数值范围大 (±3.4×10^38)，与FP32相同
├─ 缺点：精度较低，需要新硬件（A100+）
└─ 适用：大模型训练，数值稳定性要求高
</code></pre></div>

<h3 id="1032-loss-scaling">10.3.2 动态 Loss Scaling 策略</h3>
<div class="codehilite"><pre><span></span><code><span class="k">class</span> <span class="nc">DynamicLossScaler</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;自适应的 loss scaling，防止梯度下溢/上溢&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">init_scale</span><span class="o">=</span><span class="mi">2</span><span class="o">**</span><span class="mi">16</span><span class="p">,</span> <span class="n">scale_factor</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span> 
                 <span class="n">scale_window</span><span class="o">=</span><span class="mi">2000</span><span class="p">,</span> <span class="n">tolerance</span><span class="o">=</span><span class="mf">0.05</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">scale</span> <span class="o">=</span> <span class="n">init_scale</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">scale_factor</span> <span class="o">=</span> <span class="n">scale_factor</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">scale_window</span> <span class="o">=</span> <span class="n">scale_window</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tolerance</span> <span class="o">=</span> <span class="n">tolerance</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">overflow_counter</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">step_counter</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="k">def</span> <span class="nf">scale_loss</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">loss</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;放大loss防止梯度下溢&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">loss</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">scale</span>

    <span class="k">def</span> <span class="nf">unscale_gradients</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;缩小梯度到正确范围&quot;&quot;&quot;</span>
        <span class="k">for</span> <span class="n">group</span> <span class="ow">in</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">group</span><span class="p">[</span><span class="s1">&#39;params&#39;</span><span class="p">]:</span>
                <span class="k">if</span> <span class="n">param</span><span class="o">.</span><span class="n">grad</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="n">param</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">div_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">scale</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">update_scale</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">overflow</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;根据溢出情况动态调整scale&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">overflow</span><span class="p">:</span>
            <span class="c1"># 发生溢出，减小scale</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">scale</span> <span class="o">/=</span> <span class="bp">self</span><span class="o">.</span><span class="n">scale_factor</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">overflow_counter</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Gradient overflow! Reducing scale to </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">scale</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="k">return</span> <span class="kc">True</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">step_counter</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">step_counter</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">scale_window</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="c1"># 长时间无溢出，尝试增大scale</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">scale</span> <span class="o">*=</span> <span class="bp">self</span><span class="o">.</span><span class="n">scale_factor</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Increasing scale to </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">scale</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

        <span class="k">return</span> <span class="kc">False</span>
</code></pre></div>

<h3 id="1033">10.3.3 关键层的精度保护</h3>
<p>某些层必须保持 FP32 精度以确保稳定性：</p>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">configure_mixed_precision</span><span class="p">(</span><span class="n">model</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;配置混合精度训练的层级精度&quot;&quot;&quot;</span>

    <span class="c1"># 始终保持 FP32 的层</span>
    <span class="n">fp32_layers</span> <span class="o">=</span> <span class="p">[</span>
        <span class="s1">&#39;layer_norm&#39;</span><span class="p">,</span>      <span class="c1"># LayerNorm 对精度敏感</span>
        <span class="s1">&#39;softmax&#39;</span><span class="p">,</span>         <span class="c1"># Softmax 需要高精度</span>
        <span class="s1">&#39;loss&#39;</span><span class="p">,</span>            <span class="c1"># 损失计算</span>
        <span class="s1">&#39;positional&#39;</span><span class="p">,</span>      <span class="c1"># 位置编码</span>
    <span class="p">]</span>

    <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">module</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">named_modules</span><span class="p">():</span>
        <span class="c1"># 检查是否需要FP32</span>
        <span class="n">need_fp32</span> <span class="o">=</span> <span class="nb">any</span><span class="p">(</span><span class="n">fp_layer</span> <span class="ow">in</span> <span class="n">name</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> 
                        <span class="k">for</span> <span class="n">fp_layer</span> <span class="ow">in</span> <span class="n">fp32_layers</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">need_fp32</span><span class="p">:</span>
            <span class="c1"># 强制使用FP32</span>
            <span class="n">module</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
            <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">module</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span>
                <span class="n">param</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">param</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># 可以使用FP16/BF16</span>
            <span class="n">module</span><span class="o">.</span><span class="n">half</span><span class="p">()</span>  <span class="c1"># or module.bfloat16()</span>

    <span class="k">return</span> <span class="n">model</span>
</code></pre></div>

<h3 id="1034">10.3.4 梯度累积与混合精度的交互</h3>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">stable_gradient_accumulation</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">data_loader</span><span class="p">,</span> 
                                <span class="n">accumulation_steps</span><span class="o">=</span><span class="mi">4</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;稳定的梯度累积实现&quot;&quot;&quot;</span>

    <span class="n">scaler</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">amp</span><span class="o">.</span><span class="n">GradScaler</span><span class="p">()</span>
    <span class="n">accumulated_loss</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="k">for</span> <span class="n">step</span><span class="p">,</span> <span class="n">batch</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">data_loader</span><span class="p">):</span>
        <span class="c1"># 判断是否是累积的最后一步</span>
        <span class="n">is_accumulation_boundary</span> <span class="o">=</span> <span class="p">(</span><span class="n">step</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="n">accumulation_steps</span> <span class="o">==</span> <span class="mi">0</span>

        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">amp</span><span class="o">.</span><span class="n">autocast</span><span class="p">():</span>
            <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">batch</span><span class="p">[</span><span class="s1">&#39;input&#39;</span><span class="p">])</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">compute_loss</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">batch</span><span class="p">[</span><span class="s1">&#39;target&#39;</span><span class="p">])</span>
            <span class="c1"># 重要：除以累积步数</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">loss</span> <span class="o">/</span> <span class="n">accumulation_steps</span>

        <span class="c1"># Scale loss并反向传播</span>
        <span class="n">scaler</span><span class="o">.</span><span class="n">scale</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">accumulated_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>

        <span class="k">if</span> <span class="n">is_accumulation_boundary</span><span class="p">:</span>
            <span class="c1"># 梯度裁剪（在unscale之后，step之前）</span>
            <span class="n">scaler</span><span class="o">.</span><span class="n">unscale_</span><span class="p">(</span><span class="n">optimizer</span><span class="p">)</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">clip_grad_norm_</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">max_norm</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>

            <span class="c1"># 优化器步进</span>
            <span class="n">scaler</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">optimizer</span><span class="p">)</span>
            <span class="n">scaler</span><span class="o">.</span><span class="n">update</span><span class="p">()</span>
            <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

            <span class="c1"># 记录</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Step </span><span class="si">{</span><span class="n">step</span><span class="si">}</span><span class="s2">: Loss = </span><span class="si">{</span><span class="n">accumulated_loss</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="n">accumulated_loss</span> <span class="o">=</span> <span class="mi">0</span>
</code></pre></div>

<h2 id="104-checkpoint">10.4 Checkpoint 恢复与断点续训</h2>
<h3 id="1041-checkpoint">10.4.1 完整的 Checkpoint 系统</h3>
<div class="codehilite"><pre><span></span><code><span class="k">class</span> <span class="nc">CheckpointManager</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;全面的检查点管理器&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">save_dir</span><span class="p">,</span> <span class="n">keep_last_n</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">save_interval</span><span class="o">=</span><span class="mi">1000</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">save_dir</span> <span class="o">=</span> <span class="n">save_dir</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">keep_last_n</span> <span class="o">=</span> <span class="n">keep_last_n</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">save_interval</span> <span class="o">=</span> <span class="n">save_interval</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">checkpoints</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">def</span> <span class="nf">save_checkpoint</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">scheduler</span><span class="p">,</span> 
                       <span class="n">epoch</span><span class="p">,</span> <span class="n">step</span><span class="p">,</span> <span class="n">metrics</span><span class="p">,</span> <span class="n">extra_state</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;保存完整的训练状态&quot;&quot;&quot;</span>

        <span class="n">checkpoint</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s1">&#39;epoch&#39;</span><span class="p">:</span> <span class="n">epoch</span><span class="p">,</span>
            <span class="s1">&#39;step&#39;</span><span class="p">:</span> <span class="n">step</span><span class="p">,</span>
            <span class="s1">&#39;model_state_dict&#39;</span><span class="p">:</span> <span class="n">model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span>
            <span class="s1">&#39;optimizer_state_dict&#39;</span><span class="p">:</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span>
            <span class="s1">&#39;scheduler_state_dict&#39;</span><span class="p">:</span> <span class="n">scheduler</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()</span> <span class="k">if</span> <span class="n">scheduler</span> <span class="k">else</span> <span class="kc">None</span><span class="p">,</span>
            <span class="s1">&#39;metrics&#39;</span><span class="p">:</span> <span class="n">metrics</span><span class="p">,</span>
            <span class="s1">&#39;rng_state&#39;</span><span class="p">:</span> <span class="p">{</span>
                <span class="s1">&#39;python&#39;</span><span class="p">:</span> <span class="n">random</span><span class="o">.</span><span class="n">getstate</span><span class="p">(),</span>
                <span class="s1">&#39;numpy&#39;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">get_state</span><span class="p">(),</span>
                <span class="s1">&#39;torch&#39;</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">get_rng_state</span><span class="p">(),</span>
                <span class="s1">&#39;cuda&#39;</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">get_rng_state_all</span><span class="p">(),</span>
            <span class="p">},</span>
            <span class="s1">&#39;timestamp&#39;</span><span class="p">:</span> <span class="n">datetime</span><span class="o">.</span><span class="n">now</span><span class="p">()</span><span class="o">.</span><span class="n">isoformat</span><span class="p">(),</span>
        <span class="p">}</span>

        <span class="k">if</span> <span class="n">extra_state</span><span class="p">:</span>
            <span class="n">checkpoint</span><span class="p">[</span><span class="s1">&#39;extra_state&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">extra_state</span>

        <span class="c1"># 保存checkpoint</span>
        <span class="n">checkpoint_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">save_dir</span><span class="p">,</span> 
            <span class="sa">f</span><span class="s1">&#39;checkpoint_step_</span><span class="si">{</span><span class="n">step</span><span class="si">}</span><span class="s1">.pt&#39;</span>
        <span class="p">)</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">,</span> <span class="n">checkpoint_path</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">checkpoints</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">checkpoint_path</span><span class="p">)</span>

        <span class="c1"># 清理旧的checkpoints</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">checkpoints</span><span class="p">)</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">keep_last_n</span><span class="p">:</span>
            <span class="n">old_checkpoint</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">checkpoints</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">old_checkpoint</span><span class="p">):</span>
                <span class="n">os</span><span class="o">.</span><span class="n">remove</span><span class="p">(</span><span class="n">old_checkpoint</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">checkpoint_path</span>

    <span class="k">def</span> <span class="nf">load_checkpoint</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">checkpoint_path</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> 
                       <span class="n">scheduler</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">strict</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;恢复训练状态&quot;&quot;&quot;</span>

        <span class="n">checkpoint</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">checkpoint_path</span><span class="p">,</span> <span class="n">map_location</span><span class="o">=</span><span class="s1">&#39;cpu&#39;</span><span class="p">)</span>

        <span class="c1"># 恢复模型</span>
        <span class="n">model</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">[</span><span class="s1">&#39;model_state_dict&#39;</span><span class="p">],</span> <span class="n">strict</span><span class="o">=</span><span class="n">strict</span><span class="p">)</span>

        <span class="c1"># 恢复优化器</span>
        <span class="k">if</span> <span class="n">optimizer</span> <span class="ow">and</span> <span class="s1">&#39;optimizer_state_dict&#39;</span> <span class="ow">in</span> <span class="n">checkpoint</span><span class="p">:</span>
            <span class="n">optimizer</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">[</span><span class="s1">&#39;optimizer_state_dict&#39;</span><span class="p">])</span>

        <span class="c1"># 恢复学习率调度器</span>
        <span class="k">if</span> <span class="n">scheduler</span> <span class="ow">and</span> <span class="s1">&#39;scheduler_state_dict&#39;</span> <span class="ow">in</span> <span class="n">checkpoint</span><span class="p">:</span>
            <span class="n">scheduler</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">[</span><span class="s1">&#39;scheduler_state_dict&#39;</span><span class="p">])</span>

        <span class="c1"># 恢复随机数状态</span>
        <span class="k">if</span> <span class="s1">&#39;rng_state&#39;</span> <span class="ow">in</span> <span class="n">checkpoint</span><span class="p">:</span>
            <span class="n">random</span><span class="o">.</span><span class="n">setstate</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">[</span><span class="s1">&#39;rng_state&#39;</span><span class="p">][</span><span class="s1">&#39;python&#39;</span><span class="p">])</span>
            <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">set_state</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">[</span><span class="s1">&#39;rng_state&#39;</span><span class="p">][</span><span class="s1">&#39;numpy&#39;</span><span class="p">])</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">set_rng_state</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">[</span><span class="s1">&#39;rng_state&#39;</span><span class="p">][</span><span class="s1">&#39;torch&#39;</span><span class="p">])</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">set_rng_state_all</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">[</span><span class="s1">&#39;rng_state&#39;</span><span class="p">][</span><span class="s1">&#39;cuda&#39;</span><span class="p">])</span>

        <span class="k">return</span> <span class="n">checkpoint</span>
</code></pre></div>

<h3 id="1042">10.4.2 断点续训的最佳实践</h3>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">resume_training</span><span class="p">(</span><span class="n">checkpoint_path</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">data_loader</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;安全的断点续训流程&quot;&quot;&quot;</span>

    <span class="c1"># 1. 加载checkpoint</span>
    <span class="n">checkpoint</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">checkpoint_path</span><span class="p">)</span>
    <span class="n">model</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">[</span><span class="s1">&#39;model_state_dict&#39;</span><span class="p">])</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">[</span><span class="s1">&#39;optimizer_state_dict&#39;</span><span class="p">])</span>

    <span class="c1"># 2. 恢复到正确的数据位置</span>
    <span class="n">start_epoch</span> <span class="o">=</span> <span class="n">checkpoint</span><span class="p">[</span><span class="s1">&#39;epoch&#39;</span><span class="p">]</span>
    <span class="n">start_step</span> <span class="o">=</span> <span class="n">checkpoint</span><span class="p">[</span><span class="s1">&#39;step&#39;</span><span class="p">]</span>

    <span class="c1"># 3. 验证恢复是否成功</span>
    <span class="n">validation_batch</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">data_loader</span><span class="p">))</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">validation_batch</span><span class="p">[</span><span class="s1">&#39;input&#39;</span><span class="p">])</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">compute_loss</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">validation_batch</span><span class="p">[</span><span class="s1">&#39;target&#39;</span><span class="p">])</span>

    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Validation loss after resume: </span><span class="si">{</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="c1"># 4. 检查是否需要降级配置</span>
    <span class="k">if</span> <span class="n">checkpoint</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;crashed&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">):</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Previous training crashed. Applying conservative settings...&quot;</span><span class="p">)</span>
        <span class="c1"># 降低学习率</span>
        <span class="k">for</span> <span class="n">param_group</span> <span class="ow">in</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">:</span>
            <span class="n">param_group</span><span class="p">[</span><span class="s1">&#39;lr&#39;</span><span class="p">]</span> <span class="o">*=</span> <span class="mf">0.5</span>
        <span class="c1"># 增强梯度裁剪</span>
        <span class="n">max_grad_norm</span> <span class="o">=</span> <span class="mf">0.5</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">max_grad_norm</span> <span class="o">=</span> <span class="mf">1.0</span>

    <span class="k">return</span> <span class="n">start_epoch</span><span class="p">,</span> <span class="n">start_step</span><span class="p">,</span> <span class="n">max_grad_norm</span>
</code></pre></div>

<h3 id="1043">10.4.3 崩溃恢复策略</h3>
<div class="codehilite"><pre><span></span><code><span class="k">class</span> <span class="nc">CrashRecoveryTrainer</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;具有崩溃恢复能力的训练器&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">config</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">model</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">config</span> <span class="o">=</span> <span class="n">config</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">crash_counter</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_crashes</span> <span class="o">=</span> <span class="mi">3</span>

    <span class="k">def</span> <span class="nf">train_with_recovery</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data_loader</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;带自动恢复的训练循环&quot;&quot;&quot;</span>

        <span class="k">while</span> <span class="bp">self</span><span class="o">.</span><span class="n">crash_counter</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_crashes</span><span class="p">:</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="c1"># 正常训练</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_train_epoch</span><span class="p">(</span><span class="n">data_loader</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">crash_counter</span> <span class="o">=</span> <span class="mi">0</span>  <span class="c1"># 重置计数器</span>

            <span class="k">except</span> <span class="p">(</span><span class="ne">RuntimeError</span><span class="p">,</span> <span class="ne">ValueError</span><span class="p">)</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">crash_counter</span> <span class="o">+=</span> <span class="mi">1</span>
                <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Training crashed (attempt </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">crash_counter</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">max_crashes</span><span class="si">}</span><span class="s2">): </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

                <span class="c1"># 崩溃恢复策略</span>
                <span class="n">recovery_actions</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_recovery_strategy</span><span class="p">(</span><span class="n">e</span><span class="p">)</span>
                <span class="k">for</span> <span class="n">action</span> <span class="ow">in</span> <span class="n">recovery_actions</span><span class="p">:</span>
                    <span class="n">action</span><span class="p">()</span>

                <span class="c1"># 从最近的checkpoint恢复</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">last_checkpoint</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">load_checkpoint</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">last_checkpoint</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;No checkpoint available, restarting training...&quot;</span><span class="p">)</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_reinitialize_training</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">_get_recovery_strategy</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">error</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;根据错误类型确定恢复策略&quot;&quot;&quot;</span>

        <span class="n">strategies</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="k">if</span> <span class="s2">&quot;CUDA out of memory&quot;</span> <span class="ow">in</span> <span class="nb">str</span><span class="p">(</span><span class="n">error</span><span class="p">):</span>
            <span class="n">strategies</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_reduce_batch_size</span><span class="p">)</span>
            <span class="n">strategies</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_enable_gradient_checkpointing</span><span class="p">)</span>

        <span class="k">elif</span> <span class="s2">&quot;nan&quot;</span> <span class="ow">in</span> <span class="nb">str</span><span class="p">(</span><span class="n">error</span><span class="p">)</span><span class="o">.</span><span class="n">lower</span><span class="p">():</span>
            <span class="n">strategies</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_reduce_learning_rate</span><span class="p">)</span>
            <span class="n">strategies</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_reset_optimizer_state</span><span class="p">)</span>
            <span class="n">strategies</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_switch_to_fp32</span><span class="p">)</span>

        <span class="k">elif</span> <span class="s2">&quot;gradient&quot;</span> <span class="ow">in</span> <span class="nb">str</span><span class="p">(</span><span class="n">error</span><span class="p">)</span><span class="o">.</span><span class="n">lower</span><span class="p">():</span>
            <span class="n">strategies</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_enhance_gradient_clipping</span><span class="p">)</span>
            <span class="n">strategies</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_reduce_accumulation_steps</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">strategies</span>
</code></pre></div>

<h2 id="_1">本章小结</h2>
<p>在本章中，我们系统学习了 VLM 训练中崩溃和 NaN 问题的诊断与解决方法：</p>
<h3 id="_2">核心知识点</h3>
<ol>
<li>
<p><strong>5分钟快速诊断流程</strong>
   - 保存现场 → 分析Loss模式 → 定位问题层 → 检查关键数值 → 紧急处理
   - 三种典型的 Loss 爆炸模式：突然跳跃、指数增长、震荡发散
   - 不同模式对应不同的根本原因和解决方案</p>
</li>
<li>
<p><strong>梯度监控体系</strong>
   - 实时梯度统计：范数、均值、标准差、零值比例
   - 层级梯度分析：视觉编码器、投影层、语言模型的特征
   - 梯度流可视化：快速定位梯度消失或爆炸的位置</p>
</li>
<li>
<p><strong>混合精度训练稳定性</strong>
   - FP16 vs BF16 的权衡：数值范围 vs 精度
   - 动态 Loss Scaling：自适应调整防止溢出
   - 关键层精度保护：LayerNorm、Softmax 必须 FP32
   - 梯度累积的正确实现：防止精度损失累积</p>
</li>
<li>
<p><strong>Checkpoint 与容错机制</strong>
   - 完整状态保存：模型、优化器、调度器、随机数种子
   - 智能恢复策略：根据崩溃类型自动调整配置
   - 崩溃计数器：避免无限循环，设置最大重试次数</p>
</li>
</ol>
<h3 id="_3">关键公式</h3>
<ol>
<li>
<p><strong>梯度范数计算</strong>：
   $$|\nabla|_2 = \sqrt{\sum_{i} g_i^2}$$</p>
</li>
<li>
<p><strong>Loss Scaling 原理</strong>：
$$\nabla_{\text{scaled}} = \text{scale} \times \nabla_{\text{original}}$$
   $$\nabla_{\text{final}} = \nabla_{\text{scaled}} / \text{scale}$$</p>
</li>
<li>
<p><strong>梯度裁剪</strong>：
$$\nabla_{\text{clipped}} = \begin{cases}
   \nabla &amp; \text{if } |\nabla| \leq \text{max_norm} \\
   \nabla \times \frac{\text{max_norm}}{|\nabla|} &amp; \text{otherwise}
   \end{cases}$$</p>
</li>
<li>
<p><strong>数值稳定的 Softmax</strong>：
$$\text{softmax}(x_i) = \frac{e^{x_i - \max(x)}}{\sum_j e^{x_j - \max(x)}}$$</p>
</li>
</ol>
<h2 id="_4">练习题</h2>
<h3 id="_5">基础题</h3>
<p><strong>练习 10.1：Loss 模式识别</strong>
给定以下 Loss 序列，判断属于哪种爆炸模式并分析可能的原因：</p>
<div class="codehilite"><pre><span></span><code><span class="err">序列</span><span class="n">A</span><span class="o">:</span><span class="w"> </span><span class="mf">1.8</span><span class="o">,</span><span class="w"> </span><span class="mf">1.7</span><span class="o">,</span><span class="w"> </span><span class="mf">1.6</span><span class="o">,</span><span class="w"> </span><span class="mf">1.5</span><span class="o">,</span><span class="w"> </span><span class="mf">1.4</span><span class="o">,</span><span class="w"> </span><span class="mf">87234.5</span><span class="o">,</span><span class="w"> </span><span class="kc">NaN</span>
<span class="err">序列</span><span class="n">B</span><span class="o">:</span><span class="w"> </span><span class="mf">2.1</span><span class="o">,</span><span class="w"> </span><span class="mf">2.2</span><span class="o">,</span><span class="w"> </span><span class="mf">2.5</span><span class="o">,</span><span class="w"> </span><span class="mf">3.1</span><span class="o">,</span><span class="w"> </span><span class="mf">4.8</span><span class="o">,</span><span class="w"> </span><span class="mf">9.2</span><span class="o">,</span><span class="w"> </span><span class="mf">23.5</span><span class="o">,</span><span class="w"> </span><span class="mf">156.7</span><span class="o">,</span><span class="w"> </span><span class="kc">NaN</span>
<span class="err">序列</span><span class="n">C</span><span class="o">:</span><span class="w"> </span><span class="mf">2.0</span><span class="o">,</span><span class="w"> </span><span class="mf">1.8</span><span class="o">,</span><span class="w"> </span><span class="mf">2.2</span><span class="o">,</span><span class="w"> </span><span class="mf">1.6</span><span class="o">,</span><span class="w"> </span><span class="mf">2.5</span><span class="o">,</span><span class="w"> </span><span class="mf">1.4</span><span class="o">,</span><span class="w"> </span><span class="mf">3.2</span><span class="o">,</span><span class="w"> </span><span class="mf">1.2</span><span class="o">,</span><span class="w"> </span><span class="mf">5.8</span><span class="o">,</span><span class="w"> </span><span class="kc">NaN</span>
</code></pre></div>

<p>💡 <strong>提示</strong>：回顾10.1.2节的三种模式特征</p>
<details>
<summary>📝 参考答案</summary>
<ul>
<li><strong>序列A</strong>：突然跳跃模式。Loss从1.4直接跳到87234.5，表明遇到了异常样本或数值溢出。可能原因：</li>
<li>数据集中存在异常样本（如标签错误）</li>
<li>除零错误或log(0)操作</li>
<li>
<p>注意力计算中的数值溢出</p>
</li>
<li>
<p><strong>序列B</strong>：指数增长模式。Loss呈指数级增长，每步大约翻倍。可能原因：</p>
</li>
<li>学习率过大导致参数更新过激</li>
<li>梯度累积实现错误（忘记除以累积步数）</li>
<li>
<p>优化器momentum设置不当</p>
</li>
<li>
<p><strong>序列C</strong>：震荡发散模式。Loss在下降和上升之间震荡，振幅逐渐增大。可能原因：</p>
</li>
<li>优化器状态损坏（如Adam的二阶矩估计）</li>
<li>批次间数据分布差异过大</li>
<li>学习率调度器配置错误</li>
</ul>
</details>
<p><strong>练习 10.2：梯度裁剪阈值选择</strong>
你的模型正常训练时梯度范数在 0.5-2.0 之间，偶尔会达到 10-20。应该如何设置梯度裁剪的阈值？如果设置为 1.0 会发生什么？设置为 100 呢？</p>
<p>💡 <strong>提示</strong>：考虑梯度裁剪对收敛速度和稳定性的影响</p>
<details>
<summary>📝 参考答案</summary>
<p>合理的梯度裁剪阈值应该设置为 <strong>5.0-10.0</strong>，原因如下：</p>
<ul>
<li><strong>设置为 1.0 的问题</strong>：</li>
<li>会频繁触发裁剪（正常梯度就有2.0）</li>
<li>人为限制了模型的学习能力</li>
<li>可能导致收敛变慢或无法收敛到最优解</li>
<li>
<p>相当于强制降低了有效学习率</p>
</li>
<li>
<p><strong>设置为 100 的问题</strong>：</p>
</li>
<li>基本不会触发（正常最大值才20）</li>
<li>失去了防止梯度爆炸的保护作用</li>
<li>
<p>当真正出现异常时无法及时阻止</p>
</li>
<li>
<p><strong>推荐策略</strong>：
  1. 初始设置为正常最大值的 2-3 倍（如 5.0）
  2. 监控裁剪频率，如果频繁触发则适当提高
  3. 对不同层使用不同阈值（视觉编码器可以更大）</p>
</li>
</ul>
</details>
<p><strong>练习 10.3：混合精度数值范围</strong>
计算并比较 FP16 和 BF16 能表示的最大最小正数。为什么 BF16 更不容易出现梯度下溢？</p>
<p>💡 <strong>提示</strong>：查阅 IEEE 754 标准中的浮点数格式定义</p>
<details>
<summary>📝 参考答案</summary>
<p><strong>FP16（半精度）</strong>：</p>
<ul>
<li>格式：1位符号 + 5位指数 + 10位尾数</li>
<li>最大值：65,504</li>
<li>最小正规值：6.10 × 10^-5</li>
<li>最小非正规值：5.96 × 10^-8</li>
</ul>
<p><strong>BF16（Brain Float 16）</strong>：</p>
<ul>
<li>格式：1位符号 + 8位指数 + 7位尾数</li>
<li>最大值：3.39 × 10^38（与FP32相同）</li>
<li>最小正规值：1.18 × 10^-38</li>
<li>最小非正规值：9.18 × 10^-41</li>
</ul>
<p><strong>BF16 不易梯度下溢的原因</strong>：</p>
<ol>
<li>指数位数多（8位 vs 5位），数值范围大</li>
<li>可以表示极小的梯度值而不会直接变为0</li>
<li>与FP32的数值范围一致，转换时不会溢出</li>
<li>代价是尾数精度降低（7位 vs 10位），但深度学习中通常可接受</li>
</ol>
</details>
<h3 id="_6">挑战题</h3>
<p><strong>练习 10.4：设计自适应梯度裁剪算法</strong>
标准的梯度裁剪使用固定阈值，请设计一个自适应算法，根据历史梯度统计动态调整裁剪阈值。要求：</p>
<ol>
<li>能够适应训练过程中梯度范数的自然变化</li>
<li>仍然能够检测和处理异常值</li>
<li>给出伪代码实现</li>
</ol>
<p>💡 <strong>提示</strong>：可以使用移动平均和标准差</p>
<details>
<summary>📝 参考答案</summary>
<div class="codehilite"><pre><span></span><code><span class="k">class</span> <span class="nc">AdaptiveGradientClipper</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">percentile</span><span class="o">=</span><span class="mf">99.5</span><span class="p">,</span> <span class="n">history_size</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> 
                 <span class="n">min_threshold</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">max_threshold</span><span class="o">=</span><span class="mf">100.0</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">percentile</span> <span class="o">=</span> <span class="n">percentile</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">history</span> <span class="o">=</span> <span class="n">deque</span><span class="p">(</span><span class="n">maxlen</span><span class="o">=</span><span class="n">history_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">min_threshold</span> <span class="o">=</span> <span class="n">min_threshold</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_threshold</span> <span class="o">=</span> <span class="n">max_threshold</span>

    <span class="k">def</span> <span class="nf">compute_threshold</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">history</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mi">100</span><span class="p">:</span>  <span class="c1"># 初始阶段使用固定值</span>
            <span class="k">return</span> <span class="mf">10.0</span>

        <span class="c1"># 方法1：基于百分位数</span>
        <span class="n">threshold</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">percentile</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">history</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">percentile</span><span class="p">)</span>

        <span class="c1"># 方法2：基于均值和标准差（3-sigma规则）</span>
        <span class="c1"># mean = np.mean(self.history)</span>
        <span class="c1"># std = np.std(self.history)</span>
        <span class="c1"># threshold = mean + 3 * std</span>

        <span class="c1"># 限制在合理范围内</span>
        <span class="n">threshold</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span><span class="n">threshold</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">min_threshold</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_threshold</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">threshold</span>

    <span class="k">def</span> <span class="nf">clip_gradients</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="p">):</span>
        <span class="c1"># 计算当前梯度范数</span>
        <span class="n">total_norm</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span>
            <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">grad</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">total_norm</span> <span class="o">+=</span> <span class="n">p</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">**</span> <span class="mi">2</span>
        <span class="n">total_norm</span> <span class="o">=</span> <span class="n">total_norm</span> <span class="o">**</span> <span class="mf">0.5</span>

        <span class="c1"># 更新历史</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">history</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">total_norm</span><span class="p">)</span>

        <span class="c1"># 计算自适应阈值</span>
        <span class="n">clip_value</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">compute_threshold</span><span class="p">()</span>

        <span class="c1"># 执行裁剪</span>
        <span class="k">if</span> <span class="n">total_norm</span> <span class="o">&gt;</span> <span class="n">clip_value</span><span class="p">:</span>
            <span class="n">clip_coef</span> <span class="o">=</span> <span class="n">clip_value</span> <span class="o">/</span> <span class="p">(</span><span class="n">total_norm</span> <span class="o">+</span> <span class="mf">1e-6</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span>
                <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">grad</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="n">p</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">mul_</span><span class="p">(</span><span class="n">clip_coef</span><span class="p">)</span>
            <span class="k">return</span> <span class="kc">True</span><span class="p">,</span> <span class="n">clip_value</span>

        <span class="k">return</span> <span class="kc">False</span><span class="p">,</span> <span class="n">clip_value</span>
</code></pre></div>

<p><strong>优势</strong>：</p>
<ol>
<li>自动适应不同训练阶段的梯度范围</li>
<li>避免固定阈值过松或过紧</li>
<li>基于统计的异常检测更鲁棒</li>
</ol>
</details>
<p><strong>练习 10.5：实现梯度异常定位器</strong>
设计一个工具，当检测到 NaN 梯度时，能够快速定位是哪个操作产生的 NaN，并给出可能的原因。考虑 VLM 中的特殊情况。</p>
<p>💡 <strong>提示</strong>：使用 PyTorch 的 autograd 异常检测模式</p>
<details>
<summary>📝 参考答案</summary>
<div class="codehilite"><pre><span></span><code><span class="k">class</span> <span class="nc">NaNGradientLocator</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">model</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">forward_hooks</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">backward_hooks</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">problematic_layers</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">def</span> <span class="nf">enable_detection</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;启用NaN检测&quot;&quot;&quot;</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">set_detect_anomaly</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>

        <span class="c1"># 注册前向钩子</span>
        <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">module</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">named_modules</span><span class="p">():</span>
            <span class="n">handle</span> <span class="o">=</span> <span class="n">module</span><span class="o">.</span><span class="n">register_forward_hook</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_make_forward_hook</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>
            <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">forward_hooks</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">handle</span><span class="p">)</span>

            <span class="c1"># 注册反向钩子</span>
            <span class="n">handle</span> <span class="o">=</span> <span class="n">module</span><span class="o">.</span><span class="n">register_backward_hook</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_make_backward_hook</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>
            <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">backward_hooks</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">handle</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_make_forward_hook</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">layer_name</span><span class="p">):</span>
        <span class="k">def</span> <span class="nf">hook</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">output</span><span class="p">):</span>
            <span class="c1"># 检查输入</span>
            <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">inp</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
                <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">is_tensor</span><span class="p">(</span><span class="n">inp</span><span class="p">)</span> <span class="ow">and</span> <span class="n">torch</span><span class="o">.</span><span class="n">isnan</span><span class="p">(</span><span class="n">inp</span><span class="p">)</span><span class="o">.</span><span class="n">any</span><span class="p">():</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">problematic_layers</span><span class="o">.</span><span class="n">append</span><span class="p">({</span>
                        <span class="s1">&#39;layer&#39;</span><span class="p">:</span> <span class="n">layer_name</span><span class="p">,</span>
                        <span class="s1">&#39;type&#39;</span><span class="p">:</span> <span class="s1">&#39;forward_input&#39;</span><span class="p">,</span>
                        <span class="s1">&#39;index&#39;</span><span class="p">:</span> <span class="n">i</span><span class="p">,</span>
                        <span class="s1">&#39;stage&#39;</span><span class="p">:</span> <span class="s1">&#39;forward&#39;</span>
                    <span class="p">})</span>

            <span class="c1"># 检查输出</span>
            <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">is_tensor</span><span class="p">(</span><span class="n">output</span><span class="p">)</span> <span class="ow">and</span> <span class="n">torch</span><span class="o">.</span><span class="n">isnan</span><span class="p">(</span><span class="n">output</span><span class="p">)</span><span class="o">.</span><span class="n">any</span><span class="p">():</span>
                <span class="c1"># VLM特殊检查</span>
                <span class="k">if</span> <span class="s1">&#39;attention&#39;</span> <span class="ow">in</span> <span class="n">layer_name</span><span class="o">.</span><span class="n">lower</span><span class="p">():</span>
                    <span class="c1"># 检查注意力分数</span>
                    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;NaN in attention layer </span><span class="si">{</span><span class="n">layer_name</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
                    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;可能原因：1) 序列长度过长导致数值溢出&quot;</span><span class="p">)</span>
                    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;         2) 注意力mask设置错误&quot;</span><span class="p">)</span>

                <span class="k">elif</span> <span class="s1">&#39;vision&#39;</span> <span class="ow">in</span> <span class="n">layer_name</span><span class="o">.</span><span class="n">lower</span><span class="p">():</span>
                    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;NaN in vision layer </span><span class="si">{</span><span class="n">layer_name</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
                    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;可能原因：1) 图像未归一化&quot;</span><span class="p">)</span>
                    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;         2) 图像包含异常值（全黑/全白）&quot;</span><span class="p">)</span>

                <span class="k">elif</span> <span class="s1">&#39;proj&#39;</span> <span class="ow">in</span> <span class="n">layer_name</span><span class="o">.</span><span class="n">lower</span><span class="p">():</span>
                    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;NaN in projection layer </span><span class="si">{</span><span class="n">layer_name</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
                    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;可能原因：1) 维度不匹配&quot;</span><span class="p">)</span>
                    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;         2) 初始化不当&quot;</span><span class="p">)</span>

                <span class="bp">self</span><span class="o">.</span><span class="n">problematic_layers</span><span class="o">.</span><span class="n">append</span><span class="p">({</span>
                    <span class="s1">&#39;layer&#39;</span><span class="p">:</span> <span class="n">layer_name</span><span class="p">,</span>
                    <span class="s1">&#39;type&#39;</span><span class="p">:</span> <span class="s1">&#39;forward_output&#39;</span><span class="p">,</span>
                    <span class="s1">&#39;stage&#39;</span><span class="p">:</span> <span class="s1">&#39;forward&#39;</span>
                <span class="p">})</span>
        <span class="k">return</span> <span class="n">hook</span>

    <span class="k">def</span> <span class="nf">_make_backward_hook</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">layer_name</span><span class="p">):</span>
        <span class="k">def</span> <span class="nf">hook</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">grad_input</span><span class="p">,</span> <span class="n">grad_output</span><span class="p">):</span>
            <span class="c1"># 检查梯度输出</span>
            <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">grad</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">grad_output</span><span class="p">):</span>
                <span class="k">if</span> <span class="n">grad</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">torch</span><span class="o">.</span><span class="n">isnan</span><span class="p">(</span><span class="n">grad</span><span class="p">)</span><span class="o">.</span><span class="n">any</span><span class="p">():</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">problematic_layers</span><span class="o">.</span><span class="n">append</span><span class="p">({</span>
                        <span class="s1">&#39;layer&#39;</span><span class="p">:</span> <span class="n">layer_name</span><span class="p">,</span>
                        <span class="s1">&#39;type&#39;</span><span class="p">:</span> <span class="s1">&#39;grad_output&#39;</span><span class="p">,</span>
                        <span class="s1">&#39;index&#39;</span><span class="p">:</span> <span class="n">i</span><span class="p">,</span>
                        <span class="s1">&#39;stage&#39;</span><span class="p">:</span> <span class="s1">&#39;backward&#39;</span>
                    <span class="p">})</span>

                    <span class="c1"># 分析具体原因</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_analyze_nan_cause</span><span class="p">(</span><span class="n">layer_name</span><span class="p">,</span> <span class="n">module</span><span class="p">,</span> <span class="n">grad</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">hook</span>

    <span class="k">def</span> <span class="nf">_analyze_nan_cause</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">layer_name</span><span class="p">,</span> <span class="n">module</span><span class="p">,</span> <span class="n">grad</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;分析NaN的具体原因&quot;&quot;&quot;</span>

        <span class="c1"># 检查常见操作</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">):</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;LayerNorm </span><span class="si">{</span><span class="n">layer_name</span><span class="si">}</span><span class="s2">: 检查输入方差是否为0&quot;</span><span class="p">)</span>

        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Softmax</span><span class="p">):</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Softmax </span><span class="si">{</span><span class="n">layer_name</span><span class="si">}</span><span class="s2">: 检查是否有-inf输入导致exp(x)=0&quot;</span><span class="p">)</span>

        <span class="k">elif</span> <span class="s1">&#39;loss&#39;</span> <span class="ow">in</span> <span class="n">layer_name</span><span class="o">.</span><span class="n">lower</span><span class="p">():</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Loss layer </span><span class="si">{</span><span class="n">layer_name</span><span class="si">}</span><span class="s2">: 检查log(0)或除零&quot;</span><span class="p">)</span>

        <span class="c1"># 给出修复建议</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">建议修复方案:&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;1. 添加epsilon: x + 1e-8&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;2. 使用torch.clamp限制范围&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;3. 检查数据预处理流程&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;4. 降低学习率或使用梯度裁剪&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">get_report</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;生成诊断报告&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">problematic_layers</span><span class="p">:</span>
            <span class="k">return</span> <span class="s2">&quot;未检测到NaN&quot;</span>

        <span class="n">report</span> <span class="o">=</span> <span class="s2">&quot;NaN梯度诊断报告</span><span class="se">\n</span><span class="s2">&quot;</span> <span class="o">+</span> <span class="s2">&quot;=&quot;</span><span class="o">*</span><span class="mi">50</span> <span class="o">+</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span>

        <span class="c1"># 按出现顺序排序</span>
        <span class="k">for</span> <span class="n">issue</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">problematic_layers</span><span class="p">:</span>
            <span class="n">report</span> <span class="o">+=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">层: </span><span class="si">{</span><span class="n">issue</span><span class="p">[</span><span class="s1">&#39;layer&#39;</span><span class="p">]</span><span class="si">}</span><span class="se">\n</span><span class="s2">&quot;</span>
            <span class="n">report</span> <span class="o">+=</span> <span class="sa">f</span><span class="s2">&quot;类型: </span><span class="si">{</span><span class="n">issue</span><span class="p">[</span><span class="s1">&#39;type&#39;</span><span class="p">]</span><span class="si">}</span><span class="se">\n</span><span class="s2">&quot;</span>
            <span class="n">report</span> <span class="o">+=</span> <span class="sa">f</span><span class="s2">&quot;阶段: </span><span class="si">{</span><span class="n">issue</span><span class="p">[</span><span class="s1">&#39;stage&#39;</span><span class="p">]</span><span class="si">}</span><span class="se">\n</span><span class="s2">&quot;</span>
            <span class="n">report</span> <span class="o">+=</span> <span class="s2">&quot;-&quot;</span><span class="o">*</span><span class="mi">30</span> <span class="o">+</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span>

        <span class="c1"># 给出最可能的根因</span>
        <span class="n">first_issue</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">problematic_layers</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">report</span> <span class="o">+=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">最可能的根因: </span><span class="si">{</span><span class="n">first_issue</span><span class="p">[</span><span class="s1">&#39;layer&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2">层的</span><span class="si">{</span><span class="n">first_issue</span><span class="p">[</span><span class="s1">&#39;type&#39;</span><span class="p">]</span><span class="si">}</span><span class="se">\n</span><span class="s2">&quot;</span>

        <span class="k">return</span> <span class="n">report</span>
</code></pre></div>

<p>这个工具能够：</p>
<ol>
<li>精确定位产生NaN的层和操作</li>
<li>区分前向和反向传播中的NaN</li>
<li>针对VLM特有组件给出诊断</li>
<li>提供具体的修复建议</li>
</ol>
</details>
<p><strong>练习 10.6：崩溃预测系统</strong>
设计一个系统，能够在训练真正崩溃前 10-20 步预测即将发生的崩溃，并自动采取预防措施。</p>
<p>💡 <strong>提示</strong>：监控多个指标的趋势变化</p>
<details>
<summary>📝 参考答案</summary>
<div class="codehilite"><pre><span></span><code><span class="k">class</span> <span class="nc">CrashPredictor</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">window_size</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">alert_threshold</span><span class="o">=</span><span class="mf">0.8</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">window_size</span> <span class="o">=</span> <span class="n">window_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">alert_threshold</span> <span class="o">=</span> <span class="n">alert_threshold</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">metrics_history</span> <span class="o">=</span> <span class="n">defaultdict</span><span class="p">(</span><span class="k">lambda</span><span class="p">:</span> <span class="n">deque</span><span class="p">(</span><span class="n">maxlen</span><span class="o">=</span><span class="n">window_size</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">crash_probability</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="k">def</span> <span class="nf">update_metrics</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">step</span><span class="p">,</span> <span class="n">loss</span><span class="p">,</span> <span class="n">grad_norm</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;更新监控指标&quot;&quot;&quot;</span>

        <span class="c1"># 记录原始指标</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">metrics_history</span><span class="p">[</span><span class="s1">&#39;loss&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">metrics_history</span><span class="p">[</span><span class="s1">&#39;grad_norm&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">grad_norm</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">metrics_history</span><span class="p">[</span><span class="s1">&#39;lr&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">learning_rate</span><span class="p">)</span>

        <span class="c1"># 计算导数指标</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">metrics_history</span><span class="p">[</span><span class="s1">&#39;loss&#39;</span><span class="p">])</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">loss_delta</span> <span class="o">=</span> <span class="n">loss</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">metrics_history</span><span class="p">[</span><span class="s1">&#39;loss&#39;</span><span class="p">][</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">metrics_history</span><span class="p">[</span><span class="s1">&#39;loss_delta&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss_delta</span><span class="p">)</span>

            <span class="c1"># 二阶导数（加速度）</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">metrics_history</span><span class="p">[</span><span class="s1">&#39;loss_delta&#39;</span><span class="p">])</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
                <span class="n">loss_accel</span> <span class="o">=</span> <span class="n">loss_delta</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">metrics_history</span><span class="p">[</span><span class="s1">&#39;loss_delta&#39;</span><span class="p">][</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">metrics_history</span><span class="p">[</span><span class="s1">&#39;loss_accel&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss_accel</span><span class="p">)</span>

        <span class="c1"># 预测崩溃概率</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">crash_probability</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_predict_crash</span><span class="p">()</span>

        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">crash_probability</span>

    <span class="k">def</span> <span class="nf">_predict_crash</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;基于多个信号预测崩溃概率&quot;&quot;&quot;</span>

        <span class="n">signals</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="c1"># 信号1：Loss连续增长</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">metrics_history</span><span class="p">[</span><span class="s1">&#39;loss&#39;</span><span class="p">])</span> <span class="o">&gt;=</span> <span class="mi">3</span><span class="p">:</span>
            <span class="n">recent_losses</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">metrics_history</span><span class="p">[</span><span class="s1">&#39;loss&#39;</span><span class="p">])[</span><span class="o">-</span><span class="mi">3</span><span class="p">:]</span>
            <span class="k">if</span> <span class="nb">all</span><span class="p">(</span><span class="n">recent_losses</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">&lt;</span> <span class="n">recent_losses</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span> 
                   <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">recent_losses</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span><span class="p">)):</span>
                <span class="n">signals</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="mf">0.3</span><span class="p">)</span>

        <span class="c1"># 信号2：Loss增长加速</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">metrics_history</span><span class="p">[</span><span class="s1">&#39;loss_accel&#39;</span><span class="p">])</span> <span class="o">&gt;=</span> <span class="mi">2</span><span class="p">:</span>
            <span class="n">recent_accel</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">metrics_history</span><span class="p">[</span><span class="s1">&#39;loss_accel&#39;</span><span class="p">])[</span><span class="o">-</span><span class="mi">2</span><span class="p">:]</span>
            <span class="k">if</span> <span class="nb">all</span><span class="p">(</span><span class="n">a</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">a</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">metrics_history</span><span class="p">[</span><span class="s1">&#39;loss&#39;</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="mf">0.1</span> 
                   <span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="n">recent_accel</span><span class="p">):</span>
                <span class="n">signals</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="mf">0.4</span><span class="p">)</span>

        <span class="c1"># 信号3：梯度范数指数增长</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">metrics_history</span><span class="p">[</span><span class="s1">&#39;grad_norm&#39;</span><span class="p">])</span> <span class="o">&gt;=</span> <span class="mi">3</span><span class="p">:</span>
            <span class="n">recent_grads</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">metrics_history</span><span class="p">[</span><span class="s1">&#39;grad_norm&#39;</span><span class="p">])[</span><span class="o">-</span><span class="mi">3</span><span class="p">:]</span>
            <span class="k">if</span> <span class="n">recent_grads</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">&gt;</span> <span class="n">recent_grads</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="mi">5</span><span class="p">:</span>
                <span class="n">signals</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)</span>

        <span class="c1"># 信号4：梯度范数超过历史99分位</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">metrics_history</span><span class="p">[</span><span class="s1">&#39;grad_norm&#39;</span><span class="p">])</span> <span class="o">&gt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">window_size</span><span class="p">:</span>
            <span class="n">threshold</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">percentile</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">metrics_history</span><span class="p">[</span><span class="s1">&#39;grad_norm&#39;</span><span class="p">],</span> <span class="mi">99</span><span class="p">)</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">metrics_history</span><span class="p">[</span><span class="s1">&#39;grad_norm&#39;</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">&gt;</span> <span class="n">threshold</span> <span class="o">*</span> <span class="mi">2</span><span class="p">:</span>
                <span class="n">signals</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="mf">0.6</span><span class="p">)</span>

        <span class="c1"># 综合所有信号</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">signals</span><span class="p">:</span>
            <span class="k">return</span> <span class="mf">0.0</span>

        <span class="c1"># 使用概率组合公式</span>
        <span class="n">combined_prob</span> <span class="o">=</span> <span class="mf">1.0</span>
        <span class="k">for</span> <span class="n">signal</span> <span class="ow">in</span> <span class="n">signals</span><span class="p">:</span>
            <span class="n">combined_prob</span> <span class="o">*=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">signal</span><span class="p">)</span>
        <span class="n">crash_prob</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">combined_prob</span>

        <span class="k">return</span> <span class="n">crash_prob</span>

    <span class="k">def</span> <span class="nf">get_preventive_action</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;根据崩溃概率返回预防措施&quot;&quot;&quot;</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">crash_probability</span> <span class="o">&lt;</span> <span class="mf">0.3</span><span class="p">:</span>
            <span class="k">return</span> <span class="kc">None</span>

        <span class="n">actions</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">crash_probability</span> <span class="o">&gt;=</span> <span class="mf">0.3</span><span class="p">:</span>
            <span class="n">actions</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="s1">&#39;save_checkpoint&#39;</span><span class="p">,</span> <span class="s1">&#39;Preventive checkpoint&#39;</span><span class="p">))</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">crash_probability</span> <span class="o">&gt;=</span> <span class="mf">0.5</span><span class="p">:</span>
            <span class="n">actions</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="s1">&#39;reduce_lr&#39;</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">))</span>  <span class="c1"># 降低学习率50%</span>
            <span class="n">actions</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="s1">&#39;increase_grad_clip&#39;</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">))</span>  <span class="c1"># 加强梯度裁剪</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">crash_probability</span> <span class="o">&gt;=</span> <span class="mf">0.7</span><span class="p">:</span>
            <span class="n">actions</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="s1">&#39;reduce_batch_size&#39;</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">))</span>  <span class="c1"># 减小batch size</span>
            <span class="n">actions</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="s1">&#39;switch_to_fp32&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">))</span>  <span class="c1"># 切换到FP32</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">crash_probability</span> <span class="o">&gt;=</span> <span class="mf">0.9</span><span class="p">:</span>
            <span class="n">actions</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="s1">&#39;emergency_stop&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">))</span>  <span class="c1"># 紧急停止</span>

        <span class="k">return</span> <span class="n">actions</span>

    <span class="k">def</span> <span class="nf">apply_preventive_actions</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">actions</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">config</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;应用预防措施&quot;&quot;&quot;</span>

        <span class="k">for</span> <span class="n">action</span><span class="p">,</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">actions</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">action</span> <span class="o">==</span> <span class="s1">&#39;save_checkpoint&#39;</span><span class="p">:</span>
                <span class="n">save_emergency_checkpoint</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">param</span><span class="p">)</span>

            <span class="k">elif</span> <span class="n">action</span> <span class="o">==</span> <span class="s1">&#39;reduce_lr&#39;</span><span class="p">:</span>
                <span class="k">for</span> <span class="n">param_group</span> <span class="ow">in</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">:</span>
                    <span class="n">param_group</span><span class="p">[</span><span class="s1">&#39;lr&#39;</span><span class="p">]</span> <span class="o">*=</span> <span class="n">param</span>
                <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;降低学习率到 </span><span class="si">{</span><span class="n">param_group</span><span class="p">[</span><span class="s1">&#39;lr&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

            <span class="k">elif</span> <span class="n">action</span> <span class="o">==</span> <span class="s1">&#39;increase_grad_clip&#39;</span><span class="p">:</span>
                <span class="n">config</span><span class="o">.</span><span class="n">grad_clip_norm</span> <span class="o">*=</span> <span class="n">param</span>
                <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;加强梯度裁剪到 </span><span class="si">{</span><span class="n">config</span><span class="o">.</span><span class="n">grad_clip_norm</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

            <span class="k">elif</span> <span class="n">action</span> <span class="o">==</span> <span class="s1">&#39;reduce_batch_size&#39;</span><span class="p">:</span>
                <span class="n">config</span><span class="o">.</span><span class="n">batch_size</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">batch_size</span> <span class="o">*</span> <span class="n">param</span><span class="p">)</span>
                <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;减小batch size到 </span><span class="si">{</span><span class="n">config</span><span class="o">.</span><span class="n">batch_size</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

            <span class="k">elif</span> <span class="n">action</span> <span class="o">==</span> <span class="s1">&#39;switch_to_fp32&#39;</span><span class="p">:</span>
                <span class="n">model</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
                <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;切换到FP32精度&quot;</span><span class="p">)</span>

            <span class="k">elif</span> <span class="n">action</span> <span class="o">==</span> <span class="s1">&#39;emergency_stop&#39;</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;检测到即将崩溃，紧急停止训练！&quot;</span><span class="p">)</span>
                <span class="k">return</span> <span class="kc">False</span>  <span class="c1"># 停止训练</span>

        <span class="k">return</span> <span class="kc">True</span>  <span class="c1"># 继续训练</span>
</code></pre></div>

<p>该系统的特点：</p>
<ol>
<li>多指标联合监控（loss、梯度、学习率）</li>
<li>基于趋势而非单点值判断</li>
<li>分级响应机制</li>
<li>预防措施递进式增强</li>
<li>保留紧急停止选项避免资源浪费</li>
</ol>
</details>
<h2 id="_7">常见陷阱与错误</h2>
<h3 id="1">1. 忽视早期信号</h3>
<p>❌ <strong>错误</strong>：等到 Loss 完全变成 NaN 才处理
✅ <strong>正确</strong>：在 Loss 开始异常增长时就介入</p>
<h3 id="2">2. 过度依赖自动混合精度</h3>
<p>❌ <strong>错误</strong>：完全信任 AMP 的 loss scaling
✅ <strong>正确</strong>：手动检查关键操作的数值范围</p>
<h3 id="3-checkpoint">3. Checkpoint 不完整</h3>
<p>❌ <strong>错误</strong>：只保存模型权重
✅ <strong>正确</strong>：保存完整训练状态（包括优化器、随机数种子）</p>
<h3 id="4">4. 梯度裁剪时机错误</h3>
<p>❌ <strong>错误</strong>：在 loss.backward() 之前裁剪
✅ <strong>正确</strong>：在 backward 之后、optimizer.step() 之前裁剪</p>
<h3 id="5">5. 忽略数据问题</h3>
<p>❌ <strong>错误</strong>：只关注模型和优化器
✅ <strong>正确</strong>：检查数据预处理、标签正确性、异常样本</p>
<h3 id="6">6. 恢复训练后不验证</h3>
<p>❌ <strong>错误</strong>：加载 checkpoint 后直接继续训练
✅ <strong>正确</strong>：先在验证集上测试，确认状态正确</p>
<h2 id="_8">最佳实践检查清单</h2>
<h3 id="_9">训练前准备</h3>
<ul>
<li>[ ] 配置完整的 checkpoint 保存机制</li>
<li>[ ] 设置合理的梯度裁剪阈值（基于小规模实验）</li>
<li>[ ] 准备 FP32 降级方案</li>
<li>[ ] 实现梯度监控和日志记录</li>
<li>[ ] 验证数据加载和预处理流程</li>
<li>[ ] 测试 checkpoint 恢复流程</li>
</ul>
<h3 id="_10">训练中监控</h3>
<ul>
<li>[ ] 每 N 步检查梯度范数分布</li>
<li>[ ] 监控 Loss 的一阶和二阶导数</li>
<li>[ ] 关注关键层的参数和梯度统计</li>
<li>[ ] 定期保存 checkpoint（至少每小时）</li>
<li>[ ] 设置异常值报警阈值</li>
</ul>
<h3 id="_11">崩溃后恢复</h3>
<ul>
<li>[ ] 分析崩溃前的日志和指标</li>
<li>[ ] 识别崩溃模式（突发/渐进/周期）</li>
<li>[ ] 调整配置（学习率、batch size、精度）</li>
<li>[ ] 从最近的稳定 checkpoint 恢复</li>
<li>[ ] 验证恢复后的模型行为</li>
<li>[ ] 记录问题和解决方案供未来参考</li>
</ul>
<h3 id="_12">长期优化</h3>
<ul>
<li>[ ] 建立崩溃案例库</li>
<li>[ ] 总结不同模型架构的稳定性特点</li>
<li>[ ] 优化数据管道减少异常样本</li>
<li>[ ] 实现自动化的崩溃检测和恢复</li>
<li>[ ] 定期更新监控指标和阈值</li>
</ul>
            </article>
            
            <nav class="page-nav"><a href="chapter9.html" class="nav-link prev">← 第 9 章：CUDA OOM 调试完全指南</a><a href="chapter11.html" class="nav-link next">第 11 章：训练速度优化实战 →</a></nav>
        </main>
    </div>
</body>
</html>