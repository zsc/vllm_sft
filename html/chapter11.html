<!DOCTYPE html>
<html lang="zh">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <base href="./">
    <title>ç¬¬ 11 ç« ï¼šè®­ç»ƒé€Ÿåº¦ä¼˜åŒ–å®æˆ˜</title>
    <link rel="stylesheet" href="assets/style.css">
    <link rel="stylesheet" href="assets/highlight.css">
    <script src="assets/script.js" defer></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>
        window.MathJax = {
            tex: {
                inlineMath: [['$', '$']],
                displayMath: [['$$', '$$']],
                processEscapes: false,
                packages: {'[+]': ['noerrors', 'ams']}
            },
            options: {
                ignoreHtmlClass: 'tex2jax_ignore',
                processHtmlClass: 'tex2jax_process'
            },
            loader: {
                load: ['[tex]/noerrors', '[tex]/ams']
            }
        };
    </script>
</head>
<body>
    <div class="container">
        <nav id="sidebar" class="sidebar">
            <div class="sidebar-header">
                <h3>ç›®å½•</h3>
                <button id="sidebar-toggle" class="sidebar-toggle">
                    <span></span>
                    <span></span>
                    <span></span>
                </button>
            </div>
            <div class="sidebar-search">
                <input type="text" id="sidebar-search-input" placeholder="æœç´¢..." autocomplete="off">
            </div>
            <div id="tree-container">
                <nav class="tree-nav" role="tree">
                    <div class="tree-item " >
                        <a href="index.html" class="tree-link">
                            <span class="tree-icon">ğŸ“„</span>
                            <span class="tree-title">è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰çš„ç›‘ç£å¾®è°ƒä¸å¼ºåŒ–å­¦ä¹ å®æˆ˜æ•™ç¨‹</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter1.html" class="tree-link">
                            <span class="tree-icon">ğŸ“„</span>
                            <span class="tree-title">ç¬¬ 1 ç« ï¼šVLM æ¶æ„ä¸åŸç†</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter2.html" class="tree-link">
                            <span class="tree-icon">ğŸ“„</span>
                            <span class="tree-title">ç¬¬ 2 ç« ï¼šæ•°æ®å‡†å¤‡ä¸é¢„å¤„ç†</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter3.html" class="tree-link">
                            <span class="tree-icon">ğŸ“„</span>
                            <span class="tree-title">ç¬¬ 3 ç« ï¼šSFT è®­ç»ƒç­–ç•¥</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter4.html" class="tree-link">
                            <span class="tree-icon">ğŸ“„</span>
                            <span class="tree-title">ç¬¬ 4 ç« ï¼šåˆ†å¸ƒå¼è®­ç»ƒä¸ä¼˜åŒ–</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter5.html" class="tree-link">
                            <span class="tree-icon">ğŸ“„</span>
                            <span class="tree-title">ç¬¬ 5 ç« ï¼šRLHF åŸºç¡€ä¸å®ç°</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter6.html" class="tree-link">
                            <span class="tree-icon">ğŸ“„</span>
                            <span class="tree-title">ç¬¬ 6 ç« ï¼šç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter7.html" class="tree-link">
                            <span class="tree-icon">ğŸ“„</span>
                            <span class="tree-title">ç¬¬ 7 ç« ï¼šè¯„ä¼°ä½“ç³»è®¾è®¡</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter8.html" class="tree-link">
                            <span class="tree-icon">ğŸ“„</span>
                            <span class="tree-title">ç¬¬ 8 ç« ï¼šæ¨¡å‹éƒ¨ç½²ä¸æœåŠ¡åŒ–</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter9.html" class="tree-link">
                            <span class="tree-icon">ğŸ“„</span>
                            <span class="tree-title">ç¬¬ 9 ç« ï¼šCUDA OOM è°ƒè¯•å®Œå…¨æŒ‡å—</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter10.html" class="tree-link">
                            <span class="tree-icon">ğŸ“„</span>
                            <span class="tree-title">ç¬¬ 10 ç« ï¼šè®­ç»ƒå´©æºƒä¸ NaN é—®é¢˜</span>
                        </a>
                    </div>
                
                    <div class="tree-item active" >
                        <a href="chapter11.html" class="tree-link">
                            <span class="tree-icon">ğŸ“„</span>
                            <span class="tree-title">ç¬¬ 11 ç« ï¼šè®­ç»ƒé€Ÿåº¦ä¼˜åŒ–å®æˆ˜</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter12.html" class="tree-link">
                            <span class="tree-icon">ğŸ“„</span>
                            <span class="tree-title">ç¬¬ 12 ç« ï¼šå¤šæœºå¤šå¡è°ƒè¯•åœ°ç‹±</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="CLAUDE.html" class="tree-link">
                            <span class="tree-icon">ğŸ“„</span>
                            <span class="tree-title">Untitled</span>
                        </a>
                    </div>
                </nav>
            </div>
        </nav>
        
        <main class="content">
            <article>
                <h1 id="11">ç¬¬ 11 ç« ï¼šè®­ç»ƒé€Ÿåº¦ä¼˜åŒ–å®æˆ˜</h1>
<p>åœ¨ VLM è®­ç»ƒä¸­ï¼Œæ—¶é—´å°±æ˜¯é‡‘é’±ã€‚ä¸€ä¸ªéœ€è¦è¿è¡Œæ•°å‘¨çš„è®­ç»ƒä»»åŠ¡ï¼Œå¦‚æœèƒ½å¤Ÿä¼˜åŒ–åˆ°ä¸€å‘¨å†…å®Œæˆï¼Œä¸ä»…èŠ‚çœäº†å¤§é‡çš„è®¡ç®—èµ„æºæˆæœ¬ï¼Œæ›´é‡è¦çš„æ˜¯åŠ å¿«äº†æ¨¡å‹è¿­ä»£é€Ÿåº¦ã€‚æœ¬ç« å°†ä»å®æˆ˜è§’åº¦å‡ºå‘ï¼Œç³»ç»Ÿä»‹ç»å¦‚ä½•å®šä½å’Œè§£å†³ VLM è®­ç»ƒä¸­çš„æ€§èƒ½ç“¶é¢ˆï¼Œè®©æ‚¨çš„è®­ç»ƒé€Ÿåº¦æå‡ 2-5 å€ã€‚</p>
<h2 id="_1">å­¦ä¹ ç›®æ ‡</h2>
<p>å®Œæˆæœ¬ç« å­¦ä¹ åï¼Œæ‚¨å°†èƒ½å¤Ÿï¼š</p>
<ul>
<li>ä½¿ç”¨ Profile å·¥å…·ç²¾ç¡®å®šä½æ€§èƒ½ç“¶é¢ˆ</li>
<li>ä¼˜åŒ–æ•°æ®åŠ è½½ç®¡é“ï¼Œæ¶ˆé™¤ I/O ç­‰å¾…</li>
<li>å‡å°‘åˆ†å¸ƒå¼è®­ç»ƒä¸­çš„é€šä¿¡å¼€é”€</li>
<li>æ­£ç¡®ä½¿ç”¨ Flash Attention ç­‰é«˜æ•ˆç®—å­</li>
<li>å»ºç«‹ç³»ç»Ÿçš„æ€§èƒ½ä¼˜åŒ–æ€ç»´æ¡†æ¶</li>
</ul>
<h2 id="111-profile">11.1 Profile å·¥å…·å®šä½æ€§èƒ½ç“¶é¢ˆ</h2>
<p>æ€§èƒ½ä¼˜åŒ–çš„ç¬¬ä¸€æ­¥æ°¸è¿œæ˜¯æµ‹é‡ã€‚æ²¡æœ‰å‡†ç¡®çš„æ€§èƒ½æ•°æ®ï¼Œæ‰€æœ‰çš„ä¼˜åŒ–éƒ½æ˜¯ç›²ç›®çš„ã€‚æœ¬èŠ‚å°†ä»‹ç»å¦‚ä½•ä½¿ç”¨ä¸“ä¸šçš„ Profile å·¥å…·å¿«é€Ÿå®šä½ VLM è®­ç»ƒä¸­çš„æ€§èƒ½ç“¶é¢ˆã€‚</p>
<h3 id="1111-pytorch-profiler">11.1.1 PyTorch Profiler åŸºç¡€ä½¿ç”¨</h3>
<p>PyTorch Profiler æ˜¯æœ€å¸¸ç”¨çš„æ€§èƒ½åˆ†æå·¥å…·ï¼Œèƒ½å¤Ÿæä¾›è¯¦ç»†çš„ç®—å­çº§åˆ«æ€§èƒ½æ•°æ®ï¼š</p>
<div class="codehilite"><pre><span></span><code><span class="kn">import</span> <span class="nn">torch.profiler</span> <span class="k">as</span> <span class="nn">profiler</span>

<span class="c1"># åŸºç¡€ä½¿ç”¨æ¨¡å¼</span>
<span class="k">with</span> <span class="n">profiler</span><span class="o">.</span><span class="n">profile</span><span class="p">(</span>
    <span class="n">activities</span><span class="o">=</span><span class="p">[</span>
        <span class="n">profiler</span><span class="o">.</span><span class="n">ProfilerActivity</span><span class="o">.</span><span class="n">CPU</span><span class="p">,</span>
        <span class="n">profiler</span><span class="o">.</span><span class="n">ProfilerActivity</span><span class="o">.</span><span class="n">CUDA</span><span class="p">,</span>
    <span class="p">],</span>
    <span class="n">schedule</span><span class="o">=</span><span class="n">profiler</span><span class="o">.</span><span class="n">schedule</span><span class="p">(</span><span class="n">wait</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">warmup</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">active</span><span class="o">=</span><span class="mi">3</span><span class="p">),</span>
    <span class="n">on_trace_ready</span><span class="o">=</span><span class="n">profiler</span><span class="o">.</span><span class="n">tensorboard_trace_handler</span><span class="p">(</span><span class="s1">&#39;./log&#39;</span><span class="p">),</span>
    <span class="n">record_shapes</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">profile_memory</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">with_stack</span><span class="o">=</span><span class="kc">True</span>
<span class="p">)</span> <span class="k">as</span> <span class="n">prof</span><span class="p">:</span>
    <span class="k">for</span> <span class="n">step</span><span class="p">,</span> <span class="n">batch</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">dataloader</span><span class="p">):</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">targets</span><span class="p">)</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
        <span class="n">prof</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>  <span class="c1"># é€šçŸ¥ profiler è¿›å…¥ä¸‹ä¸€æ­¥</span>
</code></pre></div>

<h3 id="1112">11.1.2 å…³é”®æ€§èƒ½æŒ‡æ ‡è§£è¯»</h3>
<p>åœ¨åˆ†æ Profile ç»“æœæ—¶ï¼Œéœ€è¦é‡ç‚¹å…³æ³¨ä»¥ä¸‹æŒ‡æ ‡ï¼š</p>
<p><strong>GPU åˆ©ç”¨ç‡å±‚æ¬¡</strong>ï¼š</p>
<div class="codehilite"><pre><span></span><code>ç†æƒ³çŠ¶æ€ï¼š&gt;95% SM Occupancy
è‰¯å¥½çŠ¶æ€ï¼š85-95% 
éœ€è¦ä¼˜åŒ–ï¼š70-85%
ä¸¥é‡é—®é¢˜ï¼š&lt;70%
</code></pre></div>

<p><strong>æ—¶é—´åˆ†å¸ƒåˆ†æ</strong>ï¼š</p>
<ul>
<li><strong>è®¡ç®—æ—¶é—´</strong>ï¼šå‰å‘ä¼ æ’­ + åå‘ä¼ æ’­çš„çº¯è®¡ç®—æ—¶é—´</li>
<li><strong>é€šä¿¡æ—¶é—´</strong>ï¼šAll-Reduceã€Broadcast ç­‰é›†åˆé€šä¿¡æ—¶é—´</li>
<li><strong>æ•°æ®åŠ è½½æ—¶é—´</strong>ï¼šä» DataLoader è·å–æ•°æ®çš„æ—¶é—´</li>
<li><strong>CPU-GPU åŒæ­¥æ—¶é—´</strong>ï¼š.item()ã€.cpu() ç­‰æ“ä½œå¯¼è‡´çš„ç­‰å¾…</li>
</ul>
<h3 id="1113-vlm">11.1.3 VLM ç‰¹æœ‰çš„æ€§èƒ½ç“¶é¢ˆ</h3>
<p>VLM è®­ç»ƒç›¸æ¯”çº¯è¯­è¨€æ¨¡å‹æœ‰å…¶ç‰¹æ®Šçš„æ€§èƒ½æŒ‘æˆ˜ï¼š</p>
<ol>
<li><strong>è§†è§‰ç¼–ç å™¨ç“¶é¢ˆ</strong></li>
</ol>
<p>è§†è§‰ç¼–ç å™¨ï¼ˆå¦‚ ViTï¼‰çš„è®¡ç®—æ¨¡å¼ä¸è¯­è¨€æ¨¡å‹å·®å¼‚å¾ˆå¤§ï¼š</p>
<div class="codehilite"><pre><span></span><code>å…¸å‹é—®é¢˜ï¼š

<span class="k">-</span> Patch Embedding çš„å†…å­˜è®¿é—®æ¨¡å¼ä¸å‹å¥½
<span class="k">-</span> å¤šå°ºåº¦å›¾åƒå¯¼è‡´çš„åŠ¨æ€ batch é—®é¢˜
<span class="k">-</span> Vision Transformer çš„æ³¨æ„åŠ›è®¡ç®—å¼€é”€

è¯†åˆ«æ–¹æ³•ï¼š

1. è§‚å¯Ÿ vision_encoder.forward() å æ€»æ—¶é—´æ¯”ä¾‹
2. å¦‚æœè¶…è¿‡ 40%ï¼Œè¯´æ˜è§†è§‰ç¼–ç å™¨æ˜¯ç“¶é¢ˆ
3. æ£€æŸ¥æ˜¯å¦æ¯ä¸ª step éƒ½åœ¨è¿è¡Œè§†è§‰ç¼–ç å™¨
</code></pre></div>

<ol start="2">
<li><strong>å¤šæ¨¡æ€æŠ•å½±å±‚å¼€é”€</strong></li>
</ol>
<p>è¿æ¥è§†è§‰å’Œè¯­è¨€æ¨¡æ€çš„æŠ•å½±å±‚è™½ç„¶å‚æ•°é‡ä¸å¤§ï¼Œä½†å¯èƒ½æˆä¸ºç“¶é¢ˆï¼š</p>
<div class="codehilite"><pre><span></span><code>å¸¸è§é—®é¢˜ï¼š

- MLP Projector çš„çŸ©é˜µä¹˜æ³•æ²¡æœ‰è¾¾åˆ°æœ€ä¼˜ tile size
- Cross-attention çš„ Qã€Kã€V æŠ•å½±è®¡ç®—åˆ†æ•£
- Resampler ç±»ç»“æ„çš„é¢å¤–è®¡ç®—å¼€é”€
</code></pre></div>

<ol start="3">
<li><strong>åŠ¨æ€åºåˆ—é•¿åº¦é—®é¢˜</strong></li>
</ol>
<p>VLM çš„åºåˆ—é•¿åº¦å˜åŒ–æ¯”çº¯æ–‡æœ¬æ¨¡å‹æ›´å‰§çƒˆï¼š</p>
<div class="codehilite"><pre><span></span><code>å½±å“å› ç´ ï¼š

- å›¾åƒæ•°é‡ä¸å›ºå®šï¼ˆ0-8 å¼ å›¾ç‰‡ï¼‰
- å›¾åƒåˆ†è¾¨ç‡ä¸åŒï¼ˆ224x224 åˆ° 1344x1344ï¼‰
- æ–‡æœ¬é•¿åº¦å˜åŒ–ï¼ˆ10 tokens åˆ° 8K tokensï¼‰

ä¼˜åŒ–ç­–ç•¥ï¼š

- Padding ç­–ç•¥ï¼šé™æ€ padding vs åŠ¨æ€ padding
- Bucketingï¼šå°†ç›¸ä¼¼é•¿åº¦çš„æ ·æœ¬åˆ†ç»„
- Pack/Unpackï¼šå¤šä¸ªçŸ­åºåˆ—æ‰“åŒ…æˆä¸€ä¸ªé•¿åºåˆ—
</code></pre></div>

<h3 id="1114-nvidia-nsight-systems">11.1.4 NVIDIA Nsight Systems æ·±åº¦åˆ†æ</h3>
<p>å½“ PyTorch Profiler ä¸å¤Ÿç”¨æ—¶ï¼ŒNsight Systems æä¾›æ›´åº•å±‚çš„åˆ†æï¼š</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># æ”¶é›†æ€§èƒ½æ•°æ®</span>
nsys<span class="w"> </span>profile<span class="w"> </span>-w<span class="w"> </span><span class="nb">true</span><span class="w"> </span>-t<span class="w"> </span>cuda,cudnn,cublas,nvtx<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-o<span class="w"> </span>profile_report<span class="w"> </span>--force-overwrite<span class="w"> </span><span class="nb">true</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>python<span class="w"> </span>train_vlm.py

<span class="c1"># ç”Ÿæˆå¯è§†åŒ–æŠ¥å‘Š</span>
nsys-ui<span class="w"> </span>profile_report.nsys-rep
</code></pre></div>

<p>é‡ç‚¹å…³æ³¨çš„ Kernel çº§åˆ«æŒ‡æ ‡ï¼š</p>
<div class="codehilite"><pre><span></span><code>å…³é”® Kernel åˆ†æï¼š

1. GEMM æ“ä½œï¼š
   - æ˜¯å¦ä½¿ç”¨äº† TensorCore
   - Tile é…ç½®æ˜¯å¦åˆç†
   - è®¿å­˜æ˜¯å¦å¯¹é½

2. Attention æ“ä½œï¼š
   - æ˜¯å¦å­˜åœ¨å¤§é‡å° kernel å¯åŠ¨
   - Softmax æ˜¯å¦æˆä¸ºç“¶é¢ˆ
   - QKV è®¡ç®—æ˜¯å¦èåˆ

3. é€šä¿¡æ“ä½œï¼š
   - AllReduce æ˜¯å¦ä¸è®¡ç®—é‡å 
   - æ˜¯å¦å­˜åœ¨ä¸å¿…è¦çš„åŒæ­¥ç‚¹
</code></pre></div>

<h3 id="1115">11.1.5 æ€§èƒ½ç“¶é¢ˆå®šä½å†³ç­–æ ‘</h3>
<div class="codehilite"><pre><span></span><code>æ€§èƒ½é—®é¢˜è¯Šæ–­æµç¨‹ï¼š

GPU åˆ©ç”¨ç‡ä½ï¼Ÿ
â”œâ”€â”€ Yes â†’ æ£€æŸ¥æ•°æ®åŠ è½½
â”‚   â”œâ”€â”€ DataLoader è€—æ—¶é•¿ â†’ ä¼˜åŒ–æ•°æ®ç®¡é“ï¼ˆè§ 11.2ï¼‰
â”‚   â””â”€â”€ CPU é¢„å¤„ç†æ…¢ â†’ ä½¿ç”¨ GPU é¢„å¤„ç†
â”œâ”€â”€ No â†’ æ£€æŸ¥ GPU å†…éƒ¨æ•ˆç‡
    â”œâ”€â”€ å†…å­˜å¸¦å®½å—é™ â†’ ä½¿ç”¨ Flash Attentionï¼ˆè§ 11.4ï¼‰
    â”œâ”€â”€ è®¡ç®—æ•ˆç‡ä½ â†’ æ£€æŸ¥ Tensor Core ä½¿ç”¨ç‡
    â””â”€â”€ é€šä¿¡å¼€é”€å¤§ â†’ ä¼˜åŒ–é€šä¿¡ç­–ç•¥ï¼ˆè§ 11.3ï¼‰
</code></pre></div>

<h2 id="112">11.2 æ•°æ®åŠ è½½ä¼˜åŒ–</h2>
<p>æ•°æ®åŠ è½½æ˜¯ VLM è®­ç»ƒä¸­æœ€å®¹æ˜“è¢«å¿½è§†ä½†åˆè‡³å…³é‡è¦çš„ç¯èŠ‚ã€‚ä¸€ä¸ªä¼˜åŒ–ä¸å½“çš„æ•°æ®ç®¡é“å¯èƒ½è®©æ˜‚è´µçš„ GPU æœ‰ 30-50% çš„æ—¶é—´åœ¨ç©ºè½¬ç­‰å¾…æ•°æ®ã€‚</p>
<h3 id="1121">11.2.1 é¢„å–ä¸ç¼“å­˜ç­–ç•¥</h3>
<p><strong>å¤šçº§ç¼“å­˜è®¾è®¡</strong>ï¼š</p>
<div class="codehilite"><pre><span></span><code><span class="k">class</span> <span class="nc">OptimizedVLMDataset</span><span class="p">(</span><span class="n">Dataset</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data_path</span><span class="p">,</span> <span class="n">cache_size</span><span class="o">=</span><span class="mi">1000</span><span class="p">):</span>
        <span class="c1"># ä¸‰çº§ç¼“å­˜è®¾è®¡</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">memory_cache</span> <span class="o">=</span> <span class="p">{}</span>  <span class="c1"># ä¸€çº§ï¼šå†…å­˜ç¼“å­˜</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ssd_cache_path</span> <span class="o">=</span> <span class="s2">&quot;/ssd_cache&quot;</span>  <span class="c1"># äºŒçº§ï¼šSSD ç¼“å­˜</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">source_path</span> <span class="o">=</span> <span class="n">data_path</span>  <span class="c1"># ä¸‰çº§ï¼šåŸå§‹å­˜å‚¨</span>

        <span class="c1"># é¢„å–é˜Ÿåˆ—</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">prefetch_queue</span> <span class="o">=</span> <span class="n">Queue</span><span class="p">(</span><span class="n">maxsize</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">prefetch_thread</span> <span class="o">=</span> <span class="n">Thread</span><span class="p">(</span><span class="n">target</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_prefetch_worker</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">prefetch_thread</span><span class="o">.</span><span class="n">start</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">_prefetch_worker</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;åå°é¢„å–çº¿ç¨‹&quot;&quot;&quot;</span>
        <span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
            <span class="n">idx</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">prefetch_queue</span><span class="o">.</span><span class="n">get</span><span class="p">()</span>
            <span class="k">if</span> <span class="n">idx</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">break</span>
            <span class="c1"># é¢„åŠ è½½åˆ°å†…å­˜ç¼“å­˜</span>
            <span class="k">if</span> <span class="n">idx</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">memory_cache</span><span class="p">:</span>
                <span class="n">data</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_load_from_disk</span><span class="p">(</span><span class="n">idx</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">memory_cache</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">data</span>
</code></pre></div>

<p><strong>æ™ºèƒ½ç¼“å­˜æ·˜æ±°ç­–ç•¥</strong>ï¼š</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># LRU + é¢„æµ‹æ€§ç¼“å­˜</span>
<span class="k">class</span> <span class="nc">PredictiveCache</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">capacity</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">capacity</span> <span class="o">=</span> <span class="n">capacity</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cache</span> <span class="o">=</span> <span class="n">OrderedDict</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">access_pattern</span> <span class="o">=</span> <span class="p">[]</span>  <span class="c1"># è®°å½•è®¿é—®æ¨¡å¼</span>

    <span class="k">def</span> <span class="nf">get</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">key</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">key</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">cache</span><span class="p">:</span>
            <span class="c1"># LRU æ›´æ–°</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">cache</span><span class="o">.</span><span class="n">move_to_end</span><span class="p">(</span><span class="n">key</span><span class="p">)</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">cache</span><span class="p">[</span><span class="n">key</span><span class="p">]</span>
        <span class="k">return</span> <span class="kc">None</span>

    <span class="k">def</span> <span class="nf">put</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">cache</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">capacity</span><span class="p">:</span>
            <span class="c1"># åŸºäºè®¿é—®æ¨¡å¼é¢„æµ‹çš„æ·˜æ±°</span>
            <span class="n">victim</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_predict_victim</span><span class="p">()</span>
            <span class="k">del</span> <span class="bp">self</span><span class="o">.</span><span class="n">cache</span><span class="p">[</span><span class="n">victim</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cache</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">value</span>

    <span class="k">def</span> <span class="nf">_predict_victim</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># åˆ†æè®¿é—®æ¨¡å¼ï¼Œæ·˜æ±°æœ€ä¸å¯èƒ½è¢«è®¿é—®çš„æ•°æ®</span>
        <span class="c1"># è€ƒè™‘ï¼šé¡ºåºè®¿é—®ã€éšæœºè®¿é—®ã€å¾ªç¯è®¿é—®ç­‰æ¨¡å¼</span>
        <span class="k">pass</span>
</code></pre></div>

<h3 id="1122">11.2.2 å¤šè¿›ç¨‹æ•°æ®åŠ è½½ä¼˜åŒ–</h3>
<p><strong>æœ€ä¼˜ worker æ•°é‡ç¡®å®š</strong>ï¼š</p>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">find_optimal_num_workers</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;è‡ªåŠ¨ç¡®å®šæœ€ä¼˜çš„ DataLoader worker æ•°é‡&quot;&quot;&quot;</span>
    <span class="kn">import</span> <span class="nn">time</span>

    <span class="n">times</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">num_workers</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">33</span><span class="p">,</span> <span class="mi">2</span><span class="p">):</span>  <span class="c1"># æµ‹è¯• 2-32 ä¸ª workers</span>
        <span class="n">loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span>
            <span class="n">dataset</span><span class="p">,</span> 
            <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
            <span class="n">num_workers</span><span class="o">=</span><span class="n">num_workers</span><span class="p">,</span>
            <span class="n">pin_memory</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="n">persistent_workers</span><span class="o">=</span><span class="kc">True</span>
        <span class="p">)</span>

        <span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">batch</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">loader</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">i</span> <span class="o">&gt;=</span> <span class="mi">10</span><span class="p">:</span>  <span class="c1"># æµ‹è¯• 10 ä¸ª batch</span>
                <span class="k">break</span>
        <span class="n">end</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>

        <span class="n">times</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">num_workers</span><span class="p">,</span> <span class="n">end</span> <span class="o">-</span> <span class="n">start</span><span class="p">))</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Workers: </span><span class="si">{</span><span class="n">num_workers</span><span class="si">}</span><span class="s2">, Time: </span><span class="si">{</span><span class="n">end</span><span class="o">-</span><span class="n">start</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">s&quot;</span><span class="p">)</span>

    <span class="c1"># è¿”å›æœ€å¿«çš„é…ç½®</span>
    <span class="k">return</span> <span class="nb">min</span><span class="p">(</span><span class="n">times</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">])[</span><span class="mi">0</span><span class="p">]</span>
</code></pre></div>

<p><strong>è¿›ç¨‹é—´é€šä¿¡ä¼˜åŒ–</strong>ï¼š</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># ä½¿ç”¨å…±äº«å†…å­˜å‡å°‘è¿›ç¨‹é—´æ•°æ®æ‹·è´</span>
<span class="kn">import</span> <span class="nn">torch.multiprocessing</span> <span class="k">as</span> <span class="nn">mp</span>

<span class="k">class</span> <span class="nc">SharedMemoryDataset</span><span class="p">(</span><span class="n">Dataset</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="p">):</span>
        <span class="c1"># å°†æ•°æ®æ”¾å…¥å…±äº«å†…å­˜</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">shared_data</span> <span class="o">=</span> <span class="n">mp</span><span class="o">.</span><span class="n">Manager</span><span class="p">()</span><span class="o">.</span><span class="n">list</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
        <span class="c1"># å¯¹äºå¤§å‹å¼ é‡ï¼Œä½¿ç”¨ shared_memory</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tensor_cache</span> <span class="o">=</span> <span class="p">{}</span>

    <span class="k">def</span> <span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">idx</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">idx</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">tensor_cache</span><span class="p">:</span>
            <span class="c1"># ç¬¬ä¸€æ¬¡è®¿é—®ï¼Œåˆ›å»ºå…±äº«å†…å­˜å¼ é‡</span>
            <span class="n">tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">shared_data</span><span class="p">[</span><span class="n">idx</span><span class="p">])</span>
            <span class="n">tensor</span><span class="o">.</span><span class="n">share_memory_</span><span class="p">()</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">tensor_cache</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">tensor</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">tensor_cache</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
</code></pre></div>

<h3 id="1123">11.2.3 å›¾åƒé¢„å¤„ç†ä¼˜åŒ–</h3>
<p><strong>GPU åŠ é€Ÿé¢„å¤„ç†</strong>ï¼š</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># ä½¿ç”¨ NVIDIA DALI è¿›è¡Œ GPU é¢„å¤„ç†</span>
<span class="kn">import</span> <span class="nn">nvidia.dali</span> <span class="k">as</span> <span class="nn">dali</span>
<span class="kn">import</span> <span class="nn">nvidia.dali.fn</span> <span class="k">as</span> <span class="nn">fn</span>
<span class="kn">from</span> <span class="nn">nvidia.dali.pipeline</span> <span class="kn">import</span> <span class="n">Pipeline</span>

<span class="k">class</span> <span class="nc">VLMPreprocessPipeline</span><span class="p">(</span><span class="n">Pipeline</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">num_threads</span><span class="p">,</span> <span class="n">device_id</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_threads</span><span class="p">,</span> <span class="n">device_id</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">define_graph</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># åœ¨ GPU ä¸Šè¿›è¡Œæ‰€æœ‰é¢„å¤„ç†</span>
        <span class="n">images</span> <span class="o">=</span> <span class="n">fn</span><span class="o">.</span><span class="n">external_source</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s2">&quot;images&quot;</span><span class="p">)</span>

        <span class="c1"># GPU è§£ç </span>
        <span class="n">images</span> <span class="o">=</span> <span class="n">fn</span><span class="o">.</span><span class="n">decoders</span><span class="o">.</span><span class="n">image</span><span class="p">(</span><span class="n">images</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s2">&quot;mixed&quot;</span><span class="p">)</span>

        <span class="c1"># GPU ä¸Šçš„ resize å’Œ crop</span>
        <span class="n">images</span> <span class="o">=</span> <span class="n">fn</span><span class="o">.</span><span class="n">resize</span><span class="p">(</span>
            <span class="n">images</span><span class="p">,</span>
            <span class="n">size</span><span class="o">=</span><span class="p">[</span><span class="mi">336</span><span class="p">,</span> <span class="mi">336</span><span class="p">],</span>
            <span class="n">interp_type</span><span class="o">=</span><span class="n">dali</span><span class="o">.</span><span class="n">types</span><span class="o">.</span><span class="n">INTERP_LINEAR</span><span class="p">,</span>
            <span class="n">device</span><span class="o">=</span><span class="s2">&quot;gpu&quot;</span>
        <span class="p">)</span>

        <span class="c1"># GPU ä¸Šçš„å½’ä¸€åŒ–</span>
        <span class="n">images</span> <span class="o">=</span> <span class="n">fn</span><span class="o">.</span><span class="n">normalize</span><span class="p">(</span>
            <span class="n">images</span><span class="p">,</span>
            <span class="n">mean</span><span class="o">=</span><span class="p">[</span><span class="mf">0.485</span><span class="p">,</span> <span class="mf">0.456</span><span class="p">,</span> <span class="mf">0.406</span><span class="p">],</span>
            <span class="n">std</span><span class="o">=</span><span class="p">[</span><span class="mf">0.229</span><span class="p">,</span> <span class="mf">0.224</span><span class="p">,</span> <span class="mf">0.225</span><span class="p">],</span>
            <span class="n">device</span><span class="o">=</span><span class="s2">&quot;gpu&quot;</span>
        <span class="p">)</span>

        <span class="k">return</span> <span class="n">images</span>
</code></pre></div>

<p><strong>æ‰¹é‡åŒ–å›¾åƒå¤„ç†</strong>ï¼š</p>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">batch_image_processing</span><span class="p">(</span><span class="n">images</span><span class="p">,</span> <span class="n">target_size</span><span class="o">=</span><span class="p">(</span><span class="mi">336</span><span class="p">,</span> <span class="mi">336</span><span class="p">)):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;æ‰¹é‡å¤„ç†å›¾åƒï¼Œåˆ©ç”¨å‘é‡åŒ–æ“ä½œ&quot;&quot;&quot;</span>
    <span class="c1"># é¿å…é€ä¸ªå¤„ç†</span>
    <span class="c1"># Bad:</span>
    <span class="c1"># processed = [transform(img) for img in images]</span>

    <span class="c1"># Good: ä½¿ç”¨å‘é‡åŒ–æ“ä½œ</span>
    <span class="kn">import</span> <span class="nn">torchvision.transforms</span> <span class="k">as</span> <span class="nn">T</span>

    <span class="c1"># åˆ›å»ºæ‰¹é‡å˜æ¢</span>
    <span class="n">batch_transform</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span>
        <span class="n">T</span><span class="o">.</span><span class="n">Resize</span><span class="p">(</span><span class="n">target_size</span><span class="p">),</span>
        <span class="n">T</span><span class="o">.</span><span class="n">Normalize</span><span class="p">(</span><span class="n">mean</span><span class="o">=</span><span class="p">[</span><span class="mf">0.485</span><span class="p">,</span> <span class="mf">0.456</span><span class="p">,</span> <span class="mf">0.406</span><span class="p">],</span>
                   <span class="n">std</span><span class="o">=</span><span class="p">[</span><span class="mf">0.229</span><span class="p">,</span> <span class="mf">0.224</span><span class="p">,</span> <span class="mf">0.225</span><span class="p">])</span>
    <span class="p">])</span>

    <span class="c1"># ä¸€æ¬¡æ€§å¤„ç†æ•´ä¸ª batch</span>
    <span class="n">stacked_images</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">images</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">batch_transform</span><span class="p">(</span><span class="n">stacked_images</span><span class="p">)</span>
</code></pre></div>

<h3 id="1124">11.2.4 é«˜æ•ˆæ•°æ®æ ¼å¼</h3>
<p><strong>WebDataset æ ¼å¼ä¼˜åŒ–</strong>ï¼š</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># å°†æ•°æ®æ‰“åŒ…æˆ WebDataset æ ¼å¼</span>
<span class="kn">import</span> <span class="nn">webdataset</span> <span class="k">as</span> <span class="nn">wds</span>

<span class="k">def</span> <span class="nf">create_webdataset</span><span class="p">(</span><span class="n">data_dir</span><span class="p">,</span> <span class="n">output_dir</span><span class="p">,</span> <span class="n">shard_size</span><span class="o">=</span><span class="mi">10000</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;åˆ›å»ºé«˜æ•ˆçš„ WebDataset æ ¼å¼&quot;&quot;&quot;</span>

    <span class="n">pattern</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">output_dir</span><span class="si">}</span><span class="s2">/shard-%06d.tar&quot;</span>

    <span class="k">with</span> <span class="n">wds</span><span class="o">.</span><span class="n">ShardWriter</span><span class="p">(</span><span class="n">pattern</span><span class="p">,</span> <span class="n">maxcount</span><span class="o">=</span><span class="n">shard_size</span><span class="p">)</span> <span class="k">as</span> <span class="n">sink</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">sample</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">load_samples</span><span class="p">(</span><span class="n">data_dir</span><span class="p">)):</span>
            <span class="c1"># æ‰“åŒ…æˆ tar æ ¼å¼</span>
            <span class="n">sink</span><span class="o">.</span><span class="n">write</span><span class="p">({</span>
                <span class="s2">&quot;__key__&quot;</span><span class="p">:</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">idx</span><span class="si">:</span><span class="s2">08d</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
                <span class="s2">&quot;image.jpg&quot;</span><span class="p">:</span> <span class="n">sample</span><span class="p">[</span><span class="s2">&quot;image_bytes&quot;</span><span class="p">],</span>
                <span class="s2">&quot;text.txt&quot;</span><span class="p">:</span> <span class="n">sample</span><span class="p">[</span><span class="s2">&quot;text&quot;</span><span class="p">],</span>
                <span class="s2">&quot;metadata.json&quot;</span><span class="p">:</span> <span class="n">json</span><span class="o">.</span><span class="n">dumps</span><span class="p">(</span><span class="n">sample</span><span class="p">[</span><span class="s2">&quot;metadata&quot;</span><span class="p">])</span>
            <span class="p">})</span>

    <span class="c1"># ä½¿ç”¨æ—¶</span>
    <span class="n">dataset</span> <span class="o">=</span> <span class="n">wds</span><span class="o">.</span><span class="n">WebDataset</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">output_dir</span><span class="si">}</span><span class="s2">/shard-*.tar&quot;</span><span class="p">)</span> \
        <span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="s2">&quot;pil&quot;</span><span class="p">)</span> \
        <span class="o">.</span><span class="n">to_tuple</span><span class="p">(</span><span class="s2">&quot;image.jpg&quot;</span><span class="p">,</span> <span class="s2">&quot;text.txt&quot;</span><span class="p">)</span> \
        <span class="o">.</span><span class="n">batched</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)</span>
</code></pre></div>

<p><strong>å†…å­˜æ˜ å°„ä¼˜åŒ–</strong>ï¼š</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># ä½¿ç”¨å†…å­˜æ˜ å°„é¿å…é‡å¤åŠ è½½</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="k">class</span> <span class="nc">MemoryMappedDataset</span><span class="p">(</span><span class="n">Dataset</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data_path</span><span class="p">):</span>
        <span class="c1"># åˆ›å»ºå†…å­˜æ˜ å°„</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">images</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">memmap</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">data_path</span><span class="si">}</span><span class="s2">/images.npy&quot;</span><span class="p">,</span>
            <span class="n">dtype</span><span class="o">=</span><span class="s1">&#39;float32&#39;</span><span class="p">,</span>
            <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">,</span>
            <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">num_samples</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">336</span><span class="p">,</span> <span class="mi">336</span><span class="p">)</span>
        <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">texts</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">memmap</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">data_path</span><span class="si">}</span><span class="s2">/texts.npy&quot;</span><span class="p">,</span>
            <span class="n">dtype</span><span class="o">=</span><span class="s1">&#39;int32&#39;</span><span class="p">,</span>
            <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">,</span>
            <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">num_samples</span><span class="p">,</span> <span class="n">max_length</span><span class="p">)</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">idx</span><span class="p">):</span>
        <span class="c1"># ç›´æ¥ä»å†…å­˜æ˜ å°„è¯»å–ï¼Œæ— éœ€åŠ è½½æ•´ä¸ªæ–‡ä»¶</span>
        <span class="k">return</span> <span class="p">{</span>
            <span class="s1">&#39;image&#39;</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">images</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">copy</span><span class="p">()),</span>
            <span class="s1">&#39;text&#39;</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">texts</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">copy</span><span class="p">())</span>
        <span class="p">}</span>

<span class="c1">## 11.3 é€šä¿¡å¼€é”€ä¼˜åŒ–</span>

<span class="n">åœ¨åˆ†å¸ƒå¼è®­ç»ƒä¸­</span><span class="err">ï¼Œ</span><span class="n">é€šä¿¡å¼€é”€å¾€å¾€å æ®æ€»è®­ç»ƒæ—¶é—´çš„</span> <span class="mi">20</span><span class="o">-</span><span class="mi">40</span><span class="o">%</span><span class="err">ã€‚</span><span class="n">å¯¹äº</span> <span class="n">VLM</span> <span class="n">è¿™æ ·çš„å¤§æ¨¡å‹</span><span class="err">ï¼Œ</span><span class="n">ä¼˜åŒ–é€šä¿¡ç­–ç•¥å¯ä»¥å¸¦æ¥æ˜¾è‘—çš„æ€§èƒ½æå‡</span><span class="err">ã€‚</span>

<span class="c1">### 11.3.1 æ¢¯åº¦ç´¯ç§¯ç­–ç•¥</span>

<span class="n">æ¢¯åº¦ç´¯ç§¯ä¸ä»…èƒ½å¤Ÿæ¨¡æ‹Ÿå¤§</span> <span class="n">batch</span> <span class="n">size</span><span class="err">ï¼Œ</span><span class="n">è¿˜èƒ½å‡å°‘é€šä¿¡é¢‘ç‡</span><span class="err">ï¼š</span>

<span class="err">```</span><span class="n">python</span>
<span class="k">def</span> <span class="nf">optimized_gradient_accumulation</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">dataloader</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> 
                                    <span class="n">accumulation_steps</span><span class="o">=</span><span class="mi">4</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;ä¼˜åŒ–çš„æ¢¯åº¦ç´¯ç§¯å®ç°&quot;&quot;&quot;</span>
    <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>

    <span class="k">for</span> <span class="n">step</span><span class="p">,</span> <span class="n">batch</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">dataloader</span><span class="p">):</span>
        <span class="c1"># å½’ä¸€åŒ– lossï¼Œä¿è¯æ¢¯åº¦å¤§å°ä¸€è‡´</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">compute_loss</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">batch</span><span class="p">)</span> <span class="o">/</span> <span class="n">accumulation_steps</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

        <span class="k">if</span> <span class="p">(</span><span class="n">step</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="n">accumulation_steps</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="c1"># åªåœ¨ç´¯ç§¯å®Œæˆåè¿›è¡Œé€šä¿¡</span>
            <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
            <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

            <span class="c1"># å¯é€‰ï¼šæ¢¯åº¦è£å‰ª</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">clip_grad_norm_</span><span class="p">(</span>
                <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> 
                <span class="n">max_norm</span><span class="o">=</span><span class="mf">1.0</span>
            <span class="p">)</span>
</code></pre></div>

<p><strong>åŠ¨æ€æ¢¯åº¦ç´¯ç§¯</strong>ï¼š</p>
<div class="codehilite"><pre><span></span><code><span class="k">class</span> <span class="nc">DynamicGradientAccumulation</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;æ ¹æ® batch å¤§å°åŠ¨æ€è°ƒæ•´ç´¯ç§¯æ­¥æ•°&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">target_batch_size</span><span class="o">=</span><span class="mi">256</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">target_batch_size</span> <span class="o">=</span> <span class="n">target_batch_size</span>

    <span class="k">def</span> <span class="nf">get_accumulation_steps</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">current_batch_size</span><span class="p">):</span>
        <span class="c1"># åŠ¨æ€è®¡ç®—éœ€è¦çš„ç´¯ç§¯æ­¥æ•°</span>
        <span class="n">steps</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">target_batch_size</span> <span class="o">//</span> <span class="n">current_batch_size</span>
        <span class="k">return</span> <span class="nb">max</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">steps</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">should_update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">step</span><span class="p">,</span> <span class="n">accumulation_steps</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">step</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="n">accumulation_steps</span> <span class="o">==</span> <span class="mi">0</span>
</code></pre></div>

<h3 id="1132-all-reduce">11.3.2 All-Reduce ä¼˜åŒ–</h3>
<p><strong>é€šä¿¡å‹ç¼©</strong>ï¼š</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># ä½¿ç”¨ PowerSGD è¿›è¡Œæ¢¯åº¦å‹ç¼©</span>
<span class="kn">from</span> <span class="nn">torch.distributed.algorithms.ddp_comm_hooks</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">powerSGD_hook</span><span class="p">,</span> 
    <span class="n">default_hooks</span>
<span class="p">)</span>

<span class="k">def</span> <span class="nf">setup_gradient_compression</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">process_group</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;é…ç½®æ¢¯åº¦å‹ç¼©&quot;&quot;&quot;</span>

    <span class="c1"># PowerSGD é…ç½®</span>
    <span class="n">state</span> <span class="o">=</span> <span class="n">powerSGD_hook</span><span class="o">.</span><span class="n">PowerSGDState</span><span class="p">(</span>
        <span class="n">process_group</span><span class="o">=</span><span class="n">process_group</span><span class="p">,</span>
        <span class="n">matrix_approximation_rank</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>  <span class="c1"># å‹ç¼©ç‡</span>
        <span class="n">warm_start</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>  <span class="c1"># ä½¿ç”¨ä¸Šä¸€æ­¥çš„ Q çŸ©é˜µåˆå§‹åŒ–</span>
        <span class="n">use_error_feedback</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>  <span class="c1"># é”™è¯¯åé¦ˆæœºåˆ¶</span>
        <span class="n">start_powerSGD_iter</span><span class="o">=</span><span class="mi">1000</span>  <span class="c1"># é¢„çƒ­æ­¥æ•°</span>
    <span class="p">)</span>

    <span class="c1"># æ³¨å†Œå‹ç¼© hook</span>
    <span class="n">model</span><span class="o">.</span><span class="n">register_comm_hook</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">powerSGD_hook</span><span class="o">.</span><span class="n">powerSGD_hook</span><span class="p">)</span>
</code></pre></div>

<p><strong>æ¢¯åº¦ Bucketing ä¼˜åŒ–</strong>ï¼š</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># ä¼˜åŒ– DDP bucket å¤§å°</span>
<span class="k">def</span> <span class="nf">optimize_ddp_bucketing</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">bucket_cap_mb</span><span class="o">=</span><span class="mi">25</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;è°ƒæ•´ DDP bucket å¤§å°ä»¥ä¼˜åŒ–é€šä¿¡&quot;&quot;&quot;</span>

    <span class="n">model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">parallel</span><span class="o">.</span><span class="n">DistributedDataParallel</span><span class="p">(</span>
        <span class="n">model</span><span class="p">,</span>
        <span class="n">device_ids</span><span class="o">=</span><span class="p">[</span><span class="n">local_rank</span><span class="p">],</span>
        <span class="n">output_device</span><span class="o">=</span><span class="n">local_rank</span><span class="p">,</span>
        <span class="c1"># å…³é”®å‚æ•°</span>
        <span class="n">bucket_cap_mb</span><span class="o">=</span><span class="n">bucket_cap_mb</span><span class="p">,</span>  <span class="c1"># bucket å¤§å°</span>
        <span class="n">gradient_as_bucket_view</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>  <span class="c1"># å‡å°‘å†…å­˜æ‹·è´</span>
        <span class="n">find_unused_parameters</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>  <span class="c1"># é¿å…é¢å¤–é€šä¿¡</span>
        <span class="n">static_graph</span><span class="o">=</span><span class="kc">True</span>  <span class="c1"># é™æ€å›¾ä¼˜åŒ–</span>
    <span class="p">)</span>

    <span class="k">return</span> <span class="n">model</span>
</code></pre></div>

<h3 id="1133">11.3.3 é€šä¿¡ä¸è®¡ç®—é‡å </h3>
<p><strong>Pipeline å¹¶è¡Œä¼˜åŒ–</strong>ï¼š</p>
<div class="codehilite"><pre><span></span><code><span class="k">class</span> <span class="nc">ComputeCommunicationOverlap</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;è®¡ç®—ä¸é€šä¿¡é‡å ç­–ç•¥&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">num_micro_batches</span><span class="o">=</span><span class="mi">4</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">model</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_micro_batches</span> <span class="o">=</span> <span class="n">num_micro_batches</span>

    <span class="k">def</span> <span class="nf">forward_backward_with_overlap</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">):</span>
        <span class="c1"># å°† batch åˆ†æˆ micro-batches</span>
        <span class="n">micro_batches</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">chunk</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_micro_batches</span><span class="p">)</span>

        <span class="c1"># æµæ°´çº¿æ‰§è¡Œ</span>
        <span class="n">handles</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">micro_batch</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">micro_batches</span><span class="p">):</span>
            <span class="c1"># å‰å‘è®¡ç®—</span>
            <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">(</span><span class="n">micro_batch</span><span class="p">)</span>

            <span class="c1"># å¼‚æ­¥å¯åŠ¨åå‘ä¼ æ’­</span>
            <span class="n">handle</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">backward_async</span><span class="p">()</span>
            <span class="n">handles</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">handle</span><span class="p">)</span>

            <span class="c1"># åœ¨ç­‰å¾…å½“å‰åå‘ä¼ æ’­æ—¶ï¼Œ</span>
            <span class="c1"># å¯ä»¥å¼€å§‹ä¸‹ä¸€ä¸ª micro-batch çš„å‰å‘</span>

        <span class="c1"># ç­‰å¾…æ‰€æœ‰å¼‚æ­¥æ“ä½œå®Œæˆ</span>
        <span class="k">for</span> <span class="n">handle</span> <span class="ow">in</span> <span class="n">handles</span><span class="p">:</span>
            <span class="n">handle</span><span class="o">.</span><span class="n">wait</span><span class="p">()</span>
</code></pre></div>

<p><strong>NCCL å‚æ•°è°ƒä¼˜</strong>ï¼š</p>
<div class="codehilite"><pre><span></span><code><span class="kn">import</span> <span class="nn">os</span>

<span class="k">def</span> <span class="nf">optimize_nccl_parameters</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;ä¼˜åŒ– NCCL é€šä¿¡å‚æ•°&quot;&quot;&quot;</span>

    <span class="c1"># å¢åŠ  NCCL ç¼“å†²åŒºå¤§å°</span>
    <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;NCCL_BUFFSIZE&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;2097152&quot;</span>  <span class="c1"># 2MB</span>

    <span class="c1"># å¯ç”¨ NCCL å¼‚æ­¥é”™è¯¯å¤„ç†</span>
    <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;NCCL_ASYNC_ERROR_HANDLING&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;1&quot;</span>

    <span class="c1"># ä¼˜åŒ–æ ‘å½¢ All-Reduce ç®—æ³•</span>
    <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;NCCL_TREE_THRESHOLD&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;0&quot;</span>

    <span class="c1"># ä½¿ç”¨é«˜é€Ÿäº’è”æ—¶çš„ä¼˜åŒ–</span>
    <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;NCCL_IB_DISABLE&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;0&quot;</span>  <span class="c1"># å¯ç”¨ InfiniBand</span>
    <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;NCCL_NET_GDR_LEVEL&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;5&quot;</span>  <span class="c1"># GPU Direct RDMA</span>

    <span class="c1"># P2P ä¼˜åŒ–</span>
    <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;NCCL_P2P_LEVEL&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;NVL&quot;</span>  <span class="c1"># NVLink ä¼˜åŒ–</span>
</code></pre></div>

<h3 id="1134">11.3.4 æ··åˆç²¾åº¦é€šä¿¡ä¼˜åŒ–</h3>
<div class="codehilite"><pre><span></span><code><span class="c1"># FP16 æ¢¯åº¦é€šä¿¡</span>
<span class="k">class</span> <span class="nc">FP16GradientCommunication</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;ä½¿ç”¨ FP16 è¿›è¡Œæ¢¯åº¦é€šä¿¡ï¼Œå‡å°‘å¸¦å®½éœ€æ±‚&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">model</span>
        <span class="c1"># ä¸ºæ¯ä¸ªå‚æ•°åˆ›å»º FP16 æ¢¯åº¦ç¼“å†²åŒº</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fp16_gradients</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">():</span>
            <span class="k">if</span> <span class="n">param</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">fp16_gradients</span><span class="p">[</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span>
                    <span class="n">param</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span>
                <span class="p">)</span>

    <span class="k">def</span> <span class="nf">compress_gradients</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;å°† FP32 æ¢¯åº¦å‹ç¼©ä¸º FP16&quot;&quot;&quot;</span>
        <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">param</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">():</span>
            <span class="k">if</span> <span class="n">param</span><span class="o">.</span><span class="n">grad</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">fp16_gradients</span><span class="p">[</span><span class="n">name</span><span class="p">]</span><span class="o">.</span><span class="n">copy_</span><span class="p">(</span><span class="n">param</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">decompress_gradients</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;å°† FP16 æ¢¯åº¦è§£å‹ä¸º FP32&quot;&quot;&quot;</span>
        <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">param</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">():</span>
            <span class="k">if</span> <span class="n">name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">fp16_gradients</span><span class="p">:</span>
                <span class="n">param</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">copy_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fp16_gradients</span><span class="p">[</span><span class="n">name</span><span class="p">])</span>
</code></pre></div>

<h2 id="114-flash-attention-xformers">11.4 Flash Attention ä¸ xFormers å®è·µ</h2>
<p>æ³¨æ„åŠ›æœºåˆ¶æ˜¯ Transformer æ¨¡å‹çš„æ ¸å¿ƒï¼Œä¹Ÿæ˜¯ä¸»è¦çš„è®¡ç®—å’Œå†…å­˜ç“¶é¢ˆã€‚Flash Attention å’Œ xFormers æä¾›äº†é«˜æ•ˆçš„æ³¨æ„åŠ›å®ç°ã€‚</p>
<h3 id="1141-flash-attention">11.4.1 Flash Attention åŸç†ä¸ä½¿ç”¨</h3>
<p>Flash Attention é€šè¿‡ç®—æ³•åˆ›æ–°å‡å°‘äº† HBMï¼ˆé«˜å¸¦å®½å†…å­˜ï¼‰è®¿é—®ï¼š</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># Flash Attention 2 é›†æˆ</span>
<span class="kn">from</span> <span class="nn">flash_attn</span> <span class="kn">import</span> <span class="n">flash_attn_func</span><span class="p">,</span> <span class="n">flash_attn_varlen_func</span>

<span class="k">class</span> <span class="nc">FlashAttentionVLM</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dim</span> <span class="o">=</span> <span class="n">dim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span> <span class="o">=</span> <span class="n">num_heads</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span> <span class="o">=</span> <span class="n">dim</span> <span class="o">//</span> <span class="n">num_heads</span>

        <span class="c1"># QKV æŠ•å½±</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">qkv</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="mi">3</span> <span class="o">*</span> <span class="n">dim</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="n">B</span><span class="p">,</span> <span class="n">L</span><span class="p">,</span> <span class="n">D</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span>

        <span class="c1"># è®¡ç®— QKV</span>
        <span class="n">qkv</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">qkv</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">qkv</span> <span class="o">=</span> <span class="n">qkv</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">L</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">)</span>
        <span class="n">qkv</span> <span class="o">=</span> <span class="n">qkv</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>  <span class="c1"># [3, B, H, L, D]</span>
        <span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="o">=</span> <span class="n">qkv</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">qkv</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">qkv</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>

        <span class="c1"># ä½¿ç”¨ Flash Attention</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">flash_attn_func</span><span class="p">(</span>
            <span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span>
            <span class="n">dropout_p</span><span class="o">=</span><span class="mf">0.1</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">training</span> <span class="k">else</span> <span class="mf">0.0</span><span class="p">,</span>
            <span class="n">softmax_scale</span><span class="o">=</span><span class="mf">1.0</span> <span class="o">/</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span> <span class="o">**</span> <span class="mf">0.5</span><span class="p">),</span>
            <span class="n">causal</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>  <span class="c1"># VLM é€šå¸¸ä¸éœ€è¦ causal mask</span>
            <span class="n">window_size</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># å…¨å±€æ³¨æ„åŠ›</span>
        <span class="p">)</span>

        <span class="k">return</span> <span class="n">output</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">L</span><span class="p">,</span> <span class="n">D</span><span class="p">)</span>
</code></pre></div>

<p><strong>å˜é•¿åºåˆ—ä¼˜åŒ–</strong>ï¼š</p>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">flash_attention_with_variable_length</span><span class="p">(</span>
    <span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> 
    <span class="n">cu_seqlens_q</span><span class="p">,</span>  <span class="c1"># ç´¯ç§¯åºåˆ—é•¿åº¦</span>
    <span class="n">cu_seqlens_k</span><span class="p">,</span>
    <span class="n">max_seqlen_q</span><span class="p">,</span>
    <span class="n">max_seqlen_k</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;å¤„ç†å˜é•¿åºåˆ—çš„ Flash Attention&quot;&quot;&quot;</span>

    <span class="n">output</span> <span class="o">=</span> <span class="n">flash_attn_varlen_func</span><span class="p">(</span>
        <span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span>
        <span class="n">cu_seqlens_q</span><span class="o">=</span><span class="n">cu_seqlens_q</span><span class="p">,</span>
        <span class="n">cu_seqlens_k</span><span class="o">=</span><span class="n">cu_seqlens_k</span><span class="p">,</span>
        <span class="n">max_seqlen_q</span><span class="o">=</span><span class="n">max_seqlen_q</span><span class="p">,</span>
        <span class="n">max_seqlen_k</span><span class="o">=</span><span class="n">max_seqlen_k</span><span class="p">,</span>
        <span class="n">dropout_p</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
        <span class="n">softmax_scale</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">causal</span><span class="o">=</span><span class="kc">False</span>
    <span class="p">)</span>

    <span class="k">return</span> <span class="n">output</span>
</code></pre></div>

<h3 id="1142-xformers">11.4.2 xFormers å†…å­˜é«˜æ•ˆæ³¨æ„åŠ›</h3>
<p>xFormers æä¾›äº†å¤šç§å†…å­˜ä¼˜åŒ–çš„æ³¨æ„åŠ›å®ç°ï¼š</p>
<div class="codehilite"><pre><span></span><code><span class="kn">import</span> <span class="nn">xformers.ops</span> <span class="k">as</span> <span class="nn">xops</span>

<span class="k">class</span> <span class="nc">XFormersEfficientAttention</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dim</span> <span class="o">=</span> <span class="n">dim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span> <span class="o">=</span> <span class="n">num_heads</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">attention_bias</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="c1"># ä½¿ç”¨ xFormers çš„å†…å­˜é«˜æ•ˆæ³¨æ„åŠ›</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">xops</span><span class="o">.</span><span class="n">memory_efficient_attention</span><span class="p">(</span>
            <span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span>
            <span class="n">attn_bias</span><span class="o">=</span><span class="n">attention_bias</span><span class="p">,</span>
            <span class="n">op</span><span class="o">=</span><span class="n">xops</span><span class="o">.</span><span class="n">MemoryEfficientAttentionFlashAttentionOp</span><span class="p">,</span>
            <span class="n">scale</span><span class="o">=</span><span class="mf">1.0</span> <span class="o">/</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dim</span> <span class="o">**</span> <span class="mf">0.5</span><span class="p">)</span>
        <span class="p">)</span>

        <span class="k">return</span> <span class="n">output</span>
</code></pre></div>

<p><strong>ç¨€ç–æ³¨æ„åŠ›æ¨¡å¼</strong>ï¼š</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># ä½¿ç”¨ xFormers çš„å—ç¨€ç–æ³¨æ„åŠ›</span>
<span class="kn">from</span> <span class="nn">xformers.ops</span> <span class="kn">import</span> <span class="n">BlockDiagonalMask</span>

<span class="k">def</span> <span class="nf">create_block_sparse_mask</span><span class="p">(</span><span class="n">seq_len</span><span class="p">,</span> <span class="n">block_size</span><span class="o">=</span><span class="mi">64</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;åˆ›å»ºå—ç¨€ç–æ³¨æ„åŠ› mask&quot;&quot;&quot;</span>

    <span class="c1"># åˆ›å»ºå—å¯¹è§’ mask</span>
    <span class="n">mask</span> <span class="o">=</span> <span class="n">BlockDiagonalMask</span><span class="o">.</span><span class="n">from_seqlens</span><span class="p">(</span>
        <span class="n">q_seqlen</span><span class="o">=</span><span class="p">[</span><span class="n">block_size</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="n">seq_len</span> <span class="o">//</span> <span class="n">block_size</span><span class="p">),</span>
        <span class="n">kv_seqlen</span><span class="o">=</span><span class="p">[</span><span class="n">block_size</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="n">seq_len</span> <span class="o">//</span> <span class="n">block_size</span><span class="p">)</span>
    <span class="p">)</span>

    <span class="k">return</span> <span class="n">mask</span>

<span class="c1"># åœ¨æ³¨æ„åŠ›è®¡ç®—ä¸­ä½¿ç”¨</span>
<span class="n">sparse_mask</span> <span class="o">=</span> <span class="n">create_block_sparse_mask</span><span class="p">(</span><span class="n">seq_len</span><span class="o">=</span><span class="mi">1024</span><span class="p">)</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">xops</span><span class="o">.</span><span class="n">memory_efficient_attention</span><span class="p">(</span>
    <span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span>
    <span class="n">attn_bias</span><span class="o">=</span><span class="n">sparse_mask</span>
<span class="p">)</span>
</code></pre></div>

<h3 id="1143">11.4.3 ä¸åŒåœºæ™¯ä¸‹çš„é€‰æ‹©ç­–ç•¥</h3>
<div class="codehilite"><pre><span></span><code><span class="err">é€‰æ‹©å†³ç­–æ ‘ï¼š</span>

<span class="err">åºåˆ—é•¿åº¦ï¼Ÿ</span>
<span class="err">â”œâ”€â”€</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="mi">512</span><span class="w"> </span><span class="n">tokens</span>
<span class="err">â”‚</span><span class="w">   </span><span class="err">â””â”€â”€</span><span class="w"> </span><span class="err">æ ‡å‡†æ³¨æ„åŠ›ï¼ˆå¼€é”€ä¸å¤§ï¼‰</span>
<span class="err">â”œâ”€â”€</span><span class="w"> </span><span class="mi">512</span><span class="o">-</span><span class="mi">2048</span><span class="w"> </span><span class="n">tokens</span>
<span class="err">â”‚</span><span class="w">   </span><span class="err">â”œâ”€â”€</span><span class="w"> </span><span class="err">éœ€è¦</span><span class="w"> </span><span class="n">causal</span><span class="w"> </span><span class="n">mask</span><span class="w"> </span><span class="err">â†’</span><span class="w"> </span><span class="n">Flash</span><span class="w"> </span><span class="n">Attention</span><span class="w"> </span><span class="mi">2</span>
<span class="err">â”‚</span><span class="w">   </span><span class="err">â””â”€â”€</span><span class="w"> </span><span class="err">ä¸éœ€è¦</span><span class="w"> </span><span class="n">causal</span><span class="w"> </span><span class="err">â†’</span><span class="w"> </span><span class="n">xFormers</span>
<span class="err">â””â”€â”€</span><span class="w"> </span><span class="o">&gt;</span><span class="w"> </span><span class="mi">2048</span><span class="w"> </span><span class="n">tokens</span>
<span class="w">    </span><span class="err">â”œâ”€â”€</span><span class="w"> </span><span class="err">å†…å­˜å—é™</span><span class="w"> </span><span class="err">â†’</span><span class="w"> </span><span class="n">xFormers</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="err">æ¢¯åº¦æ£€æŸ¥ç‚¹</span>
<span class="w">    </span><span class="err">â””â”€â”€</span><span class="w"> </span><span class="err">é€Ÿåº¦ä¼˜å…ˆ</span><span class="w"> </span><span class="err">â†’</span><span class="w"> </span><span class="n">Flash</span><span class="w"> </span><span class="n">Attention</span><span class="w"> </span><span class="mi">2</span>

<span class="err">ç‰¹æ®Šæƒ…å†µï¼š</span>

<span class="o">-</span><span class="w"> </span><span class="err">åŠ¨æ€åºåˆ—é•¿åº¦</span><span class="w"> </span><span class="err">â†’</span><span class="w"> </span><span class="n">Flash</span><span class="w"> </span><span class="n">Attention</span><span class="w"> </span><span class="n">varlen</span>
<span class="o">-</span><span class="w"> </span><span class="err">éœ€è¦è‡ªå®šä¹‰</span><span class="w"> </span><span class="n">attention</span><span class="w"> </span><span class="n">bias</span><span class="w"> </span><span class="err">â†’</span><span class="w"> </span><span class="n">xFormers</span>
<span class="o">-</span><span class="w"> </span><span class="err">å¤šæŸ¥è¯¢æ³¨æ„åŠ›ï¼ˆ</span><span class="n">MQA</span><span class="o">/</span><span class="n">GQA</span><span class="err">ï¼‰â†’</span><span class="w"> </span><span class="n">Flash</span><span class="w"> </span><span class="n">Attention</span><span class="w"> </span><span class="mi">2</span>
</code></pre></div>

<h3 id="1144">11.4.4 å®é™…åŠ é€Ÿæ•ˆæœå¯¹æ¯”</h3>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">benchmark_attention_implementations</span><span class="p">(</span><span class="n">seq_len</span><span class="o">=</span><span class="mi">2048</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">4096</span><span class="p">,</span> <span class="n">num_heads</span><span class="o">=</span><span class="mi">32</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;åŸºå‡†æµ‹è¯•ä¸åŒæ³¨æ„åŠ›å®ç°&quot;&quot;&quot;</span>
    <span class="kn">import</span> <span class="nn">time</span>

    <span class="n">batch_size</span> <span class="o">=</span> <span class="mi">8</span>
    <span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span><span class="p">)</span>

    <span class="c1"># å‡†å¤‡è¾“å…¥</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">dim</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

    <span class="c1"># æ ‡å‡†æ³¨æ„åŠ›</span>
    <span class="n">standard_attn</span> <span class="o">=</span> <span class="n">StandardAttention</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

    <span class="c1"># Flash Attention</span>
    <span class="n">flash_attn</span> <span class="o">=</span> <span class="n">FlashAttentionVLM</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

    <span class="c1"># xFormers</span>
    <span class="n">xformers_attn</span> <span class="o">=</span> <span class="n">XFormersEfficientAttention</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

    <span class="c1"># æµ‹è¯•å‡½æ•°</span>
    <span class="k">def</span> <span class="nf">measure_time</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">iterations</span><span class="o">=</span><span class="mi">100</span><span class="p">):</span>
        <span class="c1"># é¢„çƒ­</span>
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
            <span class="n">_</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">synchronize</span><span class="p">()</span>
        <span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>

        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">iterations</span><span class="p">):</span>
            <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
            <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

        <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">synchronize</span><span class="p">()</span>
        <span class="n">end</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>

        <span class="n">avg_time</span> <span class="o">=</span> <span class="p">(</span><span class="n">end</span> <span class="o">-</span> <span class="n">start</span><span class="p">)</span> <span class="o">/</span> <span class="n">iterations</span> <span class="o">*</span> <span class="mi">1000</span>  <span class="c1"># ms</span>

        <span class="c1"># æµ‹é‡å†…å­˜</span>
        <span class="n">memory</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">max_memory_allocated</span><span class="p">()</span> <span class="o">/</span> <span class="mi">1024</span><span class="o">**</span><span class="mi">3</span>  <span class="c1"># GB</span>

        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2">:&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  æ—¶é—´: </span><span class="si">{</span><span class="n">avg_time</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> ms&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  å†…å­˜: </span><span class="si">{</span><span class="n">memory</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> GB&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  ç›¸å¯¹é€Ÿåº¦: </span><span class="si">{</span><span class="n">baseline_time</span><span class="o">/</span><span class="n">avg_time</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">x&quot;</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">avg_time</span>

    <span class="c1"># è¿è¡ŒåŸºå‡†æµ‹è¯•</span>
    <span class="n">baseline_time</span> <span class="o">=</span> <span class="n">measure_time</span><span class="p">(</span><span class="n">standard_attn</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="s2">&quot;æ ‡å‡†æ³¨æ„åŠ›&quot;</span><span class="p">)</span>
    <span class="n">measure_time</span><span class="p">(</span><span class="n">flash_attn</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="s2">&quot;Flash Attention 2&quot;</span><span class="p">)</span>
    <span class="n">measure_time</span><span class="p">(</span><span class="n">xformers_attn</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="s2">&quot;xFormers&quot;</span><span class="p">)</span>
</code></pre></div>

<p>å…¸å‹ç»“æœï¼ˆA100-80GB, seq_len=2048ï¼‰ï¼š</p>
<div class="codehilite"><pre><span></span><code><span class="err">æ ‡å‡†æ³¨æ„åŠ›</span><span class="o">:</span>
<span class="w">  </span><span class="err">æ—¶é—´</span><span class="o">:</span><span class="w"> </span><span class="mf">45.32</span><span class="w"> </span><span class="n">ms</span>
<span class="w">  </span><span class="err">å†…å­˜</span><span class="o">:</span><span class="w"> </span><span class="mf">12.45</span><span class="w"> </span><span class="n">GB</span>
<span class="w">  </span><span class="err">ç›¸å¯¹é€Ÿåº¦</span><span class="o">:</span><span class="w"> </span><span class="mf">1.00</span><span class="n">x</span>

<span class="n">Flash</span><span class="w"> </span><span class="n">Attention</span><span class="w"> </span><span class="mi">2</span><span class="o">:</span>
<span class="w">  </span><span class="err">æ—¶é—´</span><span class="o">:</span><span class="w"> </span><span class="mf">12.18</span><span class="w"> </span><span class="n">ms</span><span class="w">  </span>
<span class="w">  </span><span class="err">å†…å­˜</span><span class="o">:</span><span class="w"> </span><span class="mf">4.32</span><span class="w"> </span><span class="n">GB</span>
<span class="w">  </span><span class="err">ç›¸å¯¹é€Ÿåº¦</span><span class="o">:</span><span class="w"> </span><span class="mf">3.72</span><span class="n">x</span>

<span class="n">xFormers</span><span class="o">:</span>
<span class="w">  </span><span class="err">æ—¶é—´</span><span class="o">:</span><span class="w"> </span><span class="mf">15.67</span><span class="w"> </span><span class="n">ms</span>
<span class="w">  </span><span class="err">å†…å­˜</span><span class="o">:</span><span class="w"> </span><span class="mf">5.18</span><span class="w"> </span><span class="n">GB</span>
<span class="w">  </span><span class="err">ç›¸å¯¹é€Ÿåº¦</span><span class="o">:</span><span class="w"> </span><span class="mf">2.89</span><span class="n">x</span>
</code></pre></div>

<h2 id="_2">æœ¬ç« å°ç»“</h2>
<p>åœ¨æœ¬ç« ä¸­ï¼Œæˆ‘ä»¬ç³»ç»Ÿåœ°å­¦ä¹ äº† VLM è®­ç»ƒé€Ÿåº¦ä¼˜åŒ–çš„å…³é”®æŠ€æœ¯ï¼š</p>
<h3 id="_3">æ ¸å¿ƒè¦ç‚¹</h3>
<ol>
<li><strong>æ€§èƒ½åˆ†æå…ˆè¡Œ</strong>ï¼šä½¿ç”¨ PyTorch Profiler å’Œ Nsight Systems ç²¾ç¡®å®šä½ç“¶é¢ˆï¼Œé¿å…ç›²ç›®ä¼˜åŒ–</li>
<li><strong>æ•°æ®ç®¡é“ä¼˜åŒ–</strong>ï¼šé€šè¿‡é¢„å–ã€ç¼“å­˜ã€GPU é¢„å¤„ç†ç­‰æŠ€æœ¯æ¶ˆé™¤ I/O ç“¶é¢ˆ</li>
<li><strong>é€šä¿¡ç­–ç•¥ä¼˜åŒ–</strong>ï¼šæ¢¯åº¦ç´¯ç§¯ã€é€šä¿¡å‹ç¼©ã€è®¡ç®—é€šä¿¡é‡å æ˜¾è‘—å‡å°‘åˆ†å¸ƒå¼è®­ç»ƒå¼€é”€</li>
<li><strong>é«˜æ•ˆæ³¨æ„åŠ›æœºåˆ¶</strong>ï¼šFlash Attention å’Œ xFormers å¯å¸¦æ¥ 3-4 å€çš„åŠ é€Ÿ</li>
</ol>
<h3 id="_4">å…³é”®å…¬å¼</h3>
<p><strong>Roofline æ¨¡å‹</strong>ï¼š
$$\text{Performance} = \min(\text{Peak FLOPS}, \text{Bandwidth} \times \text{Arithmetic Intensity})$$
<strong>é€šä¿¡ä¸è®¡ç®—æ¯”</strong>ï¼š
$$\text{é€šä¿¡è®¡ç®—æ¯”} = \frac{T_{\text{comm}}}{T_{\text{comp}}} = \frac{2 \times \text{Model Size}}{\text{Bandwidth} \times \text{Batch Size} \times \text{FLOPS}}$$
<strong>Flash Attention å¤æ‚åº¦</strong>ï¼š
$$\text{Memory}: O(N) \text{ vs } O(N^2), \quad \text{I/O}: O(N^2d^{1/2}M^{-1/2}) \text{ vs } O(N^2d)$$</p>
<h3 id="_5">æ€§èƒ½ä¼˜åŒ–æ£€æŸ¥è¡¨</h3>
<ul>
<li>[ ] GPU åˆ©ç”¨ç‡æ˜¯å¦è¾¾åˆ° 90% ä»¥ä¸Šï¼Ÿ</li>
<li>[ ] æ˜¯å¦å­˜åœ¨ CPU-GPU åŒæ­¥å¯¼è‡´çš„ç­‰å¾…ï¼Ÿ</li>
<li>[ ] æ•°æ®åŠ è½½æ˜¯å¦æˆä¸ºç“¶é¢ˆï¼Ÿ</li>
<li>[ ] é€šä¿¡æ—¶é—´å æ¯”æ˜¯å¦è¶…è¿‡ 30%ï¼Ÿ</li>
<li>[ ] æ˜¯å¦ä½¿ç”¨äº†é«˜æ•ˆçš„æ³¨æ„åŠ›å®ç°ï¼Ÿ</li>
<li>[ ] å†…å­˜å¸¦å®½åˆ©ç”¨ç‡æ˜¯å¦åˆç†ï¼Ÿ</li>
</ul>
<h2 id="_6">ç»ƒä¹ é¢˜</h2>
<h3 id="_7">åŸºç¡€é¢˜</h3>
<p><strong>ç»ƒä¹  11.1ï¼šProfile ç»“æœåˆ†æ</strong></p>
<p>ç»™å®šä»¥ä¸‹ PyTorch Profiler è¾“å‡ºï¼š</p>
<div class="codehilite"><pre><span></span><code>Name                          CPU time  CUDA time  Calls
aten::matmul                  45.2%     52.1%      1000
aten::softmax                 12.3%     15.2%      500  
DataLoader.__next__           25.1%     0.0%       100
aten::all_reduce              8.5%      18.3%      200
</code></pre></div>

<p>è¯·åˆ†æä¸»è¦çš„æ€§èƒ½ç“¶é¢ˆåœ¨å“ªé‡Œï¼Ÿåº”è¯¥é‡‡å–ä»€ä¹ˆä¼˜åŒ–ç­–ç•¥ï¼Ÿ</p>
<details>
<summary>ğŸ’¡ æç¤º</summary>
<p>è§‚å¯Ÿ CPU æ—¶é—´å’Œ CUDA æ—¶é—´çš„åˆ†å¸ƒï¼Œæ³¨æ„ DataLoader å ç”¨çš„ CPU æ—¶é—´æ¯”ä¾‹ã€‚</p>
</details>
<details>
<summary>ğŸ“ å‚è€ƒç­”æ¡ˆ</summary>
<p>ä¸»è¦ç“¶é¢ˆï¼š</p>
<ol>
<li><strong>æ•°æ®åŠ è½½</strong>ï¼šDataLoader å ç”¨ 25.1% CPU æ—¶é—´ï¼Œè¯´æ˜ GPU åœ¨ç­‰å¾…æ•°æ®</li>
<li><strong>é€šä¿¡å¼€é”€</strong>ï¼šall_reduce å ç”¨ 18.3% CUDA æ—¶é—´ï¼Œé€šä¿¡å¼€é”€è¾ƒå¤§</li>
</ol>
<p>ä¼˜åŒ–ç­–ç•¥ï¼š</p>
<ol>
<li>å¢åŠ  DataLoader çš„ num_workers</li>
<li>ä½¿ç”¨ pin_memory å’Œ persistent_workers</li>
<li>è€ƒè™‘ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯å‡å°‘ all_reduce é¢‘ç‡</li>
<li>æ£€æŸ¥æ˜¯å¦å¯ä»¥ä½¿ç”¨ Flash Attention ä¼˜åŒ– softmax</li>
</ol>
</details>
<p><strong>ç»ƒä¹  11.2ï¼šè®¡ç®—æœ€ä¼˜ batch size</strong></p>
<p>å‡è®¾æ¨¡å‹å‚æ•°é‡ä¸º 7Bï¼Œä½¿ç”¨ FP16 è®­ç»ƒï¼Œæ¢¯åº¦ç´¯ç§¯æ­¥æ•°ä¸º 4ï¼Œå•å¡æ˜¾å­˜ 80GBã€‚è¯·è®¡ç®—ï¼š</p>
<ol>
<li>æ¨¡å‹æƒé‡å ç”¨æ˜¾å­˜</li>
<li>æ¢¯åº¦å’Œä¼˜åŒ–å™¨çŠ¶æ€å ç”¨æ˜¾å­˜ï¼ˆä½¿ç”¨ AdamWï¼‰</li>
<li>å¯ç”¨äºæ¿€æ´»å€¼çš„æ˜¾å­˜</li>
<li>ä¼°ç®—æœ€å¤§ batch sizeï¼ˆå‡è®¾åºåˆ—é•¿åº¦ 2048ï¼‰</li>
</ol>
<details>
<summary>ğŸ’¡ æç¤º</summary>
<p>è®°ä½ AdamW éœ€è¦å­˜å‚¨ä¸¤ä¸ªåŠ¨é‡é¡¹ï¼Œæ¿€æ´»å€¼å†…å­˜ä¸ batch size å’Œåºåˆ—é•¿åº¦æˆæ­£æ¯”ã€‚</p>
</details>
<details>
<summary>ğŸ“ å‚è€ƒç­”æ¡ˆ</summary>
<ol>
<li>
<p><strong>æ¨¡å‹æƒé‡</strong>ï¼š7B Ã— 2 bytes (FP16) = 14 GB</p>
</li>
<li>
<p><strong>æ¢¯åº¦å’Œä¼˜åŒ–å™¨çŠ¶æ€</strong>ï¼š
   - æ¢¯åº¦ï¼š7B Ã— 2 bytes = 14 GB
   - Adam åŠ¨é‡ï¼š7B Ã— 4 bytes Ã— 2 = 56 GB
   - æ€»è®¡ï¼š70 GB</p>
</li>
<li>
<p><strong>å¯ç”¨äºæ¿€æ´»å€¼</strong>ï¼š80 - 14 - 70 = -4 GBï¼ˆæ˜¾å­˜ä¸è¶³ï¼ï¼‰</p>
</li>
</ol>
<p>éœ€è¦ä¼˜åŒ–ç­–ç•¥ï¼š</p>
<ul>
<li>ä½¿ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹ï¼šé‡Šæ”¾çº¦ 50% æ¿€æ´»å€¼å†…å­˜</li>
<li>ä½¿ç”¨ ZeRO-2ï¼šå°†ä¼˜åŒ–å™¨çŠ¶æ€åˆ†ç‰‡ï¼Œæ¯å¡åªéœ€ 56/N GB</li>
<li>ä½¿ç”¨ LoRAï¼šå¤§å¹…å‡å°‘å¯è®­ç»ƒå‚æ•°</li>
</ul>
<p>å‡è®¾ä½¿ç”¨ ZeRO-2ï¼ˆ8å¡ï¼‰+ æ¢¯åº¦æ£€æŸ¥ç‚¹ï¼š</p>
<ul>
<li>ä¼˜åŒ–å™¨çŠ¶æ€ï¼š56/8 = 7 GB</li>
<li>å¯ç”¨æ˜¾å­˜ï¼š80 - 14 - 14 - 7 = 45 GB</li>
<li>ä¼°ç®— batch sizeï¼šçº¦ 8-16ï¼ˆå–å†³äºæ¨¡å‹æ¶æ„ï¼‰</li>
</ul>
</details>
<p><strong>ç»ƒä¹  11.3ï¼šæ•°æ®åŠ è½½ä¼˜åŒ–</strong></p>
<p>æŸ VLM è®­ç»ƒä»»åŠ¡ï¼Œæ¯ä¸ª batch éœ€è¦åŠ è½½ 32 å¼ å›¾ç‰‡ï¼ˆæ¯å¼  3Ã—336Ã—336ï¼‰ï¼Œå¤„ç†æ—¶é—´å¦‚ä¸‹ï¼š</p>
<ul>
<li>ç£ç›˜è¯»å–ï¼š50ms</li>
<li>è§£ç ï¼š30ms</li>
<li>é¢„å¤„ç†ï¼ˆresizeã€normalizeï¼‰ï¼š40ms</li>
<li>ä¼ è¾“åˆ° GPUï¼š20ms</li>
</ul>
<p>å¦‚ä½•ä¼˜åŒ–ä½¿æ€»æ—¶é—´ä» 140ms é™åˆ° 40ms ä»¥å†…ï¼Ÿ</p>
<details>
<summary>ğŸ’¡ æç¤º</summary>
<p>è€ƒè™‘å¹¶è¡ŒåŒ–å’Œ GPU åŠ é€Ÿé¢„å¤„ç†ã€‚</p>
</details>
<details>
<summary>ğŸ“ å‚è€ƒç­”æ¡ˆ</summary>
<p>ä¼˜åŒ–æ–¹æ¡ˆï¼š</p>
<ol>
<li>
<p><strong>å¹¶è¡Œæ•°æ®åŠ è½½</strong>ï¼ˆnum_workers=4ï¼‰ï¼š
   - 4 ä¸ªè¿›ç¨‹å¹¶è¡Œè¯»å–ï¼Œæ¯ä¸ªå¤„ç† 8 å¼ å›¾
   - ç£ç›˜è¯»å–ï¼š50msï¼ˆå¹¶è¡Œï¼‰</p>
</li>
<li>
<p><strong>GPU é¢„å¤„ç†</strong>ï¼š
   - ä½¿ç”¨ NVIDIA DALI æˆ– torchvision GPU transforms
   - è§£ç  + é¢„å¤„ç†ï¼š15msï¼ˆGPU æ›´å¿«ï¼‰</p>
</li>
<li>
<p><strong>é¢„å–å’Œæµæ°´çº¿</strong>ï¼š
   - ä½¿ç”¨ pin_memory + non_blocking ä¼ è¾“
   - ä¼ è¾“æ—¶é—´ä¸è®¡ç®—é‡å </p>
</li>
</ol>
<p>æœ€ç»ˆæ—¶é—´çº¿ï¼š</p>
<ul>
<li>T0-T50ï¼šå¹¶è¡Œè¯»å–ï¼ˆ50msï¼‰</li>
<li>T50-T65ï¼šGPU å¤„ç†ï¼ˆ15msï¼Œä¸ä¸‹ä¸€æ‰¹è¯»å–é‡å ï¼‰</li>
<li>å®é™…å»¶è¿Ÿï¼šçº¦ 35-40ms</li>
</ul>
</details>
<h3 id="_8">æŒ‘æˆ˜é¢˜</h3>
<p><strong>ç»ƒä¹  11.4ï¼šé€šä¿¡ä¼˜åŒ–æ–¹æ¡ˆè®¾è®¡</strong></p>
<p>æŸå…¬å¸ä½¿ç”¨ 8Ã—A100 è®­ç»ƒ VLMï¼Œæ¨¡å‹å¤§å° 13Bï¼Œç°æœ‰é…ç½®ï¼š</p>
<ul>
<li>å…¨å±€ batch sizeï¼š256</li>
<li>å¾®æ‰¹æ¬¡å¤§å°ï¼š4</li>
<li>é€šä¿¡å¸¦å®½ï¼š600 GB/s (NVLink)</li>
<li>All-Reduce æ—¶é—´ï¼šçº¦ 500ms</li>
</ul>
<p>è¯·è®¾è®¡ä¼˜åŒ–æ–¹æ¡ˆï¼Œå°†é€šä¿¡å¼€é”€é™ä½ 50%ã€‚</p>
<details>
<summary>ğŸ’¡ æç¤º</summary>
<p>è€ƒè™‘æ¢¯åº¦ç´¯ç§¯ã€é€šä¿¡å‹ç¼©ã€ä»¥åŠé€šä¿¡ä¸è®¡ç®—çš„é‡å ã€‚</p>
</details>
<details>
<summary>ğŸ“ å‚è€ƒç­”æ¡ˆ</summary>
<p>ç»¼åˆä¼˜åŒ–æ–¹æ¡ˆï¼š</p>
<ol>
<li>
<p><strong>å¢åŠ æ¢¯åº¦ç´¯ç§¯æ­¥æ•°</strong>ï¼š
   - ä» 256/8/4=8 æ­¥å¢åŠ åˆ° 16 æ­¥
   - All-Reduce é¢‘ç‡å‡åŠï¼š500ms â†’ 250msï¼ˆå¹³å‡ï¼‰</p>
</li>
<li>
<p><strong>æ¢¯åº¦å‹ç¼©ï¼ˆPowerSGDï¼‰</strong>ï¼š
   - å‹ç¼©ç‡è®¾ä¸º 4ï¼Œé€šä¿¡é‡å‡å°‘ 75%
   - å®é™…æ—¶é—´ï¼š250ms Ã— 0.25 = 62.5ms
   - è§£å‹ç¼©å¼€é”€ï¼šçº¦ 20ms</p>
</li>
<li>
<p><strong>é€šä¿¡è®¡ç®—é‡å </strong>ï¼š
   - ä½¿ç”¨ bucketingï¼Œå°†æ¢¯åº¦åˆ†æˆ 4 ä¸ª bucket
   - æ¯ä¸ª bucket å®Œæˆåç«‹å³å¯åŠ¨ All-Reduce
   - é‡å ç‡çº¦ 30%ï¼š82.5ms Ã— 0.7 = 58ms</p>
</li>
<li>
<p><strong>ä¼˜åŒ– NCCL å‚æ•°</strong>ï¼š
   - è°ƒæ•´ NCCL_BUFFSIZE å’Œæ ‘å½¢ç®—æ³•
   - é¢å¤–å‡å°‘ 10-15%</p>
</li>
</ol>
<p>æœ€ç»ˆé€šä¿¡æ—¶é—´ï¼šçº¦ 50msï¼Œé™ä½ 90%ï¼</p>
<p>æ³¨æ„æƒè¡¡ï¼š</p>
<ul>
<li>æ¢¯åº¦ç´¯ç§¯å¢åŠ ä¼šå»¶é•¿æ”¶æ•›</li>
<li>å‹ç¼©å¯èƒ½å½±å“ç²¾åº¦</li>
<li>éœ€è¦ä»”ç»†è°ƒè¯•å’ŒéªŒè¯</li>
</ul>
</details>
<p><strong>ç»ƒä¹  11.5ï¼šFlash Attention é€‚ç”¨æ€§åˆ†æ</strong></p>
<p>åˆ†æä»¥ä¸‹åœºæ™¯æ˜¯å¦é€‚åˆä½¿ç”¨ Flash Attentionï¼Œå¹¶è¯´æ˜ç†ç”±ï¼š</p>
<ol>
<li>åºåˆ—é•¿åº¦ 256ï¼Œbatch size 128</li>
<li>åºåˆ—é•¿åº¦ 8192ï¼Œéœ€è¦ block-sparse attention</li>
<li>éœ€è¦è¿”å› attention weights ç”¨äºå¯è§†åŒ–</li>
<li>ä½¿ç”¨ GQA (Grouped Query Attention)ï¼Œç»„æ•°ä¸º 8</li>
<li>æ¨ç†é˜¶æ®µï¼Œéœ€è¦ KV cache</li>
</ol>
<details>
<summary>ğŸ’¡ æç¤º</summary>
<p>Flash Attention çš„é™åˆ¶åŒ…æ‹¬ä¸è¿”å› attention weightsã€å¯¹æŸäº› attention æ¨¡å¼æ”¯æŒæœ‰é™ç­‰ã€‚</p>
</details>
<details>
<summary>ğŸ“ å‚è€ƒç­”æ¡ˆ</summary>
<ol>
<li>
<p><strong>ä¸é€‚åˆ</strong>ï¼šåºåˆ—å¤ªçŸ­ï¼Œæ ‡å‡†æ³¨æ„åŠ›è¶³å¤Ÿå¿«ï¼ŒFlash Attention çš„å¯åŠ¨å¼€é”€å¯èƒ½æ›´å¤§</p>
</li>
<li>
<p><strong>éƒ¨åˆ†é€‚åˆ</strong>ï¼šFlash Attention 2 æ”¯æŒæŸäº›ç¨€ç–æ¨¡å¼ï¼Œä½† xFormers çš„ BlockDiagonalMask å¯èƒ½æ›´çµæ´»</p>
</li>
<li>
<p><strong>ä¸é€‚åˆ</strong>ï¼šFlash Attention ä¸è¿”å›ä¸­é—´çš„ attention weightsï¼Œéœ€è¦ä½¿ç”¨æ ‡å‡†å®ç°</p>
</li>
<li>
<p><strong>éå¸¸é€‚åˆ</strong>ï¼šFlash Attention 2 åŸç”Ÿæ”¯æŒ GQA/MQAï¼Œæ€§èƒ½ä¼˜ç§€</p>
</li>
<li>
<p><strong>é€‚åˆ</strong>ï¼šFlash Attention 2 æ”¯æŒæ¨ç†ä¼˜åŒ–ï¼ŒåŒ…æ‹¬ KV cache çš„é«˜æ•ˆå®ç°</p>
</li>
</ol>
<p>å»ºè®®ç­–ç•¥ï¼š</p>
<ul>
<li>è®­ç»ƒæ—¶é»˜è®¤ä½¿ç”¨ Flash Attention 2</li>
<li>éœ€è¦ attention å¯è§†åŒ–æ—¶ä¸´æ—¶åˆ‡æ¢</li>
<li>çŸ­åºåˆ—åœºæ™¯å¯ä»¥æ ¹æ®åŸºå‡†æµ‹è¯•å†³å®š</li>
</ul>
</details>
<p><strong>ç»ƒä¹  11.6ï¼šç«¯åˆ°ç«¯ä¼˜åŒ–æ–¹æ¡ˆ</strong></p>
<p>æŸå›¢é˜Ÿçš„ VLM è®­ç»ƒé…ç½®å¦‚ä¸‹ï¼š</p>
<ul>
<li>æ¨¡å‹ï¼šVision Encoder (ViT-L) + LLM (7B)</li>
<li>ç¡¬ä»¶ï¼š4Ã—A100-40GB</li>
<li>æ•°æ®ï¼š100k å›¾æ–‡å¯¹ï¼Œå›¾ç‰‡åˆ†è¾¨ç‡ 224-1344 ä¸ç­‰</li>
<li>å½“å‰é€Ÿåº¦ï¼š2.5 samples/ç§’</li>
<li>ç›®æ ‡ï¼šè¾¾åˆ° 10 samples/ç§’</li>
</ul>
<p>è¯·è®¾è®¡å®Œæ•´çš„ä¼˜åŒ–æ–¹æ¡ˆã€‚</p>
<details>
<summary>ğŸ’¡ æç¤º</summary>
<p>éœ€è¦ä»æ•°æ®ã€æ¨¡å‹ã€åˆ†å¸ƒå¼ç­‰å¤šä¸ªè§’åº¦ç»¼åˆä¼˜åŒ–ã€‚</p>
</details>
<details>
<summary>ğŸ“ å‚è€ƒç­”æ¡ˆ</summary>
<p><strong>é˜¶æ®µä¸€ï¼šå¿«é€Ÿä¼˜åŒ–ï¼ˆé¢„æœŸ 2.5 â†’ 5 samples/sï¼‰</strong></p>
<ol>
<li>
<p><strong>æ•°æ®ä¼˜åŒ–</strong>ï¼š
   - WebDataset æ ¼å¼ï¼Œå‡å°‘éšæœºè¯»å–
   - å›¾ç‰‡é¢„å…ˆ resize åˆ°æœ€å¤§ 672Ã—672
   - num_workers=8, pin_memory=True</p>
</li>
<li>
<p><strong>æ˜¾å­˜ä¼˜åŒ–</strong>ï¼š
   - å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹
   - æ··åˆç²¾åº¦è®­ç»ƒ (AMP)
   - æ‰¹æ¬¡å¤§å°ä» 4 å¢åŠ åˆ° 8</p>
</li>
</ol>
<p><strong>é˜¶æ®µäºŒï¼šæ¨¡å‹ä¼˜åŒ–ï¼ˆé¢„æœŸ 5 â†’ 7.5 samples/sï¼‰</strong></p>
<ol start="3">
<li>
<p><strong>æ³¨æ„åŠ›ä¼˜åŒ–</strong>ï¼š
   - Vision Encoder ä½¿ç”¨ Flash Attention
   - LLM ä½¿ç”¨ Flash Attention 2
   - ç§»é™¤ä¸å¿…è¦çš„ attention mask è®¡ç®—</p>
</li>
<li>
<p><strong>LoRA å¾®è°ƒ</strong>ï¼š
   - Vision Encoder å†»ç»“ï¼Œåªè°ƒ LLM
   - LoRA rank=64ï¼Œå‡å°‘ 95% å¯è®­ç»ƒå‚æ•°
   - ä¼˜åŒ–å™¨å†…å­˜ä» 28GB é™åˆ° 2GB</p>
</li>
</ol>
<p><strong>é˜¶æ®µä¸‰ï¼šåˆ†å¸ƒå¼ä¼˜åŒ–ï¼ˆé¢„æœŸ 7.5 â†’ 10+ samples/sï¼‰</strong></p>
<ol start="5">
<li>
<p><strong>é€šä¿¡ä¼˜åŒ–</strong>ï¼š
   - æ¢¯åº¦ç´¯ç§¯ä» 1 å¢åŠ åˆ° 4
   - å¯ç”¨æ¢¯åº¦å‹ç¼©ï¼ˆPowerSGDï¼‰
   - DDP é™æ€å›¾ä¼˜åŒ–</p>
</li>
<li>
<p><strong>Pipeline å¹¶è¡Œ</strong>ï¼š
   - Vision Encoder æ”¾ GPU 0-1
   - LLM æ”¾ GPU 2-3
   - å¾®æ‰¹æ¬¡æµæ°´çº¿å¤„ç†</p>
</li>
</ol>
<p><strong>éªŒè¯æ£€æŸ¥</strong>ï¼š</p>
<ul>
<li>Profile ç¡®è®¤ GPU åˆ©ç”¨ç‡ &gt;95%</li>
<li>ç›‘æ§æ”¶æ•›æ›²çº¿ç¡®ä¿ä¼˜åŒ–ä¸å½±å“æ•ˆæœ</li>
<li>A/B æµ‹è¯•éªŒè¯æ¨¡å‹è´¨é‡</li>
</ul>
<p>é¢„æœŸæœ€ç»ˆï¼š12-15 samples/ç§’</p>
</details>
<h2 id="_9">å¸¸è§é™·é˜±ä¸é”™è¯¯</h2>
<h3 id="1-profile">1. Profile è¯¯åŒº</h3>
<p>âŒ <strong>é”™è¯¯</strong>ï¼šåªçœ‹å¹³å‡å€¼</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># é”™è¯¯ï¼šå¿½ç•¥äº†é•¿å°¾å»¶è¿Ÿ</span>
<span class="n">avg_time</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">times</span><span class="p">)</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">times</span><span class="p">)</span>
</code></pre></div>

<p>âœ… <strong>æ­£ç¡®</strong>ï¼šåˆ†æå®Œæ•´åˆ†å¸ƒ</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># æŸ¥çœ‹ P50, P90, P99</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="n">p50</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">percentile</span><span class="p">(</span><span class="n">times</span><span class="p">,</span> <span class="mi">50</span><span class="p">)</span>
<span class="n">p90</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">percentile</span><span class="p">(</span><span class="n">times</span><span class="p">,</span> <span class="mi">90</span><span class="p">)</span>  
<span class="n">p99</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">percentile</span><span class="p">(</span><span class="n">times</span><span class="p">,</span> <span class="mi">99</span><span class="p">)</span>
</code></pre></div>

<h3 id="2">2. æ•°æ®åŠ è½½é™·é˜±</h3>
<p>âŒ <strong>é”™è¯¯</strong>ï¼šè¿‡å¤šçš„ workers</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># å¯èƒ½å¯¼è‡´ CPU ç«äº‰</span>
<span class="n">DataLoader</span><span class="p">(</span><span class="n">num_workers</span><span class="o">=</span><span class="mi">32</span><span class="p">)</span>
</code></pre></div>

<p>âœ… <strong>æ­£ç¡®</strong>ï¼šæ ¹æ® CPU æ ¸æ•°è°ƒæ•´</p>
<div class="codehilite"><pre><span></span><code><span class="kn">import</span> <span class="nn">os</span>
<span class="n">num_workers</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">cpu_count</span><span class="p">()</span> <span class="o">//</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">8</span><span class="p">)</span>
</code></pre></div>

<h3 id="3">3. é€šä¿¡ä¼˜åŒ–è¯¯åŒº</h3>
<p>âŒ <strong>é”™è¯¯</strong>ï¼šç›²ç›®å¢åŠ æ¢¯åº¦ç´¯ç§¯</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># å¯èƒ½å¯¼è‡´æ”¶æ•›å˜æ…¢</span>
<span class="n">accumulation_steps</span> <span class="o">=</span> <span class="mi">32</span>
</code></pre></div>

<p>âœ… <strong>æ­£ç¡®</strong>ï¼šå¹³è¡¡é€šä¿¡å’Œæ”¶æ•›</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># æ ¹æ®å®é™…é€šä¿¡å æ¯”å†³å®š</span>
<span class="k">if</span> <span class="n">comm_time_ratio</span> <span class="o">&gt;</span> <span class="mf">0.3</span><span class="p">:</span>
    <span class="n">accumulation_steps</span> <span class="o">=</span> <span class="mi">8</span>
<span class="k">else</span><span class="p">:</span>
    <span class="n">accumulation_steps</span> <span class="o">=</span> <span class="mi">4</span>
</code></pre></div>

<h3 id="4-flash-attention">4. Flash Attention ä½¿ç”¨é”™è¯¯</h3>
<p>âŒ <strong>é”™è¯¯</strong>ï¼šçŸ­åºåˆ—ä½¿ç”¨ Flash Attention</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># åºåˆ—é•¿åº¦ 128ï¼Œåè€Œæ›´æ…¢</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">flash_attn_func</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>
</code></pre></div>

<p>âœ… <strong>æ­£ç¡®</strong>ï¼šæ ¹æ®åºåˆ—é•¿åº¦é€‰æ‹©</p>
<div class="codehilite"><pre><span></span><code><span class="k">if</span> <span class="n">seq_len</span> <span class="o">&gt;</span> <span class="mi">512</span><span class="p">:</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">flash_attn_func</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">standard_attention</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>
</code></pre></div>

<h2 id="_10">æœ€ä½³å®è·µæ£€æŸ¥æ¸…å•</h2>
<h3 id="_11">è®­ç»ƒå‰å‡†å¤‡</h3>
<ul>
<li>[ ] è¿è¡Œ benchmark ç¡®å®šæœ€ä¼˜ num_workers</li>
<li>[ ] æµ‹è¯•ä¸åŒ batch size çš„é€Ÿåº¦å’Œæ˜¾å­˜å ç”¨</li>
<li>[ ] Profile ä¸€ä¸ª epoch æ‰¾å‡ºç“¶é¢ˆ</li>
<li>[ ] å‡†å¤‡ç›‘æ§è„šæœ¬ï¼ˆGPU åˆ©ç”¨ç‡ã€é€šä¿¡æ—¶é—´ç­‰ï¼‰</li>
</ul>
<h3 id="_12">è®­ç»ƒä¸­ç›‘æ§</h3>
<ul>
<li>[ ] GPU åˆ©ç”¨ç‡æ˜¯å¦æŒç»­ &gt;90%ï¼Ÿ</li>
<li>[ ] æ˜¯å¦å­˜åœ¨æ˜¾å­˜ç¢ç‰‡åŒ–ï¼Ÿ</li>
<li>[ ] DataLoader æ˜¯å¦æˆä¸ºç“¶é¢ˆï¼Ÿ</li>
<li>[ ] é€šä¿¡æ—¶é—´å æ¯”æ˜¯å¦åˆç†ï¼Ÿ</li>
<li>[ ] æ˜¯å¦æœ‰å¼‚å¸¸çš„ GPU åŒæ­¥ï¼Ÿ</li>
</ul>
<h3 id="_13">ä¼˜åŒ–å†³ç­–</h3>
<ul>
<li>[ ] å…ˆä¼˜åŒ–æœ€å¤§çš„ç“¶é¢ˆ</li>
<li>[ ] æ¯æ¬¡ä¼˜åŒ–åé‡æ–° Profile</li>
<li>[ ] è®°å½•ä¼˜åŒ–å‰åçš„æŒ‡æ ‡å¯¹æ¯”</li>
<li>[ ] ç¡®ä¿ä¼˜åŒ–ä¸å½±å“æ¨¡å‹æ”¶æ•›</li>
<li>[ ] ä¿ç•™å¯å›æ»šçš„é…ç½®</li>
</ul>
<h3 id="_14">é•¿æœŸç»´æŠ¤</h3>
<ul>
<li>[ ] å»ºç«‹æ€§èƒ½åŸºå‡†çº¿</li>
<li>[ ] å®šæœŸæ›´æ–°ä¾èµ–åº“ç‰ˆæœ¬</li>
<li>[ ] è·Ÿè¸ªæ–°çš„ä¼˜åŒ–æŠ€æœ¯</li>
<li>[ ] åˆ†äº«ä¼˜åŒ–ç»éªŒåˆ°å›¢é˜ŸçŸ¥è¯†åº“</li>
</ul>
            </article>
            
            <nav class="page-nav"><a href="chapter10.html" class="nav-link prev">â† ç¬¬ 10 ç« ï¼šè®­ç»ƒå´©æºƒä¸ NaN é—®é¢˜</a><a href="chapter12.html" class="nav-link next">ç¬¬ 12 ç« ï¼šå¤šæœºå¤šå¡è°ƒè¯•åœ°ç‹± â†’</a></nav>
        </main>
    </div>
</body>
</html>