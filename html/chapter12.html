<!DOCTYPE html>
<html lang="zh">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <base href="./">
    <title>ç¬¬ 12 ç« ï¼šå¤šæœºå¤šå¡è°ƒè¯•åœ°ç‹±</title>
    <link rel="stylesheet" href="assets/style.css">
    <link rel="stylesheet" href="assets/highlight.css">
    <script src="assets/script.js" defer></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>
        window.MathJax = {
            tex: {
                inlineMath: [['$', '$']],
                displayMath: [['$$', '$$']],
                processEscapes: false,
                packages: {'[+]': ['noerrors', 'ams']}
            },
            options: {
                ignoreHtmlClass: 'tex2jax_ignore',
                processHtmlClass: 'tex2jax_process'
            },
            loader: {
                load: ['[tex]/noerrors', '[tex]/ams']
            }
        };
    </script>
</head>
<body>
    <div class="container">
        <nav id="sidebar" class="sidebar">
            <div class="sidebar-header">
                <h3>ç›®å½•</h3>
                <button id="sidebar-toggle" class="sidebar-toggle">
                    <span></span>
                    <span></span>
                    <span></span>
                </button>
            </div>
            <div class="sidebar-search">
                <input type="text" id="sidebar-search-input" placeholder="æœç´¢..." autocomplete="off">
            </div>
            <div id="tree-container">
                <nav class="tree-nav" role="tree">
                    <div class="tree-item " >
                        <a href="index.html" class="tree-link">
                            <span class="tree-icon">ğŸ“„</span>
                            <span class="tree-title">è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰çš„ç›‘ç£å¾®è°ƒä¸å¼ºåŒ–å­¦ä¹ å®æˆ˜æ•™ç¨‹</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter1.html" class="tree-link">
                            <span class="tree-icon">ğŸ“„</span>
                            <span class="tree-title">ç¬¬ 1 ç« ï¼šVLM æ¶æ„ä¸åŸç†</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter2.html" class="tree-link">
                            <span class="tree-icon">ğŸ“„</span>
                            <span class="tree-title">ç¬¬ 2 ç« ï¼šæ•°æ®å‡†å¤‡ä¸é¢„å¤„ç†</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter3.html" class="tree-link">
                            <span class="tree-icon">ğŸ“„</span>
                            <span class="tree-title">ç¬¬ 3 ç« ï¼šSFT è®­ç»ƒç­–ç•¥</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter4.html" class="tree-link">
                            <span class="tree-icon">ğŸ“„</span>
                            <span class="tree-title">ç¬¬ 4 ç« ï¼šåˆ†å¸ƒå¼è®­ç»ƒä¸ä¼˜åŒ–</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter5.html" class="tree-link">
                            <span class="tree-icon">ğŸ“„</span>
                            <span class="tree-title">ç¬¬ 5 ç« ï¼šRLHF åŸºç¡€ä¸å®ç°</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter6.html" class="tree-link">
                            <span class="tree-icon">ğŸ“„</span>
                            <span class="tree-title">ç¬¬ 6 ç« ï¼šç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter7.html" class="tree-link">
                            <span class="tree-icon">ğŸ“„</span>
                            <span class="tree-title">ç¬¬ 7 ç« ï¼šè¯„ä¼°ä½“ç³»è®¾è®¡</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter8.html" class="tree-link">
                            <span class="tree-icon">ğŸ“„</span>
                            <span class="tree-title">ç¬¬ 8 ç« ï¼šæ¨¡å‹éƒ¨ç½²ä¸æœåŠ¡åŒ–</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter9.html" class="tree-link">
                            <span class="tree-icon">ğŸ“„</span>
                            <span class="tree-title">ç¬¬ 9 ç« ï¼šCUDA OOM è°ƒè¯•å®Œå…¨æŒ‡å—</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter10.html" class="tree-link">
                            <span class="tree-icon">ğŸ“„</span>
                            <span class="tree-title">ç¬¬ 10 ç« ï¼šè®­ç»ƒå´©æºƒä¸ NaN é—®é¢˜</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter11.html" class="tree-link">
                            <span class="tree-icon">ğŸ“„</span>
                            <span class="tree-title">ç¬¬ 11 ç« ï¼šè®­ç»ƒé€Ÿåº¦ä¼˜åŒ–å®æˆ˜</span>
                        </a>
                    </div>
                
                    <div class="tree-item active" >
                        <a href="chapter12.html" class="tree-link">
                            <span class="tree-icon">ğŸ“„</span>
                            <span class="tree-title">ç¬¬ 12 ç« ï¼šå¤šæœºå¤šå¡è°ƒè¯•åœ°ç‹±</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="CLAUDE.html" class="tree-link">
                            <span class="tree-icon">ğŸ“„</span>
                            <span class="tree-title">Untitled</span>
                        </a>
                    </div>
                </nav>
            </div>
        </nav>
        
        <main class="content">
            <article>
                <h1 id="12">ç¬¬ 12 ç« ï¼šå¤šæœºå¤šå¡è°ƒè¯•åœ°ç‹±</h1>
<p>å½“ä½ çš„ VLM è®­ç»ƒæ‰©å±•åˆ°å¤šæœºå¤šå¡æ—¶ï¼Œä½ å°†è¿›å…¥ä¸€ä¸ªå……æ»¡æŒ‘æˆ˜çš„è°ƒè¯•ä¸–ç•Œã€‚æœ¬ç« å°†ç³»ç»Ÿåœ°ä»‹ç»åˆ†å¸ƒå¼è®­ç»ƒä¸­æœ€å¸¸è§çš„é—®é¢˜å’Œè§£å†³æ–¹æ¡ˆï¼Œä» NCCL é€šä¿¡é”™è¯¯åˆ°è¿›ç¨‹æ­»é”ï¼Œä»å¼‚æ„ GPU æ··è®­åˆ°æ¡†æ¶é€‰æ‹©ï¼Œå¸®åŠ©ä½ å¿«é€Ÿå®šä½å’Œè§£å†³å¤šæœºè®­ç»ƒä¸­çš„å„ç§"åœ°ç‹±"çº§é—®é¢˜ã€‚æ— è®ºæ˜¯å‡Œæ™¨ä¸‰ç‚¹çš„ NCCL timeoutï¼Œè¿˜æ˜¯è«åå…¶å¦™çš„è¿›ç¨‹æŒ‚èµ·ï¼Œæœ¬ç« éƒ½èƒ½è®©ä½ åœ¨ 5 åˆ†é’Ÿå†…æ‰¾åˆ°è§£å†³æ€è·¯ã€‚</p>
<h2 id="121-nccl">12.1 NCCL é”™è¯¯çš„å¸¸è§åŸå› ä¸è§£å†³</h2>
<p>NCCLï¼ˆNVIDIA Collective Communications Libraryï¼‰æ˜¯å¤š GPU è®­ç»ƒçš„æ ¸å¿ƒé€šä¿¡åº“ã€‚å½“ä½ çœ‹åˆ° "NCCL Error" æ—¶ï¼Œä¸è¦æ…Œå¼ ï¼ŒæŒ‰ç…§ä»¥ä¸‹æµç¨‹ç³»ç»Ÿæ’æŸ¥ã€‚</p>
<h3 id="1211">12.1.1 å¿«é€Ÿè¯Šæ–­æµç¨‹</h3>
<p>å½“é‡åˆ° NCCL é”™è¯¯æ—¶ï¼Œé¦–å…ˆæ‰§è¡Œä»¥ä¸‹è¯Šæ–­æ­¥éª¤ï¼š</p>
<p><strong>ç¬¬ä¸€æ­¥ï¼šå¯ç”¨è¯¦ç»†æ—¥å¿—</strong></p>
<div class="codehilite"><pre><span></span><code><span class="nb">export</span><span class="w"> </span><span class="nv">NCCL_DEBUG</span><span class="o">=</span>INFO
<span class="nb">export</span><span class="w"> </span><span class="nv">NCCL_DEBUG_SUBSYS</span><span class="o">=</span>ALL
<span class="nb">export</span><span class="w"> </span><span class="nv">TORCH_DISTRIBUTED_DEBUG</span><span class="o">=</span>DETAIL
</code></pre></div>

<p><strong>ç¬¬äºŒæ­¥ï¼šæ£€æŸ¥åŸºç¡€è¿é€šæ€§</strong></p>
<div class="codehilite"><pre><span></span><code><span class="c1"># æ£€æŸ¥èŠ‚ç‚¹é—´ç½‘ç»œè¿é€šæ€§</span>
ping<span class="w"> </span>&lt;other_node_ip&gt;
nc<span class="w"> </span>-zv<span class="w"> </span>&lt;other_node_ip&gt;<span class="w"> </span><span class="m">29500</span><span class="w">  </span><span class="c1"># æ£€æŸ¥ç«¯å£æ˜¯å¦å¼€æ”¾</span>

<span class="c1"># æ£€æŸ¥ GPU å¯è§æ€§</span>
nvidia-smi<span class="w"> </span>-L
<span class="nb">echo</span><span class="w"> </span><span class="nv">$CUDA_VISIBLE_DEVICES</span>
</code></pre></div>

<p><strong>ç¬¬ä¸‰æ­¥ï¼šè¿è¡Œ NCCL æµ‹è¯•</strong></p>
<div class="codehilite"><pre><span></span><code><span class="c1"># å•æœºæµ‹è¯•</span>
python<span class="w"> </span>-m<span class="w"> </span>torch.distributed.run<span class="w"> </span>--nproc_per_node<span class="o">=</span><span class="m">8</span><span class="w"> </span>--nnodes<span class="o">=</span><span class="m">1</span><span class="w"> </span>test_nccl.py

<span class="c1"># å¤šæœºæµ‹è¯•ï¼ˆåœ¨ä¸»èŠ‚ç‚¹è¿è¡Œï¼‰</span>
python<span class="w"> </span>-m<span class="w"> </span>torch.distributed.run<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--nproc_per_node<span class="o">=</span><span class="m">8</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--nnodes<span class="o">=</span><span class="m">2</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--node_rank<span class="o">=</span><span class="m">0</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--master_addr<span class="o">=</span>&lt;master_ip&gt;<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--master_port<span class="o">=</span><span class="m">29500</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>test_nccl.py
</code></pre></div>

<h3 id="1212">12.1.2 å¸¸è§é”™è¯¯ç±»å‹ä¸è§£å†³æ–¹æ¡ˆ</h3>
<p><strong>é”™è¯¯ 1ï¼šNCCL Init Failed</strong></p>
<p>ç—‡çŠ¶ï¼š</p>
<div class="codehilite"><pre><span></span><code><span class="n">RuntimeError</span><span class="o">:</span><span class="w"> </span><span class="n">NCCL</span><span class="w"> </span><span class="n">error</span><span class="w"> </span><span class="k">in</span><span class="o">:</span><span class="w"> </span><span class="o">../</span><span class="n">torch</span><span class="sr">/csrc/distributed/c10d/</span><span class="n">ProcessGroupNCCL</span><span class="o">.</span><span class="na">cpp</span><span class="o">:</span><span class="mi">1191</span>
<span class="n">unhandled</span><span class="w"> </span><span class="n">system</span><span class="w"> </span><span class="n">error</span><span class="o">,</span><span class="w"> </span><span class="n">NCCL</span><span class="w"> </span><span class="n">version</span><span class="w"> </span><span class="mf">2.14</span><span class="o">.</span><span class="mi">3</span>
</code></pre></div>

<p>åŸå› åˆ†æï¼š</p>
<ol>
<li>GPU ä¸å¯è§æˆ–ç¼–å·é”™è¯¯</li>
<li>NCCL ç‰ˆæœ¬ä¸å…¼å®¹</li>
<li>å…±äº«å†…å­˜ä¸è¶³</li>
</ol>
<p>è§£å†³æ–¹æ¡ˆï¼š</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># 1. ç¡®ä¿ GPU ç¼–å·æ­£ç¡®</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">CUDA_VISIBLE_DEVICES</span><span class="o">=</span><span class="m">0</span>,1,2,3,4,5,6,7

<span class="c1"># 2. å¢åŠ å…±äº«å†…å­˜</span>
docker<span class="w"> </span>run<span class="w"> </span>--shm-size<span class="o">=</span>32gb<span class="w"> </span>...<span class="w">  </span><span class="c1"># Docker ç¯å¢ƒ</span>
<span class="c1"># æˆ–ä¿®æ”¹ç³»ç»Ÿé…ç½®</span>
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;kernel.shmmax = 68719476736&quot;</span><span class="w"> </span>&gt;&gt;<span class="w"> </span>/etc/sysctl.conf
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;kernel.shmall = 4294967296&quot;</span><span class="w"> </span>&gt;&gt;<span class="w"> </span>/etc/sysctl.conf
sysctl<span class="w"> </span>-p

<span class="c1"># 3. é™çº§æˆ–å‡çº§ NCCL</span>
pip<span class="w"> </span>install<span class="w"> </span><span class="nv">torch</span><span class="o">==</span><span class="m">2</span>.0.1+cu118<span class="w">  </span><span class="c1"># ä½¿ç”¨å…¼å®¹ç‰ˆæœ¬</span>
</code></pre></div>

<p><strong>é”™è¯¯ 2ï¼šNCCL Timeout</strong></p>
<p>ç—‡çŠ¶ï¼š</p>
<div class="codehilite"><pre><span></span><code>torch.distributed.DistBackendError: NCCL timeout after 1800 seconds
</code></pre></div>

<p>åŸå› åˆ†æï¼š</p>
<ol>
<li>ç½‘ç»œå¸¦å®½ä¸è¶³æˆ–å»¶è¿Ÿè¿‡é«˜</li>
<li>æŸä¸ªè¿›ç¨‹å¡ä½æˆ–å´©æºƒ</li>
<li>æ•°æ®ä¸å‡è¡¡å¯¼è‡´ç­‰å¾…</li>
</ol>
<p>è§£å†³æ–¹æ¡ˆï¼š</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># 1. å¢åŠ è¶…æ—¶æ—¶é—´</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">NCCL_TIMEOUT</span><span class="o">=</span><span class="m">3600</span><span class="w">  </span><span class="c1"># å•ä½ï¼šç§’</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">NCCL_ASYNC_ERROR_HANDLING</span><span class="o">=</span><span class="m">1</span>

<span class="c1"># 2. ä½¿ç”¨æ›´å¿«çš„ç½‘ç»œåè®®</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">NCCL_IB_DISABLE</span><span class="o">=</span><span class="m">0</span><span class="w">  </span><span class="c1"># å¯ç”¨ InfiniBand</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">NCCL_SOCKET_IFNAME</span><span class="o">=</span>eth0<span class="w">  </span><span class="c1"># æŒ‡å®šç½‘ç»œæ¥å£</span>

<span class="c1"># 3. æ£€æŸ¥è¿›ç¨‹çŠ¶æ€</span>
ps<span class="w"> </span>aux<span class="w"> </span><span class="p">|</span><span class="w"> </span>grep<span class="w"> </span>python<span class="w">  </span><span class="c1"># æŸ¥çœ‹æ˜¯å¦æœ‰åƒµå°¸è¿›ç¨‹</span>
htop<span class="w">  </span><span class="c1"># æŸ¥çœ‹ CPU/å†…å­˜ä½¿ç”¨æƒ…å†µ</span>
</code></pre></div>

<p><strong>é”™è¯¯ 3ï¼šNCCL AllReduce Failed</strong></p>
<p>ç—‡çŠ¶ï¼š</p>
<div class="codehilite"><pre><span></span><code><span class="nv">NCCL</span><span class="w"> </span><span class="nv">WARN</span><span class="w"> </span><span class="nv">Unhandled</span><span class="w"> </span><span class="nv">System</span><span class="w"> </span><span class="nv">Error</span><span class="w"> </span><span class="k">while</span><span class="w"> </span><span class="nv">waiting</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="nv">event</span>
</code></pre></div>

<p>åŸå› åˆ†æï¼š</p>
<ol>
<li>PCIe é€šä¿¡é—®é¢˜</li>
<li>NVLink é…ç½®é”™è¯¯</li>
<li>æ‹“æ‰‘ç»“æ„ä¸ä¼˜åŒ–</li>
</ol>
<p>è§£å†³æ–¹æ¡ˆï¼š</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># 1. æ£€æŸ¥ PCIe å’Œ NVLink çŠ¶æ€</span>
nvidia-smi<span class="w"> </span>topo<span class="w"> </span>-m

<span class="c1"># 2. ä¼˜åŒ– P2P é€šä¿¡</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">NCCL_P2P_DISABLE</span><span class="o">=</span><span class="m">0</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">NCCL_P2P_LEVEL</span><span class="o">=</span>NVL<span class="w">  </span><span class="c1"># ä½¿ç”¨ NVLink</span>

<span class="c1"># 3. è®¾ç½®åˆç†çš„ NCCL æ ‘å½¢æ‹“æ‰‘</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">NCCL_TREE_THRESHOLD</span><span class="o">=</span><span class="m">0</span><span class="w">  </span><span class="c1"># æ€»æ˜¯ä½¿ç”¨æ ‘å½¢ç®—æ³•</span>
</code></pre></div>

<h3 id="1213">12.1.3 ç½‘ç»œé…ç½®ä¼˜åŒ–</h3>
<p><strong>InfiniBand é…ç½®</strong></p>
<p>InfiniBand æ˜¯é«˜æ€§èƒ½è®¡ç®—çš„é¦–é€‰ç½‘ç»œï¼š</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># æ£€æŸ¥ IB çŠ¶æ€</span>
ibstat
ibping<span class="w"> </span>-S<span class="w">  </span><span class="c1"># åœ¨ä¸€ä¸ªèŠ‚ç‚¹å¯åŠ¨æœåŠ¡å™¨</span>
ibping<span class="w"> </span>-c<span class="w"> </span>&lt;server_guid&gt;<span class="w">  </span><span class="c1"># åœ¨å¦ä¸€ä¸ªèŠ‚ç‚¹æµ‹è¯•</span>

<span class="c1"># NCCL IB é…ç½®</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">NCCL_IB_HCA</span><span class="o">=</span>mlx5_0,mlx5_1<span class="w">  </span><span class="c1"># æŒ‡å®š HCA</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">NCCL_IB_GID_INDEX</span><span class="o">=</span><span class="m">3</span><span class="w">  </span><span class="c1"># RoCE ç¯å¢ƒéœ€è¦</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">NCCL_IB_TC</span><span class="o">=</span><span class="m">106</span><span class="w">  </span><span class="c1"># Traffic Class</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">NCCL_IB_SL</span><span class="o">=</span><span class="m">0</span><span class="w">  </span><span class="c1"># Service Level</span>
</code></pre></div>

<p><strong>TCP ç½‘ç»œä¼˜åŒ–</strong></p>
<p>å½“åªæœ‰ä»¥å¤ªç½‘æ—¶çš„ä¼˜åŒ–ç­–ç•¥ï¼š</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># 1. é€‰æ‹©æ­£ç¡®çš„ç½‘ç»œæ¥å£</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">NCCL_SOCKET_IFNAME</span><span class="o">=</span>eth0,eth1<span class="w">  </span><span class="c1"># å¯ä»¥æŒ‡å®šå¤šä¸ª</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">NCCL_SOCKET_NTHREADS</span><span class="o">=</span><span class="m">8</span><span class="w">  </span><span class="c1"># å¢åŠ  socket çº¿ç¨‹æ•°</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">NCCL_NSOCKS_PERTHREAD</span><span class="o">=</span><span class="m">4</span><span class="w">  </span><span class="c1"># æ¯çº¿ç¨‹ socket æ•°</span>

<span class="c1"># 2. TCP ç¼“å†²åŒºä¼˜åŒ–</span>
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;net.core.rmem_max = 134217728&quot;</span><span class="w"> </span>&gt;&gt;<span class="w"> </span>/etc/sysctl.conf
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;net.core.wmem_max = 134217728&quot;</span><span class="w"> </span>&gt;&gt;<span class="w"> </span>/etc/sysctl.conf
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;net.ipv4.tcp_rmem = 4096 87380 134217728&quot;</span><span class="w"> </span>&gt;&gt;<span class="w"> </span>/etc/sysctl.conf
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;net.ipv4.tcp_wmem = 4096 65536 134217728&quot;</span><span class="w"> </span>&gt;&gt;<span class="w"> </span>/etc/sysctl.conf
sysctl<span class="w"> </span>-p
</code></pre></div>

<h3 id="1214">12.1.4 ç¯å¢ƒå˜é‡é€ŸæŸ¥è¡¨</h3>
<p>å…³é”® NCCL ç¯å¢ƒå˜é‡åŠå…¶ä½œç”¨ï¼š</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># è°ƒè¯•ç›¸å…³</span>
<span class="nv">NCCL_DEBUG</span><span class="o">=</span>INFO/WARN/ERROR<span class="w">  </span><span class="c1"># æ—¥å¿—çº§åˆ«</span>
<span class="nv">NCCL_DEBUG_FILE</span><span class="o">=</span>/path/to/log<span class="w">  </span><span class="c1"># æ—¥å¿—è¾“å‡ºæ–‡ä»¶</span>

<span class="c1"># æ€§èƒ½è°ƒä¼˜</span>
<span class="nv">NCCL_BUFFSIZE</span><span class="o">=</span><span class="m">8388608</span><span class="w">  </span><span class="c1"># ç¼“å†²åŒºå¤§å°ï¼ˆé»˜è®¤ 4MBï¼‰</span>
<span class="nv">NCCL_NTHREADS</span><span class="o">=</span><span class="m">512</span><span class="w">  </span><span class="c1"># NCCL çº¿ç¨‹æ•°</span>
<span class="nv">NCCL_MAX_NCHANNELS</span><span class="o">=</span><span class="m">16</span><span class="w">  </span><span class="c1"># æœ€å¤§é€šé“æ•°</span>

<span class="c1"># ç½‘ç»œé€‰æ‹©</span>
<span class="nv">NCCL_NET_GDR_LEVEL</span><span class="o">=</span><span class="m">5</span><span class="w">  </span><span class="c1"># GPUDirect RDMA çº§åˆ«</span>
<span class="nv">NCCL_CROSS_NIC</span><span class="o">=</span><span class="m">1</span><span class="w">  </span><span class="c1"># å…è®¸è·¨ NIC é€šä¿¡</span>

<span class="c1"># ç®—æ³•é€‰æ‹©</span>
<span class="nv">NCCL_ALGO</span><span class="o">=</span>Ring/Tree/CollNet<span class="w">  </span><span class="c1"># æŒ‡å®šç®—æ³•</span>
<span class="nv">NCCL_PROTO</span><span class="o">=</span>LL/LL128/Simple<span class="w">  </span><span class="c1"># åè®®é€‰æ‹©</span>
</code></pre></div>

<h3 id="1215-8a100">12.1.5 å®æˆ˜æ¡ˆä¾‹ï¼š8Ã—A100 é›†ç¾¤è°ƒè¯•</h3>
<p>çœŸå®æ¡ˆä¾‹ï¼šæŸå›¢é˜Ÿåœ¨ 2 èŠ‚ç‚¹ 8Ã—A100 é›†ç¾¤ä¸Šè®­ç»ƒ VLMï¼Œé‡åˆ°é—´æ­‡æ€§ NCCL é”™è¯¯ã€‚</p>
<p><strong>é—®é¢˜è¡¨ç°</strong>ï¼š</p>
<ul>
<li>è®­ç»ƒè¿›è¡Œåˆ° 30% æ—¶éšæœºå‡ºç° NCCL timeout</li>
<li>é”™è¯¯æ—¥å¿—æ˜¾ç¤º <code>unhandled cuda error</code></li>
<li>é‡å¯åèƒ½ç»§ç»­è®­ç»ƒä¸€æ®µæ—¶é—´</li>
</ul>
<p><strong>æ’æŸ¥è¿‡ç¨‹</strong>ï¼š</p>
<ol>
<li>å¯ç”¨ NCCL_DEBUG=INFOï¼Œå‘ç°ç‰¹å®š GPU å¯¹é€šä¿¡è¶…æ—¶</li>
<li>nvidia-smi topo -m æ˜¾ç¤º GPU 6-7 ä¹‹é—´æ˜¯ PIX è¿æ¥ï¼ˆæœ€æ…¢ï¼‰</li>
<li>æ£€æŸ¥æ¸©åº¦æ—¥å¿—ï¼Œå‘ç° GPU 6 ç»å¸¸è§¦å‘æ¸©åº¦ä¿æŠ¤ï¼ˆthrottlingï¼‰</li>
</ol>
<p><strong>è§£å†³æ–¹æ¡ˆ</strong>ï¼š</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># 1. è°ƒæ•´ GPU æ˜ å°„ï¼Œé¿å…ä½¿ç”¨é—®é¢˜ GPU å¯¹</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">CUDA_VISIBLE_DEVICES</span><span class="o">=</span><span class="m">0</span>,1,2,3,4,5<span class="w">  </span><span class="c1"># è·³è¿‡ 6,7</span>

<span class="c1"># 2. é™ä½åŠŸç‡ä¸Šé™é˜²æ­¢è¿‡çƒ­</span>
nvidia-smi<span class="w"> </span>-pl<span class="w"> </span><span class="m">300</span><span class="w">  </span><span class="c1"># è®¾ç½®åŠŸç‡ä¸Šé™ 300W</span>

<span class="c1"># 3. ä¼˜åŒ–é€šä¿¡æ¨¡å¼</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">NCCL_P2P_LEVEL</span><span class="o">=</span>PHB<span class="w">  </span><span class="c1"># åªä½¿ç”¨åŒä¸€ PCIe æ¡¥ä¸‹çš„ P2P</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">NCCL_ALGO</span><span class="o">=</span>Tree<span class="w">  </span><span class="c1"># ä½¿ç”¨æ ‘å½¢ç®—æ³•å‡å°‘ P2P ä¾èµ–</span>

<span class="c1"># 4. ç›‘æ§è„šæœ¬</span>
watch<span class="w"> </span>-n<span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="s1">&#39;nvidia-smi --query-gpu=index,name,temperature.gpu,power.draw --format=csv&#39;</span>
</code></pre></div>

<p><strong>æœ€ç»ˆæ•ˆæœ</strong>ï¼š</p>
<ul>
<li>è®­ç»ƒç¨³å®šæ€§æå‡ï¼Œæœªå†å‡ºç° timeout</li>
<li>è™½ç„¶ä½¿ç”¨ 6 ä¸ª GPUï¼Œä½†æ•´ä½“ååé‡åè€Œæå‡ 15%ï¼ˆé¿å…äº†é€šä¿¡ç“¶é¢ˆï¼‰</li>
</ul>
<h2 id="122">12.2 è¿›ç¨‹åŒæ­¥ä¸æ­»é”æ’æŸ¥</h2>
<p>åˆ†å¸ƒå¼è®­ç»ƒä¸­ï¼Œè¿›ç¨‹åŒæ­¥é—®é¢˜æ˜¯ä»…æ¬¡äº NCCL é”™è¯¯çš„ç¬¬äºŒå¤§"æ€æ‰‹"ã€‚æœ¬èŠ‚å°†æ·±å…¥å‰–ææ­»é”çš„æˆå› å’Œå¿«é€Ÿå®šä½æ–¹æ³•ã€‚</p>
<h3 id="1221">12.2.1 åˆ†å¸ƒå¼è®­ç»ƒçš„åŒæ­¥æœºåˆ¶</h3>
<p>ç†è§£åŒæ­¥ç‚¹æ˜¯æ’æŸ¥æ­»é”çš„åŸºç¡€ã€‚VLM è®­ç»ƒä¸­çš„ä¸»è¦åŒæ­¥ç‚¹ï¼š</p>
<p><strong>æ˜¾å¼åŒæ­¥ç‚¹</strong>ï¼š</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># 1. Barrier åŒæ­¥</span>
<span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">barrier</span><span class="p">()</span>  <span class="c1"># æ‰€æœ‰è¿›ç¨‹å¿…é¡»åˆ°è¾¾</span>

<span class="c1"># 2. All-Reduce æ“ä½œ</span>
<span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">all_reduce</span><span class="p">(</span><span class="n">tensor</span><span class="p">)</span>  <span class="c1"># æ¢¯åº¦åŒæ­¥</span>

<span class="c1"># 3. Broadcast æ“ä½œ  </span>
<span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">broadcast</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">src</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># å‚æ•°å¹¿æ’­</span>
</code></pre></div>

<p><strong>éšå¼åŒæ­¥ç‚¹</strong>ï¼š</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># 1. ä¼˜åŒ–å™¨æ­¥è¿›</span>
<span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>  <span class="c1"># DDP ä¼šè‡ªåŠ¨åŒæ­¥æ¢¯åº¦</span>

<span class="c1"># 2. æ¨¡å‹ä¿å­˜</span>
<span class="k">if</span> <span class="n">rank</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span> <span class="n">path</span><span class="p">)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">barrier</span><span class="p">()</span>  <span class="c1"># ç­‰å¾…ä¿å­˜å®Œæˆ</span>

<span class="c1"># 3. æ•°æ®åŠ è½½</span>
<span class="n">dataloader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">sampler</span><span class="o">=</span><span class="n">DistributedSampler</span><span class="p">(</span><span class="n">dataset</span><span class="p">))</span>
<span class="c1"># DistributedSampler ç¡®ä¿å„è¿›ç¨‹æ•°æ®ä¸é‡å¤</span>
</code></pre></div>

<h3 id="1222">12.2.2 æ­»é”çš„å…¸å‹åœºæ™¯</h3>
<p><strong>åœºæ™¯ 1ï¼šæ¡ä»¶åˆ†æ”¯ä¸ä¸€è‡´</strong></p>
<p>é”™è¯¯ä»£ç ï¼š</p>
<div class="codehilite"><pre><span></span><code><span class="k">if</span> <span class="n">rank</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">epoch</span> <span class="o">%</span> <span class="mi">10</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
    <span class="c1"># åªæœ‰ rank 0 æ‰§è¡ŒéªŒè¯</span>
    <span class="n">val_loss</span> <span class="o">=</span> <span class="n">validate</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">val_loader</span><span class="p">)</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">broadcast</span><span class="p">(</span><span class="n">val_loss</span><span class="p">,</span> <span class="n">src</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># æ­»é”ï¼å…¶ä»–è¿›ç¨‹æœªæ‰§è¡Œ</span>
</code></pre></div>

<p>æ­£ç¡®åšæ³•ï¼š</p>
<div class="codehilite"><pre><span></span><code><span class="k">if</span> <span class="n">epoch</span> <span class="o">%</span> <span class="mi">10</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">rank</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">val_loss</span> <span class="o">=</span> <span class="n">validate</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">val_loader</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">val_loss</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">broadcast</span><span class="p">(</span><span class="n">val_loss</span><span class="p">,</span> <span class="n">src</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># æ‰€æœ‰è¿›ç¨‹éƒ½å‚ä¸</span>
</code></pre></div>

<p><strong>åœºæ™¯ 2ï¼šæ•°æ®ä¸å‡è¡¡å¯¼è‡´çš„æ­»é”</strong></p>
<p>é—®é¢˜ä»£ç ï¼š</p>
<div class="codehilite"><pre><span></span><code><span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">dataloader</span><span class="p">:</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>  <span class="c1"># æŸäº›è¿›ç¨‹æ•°æ®æå‰ç»“æŸï¼Œæœªå‚ä¸åŒæ­¥</span>
</code></pre></div>

<p>è§£å†³æ–¹æ¡ˆï¼š</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># æ–¹æ¡ˆ 1ï¼šä½¿ç”¨ drop_last</span>
<span class="n">dataloader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">drop_last</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="o">...</span><span class="p">)</span>

<span class="c1"># æ–¹æ¡ˆ 2ï¼šå¡«å……æ•°æ®</span>
<span class="n">total_samples</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>
<span class="n">samples_per_rank</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="n">total_samples</span> <span class="o">/</span> <span class="n">world_size</span><span class="p">)</span>
<span class="c1"># ç¡®ä¿æ¯ä¸ªè¿›ç¨‹æœ‰ç›¸åŒæ•°é‡çš„æ‰¹æ¬¡</span>
</code></pre></div>

<p><strong>åœºæ™¯ 3ï¼šå¼‚å¸¸å¤„ç†ä¸å½“</strong></p>
<p>å±é™©ä»£ç ï¼š</p>
<div class="codehilite"><pre><span></span><code><span class="k">try</span><span class="p">:</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Error on rank </span><span class="si">{</span><span class="n">rank</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="k">continue</span>  <span class="c1"># è·³è¿‡è¿™ä¸ªæ‰¹æ¬¡ï¼Œä½†å…¶ä»–è¿›ç¨‹è¿˜åœ¨ç­‰å¾…åŒæ­¥ï¼</span>
</code></pre></div>

<p>å®‰å…¨å¤„ç†ï¼š</p>
<div class="codehilite"><pre><span></span><code><span class="k">try</span><span class="p">:</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Error on rank </span><span class="si">{</span><span class="n">rank</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="c1"># é€šçŸ¥æ‰€æœ‰è¿›ç¨‹å‡ºç°å¼‚å¸¸</span>
    <span class="n">error_flag</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.0</span><span class="p">])</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">all_reduce</span><span class="p">(</span><span class="n">error_flag</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">error_flag</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="c1"># æ‰€æœ‰è¿›ç¨‹ä¸€èµ·é€€å‡º</span>
        <span class="n">cleanup_distributed</span><span class="p">()</span>
        <span class="n">sys</span><span class="o">.</span><span class="n">exit</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div>

<h3 id="1223">12.2.3 æ­»é”å¿«é€Ÿè¯Šæ–­æ–¹æ³•</h3>
<p><strong>æ–¹æ³• 1ï¼šæ·»åŠ è¶…æ—¶å’Œæ—¥å¿—</strong></p>
<div class="codehilite"><pre><span></span><code><span class="kn">import</span> <span class="nn">functools</span>
<span class="kn">import</span> <span class="nn">torch.distributed</span> <span class="k">as</span> <span class="nn">dist</span>

<span class="k">def</span> <span class="nf">timeout_wrapper</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="n">timeout</span><span class="o">=</span><span class="mi">300</span><span class="p">):</span>
    <span class="nd">@functools</span><span class="o">.</span><span class="n">wraps</span><span class="p">(</span><span class="n">func</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">wrapper</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="kn">import</span> <span class="nn">signal</span>

        <span class="k">def</span> <span class="nf">timeout_handler</span><span class="p">(</span><span class="n">signum</span><span class="p">,</span> <span class="n">frame</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">TimeoutError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">func</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2"> timeout after </span><span class="si">{</span><span class="n">timeout</span><span class="si">}</span><span class="s2">s&quot;</span><span class="p">)</span>

        <span class="n">signal</span><span class="o">.</span><span class="n">signal</span><span class="p">(</span><span class="n">signal</span><span class="o">.</span><span class="n">SIGALRM</span><span class="p">,</span> <span class="n">timeout_handler</span><span class="p">)</span>
        <span class="n">signal</span><span class="o">.</span><span class="n">alarm</span><span class="p">(</span><span class="n">timeout</span><span class="p">)</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">result</span> <span class="o">=</span> <span class="n">func</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="k">finally</span><span class="p">:</span>
            <span class="n">signal</span><span class="o">.</span><span class="n">alarm</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">result</span>
    <span class="k">return</span> <span class="n">wrapper</span>

<span class="c1"># ä½¿ç”¨</span>
<span class="nd">@timeout_wrapper</span>
<span class="k">def</span> <span class="nf">training_step</span><span class="p">(</span><span class="n">batch</span><span class="p">):</span>
    <span class="c1"># è®­ç»ƒä»£ç </span>
    <span class="k">pass</span>
</code></pre></div>

<p><strong>æ–¹æ³• 2ï¼šè¿›ç¨‹çŠ¶æ€ç›‘æ§</strong></p>
<div class="codehilite"><pre><span></span><code><span class="kn">import</span> <span class="nn">threading</span>
<span class="kn">import</span> <span class="nn">time</span>

<span class="k">def</span> <span class="nf">monitor_thread</span><span class="p">(</span><span class="n">rank</span><span class="p">,</span> <span class="n">interval</span><span class="o">=</span><span class="mi">30</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;ç›‘æ§çº¿ç¨‹ï¼Œå®šæœŸæ‰“å°è¿›ç¨‹çŠ¶æ€&quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="nf">run</span><span class="p">():</span>
        <span class="n">step</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
            <span class="n">time</span><span class="o">.</span><span class="n">sleep</span><span class="p">(</span><span class="n">interval</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;[Rank </span><span class="si">{</span><span class="n">rank</span><span class="si">}</span><span class="s2">] Heartbeat at step </span><span class="si">{</span><span class="n">step</span><span class="si">}</span><span class="s2">, &quot;</span>
                  <span class="sa">f</span><span class="s2">&quot;Memory: </span><span class="si">{</span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">memory_allocated</span><span class="p">()</span><span class="o">/</span><span class="mf">1e9</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">GB&quot;</span><span class="p">)</span>
            <span class="n">step</span> <span class="o">+=</span> <span class="mi">1</span>

    <span class="n">thread</span> <span class="o">=</span> <span class="n">threading</span><span class="o">.</span><span class="n">Thread</span><span class="p">(</span><span class="n">target</span><span class="o">=</span><span class="n">run</span><span class="p">,</span> <span class="n">daemon</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">thread</span><span class="o">.</span><span class="n">start</span><span class="p">()</span>

<span class="c1"># åœ¨è®­ç»ƒå¼€å§‹æ—¶å¯åŠ¨</span>
<span class="n">monitor_thread</span><span class="p">(</span><span class="n">rank</span><span class="p">)</span>
</code></pre></div>

<p><strong>æ–¹æ³• 3ï¼šä½¿ç”¨ py-spy åˆ†æ</strong></p>
<div class="codehilite"><pre><span></span><code><span class="c1"># å®‰è£… py-spy</span>
pip<span class="w"> </span>install<span class="w"> </span>py-spy

<span class="c1"># åˆ†ææŒ‚èµ·çš„è¿›ç¨‹</span>
py-spy<span class="w"> </span>dump<span class="w"> </span>--pid<span class="w"> </span>&lt;process_id&gt;

<span class="c1"># ç”Ÿæˆç«ç„°å›¾</span>
py-spy<span class="w"> </span>record<span class="w"> </span>-d<span class="w"> </span><span class="m">30</span><span class="w"> </span>-o<span class="w"> </span>profile.svg<span class="w"> </span>--pid<span class="w"> </span>&lt;process_id&gt;
</code></pre></div>

<h3 id="1224-barrier">12.2.4 Barrier è¶…æ—¶é—®é¢˜</h3>
<p><strong>å¸¸è§åŸå› </strong>ï¼š</p>
<ol>
<li><strong>ä¸å‡åŒ€çš„è®¡ç®—è´Ÿè½½</strong></li>
</ol>
<div class="codehilite"><pre><span></span><code><span class="c1"># é—®é¢˜ï¼šrank 0 åšé¢å¤–çš„æ—¥å¿—è®°å½•</span>
<span class="k">if</span> <span class="n">rank</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
    <span class="c1"># å¤æ‚çš„æ—¥å¿—è®¡ç®—</span>
    <span class="n">log_metrics</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">epoch</span><span class="p">,</span> <span class="n">step</span><span class="p">)</span>

<span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">barrier</span><span class="p">()</span>  <span class="c1"># rank 0 å¤ªæ…¢ï¼Œå…¶ä»–è¿›ç¨‹è¶…æ—¶</span>
</code></pre></div>

<ol start="2">
<li><strong>I/O æ“ä½œä¸å½“</strong></li>
</ol>
<div class="codehilite"><pre><span></span><code><span class="c1"># é—®é¢˜ï¼šæ‰€æœ‰è¿›ç¨‹åŒæ—¶å†™å…¥</span>
<span class="k">for</span> <span class="n">rank</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">world_size</span><span class="p">):</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;log_</span><span class="si">{</span><span class="n">rank</span><span class="si">}</span><span class="s2">.txt&quot;</span><span class="p">,</span> <span class="s2">&quot;a&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
        <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">metrics</span><span class="p">)</span>  <span class="c1"># æ–‡ä»¶ç³»ç»Ÿå‹åŠ›å¯¼è‡´æŸäº›è¿›ç¨‹é˜»å¡</span>
</code></pre></div>

<p><strong>è§£å†³ç­–ç•¥</strong>ï¼š</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># 1. ä½¿ç”¨å¼‚æ­¥ I/O</span>
<span class="kn">import</span> <span class="nn">asyncio</span>

<span class="k">async</span> <span class="k">def</span> <span class="nf">async_log</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">filename</span><span class="p">):</span>
    <span class="k">async</span> <span class="k">with</span> <span class="n">aiofiles</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="n">filename</span><span class="p">,</span> <span class="s1">&#39;a&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
        <span class="k">await</span> <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

<span class="c1"># 2. é”™å¼€ I/O æ—¶æœº</span>
<span class="k">if</span> <span class="n">step</span> <span class="o">%</span> <span class="mi">100</span> <span class="o">==</span> <span class="n">rank</span><span class="p">:</span>  <span class="c1"># ä¸åŒ rank åœ¨ä¸åŒæ­¥æ•°è®°å½•</span>
    <span class="n">save_checkpoint</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;ckpt_</span><span class="si">{</span><span class="n">rank</span><span class="si">}</span><span class="s2">_</span><span class="si">{</span><span class="n">step</span><span class="si">}</span><span class="s2">.pt&quot;</span><span class="p">)</span>

<span class="c1"># 3. è®¾ç½®åˆç†çš„è¶…æ—¶</span>
<span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s1">&#39;TORCH_DISTRIBUTED_TIMEOUT&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;3600&#39;</span>  <span class="c1"># 1 å°æ—¶</span>
</code></pre></div>

<h3 id="1225-64-gpu">12.2.5 å®æˆ˜æ¡ˆä¾‹ï¼š64 GPU è®­ç»ƒæ­»é”æ’æŸ¥</h3>
<p><strong>èƒŒæ™¯</strong>ï¼šæŸå›¢é˜Ÿä½¿ç”¨ 8 èŠ‚ç‚¹ Ã— 8 V100 è®­ç»ƒ 13B VLMï¼Œåœ¨ç¬¬ 1000 æ­¥çªç„¶æŒ‚èµ·ã€‚</p>
<p><strong>ç—‡çŠ¶</strong>ï¼š</p>
<ul>
<li>æ‰€æœ‰ GPU åˆ©ç”¨ç‡é™ä¸º 0%</li>
<li>CPU å ç”¨ç‡æ­£å¸¸</li>
<li>æ— é”™è¯¯æ—¥å¿—è¾“å‡º</li>
</ul>
<p><strong>æ’æŸ¥æ­¥éª¤</strong>ï¼š</p>
<ol>
<li><strong>ç¡®è®¤æŒ‚èµ·ä½ç½®</strong></li>
</ol>
<div class="codehilite"><pre><span></span><code><span class="c1"># ä½¿ç”¨ gdb é™„åŠ åˆ°è¿›ç¨‹</span>
gdb<span class="w"> </span>-p<span class="w"> </span>&lt;pid&gt;
<span class="o">(</span>gdb<span class="o">)</span><span class="w"> </span>py-bt<span class="w">  </span><span class="c1"># æŸ¥çœ‹ Python è°ƒç”¨æ ˆ</span>

<span class="c1"># å‘ç°å¡åœ¨ï¼š</span>
File<span class="w"> </span><span class="s2">&quot;torch/distributed/distributed_c10d.py&quot;</span>,<span class="w"> </span>line<span class="w"> </span><span class="m">2838</span>,<span class="w"> </span><span class="k">in</span><span class="w"> </span>barrier
<span class="w">    </span>work.wait<span class="o">()</span>
</code></pre></div>

<ol start="2">
<li><strong>æ£€æŸ¥å„è¿›ç¨‹çŠ¶æ€</strong></li>
</ol>
<div class="codehilite"><pre><span></span><code><span class="c1"># æ·»åŠ è°ƒè¯•ä»£ç </span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;[Rank </span><span class="si">{</span><span class="n">rank</span><span class="si">}</span><span class="s2">] Entering barrier at step </span><span class="si">{</span><span class="n">step</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">barrier</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;[Rank </span><span class="si">{</span><span class="n">rank</span><span class="si">}</span><span class="s2">] Exited barrier&quot;</span><span class="p">)</span>

<span class="c1"># å‘ç° rank 43 æœªè¿›å…¥ barrier</span>
</code></pre></div>

<ol start="3">
<li><strong>å®šä½é—®é¢˜ä»£ç </strong></li>
</ol>
<div class="codehilite"><pre><span></span><code><span class="c1"># åŸä»£ç </span>
<span class="k">if</span> <span class="n">batch_idx</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">batch_idx</span> <span class="o">%</span> <span class="n">gradient_accumulation_steps</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

<span class="c1"># é—®é¢˜ï¼šrank 43 çš„æ•°æ®å°‘ä¸€ä¸ª batchï¼Œæœªæ‰§è¡Œæœ€åä¸€æ¬¡ optimizer.step()</span>
</code></pre></div>

<p><strong>è§£å†³æ–¹æ¡ˆ</strong>ï¼š</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># 1. ç¡®ä¿æ‰€æœ‰ rank æ‰§è¡Œç›¸åŒæ¬¡æ•°çš„ä¼˜åŒ–æ­¥éª¤</span>
<span class="n">total_steps</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">dataloader</span><span class="p">)</span> <span class="o">//</span> <span class="n">gradient_accumulation_steps</span>
<span class="k">for</span> <span class="n">step</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">total_steps</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">gradient_accumulation_steps</span><span class="p">):</span>
        <span class="n">batch</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">dataloader</span><span class="p">),</span> <span class="kc">None</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">batch</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>
            <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

    <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

<span class="c1"># 2. æ·»åŠ åŒæ­¥æ£€æŸ¥ç‚¹</span>
<span class="k">if</span> <span class="n">step</span> <span class="o">%</span> <span class="mi">100</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
    <span class="c1"># åŒæ­¥æ£€æŸ¥ï¼Œç¡®ä¿æ‰€æœ‰è¿›ç¨‹è¿›åº¦ä¸€è‡´</span>
    <span class="n">progress_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">step</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;cuda&#39;</span><span class="p">)</span>
    <span class="n">progress_list</span> <span class="o">=</span> <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">progress_tensor</span><span class="p">)</span> 
                    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">world_size</span><span class="p">)]</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">all_gather</span><span class="p">(</span><span class="n">progress_list</span><span class="p">,</span> <span class="n">progress_tensor</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">rank</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">progress</span> <span class="o">=</span> <span class="p">[</span><span class="n">p</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">progress_list</span><span class="p">]</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">progress</span><span class="p">))</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Warning: Progress mismatch: </span><span class="si">{</span><span class="n">progress</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</code></pre></div>

<h2 id="123-gpu">12.3 ä¸åŒ GPU å‹å·æ··åˆè®­ç»ƒçš„å‘</h2>
<p>åœ¨å®é™…ç”Ÿäº§ç¯å¢ƒä¸­ï¼Œä½ å¯èƒ½é¢ä¸´ A100 å’Œ V100 æ··ç”¨ã€3090 å’Œ 4090 å¹¶å­˜çš„æƒ…å†µã€‚å¼‚æ„ GPU è®­ç»ƒå……æ»¡æŒ‘æˆ˜ï¼Œæœ¬èŠ‚å°†æ­ç¤ºæ‰€æœ‰éšè—çš„é™·é˜±ã€‚</p>
<h3 id="1231-gpu">12.3.1 å¼‚æ„ GPU çš„ä¸»è¦æŒ‘æˆ˜</h3>
<p><strong>ç¡¬ä»¶å·®å¼‚å¸¦æ¥çš„é—®é¢˜</strong>ï¼š</p>
<p>| å·®å¼‚ç»´åº¦ | å½±å“ | å…¸å‹åœºæ™¯ |</p>
<table>
<thead>
<tr>
<th>å·®å¼‚ç»´åº¦</th>
<th>å½±å“</th>
<th>å…¸å‹åœºæ™¯</th>
</tr>
</thead>
<tbody>
<tr>
<td>æ˜¾å­˜å¤§å°</td>
<td>OOM é£é™©</td>
<td>A100-80G vs A100-40G</td>
</tr>
<tr>
<td>è®¡ç®—èƒ½åŠ›</td>
<td>é€Ÿåº¦ç“¶é¢ˆ</td>
<td>V100 vs A100 (2.5x å·®è·)</td>
</tr>
<tr>
<td>ç²¾åº¦æ”¯æŒ</td>
<td>è®­ç»ƒä¸ç¨³å®š</td>
<td>3090 (FP16) vs A100 (BF16)</td>
</tr>
<tr>
<td>äº’è”å¸¦å®½</td>
<td>é€šä¿¡ç“¶é¢ˆ</td>
<td>NVLink vs PCIe</td>
</tr>
<tr>
<td>æ¶æ„å·®å¼‚</td>
<td>åŠŸèƒ½ä¸å…¼å®¹</td>
<td>Ampere vs Volta</td>
</tr>
</tbody>
</table>
<h3 id="1232">12.3.2 æ€§èƒ½ç“¶é¢ˆä¸è´Ÿè½½å‡è¡¡</h3>
<p><strong>é—®é¢˜ 1ï¼šæœ¨æ¡¶æ•ˆåº”</strong></p>
<p>æœ€æ…¢çš„ GPU å†³å®šæ•´ä½“è®­ç»ƒé€Ÿåº¦ï¼š</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># è¯Šæ–­ä»£ç </span>
<span class="kn">import</span> <span class="nn">time</span>
<span class="kn">import</span> <span class="nn">torch.distributed</span> <span class="k">as</span> <span class="nn">dist</span>

<span class="k">def</span> <span class="nf">measure_gpu_speed</span><span class="p">(</span><span class="n">rank</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">dummy_batch</span><span class="p">):</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">synchronize</span><span class="p">()</span>
    <span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>

    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">100</span><span class="p">):</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">dummy_batch</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

    <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">synchronize</span><span class="p">()</span>
    <span class="n">elapsed</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">start</span>

    <span class="c1"># æ”¶é›†æ‰€æœ‰ GPU çš„æ—¶é—´</span>
    <span class="n">times</span> <span class="o">=</span> <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">dist</span><span class="o">.</span><span class="n">get_world_size</span><span class="p">())]</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">all_gather</span><span class="p">(</span><span class="n">times</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">elapsed</span><span class="p">])</span><span class="o">.</span><span class="n">cuda</span><span class="p">())</span>

    <span class="k">if</span> <span class="n">rank</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">times</span> <span class="o">=</span> <span class="p">[</span><span class="n">t</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">times</span><span class="p">]</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;GPU speeds: </span><span class="si">{</span><span class="n">times</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Slowest/Fastest ratio: </span><span class="si">{</span><span class="nb">max</span><span class="p">(</span><span class="n">times</span><span class="p">)</span><span class="o">/</span><span class="nb">min</span><span class="p">(</span><span class="n">times</span><span class="p">)</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">x&quot;</span><span class="p">)</span>
</code></pre></div>

<p><strong>è§£å†³æ–¹æ¡ˆï¼šåŠ¨æ€æ‰¹å¤§å°</strong></p>
<div class="codehilite"><pre><span></span><code><span class="k">class</span> <span class="nc">HeterogeneousDataLoader</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dataset</span><span class="p">,</span> <span class="n">gpu_configs</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        gpu_configs: {</span>
<span class="sd">            0: {&#39;type&#39;: &#39;A100&#39;, &#39;memory&#39;: 80, &#39;batch_size&#39;: 8},</span>
<span class="sd">            1: {&#39;type&#39;: &#39;V100&#39;, &#39;memory&#39;: 32, &#39;batch_size&#39;: 4},</span>
<span class="sd">        }</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gpu_configs</span> <span class="o">=</span> <span class="n">gpu_configs</span>

    <span class="k">def</span> <span class="nf">get_batch_size</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">rank</span><span class="p">):</span>
        <span class="c1"># æ ¹æ® GPU èƒ½åŠ›åˆ†é…ä¸åŒæ‰¹å¤§å°</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">gpu_configs</span><span class="p">[</span><span class="n">rank</span><span class="p">][</span><span class="s1">&#39;batch_size&#39;</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">create_loader</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">rank</span><span class="p">):</span>
        <span class="n">batch_size</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_batch_size</span><span class="p">(</span><span class="n">rank</span><span class="p">)</span>
        <span class="n">sampler</span> <span class="o">=</span> <span class="n">DistributedSampler</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">dataset</span><span class="p">,</span>
            <span class="n">num_replicas</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">gpu_configs</span><span class="p">),</span>
            <span class="n">rank</span><span class="o">=</span><span class="n">rank</span><span class="p">,</span>
            <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">DataLoader</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">dataset</span><span class="p">,</span>
            <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
            <span class="n">sampler</span><span class="o">=</span><span class="n">sampler</span>
        <span class="p">)</span>
</code></pre></div>

<p><strong>é—®é¢˜ 2ï¼šæ˜¾å­˜ä¸å‡è¡¡</strong></p>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">adaptive_gradient_accumulation</span><span class="p">(</span><span class="n">rank</span><span class="p">,</span> <span class="n">base_batch_size</span><span class="o">=</span><span class="mi">8</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;æ ¹æ® GPU æ˜¾å­˜åŠ¨æ€è°ƒæ•´æ¢¯åº¦ç´¯ç§¯&quot;&quot;&quot;</span>
    <span class="n">gpu_memory</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">get_device_properties</span><span class="p">(</span><span class="n">rank</span><span class="p">)</span><span class="o">.</span><span class="n">total_memory</span>

    <span class="k">if</span> <span class="n">gpu_memory</span> <span class="o">&gt;</span> <span class="mi">80</span> <span class="o">*</span> <span class="mi">1024</span><span class="o">**</span><span class="mi">3</span><span class="p">:</span>  <span class="c1"># 80GB</span>
        <span class="n">micro_batch_size</span> <span class="o">=</span> <span class="n">base_batch_size</span>
        <span class="n">accumulation_steps</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="k">elif</span> <span class="n">gpu_memory</span> <span class="o">&gt;</span> <span class="mi">40</span> <span class="o">*</span> <span class="mi">1024</span><span class="o">**</span><span class="mi">3</span><span class="p">:</span>  <span class="c1"># 40GB</span>
        <span class="n">micro_batch_size</span> <span class="o">=</span> <span class="n">base_batch_size</span> <span class="o">//</span> <span class="mi">2</span>
        <span class="n">accumulation_steps</span> <span class="o">=</span> <span class="mi">2</span>
    <span class="k">elif</span> <span class="n">gpu_memory</span> <span class="o">&gt;</span> <span class="mi">24</span> <span class="o">*</span> <span class="mi">1024</span><span class="o">**</span><span class="mi">3</span><span class="p">:</span>  <span class="c1"># 24GB</span>
        <span class="n">micro_batch_size</span> <span class="o">=</span> <span class="n">base_batch_size</span> <span class="o">//</span> <span class="mi">4</span>
        <span class="n">accumulation_steps</span> <span class="o">=</span> <span class="mi">4</span>
    <span class="k">else</span><span class="p">:</span>  <span class="c1"># 16GB or less</span>
        <span class="n">micro_batch_size</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="n">accumulation_steps</span> <span class="o">=</span> <span class="n">base_batch_size</span>

    <span class="k">return</span> <span class="n">micro_batch_size</span><span class="p">,</span> <span class="n">accumulation_steps</span>
</code></pre></div>

<h3 id="1233">12.3.3 æ··åˆç²¾åº¦çš„å…¼å®¹æ€§é—®é¢˜</h3>
<p><strong>BF16 vs FP16 æ··ç”¨é™·é˜±</strong>ï¼š</p>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">setup_mixed_precision</span><span class="p">(</span><span class="n">rank</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;å¤„ç†ä¸åŒ GPU çš„ç²¾åº¦å·®å¼‚&quot;&quot;&quot;</span>
    <span class="n">gpu_name</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">get_device_name</span><span class="p">(</span><span class="n">rank</span><span class="p">)</span>

    <span class="k">if</span> <span class="s1">&#39;A100&#39;</span> <span class="ow">in</span> <span class="n">gpu_name</span> <span class="ow">or</span> <span class="s1">&#39;H100&#39;</span> <span class="ow">in</span> <span class="n">gpu_name</span><span class="p">:</span>
        <span class="c1"># æ”¯æŒ BF16</span>
        <span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span>
        <span class="n">use_bf16</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># åªæ”¯æŒ FP16</span>
        <span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float16</span>
        <span class="n">use_bf16</span> <span class="o">=</span> <span class="kc">False</span>

    <span class="c1"># ç»Ÿä¸€ç²¾åº¦è®¾ç½®</span>
    <span class="n">all_use_bf16</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">use_bf16</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bool</span><span class="p">)</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>
    <span class="n">dist</span><span class="o">.</span><span class="n">all_reduce</span><span class="p">(</span><span class="n">all_use_bf16</span><span class="p">,</span> <span class="n">op</span><span class="o">=</span><span class="n">dist</span><span class="o">.</span><span class="n">ReduceOp</span><span class="o">.</span><span class="n">MIN</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">all_use_bf16</span><span class="o">.</span><span class="n">item</span><span class="p">():</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># é™çº§åˆ° FP16</span>
        <span class="k">if</span> <span class="n">rank</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Warning: Falling back to FP16 due to hardware limitations&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">float16</span>
</code></pre></div>

<p><strong>æ¢¯åº¦åŒæ­¥ç²¾åº¦é—®é¢˜</strong>ï¼š</p>
<div class="codehilite"><pre><span></span><code><span class="k">class</span> <span class="nc">MixedPrecisionOptimizer</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">dtype</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">optimizer</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">model</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">dtype</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">grad_scaler</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">amp</span><span class="o">.</span><span class="n">GradScaler</span><span class="p">(</span><span class="n">enabled</span><span class="o">=</span><span class="p">(</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">step</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># æ¢¯åº¦è½¬æ¢åˆ°ç»Ÿä¸€ç²¾åº¦</span>
        <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span>
            <span class="k">if</span> <span class="n">param</span><span class="o">.</span><span class="n">grad</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="c1"># ç¡®ä¿æ¢¯åº¦ç²¾åº¦ä¸€è‡´</span>
                <span class="n">param</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="n">param</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

        <span class="c1"># åŒæ­¥å‰è½¬æ¢</span>
        <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span>
            <span class="k">if</span> <span class="n">param</span><span class="o">.</span><span class="n">grad</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">dist</span><span class="o">.</span><span class="n">all_reduce</span><span class="p">(</span><span class="n">param</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span>
                <span class="n">param</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="n">param</span><span class="o">.</span><span class="n">grad</span> <span class="o">/</span> <span class="n">dist</span><span class="o">.</span><span class="n">get_world_size</span><span class="p">()</span>

        <span class="c1"># ä¼˜åŒ–å™¨æ­¥è¿›</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">grad_scaler</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">grad_scaler</span><span class="o">.</span><span class="n">update</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</code></pre></div>

<h3 id="1234">12.3.4 å®æˆ˜é…ç½®ç¤ºä¾‹</h3>
<p><strong>åœºæ™¯ï¼š4Ã—A100-80G + 4Ã—V100-32G æ··åˆé›†ç¾¤</strong></p>
<div class="codehilite"><pre><span></span><code><span class="c1"># é…ç½®æ–‡ä»¶ heterogeneous_config.yaml</span>
<span class="n">gpu_groups</span><span class="p">:</span>
  <span class="n">high_tier</span><span class="p">:</span>  <span class="c1"># A100-80G</span>
    <span class="n">ranks</span><span class="p">:</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]</span>
    <span class="n">batch_size</span><span class="p">:</span> <span class="mi">8</span>
    <span class="n">gradient_accumulation</span><span class="p">:</span> <span class="mi">1</span>
    <span class="n">precision</span><span class="p">:</span> <span class="n">bfloat16</span>

  <span class="n">low_tier</span><span class="p">:</span>  <span class="c1"># V100-32G  </span>
    <span class="n">ranks</span><span class="p">:</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">7</span><span class="p">]</span>
    <span class="n">batch_size</span><span class="p">:</span> <span class="mi">3</span>
    <span class="n">gradient_accumulation</span><span class="p">:</span> <span class="mi">3</span>
    <span class="n">precision</span><span class="p">:</span> <span class="n">float16</span>

<span class="n">communication</span><span class="p">:</span>
  <span class="c1"># åˆ†ç»„é€šä¿¡ç­–ç•¥</span>
  <span class="n">allreduce_groups</span><span class="p">:</span>

    <span class="o">-</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]</span>  <span class="c1"># A100 å†…éƒ¨å…ˆåŒæ­¥</span>
    <span class="o">-</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">7</span><span class="p">]</span>  <span class="c1"># V100 å†…éƒ¨å…ˆåŒæ­¥</span>

  <span class="c1"># è·¨ç»„åŒæ­¥ä½¿ç”¨å¼‚æ­¥æ¨¡å¼</span>
  <span class="n">cross_group_async</span><span class="p">:</span> <span class="n">true</span>

<span class="n">training</span><span class="p">:</span>
  <span class="c1"># ä½¿ç”¨ pipeline å¹¶è¡Œç¼“è§£ä¸å‡è¡¡</span>
  <span class="n">pipeline_parallel</span><span class="p">:</span> <span class="n">true</span>
  <span class="n">pipeline_stages</span><span class="p">:</span>

    <span class="o">-</span> <span class="n">ranks</span><span class="p">:</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>  <span class="c1"># A100 å¤„ç†å‰é¢å±‚</span>
    <span class="o">-</span> <span class="n">ranks</span><span class="p">:</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]</span>  <span class="c1"># A100 å¤„ç†ä¸­é—´å±‚</span>
    <span class="o">-</span> <span class="n">ranks</span><span class="p">:</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">7</span><span class="p">]</span>  <span class="c1"># V100 å¤„ç†åé¢å±‚ï¼ˆè®¡ç®—é‡è¾ƒå°ï¼‰</span>
</code></pre></div>

<p>å®ç°ä»£ç ï¼š</p>
<div class="codehilite"><pre><span></span><code><span class="k">class</span> <span class="nc">HeterogeneousTrainer</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config_path</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">config</span> <span class="o">=</span> <span class="n">yaml</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="nb">open</span><span class="p">(</span><span class="n">config_path</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">rank</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">get_rank</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">setup_gpu_group</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">setup_gpu_group</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># ç¡®å®šå½“å‰ GPU æ‰€å±ç»„</span>
        <span class="k">for</span> <span class="n">group_name</span><span class="p">,</span> <span class="n">group_config</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s1">&#39;gpu_groups&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">rank</span> <span class="ow">in</span> <span class="n">group_config</span><span class="p">[</span><span class="s1">&#39;ranks&#39;</span><span class="p">]:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">group_name</span> <span class="o">=</span> <span class="n">group_name</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">group_config</span> <span class="o">=</span> <span class="n">group_config</span>
                <span class="k">break</span>

        <span class="c1"># åˆ›å»ºé€šä¿¡ç»„</span>
        <span class="k">for</span> <span class="n">group_ranks</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s1">&#39;communication&#39;</span><span class="p">][</span><span class="s1">&#39;allreduce_groups&#39;</span><span class="p">]:</span>
            <span class="n">group</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">new_group</span><span class="p">(</span><span class="n">ranks</span><span class="o">=</span><span class="n">group_ranks</span><span class="p">)</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">rank</span> <span class="ow">in</span> <span class="n">group_ranks</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">local_group</span> <span class="o">=</span> <span class="n">group</span>

    <span class="k">def</span> <span class="nf">train_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">):</span>
        <span class="c1"># æ ¹æ®ç»„é…ç½®è°ƒæ•´æ‰¹å¤§å°</span>
        <span class="n">micro_batch</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_split_batch</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">group_config</span><span class="p">[</span><span class="s1">&#39;batch_size&#39;</span><span class="p">])</span>

        <span class="n">total_loss</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">step</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">group_config</span><span class="p">[</span><span class="s1">&#39;gradient_accumulation&#39;</span><span class="p">]):</span>
            <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">amp</span><span class="o">.</span><span class="n">autocast</span><span class="p">(</span>
                <span class="n">dtype</span><span class="o">=</span><span class="nb">getattr</span><span class="p">(</span><span class="n">torch</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">group_config</span><span class="p">[</span><span class="s1">&#39;precision&#39;</span><span class="p">])</span>
            <span class="p">):</span>
                <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">(</span><span class="n">micro_batch</span><span class="p">[</span><span class="n">step</span><span class="p">])</span>
                <span class="n">loss</span> <span class="o">=</span> <span class="n">loss</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">group_config</span><span class="p">[</span><span class="s1">&#39;gradient_accumulation&#39;</span><span class="p">]</span>

            <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
            <span class="n">total_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>

        <span class="c1"># åˆ†å±‚åŒæ­¥ç­–ç•¥</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_hierarchical_allreduce</span><span class="p">()</span>

        <span class="k">return</span> <span class="n">total_loss</span>

    <span class="k">def</span> <span class="nf">_hierarchical_allreduce</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># ç¬¬ä¸€æ­¥ï¼šç»„å†…åŒæ­¥</span>
        <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span>
            <span class="k">if</span> <span class="n">param</span><span class="o">.</span><span class="n">grad</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">dist</span><span class="o">.</span><span class="n">all_reduce</span><span class="p">(</span><span class="n">param</span><span class="o">.</span><span class="n">grad</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">local_group</span><span class="p">)</span>

        <span class="c1"># ç¬¬äºŒæ­¥ï¼šè·¨ç»„åŒæ­¥ï¼ˆä»…ç»„é•¿å‚ä¸ï¼‰</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">rank</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">group_config</span><span class="p">[</span><span class="s1">&#39;ranks&#39;</span><span class="p">][</span><span class="mi">0</span><span class="p">]:</span>
            <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span>
                <span class="k">if</span> <span class="n">param</span><span class="o">.</span><span class="n">grad</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="n">dist</span><span class="o">.</span><span class="n">all_reduce</span><span class="p">(</span><span class="n">param</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span>  <span class="c1"># å…¨å±€åŒæ­¥</span>

        <span class="c1"># ç¬¬ä¸‰æ­¥ï¼šç»„å†…å¹¿æ’­</span>
        <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span>
            <span class="k">if</span> <span class="n">param</span><span class="o">.</span><span class="n">grad</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">dist</span><span class="o">.</span><span class="n">broadcast</span><span class="p">(</span><span class="n">param</span><span class="o">.</span><span class="n">grad</span><span class="p">,</span> 
                             <span class="n">src</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">group_config</span><span class="p">[</span><span class="s1">&#39;ranks&#39;</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span>
                             <span class="n">group</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">local_group</span><span class="p">)</span>
</code></pre></div>

<h3 id="1235">12.3.5 è°ƒè¯•æŠ€å·§ä¸ç›‘æ§</h3>
<p><strong>å¼‚æ„é›†ç¾¤ç›‘æ§è„šæœ¬</strong>ï¼š</p>
<div class="codehilite"><pre><span></span><code><span class="ch">#!/bin/bash</span>
<span class="c1"># monitor_heterogeneous.sh</span>

<span class="k">while</span><span class="w"> </span>true<span class="p">;</span><span class="w"> </span><span class="k">do</span>
<span class="w">    </span><span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;=== GPU Status at </span><span class="k">$(</span>date<span class="k">)</span><span class="s2"> ===&quot;</span>

<span class="w">    </span><span class="c1"># æ”¶é›†æ‰€æœ‰èŠ‚ç‚¹çš„ GPU ä¿¡æ¯</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span>node<span class="w"> </span><span class="k">in</span><span class="w"> </span>node1<span class="w"> </span>node2<span class="p">;</span><span class="w"> </span><span class="k">do</span>
<span class="w">        </span><span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;Node: </span><span class="nv">$node</span><span class="s2">&quot;</span>
<span class="w">        </span>ssh<span class="w"> </span><span class="nv">$node</span><span class="w"> </span><span class="s2">&quot;nvidia-smi --query-gpu=index,name,memory.used,memory.total,utilization.gpu,temperature.gpu,power.draw --format=csv&quot;</span>
<span class="w">    </span><span class="k">done</span>

<span class="w">    </span><span class="c1"># æ£€æŸ¥é€Ÿåº¦å·®å¼‚</span>
<span class="w">    </span><span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;=== Training Speed ===&quot;</span>
<span class="w">    </span>tail<span class="w"> </span>-n<span class="w"> </span><span class="m">8</span><span class="w"> </span>training.log<span class="w"> </span><span class="p">|</span><span class="w"> </span>grep<span class="w"> </span><span class="s2">&quot;step_time&quot;</span>

<span class="w">    </span><span class="c1"># æ£€æŸ¥æ˜¯å¦æœ‰ OOM</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span>grep<span class="w"> </span>-q<span class="w"> </span><span class="s2">&quot;out of memory&quot;</span><span class="w"> </span>training.log<span class="p">;</span><span class="w"> </span><span class="k">then</span>
<span class="w">        </span><span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;WARNING: OOM detected!&quot;</span>
<span class="w">        </span>grep<span class="w"> </span><span class="s2">&quot;out of memory&quot;</span><span class="w"> </span>training.log<span class="w"> </span><span class="p">|</span><span class="w"> </span>tail<span class="w"> </span>-n<span class="w"> </span><span class="m">5</span>
<span class="w">    </span><span class="k">fi</span>

<span class="w">    </span>sleep<span class="w"> </span><span class="m">30</span>
<span class="k">done</span>
</code></pre></div>

<h2 id="124-fsdp-vs-deepspeed">12.4 FSDP vs DeepSpeed å®æˆ˜å¯¹æ¯”</h2>
<p>é€‰æ‹© FSDP è¿˜æ˜¯ DeepSpeedï¼Ÿè¿™æ˜¯æ¯ä¸ªå¤§æ¨¡å‹è®­ç»ƒè€…éƒ½ä¼šé¢ä¸´çš„é—®é¢˜ã€‚æœ¬èŠ‚é€šè¿‡å®æˆ˜å¯¹æ¯”ï¼Œå¸®ä½ åšå‡ºæœ€ä½³é€‰æ‹©ã€‚</p>
<h3 id="1241">12.4.1 æ¶æ„å·®å¼‚ä¸è®¾è®¡ç†å¿µ</h3>
<p>| ç‰¹æ€§ | FSDP | DeepSpeed |</p>
<table>
<thead>
<tr>
<th>ç‰¹æ€§</th>
<th>FSDP</th>
<th>DeepSpeed</th>
</tr>
</thead>
<tbody>
<tr>
<td>å¼€å‘è€…</td>
<td>Meta/PyTorch åŸç”Ÿ</td>
<td>Microsoft</td>
</tr>
<tr>
<td>é›†æˆåº¦</td>
<td>PyTorch å†…ç½®</td>
<td>éœ€è¦é¢å¤–å®‰è£…</td>
</tr>
<tr>
<td>å­¦ä¹ æ›²çº¿</td>
<td>ç›¸å¯¹ç®€å•</td>
<td>åŠŸèƒ½ä¸°å¯Œä½†å¤æ‚</td>
</tr>
<tr>
<td>ä¼˜åŒ–å™¨çŠ¶æ€åˆ†ç‰‡</td>
<td>âœ…</td>
<td>âœ… (ZeRO-2/3)</td>
</tr>
<tr>
<td>å‚æ•°åˆ†ç‰‡</td>
<td>âœ…</td>
<td>âœ… (ZeRO-3)</td>
</tr>
<tr>
<td>æ¿€æ´»å€¼åˆ†ç‰‡</td>
<td>éƒ¨åˆ†æ”¯æŒ</td>
<td>âœ… (ZeRO-R)</td>
</tr>
<tr>
<td>CPU Offload</td>
<td>âœ…</td>
<td>âœ… æ›´æˆç†Ÿ</td>
</tr>
<tr>
<td>æ··åˆç²¾åº¦</td>
<td>åŸç”Ÿæ”¯æŒ</td>
<td>éœ€è¦é…ç½®</td>
</tr>
<tr>
<td>Pipeline å¹¶è¡Œ</td>
<td>âŒ</td>
<td>âœ…</td>
</tr>
</tbody>
</table>
<h3 id="1242-vlm">12.4.2 VLM è®­ç»ƒé…ç½®å¯¹æ¯”</h3>
<p><strong>FSDP é…ç½®ç¤ºä¾‹</strong>ï¼š</p>
<div class="codehilite"><pre><span></span><code><span class="kn">from</span> <span class="nn">torch.distributed.fsdp</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">FullyShardedDataParallel</span> <span class="k">as</span> <span class="n">FSDP</span><span class="p">,</span>
    <span class="n">MixedPrecision</span><span class="p">,</span>
    <span class="n">BackwardPrefetch</span><span class="p">,</span>
    <span class="n">ShardingStrategy</span><span class="p">,</span>
    <span class="n">CPUOffload</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">torch.distributed.fsdp.wrap</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">transformer_auto_wrap_policy</span><span class="p">,</span>
    <span class="n">size_based_auto_wrap_policy</span>
<span class="p">)</span>

<span class="k">def</span> <span class="nf">setup_fsdp_for_vlm</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">vision_encoder</span><span class="p">,</span> <span class="n">language_model</span><span class="p">):</span>
    <span class="c1"># æ··åˆç²¾åº¦é…ç½®</span>
    <span class="n">mp_policy</span> <span class="o">=</span> <span class="n">MixedPrecision</span><span class="p">(</span>
        <span class="n">param_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">,</span>
        <span class="n">reduce_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">,</span>
        <span class="n">buffer_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">,</span>
        <span class="n">cast_forward_inputs</span><span class="o">=</span><span class="kc">True</span>
    <span class="p">)</span>

    <span class="c1"># è‡ªåŠ¨åŒ…è£…ç­–ç•¥ - å…³é”®ï¼</span>
    <span class="n">auto_wrap_policy</span> <span class="o">=</span> <span class="n">functools</span><span class="o">.</span><span class="n">partial</span><span class="p">(</span>
        <span class="n">transformer_auto_wrap_policy</span><span class="p">,</span>
        <span class="n">transformer_layer_cls</span><span class="o">=</span><span class="p">{</span>
            <span class="c1"># è§†è§‰ç¼–ç å™¨å±‚</span>
            <span class="n">vision_encoder</span><span class="o">.</span><span class="vm">__class__</span><span class="p">,</span>
            <span class="c1"># è¯­è¨€æ¨¡å‹å±‚  </span>
            <span class="nb">type</span><span class="p">(</span><span class="n">language_model</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span>
        <span class="p">}</span>
    <span class="p">)</span>

    <span class="c1"># CPU Offloadï¼ˆæ˜¾å­˜ä¸è¶³æ—¶å¯ç”¨ï¼‰</span>
    <span class="n">cpu_offload</span> <span class="o">=</span> <span class="n">CPUOffload</span><span class="p">(</span><span class="n">offload_params</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="c1"># FSDP é…ç½®</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">FSDP</span><span class="p">(</span>
        <span class="n">model</span><span class="p">,</span>
        <span class="n">auto_wrap_policy</span><span class="o">=</span><span class="n">auto_wrap_policy</span><span class="p">,</span>
        <span class="n">mixed_precision</span><span class="o">=</span><span class="n">mp_policy</span><span class="p">,</span>
        <span class="n">sharding_strategy</span><span class="o">=</span><span class="n">ShardingStrategy</span><span class="o">.</span><span class="n">FULL_SHARD</span><span class="p">,</span>  <span class="c1"># å®Œå…¨åˆ†ç‰‡</span>
        <span class="n">cpu_offload</span><span class="o">=</span><span class="n">cpu_offload</span><span class="p">,</span>
        <span class="n">backward_prefetch</span><span class="o">=</span><span class="n">BackwardPrefetch</span><span class="o">.</span><span class="n">BACKWARD_PRE</span><span class="p">,</span>  <span class="c1"># é¢„å–ä¼˜åŒ–</span>
        <span class="n">limit_all_gathers</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>  <span class="c1"># é™åˆ¶ all-gather é˜²æ­¢ OOM</span>
        <span class="n">use_orig_params</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>  <span class="c1"># ä¿æŒåŸå§‹å‚æ•°ï¼ˆé‡è¦ï¼ï¼‰</span>
    <span class="p">)</span>

    <span class="k">return</span> <span class="n">model</span>
</code></pre></div>

<p><strong>DeepSpeed é…ç½®ç¤ºä¾‹</strong>ï¼š</p>
<div class="codehilite"><pre><span></span><code><span class="p">{</span>
<span class="w">    </span><span class="nt">&quot;train_batch_size&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">64</span><span class="p">,</span>
<span class="w">    </span><span class="nt">&quot;gradient_accumulation_steps&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">8</span><span class="p">,</span>
<span class="w">    </span><span class="nt">&quot;train_micro_batch_size_per_gpu&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">2</span><span class="p">,</span>

<span class="w">    </span><span class="nt">&quot;bf16&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="nt">&quot;enabled&quot;</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span>
<span class="w">    </span><span class="p">},</span>

<span class="w">    </span><span class="nt">&quot;zero_optimization&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="nt">&quot;stage&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">3</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;offload_optimizer&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">            </span><span class="nt">&quot;device&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;cpu&quot;</span><span class="p">,</span>
<span class="w">            </span><span class="nt">&quot;pin_memory&quot;</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span>
<span class="w">        </span><span class="p">},</span>
<span class="w">        </span><span class="nt">&quot;offload_param&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">            </span><span class="nt">&quot;device&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;cpu&quot;</span><span class="p">,</span>
<span class="w">            </span><span class="nt">&quot;pin_memory&quot;</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span>
<span class="w">        </span><span class="p">},</span>
<span class="w">        </span><span class="nt">&quot;overlap_comm&quot;</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;contiguous_gradients&quot;</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;sub_group_size&quot;</span><span class="p">:</span><span class="w"> </span><span class="mf">1e9</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;reduce_bucket_size&quot;</span><span class="p">:</span><span class="w"> </span><span class="mf">1e9</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;stage3_prefetch_bucket_size&quot;</span><span class="p">:</span><span class="w"> </span><span class="mf">1e8</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;stage3_param_persistence_threshold&quot;</span><span class="p">:</span><span class="w"> </span><span class="mf">1e6</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;stage3_max_live_parameters&quot;</span><span class="p">:</span><span class="w"> </span><span class="mf">1e9</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;stage3_max_reuse_distance&quot;</span><span class="p">:</span><span class="w"> </span><span class="mf">1e9</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;stage3_gather_16bit_weights_on_model_save&quot;</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span>
<span class="w">    </span><span class="p">},</span>

<span class="w">    </span><span class="nt">&quot;gradient_clipping&quot;</span><span class="p">:</span><span class="w"> </span><span class="mf">1.0</span><span class="p">,</span>

<span class="w">    </span><span class="nt">&quot;optimizer&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="nt">&quot;type&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;AdamW&quot;</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;params&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">            </span><span class="nt">&quot;lr&quot;</span><span class="p">:</span><span class="w"> </span><span class="mf">2e-5</span><span class="p">,</span>
<span class="w">            </span><span class="nt">&quot;betas&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="mf">0.9</span><span class="p">,</span><span class="w"> </span><span class="mf">0.999</span><span class="p">],</span>
<span class="w">            </span><span class="nt">&quot;eps&quot;</span><span class="p">:</span><span class="w"> </span><span class="mf">1e-8</span><span class="p">,</span>
<span class="w">            </span><span class="nt">&quot;weight_decay&quot;</span><span class="p">:</span><span class="w"> </span><span class="mf">0.01</span>
<span class="w">        </span><span class="p">}</span>
<span class="w">    </span><span class="p">},</span>

<span class="w">    </span><span class="nt">&quot;scheduler&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="nt">&quot;type&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;WarmupCosineLR&quot;</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;params&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">            </span><span class="nt">&quot;warmup_min_lr&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span>
<span class="w">            </span><span class="nt">&quot;warmup_max_lr&quot;</span><span class="p">:</span><span class="w"> </span><span class="mf">2e-5</span><span class="p">,</span>
<span class="w">            </span><span class="nt">&quot;warmup_num_steps&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">1000</span><span class="p">,</span>
<span class="w">            </span><span class="nt">&quot;total_num_steps&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">10000</span>
<span class="w">        </span><span class="p">}</span>
<span class="w">    </span><span class="p">},</span>

<span class="w">    </span><span class="nt">&quot;activation_checkpointing&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="nt">&quot;partition_activations&quot;</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;cpu_checkpointing&quot;</span><span class="p">:</span><span class="w"> </span><span class="kc">false</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;contiguous_memory_optimization&quot;</span><span class="p">:</span><span class="w"> </span><span class="kc">false</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;number_checkpoints&quot;</span><span class="p">:</span><span class="w"> </span><span class="kc">null</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;synchronize_checkpoint_boundary&quot;</span><span class="p">:</span><span class="w"> </span><span class="kc">false</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;profile&quot;</span><span class="p">:</span><span class="w"> </span><span class="kc">false</span>
<span class="w">    </span><span class="p">}</span>
<span class="p">}</span>
</code></pre></div>

<h3 id="1243">12.4.3 æ€§èƒ½åŸºå‡†æµ‹è¯•</h3>
<p><strong>æµ‹è¯•ç¯å¢ƒ</strong>ï¼š</p>
<ul>
<li>æ¨¡å‹ï¼šLLaVA-13B</li>
<li>ç¡¬ä»¶ï¼š8Ã—A100-40G</li>
<li>æ•°æ®ï¼šå›¾æ–‡å¯¹ï¼Œæ‰¹å¤§å° 64</li>
</ul>
<p><strong>æµ‹è¯•ç»“æœ</strong>ï¼š</p>
<p>| æŒ‡æ ‡ | FSDP | DeepSpeed ZeRO-2 | DeepSpeed ZeRO-3 |</p>
<table>
<thead>
<tr>
<th>æŒ‡æ ‡</th>
<th>FSDP</th>
<th>DeepSpeed ZeRO-2</th>
<th>DeepSpeed ZeRO-3</th>
</tr>
</thead>
<tbody>
<tr>
<td>ååé‡ (samples/s)</td>
<td>28.5</td>
<td>31.2</td>
<td>26.8</td>
</tr>
<tr>
<td>æ˜¾å­˜å ç”¨ (GB)</td>
<td>35.2</td>
<td>32.1</td>
<td>28.9</td>
</tr>
<tr>
<td>é€šä¿¡å¼€é”€ (%)</td>
<td>18%</td>
<td>15%</td>
<td>22%</td>
</tr>
<tr>
<td>å¯åŠ¨æ—¶é—´ (s)</td>
<td>45</td>
<td>62</td>
<td>78</td>
</tr>
<tr>
<td>Checkpoint å¤§å° (GB)</td>
<td>26</td>
<td>26</td>
<td>52 (åˆ†ç‰‡)</td>
</tr>
</tbody>
</table>
<p><strong>æ€§èƒ½åˆ†æä»£ç </strong>ï¼š</p>
<div class="codehilite"><pre><span></span><code><span class="kn">import</span> <span class="nn">time</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">contextlib</span> <span class="kn">import</span> <span class="n">contextmanager</span>

<span class="nd">@contextmanager</span>
<span class="k">def</span> <span class="nf">profile_time</span><span class="p">(</span><span class="n">name</span><span class="p">):</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">synchronize</span><span class="p">()</span>
    <span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
    <span class="k">yield</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">synchronize</span><span class="p">()</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">start</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">s&quot;</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">benchmark_training_step</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">use_fsdp</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="n">metrics</span> <span class="o">=</span> <span class="p">{}</span>

    <span class="c1"># Forward</span>
    <span class="k">with</span> <span class="n">profile_time</span><span class="p">(</span><span class="s2">&quot;Forward&quot;</span><span class="p">):</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">loss</span>
    <span class="n">metrics</span><span class="p">[</span><span class="s1">&#39;loss&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>

    <span class="c1"># Backward</span>
    <span class="k">with</span> <span class="n">profile_time</span><span class="p">(</span><span class="s2">&quot;Backward&quot;</span><span class="p">):</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

    <span class="c1"># Optimizer step</span>
    <span class="k">with</span> <span class="n">profile_time</span><span class="p">(</span><span class="s2">&quot;Optimizer&quot;</span><span class="p">):</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

    <span class="c1"># å†…å­˜ç»Ÿè®¡</span>
    <span class="n">metrics</span><span class="p">[</span><span class="s1">&#39;memory_allocated&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">memory_allocated</span><span class="p">()</span> <span class="o">/</span> <span class="mf">1e9</span>
    <span class="n">metrics</span><span class="p">[</span><span class="s1">&#39;memory_reserved&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">memory_reserved</span><span class="p">()</span> <span class="o">/</span> <span class="mf">1e9</span>

    <span class="k">if</span> <span class="n">use_fsdp</span><span class="p">:</span>
        <span class="c1"># FSDP ç‰¹å®šæŒ‡æ ‡</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="s1">&#39;_fsdp_wrapped_module&#39;</span><span class="p">):</span>
            <span class="n">metrics</span><span class="p">[</span><span class="s1">&#39;communication_time&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">_communication_time</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># DeepSpeed ç‰¹å®šæŒ‡æ ‡</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="s1">&#39;timer_names&#39;</span><span class="p">):</span>
            <span class="k">for</span> <span class="n">name</span> <span class="ow">in</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">timer_names</span><span class="p">:</span>
                <span class="n">metrics</span><span class="p">[</span><span class="sa">f</span><span class="s1">&#39;ds_</span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">timers</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">metrics</span>
</code></pre></div>

<h3 id="1244">12.4.4 é€‰æ‹©å»ºè®®ä¸è¿ç§»æŒ‡å—</h3>
<p><strong>ä½•æ—¶é€‰æ‹© FSDP</strong>ï¼š</p>
<ol>
<li>PyTorch åŸç”Ÿé¡¹ç›®ï¼Œä¸æƒ³å¼•å…¥é¢å¤–ä¾èµ–</li>
<li>æ¨¡å‹ç›¸å¯¹ç®€å•ï¼Œä¸éœ€è¦å¤æ‚çš„å¹¶è¡Œç­–ç•¥</li>
<li>å›¢é˜Ÿç†Ÿæ‚‰ PyTorchï¼Œå­¦ä¹ æˆæœ¬ä½</li>
<li>éœ€è¦ä¸å…¶ä»– PyTorch ç”Ÿæ€å·¥å…·é›†æˆ</li>
</ol>
<p><strong>ä½•æ—¶é€‰æ‹© DeepSpeed</strong>ï¼š</p>
<ol>
<li>è¶…å¤§æ¨¡å‹ï¼ˆ&gt;30Bï¼‰ï¼Œéœ€è¦æè‡´ä¼˜åŒ–</li>
<li>éœ€è¦ Pipeline å¹¶è¡Œç­‰é«˜çº§ç‰¹æ€§</li>
<li>æ··åˆè®­ç»ƒç¯å¢ƒï¼Œéœ€è¦æ›´ç»†ç²’åº¦æ§åˆ¶</li>
<li>å·²æœ‰ DeepSpeed ç»éªŒç§¯ç´¯</li>
</ol>
<p><strong>ä» FSDP è¿ç§»åˆ° DeepSpeed</strong>ï¼š</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># è¿ç§»æ£€æŸ¥æ¸…å•</span>
<span class="n">migration_checklist</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;æ¨¡å‹åŒ…è£…&quot;</span><span class="p">:</span> <span class="s2">&quot;FSDP(...) -&gt; deepspeed.initialize(...)&quot;</span><span class="p">,</span>
    <span class="s2">&quot;ä¼˜åŒ–å™¨&quot;</span><span class="p">:</span> <span class="s2">&quot;éœ€è¦åœ¨ config ä¸­é…ç½®ï¼Œä¸èƒ½ç›´æ¥ä¼ å…¥&quot;</span><span class="p">,</span>
    <span class="s2">&quot;æ¢¯åº¦ç´¯ç§¯&quot;</span><span class="p">:</span> <span class="s2">&quot;è‡ªåŠ¨å¤„ç† -&gt; éœ€è¦æ˜¾å¼é…ç½®&quot;</span><span class="p">,</span>
    <span class="s2">&quot;Checkpoint&quot;</span><span class="p">:</span> <span class="s2">&quot;torch.save -&gt; model.save_checkpoint&quot;</span><span class="p">,</span>
    <span class="s2">&quot;æ··åˆç²¾åº¦&quot;</span><span class="p">:</span> <span class="s2">&quot;MixedPrecision -&gt; fp16/bf16 config&quot;</span><span class="p">,</span>
    <span class="s2">&quot;å­¦ä¹ ç‡è°ƒåº¦&quot;</span><span class="p">:</span> <span class="s2">&quot;æ‰‹åŠ¨ -&gt; é…ç½®æ–‡ä»¶&quot;</span><span class="p">,</span>
<span class="p">}</span>

<span class="c1"># è¿ç§»ç¤ºä¾‹</span>
<span class="k">def</span> <span class="nf">migrate_fsdp_to_deepspeed</span><span class="p">(</span><span class="n">fsdp_model</span><span class="p">,</span> <span class="n">fsdp_optimizer</span><span class="p">):</span>
    <span class="c1"># 1. æå–åŸå§‹æ¨¡å‹</span>
    <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">fsdp_model</span><span class="p">,</span> <span class="s1">&#39;module&#39;</span><span class="p">):</span>
        <span class="n">base_model</span> <span class="o">=</span> <span class="n">fsdp_model</span><span class="o">.</span><span class="n">module</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">base_model</span> <span class="o">=</span> <span class="n">fsdp_model</span>

    <span class="c1"># 2. åˆ›å»º DeepSpeed é…ç½®</span>
    <span class="n">ds_config</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">&quot;train_batch_size&quot;</span><span class="p">:</span> <span class="n">world_size</span> <span class="o">*</span> <span class="n">batch_size</span><span class="p">,</span>
        <span class="s2">&quot;gradient_accumulation_steps&quot;</span><span class="p">:</span> <span class="n">grad_acc_steps</span><span class="p">,</span>
        <span class="s2">&quot;zero_optimization&quot;</span><span class="p">:</span> <span class="p">{</span>
            <span class="s2">&quot;stage&quot;</span><span class="p">:</span> <span class="mi">3</span> <span class="k">if</span> <span class="n">was_full_shard</span> <span class="k">else</span> <span class="mi">2</span><span class="p">,</span>
            <span class="c1"># å…¶ä»–é…ç½®...</span>
        <span class="p">}</span>
    <span class="p">}</span>

    <span class="c1"># 3. åˆå§‹åŒ– DeepSpeed</span>
    <span class="n">model</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">deepspeed</span><span class="o">.</span><span class="n">initialize</span><span class="p">(</span>
        <span class="n">model</span><span class="o">=</span><span class="n">base_model</span><span class="p">,</span>
        <span class="n">config</span><span class="o">=</span><span class="n">ds_config</span><span class="p">,</span>
        <span class="n">model_parameters</span><span class="o">=</span><span class="n">base_model</span><span class="o">.</span><span class="n">parameters</span><span class="p">()</span>
    <span class="p">)</span>

    <span class="k">return</span> <span class="n">model</span><span class="p">,</span> <span class="n">optimizer</span>
</code></pre></div>

<h3 id="1245">12.4.5 å¸¸è§é—®é¢˜ä¸è§£å†³æ–¹æ¡ˆ</h3>
<p><strong>é—®é¢˜ 1ï¼šFSDP OOM ä½† DeepSpeed æ­£å¸¸</strong></p>
<p>åŸå› ï¼šFSDP çš„ all-gather æ“ä½œå¯èƒ½å¯¼è‡´ç¬æ—¶æ˜¾å­˜å³°å€¼ã€‚</p>
<p>è§£å†³æ–¹æ¡ˆï¼š</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># FSDP é™åˆ¶ all-gather</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">FSDP</span><span class="p">(</span>
    <span class="n">model</span><span class="p">,</span>
    <span class="n">limit_all_gathers</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">forward_prefetch</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>  <span class="c1"># å‰å‘é¢„å–</span>
    <span class="n">backward_prefetch</span><span class="o">=</span><span class="n">BackwardPrefetch</span><span class="o">.</span><span class="n">BACKWARD_PRE</span><span class="p">,</span>
<span class="p">)</span>
</code></pre></div>

<p><strong>é—®é¢˜ 2ï¼šDeepSpeed è®­ç»ƒé€Ÿåº¦æ…¢</strong></p>
<p>åŸå› ï¼šZeRO-3 çš„å‚æ•°æ”¶é›†å¼€é”€ã€‚</p>
<p>è§£å†³æ–¹æ¡ˆï¼š</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># ä¼˜åŒ– ZeRO-3 é…ç½®</span>
<span class="s2">&quot;zero_optimization&quot;</span><span class="p">:</span> <span class="p">{</span>
    <span class="s2">&quot;stage&quot;</span><span class="p">:</span> <span class="mi">3</span><span class="p">,</span>
    <span class="s2">&quot;stage3_max_live_parameters&quot;</span><span class="p">:</span> <span class="mf">2e9</span><span class="p">,</span>  <span class="c1"># å¢åŠ ç¼“å­˜</span>
    <span class="s2">&quot;stage3_max_reuse_distance&quot;</span><span class="p">:</span> <span class="mf">2e9</span><span class="p">,</span>
    <span class="s2">&quot;stage3_prefetch_bucket_size&quot;</span><span class="p">:</span> <span class="mf">2e8</span><span class="p">,</span>  <span class="c1"># å¢åŠ é¢„å–</span>
<span class="p">}</span>
</code></pre></div>

<p><strong>é—®é¢˜ 3ï¼šCheckpoint ä¸å…¼å®¹</strong></p>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">convert_checkpoint</span><span class="p">(</span><span class="n">checkpoint_path</span><span class="p">,</span> <span class="n">from_format</span><span class="o">=</span><span class="s2">&quot;fsdp&quot;</span><span class="p">,</span> <span class="n">to_format</span><span class="o">=</span><span class="s2">&quot;deepspeed&quot;</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;è½¬æ¢ä¸åŒæ ¼å¼çš„ checkpoint&quot;&quot;&quot;</span>

    <span class="k">if</span> <span class="n">from_format</span> <span class="o">==</span> <span class="s2">&quot;fsdp&quot;</span> <span class="ow">and</span> <span class="n">to_format</span> <span class="o">==</span> <span class="s2">&quot;deepspeed&quot;</span><span class="p">:</span>
        <span class="c1"># FSDP -&gt; DeepSpeed</span>
        <span class="n">state_dict</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">checkpoint_path</span><span class="p">)</span>

        <span class="c1"># FSDP å¯èƒ½æœ‰ _fsdp å‰ç¼€</span>
        <span class="n">new_state_dict</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="n">new_key</span> <span class="o">=</span> <span class="n">key</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot;_fsdp_wrapped_module.&quot;</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">)</span>
            <span class="n">new_key</span> <span class="o">=</span> <span class="n">new_key</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot;_fpw_module.&quot;</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">)</span>
            <span class="n">new_state_dict</span><span class="p">[</span><span class="n">new_key</span><span class="p">]</span> <span class="o">=</span> <span class="n">value</span>

        <span class="c1"># DeepSpeed æœŸæœ›çš„æ ¼å¼</span>
        <span class="n">ds_checkpoint</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">&quot;module&quot;</span><span class="p">:</span> <span class="n">new_state_dict</span><span class="p">,</span>
            <span class="s2">&quot;epoch&quot;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
            <span class="s2">&quot;global_step&quot;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
        <span class="p">}</span>

        <span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">ds_checkpoint</span><span class="p">,</span> <span class="n">checkpoint_path</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot;.pt&quot;</span><span class="p">,</span> <span class="s2">&quot;_ds.pt&quot;</span><span class="p">))</span>

    <span class="k">elif</span> <span class="n">from_format</span> <span class="o">==</span> <span class="s2">&quot;deepspeed&quot;</span> <span class="ow">and</span> <span class="n">to_format</span> <span class="o">==</span> <span class="s2">&quot;fsdp&quot;</span><span class="p">:</span>
        <span class="c1"># DeepSpeed -&gt; FSDP</span>
        <span class="c1"># DeepSpeed ZeRO-3 éœ€è¦å…ˆæ”¶é›†åˆ†ç‰‡</span>
        <span class="kn">from</span> <span class="nn">deepspeed.utils.zero_to_fp32</span> <span class="kn">import</span> <span class="n">convert_zero_checkpoint_to_fp32_state_dict</span>

        <span class="n">convert_zero_checkpoint_to_fp32_state_dict</span><span class="p">(</span>
            <span class="n">checkpoint_path</span><span class="p">,</span>
            <span class="n">checkpoint_path</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot;ds&quot;</span><span class="p">,</span> <span class="s2">&quot;fsdp.pt&quot;</span><span class="p">)</span>
        <span class="p">)</span>
</code></pre></div>

<h2 id="_1">æœ¬ç« å°ç»“</h2>
<p>å¤šæœºå¤šå¡è®­ç»ƒæ˜¯ VLM æ‰©å±•çš„å¿…ç»ä¹‹è·¯ï¼Œä½†ä¹Ÿå……æ»¡æŒ‘æˆ˜ã€‚æœ¬ç« ç³»ç»Ÿä»‹ç»äº†åˆ†å¸ƒå¼è®­ç»ƒä¸­æœ€å¸¸è§çš„å››ç±»é—®é¢˜ï¼š</p>
<ol>
<li>
<p><strong>NCCL é€šä¿¡é”™è¯¯</strong>ï¼šæŒæ¡äº†å¿«é€Ÿè¯Šæ–­æµç¨‹ã€ç¯å¢ƒå˜é‡é…ç½®å’Œç½‘ç»œä¼˜åŒ–ç­–ç•¥ã€‚è®°ä½ï¼Œå¤§éƒ¨åˆ† NCCL é”™è¯¯éƒ½å¯ä»¥é€šè¿‡æ­£ç¡®çš„ç¯å¢ƒå˜é‡å’Œç½‘ç»œé…ç½®è§£å†³ã€‚</p>
</li>
<li>
<p><strong>è¿›ç¨‹åŒæ­¥ä¸æ­»é”</strong>ï¼šç†è§£äº†åˆ†å¸ƒå¼è®­ç»ƒçš„åŒæ­¥æœºåˆ¶ï¼Œå­¦ä¼šäº†è¯†åˆ«å’Œé¿å…æ­»é”çš„å…¸å‹åœºæ™¯ã€‚å…³é”®æ˜¯ç¡®ä¿æ‰€æœ‰è¿›ç¨‹æ‰§è¡Œç›¸åŒçš„é›†åˆé€šä¿¡æ“ä½œã€‚</p>
</li>
<li>
<p><strong>å¼‚æ„ GPU è®­ç»ƒ</strong>ï¼šäº†è§£äº†æ··åˆ GPU è®­ç»ƒçš„æŒ‘æˆ˜å’Œè§£å†³æ–¹æ¡ˆã€‚æ ¸å¿ƒæ€æƒ³æ˜¯æ ¹æ®ç¡¬ä»¶èƒ½åŠ›åŠ¨æ€è°ƒæ•´æ‰¹å¤§å°å’Œæ¢¯åº¦ç´¯ç§¯ç­–ç•¥ã€‚</p>
</li>
<li>
<p><strong>FSDP vs DeepSpeed</strong>ï¼šé€šè¿‡å®æˆ˜å¯¹æ¯”ï¼Œæ˜ç¡®äº†ä¸¤ç§æ¡†æ¶çš„ä¼˜åŠ£å’Œé€‚ç”¨åœºæ™¯ã€‚FSDP æ›´ç®€å•ç›´æ¥ï¼ŒDeepSpeed åŠŸèƒ½æ›´ä¸°å¯Œã€‚</p>
</li>
</ol>
<p><strong>å…³é”®å…¬å¼å›é¡¾</strong>ï¼š</p>
<p>æœ‰æ•ˆæ‰¹å¤§å°è®¡ç®—ï¼š
$$\text{Effective Batch Size} = \text{World Size} \times \text{Micro Batch Size} \times \text{Gradient Accumulation Steps}$$
é€šä¿¡æ—¶é—´ä¼°ç®—ï¼š
$$T_{\text{comm}} = \frac{\text{Data Size}}{\text{Bandwidth}} + \text{Latency} \times \text{Num Operations}$$
æ˜¾å­˜å ç”¨ï¼ˆZeRO-3ï¼‰ï¼š
$$M_{\text{per GPU}} = \frac{M_{\text{model}} + M_{\text{optimizer}} + M_{\text{gradients}}}{\text{World Size}} + M_{\text{activations}}$$</p>
<h2 id="_2">ç»ƒä¹ é¢˜</h2>
<h3 id="_3">åŸºç¡€é¢˜</h3>
<p><strong>ç»ƒä¹  12.1ï¼šNCCL ç¯å¢ƒå˜é‡é…ç½®</strong></p>
<p>ä½ çš„ 8 å¡ V100 æœåŠ¡å™¨è®­ç»ƒæ—¶ç»å¸¸å‡ºç° NCCL timeoutï¼Œè¯·å†™å‡ºå®Œæ•´çš„ç¯å¢ƒå˜é‡é…ç½®æ¥ä¼˜åŒ–é€šä¿¡ã€‚</p>
<p>ğŸ’¡ <strong>æç¤º</strong>ï¼šè€ƒè™‘è¶…æ—¶æ—¶é—´ã€æ—¥å¿—çº§åˆ«ã€P2P é€šä¿¡å’Œç½‘ç»œæ¥å£é€‰æ‹©ã€‚</p>
<details>
<summary>ğŸ“ å‚è€ƒç­”æ¡ˆ</summary>
<div class="codehilite"><pre><span></span><code><span class="c1"># å¢åŠ è¶…æ—¶æ—¶é—´</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">NCCL_TIMEOUT</span><span class="o">=</span><span class="m">7200</span><span class="w">  </span><span class="c1"># 2å°æ—¶</span>

<span class="c1"># å¯ç”¨è°ƒè¯•æ—¥å¿—</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">NCCL_DEBUG</span><span class="o">=</span>INFO
<span class="nb">export</span><span class="w"> </span><span class="nv">NCCL_DEBUG_SUBSYS</span><span class="o">=</span>INIT,GRAPH

<span class="c1"># ä¼˜åŒ– P2P é€šä¿¡</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">NCCL_P2P_DISABLE</span><span class="o">=</span><span class="m">0</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">NCCL_P2P_LEVEL</span><span class="o">=</span>NVL

<span class="c1"># æŒ‡å®šç½‘ç»œæ¥å£</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">NCCL_SOCKET_IFNAME</span><span class="o">=</span>eth0
<span class="nb">export</span><span class="w"> </span><span class="nv">NCCL_IB_DISABLE</span><span class="o">=</span><span class="m">1</span><span class="w">  </span><span class="c1"># å¦‚æœæ²¡æœ‰ InfiniBand</span>

<span class="c1"># ä¼˜åŒ–ç¼“å†²åŒº</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">NCCL_BUFFSIZE</span><span class="o">=</span><span class="m">8388608</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">NCCL_NTHREADS</span><span class="o">=</span><span class="m">256</span>

<span class="c1"># æ ‘å½¢ç®—æ³•ä¼˜åŒ–</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">NCCL_TREE_THRESHOLD</span><span class="o">=</span><span class="m">0</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">NCCL_ALGO</span><span class="o">=</span>Tree
</code></pre></div>

<p>è¿™å¥—é…ç½®å¢åŠ äº†è¶…æ—¶å®¹å¿åº¦ï¼Œå¯ç”¨äº†è¯¦ç»†æ—¥å¿—ä¾¿äºè°ƒè¯•ï¼Œä¼˜åŒ–äº† P2P å’Œç½‘ç»œé€šä¿¡ï¼Œé€‚åˆå¤§å¤šæ•° V100 é›†ç¾¤ã€‚</p>
</details>
<p><strong>ç»ƒä¹  12.2ï¼šæ­»é”è¯Šæ–­</strong></p>
<p>ä»¥ä¸‹ä»£ç åœ¨ 4 å¡è®­ç»ƒæ—¶ä¼šæ­»é”ï¼Œè¯·æ‰¾å‡ºåŸå› å¹¶ä¿®å¤ï¼š</p>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">validation_step</span><span class="p">(</span><span class="n">rank</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">val_loader</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">rank</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
        <span class="n">total_loss</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">val_loader</span><span class="p">:</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span><span class="o">.</span><span class="n">loss</span>
            <span class="n">total_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        <span class="n">avg_loss</span> <span class="o">=</span> <span class="n">total_loss</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">val_loader</span><span class="p">)</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">broadcast</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">avg_loss</span><span class="p">])</span><span class="o">.</span><span class="n">cuda</span><span class="p">(),</span> <span class="n">src</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">avg_loss</span>
</code></pre></div>

<p>ğŸ’¡ <strong>æç¤º</strong>ï¼šè€ƒè™‘æ‰€æœ‰è¿›ç¨‹çš„æ‰§è¡Œè·¯å¾„ã€‚</p>
<details>
<summary>ğŸ“ å‚è€ƒç­”æ¡ˆ</summary>
<p>é—®é¢˜ï¼šåªæœ‰ rank 0 æ‰§è¡Œ broadcastï¼Œå…¶ä»–è¿›ç¨‹æ²¡æœ‰å¯¹åº”çš„ broadcast è°ƒç”¨ï¼Œå¯¼è‡´æ­»é”ã€‚</p>
<p>ä¿®å¤æ–¹æ¡ˆï¼š</p>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">validation_step</span><span class="p">(</span><span class="n">rank</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">val_loader</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">rank</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
        <span class="n">total_loss</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">val_loader</span><span class="p">:</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span><span class="o">.</span><span class="n">loss</span>
            <span class="n">total_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        <span class="n">avg_loss</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">total_loss</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">val_loader</span><span class="p">)])</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">avg_loss</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>

    <span class="c1"># æ‰€æœ‰è¿›ç¨‹éƒ½å‚ä¸ broadcast</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">broadcast</span><span class="p">(</span><span class="n">avg_loss</span><span class="p">,</span> <span class="n">src</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">avg_loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
</code></pre></div>

</details>
<p><strong>ç»ƒä¹  12.3ï¼šå¼‚æ„ GPU æ‰¹å¤§å°è®¡ç®—</strong></p>
<p>ä½ æœ‰ 2 å¼  A100-80G å’Œ 2 å¼  V100-32Gï¼Œç›®æ ‡æ˜¯æ€»æ‰¹å¤§å° 64ã€‚è¯·è®¾è®¡æ¯ä¸ª GPU çš„ micro batch size å’Œæ¢¯åº¦ç´¯ç§¯æ­¥æ•°ã€‚</p>
<p>ğŸ’¡ <strong>æç¤º</strong>ï¼šA100 çš„è®¡ç®—èƒ½åŠ›çº¦æ˜¯ V100 çš„ 2.5 å€ã€‚</p>
<details>
<summary>ğŸ“ å‚è€ƒç­”æ¡ˆ</summary>
<p>æ ¹æ®æ˜¾å­˜å’Œè®¡ç®—èƒ½åŠ›åˆ†é…ï¼š</p>
<div class="codehilite"><pre><span></span><code><span class="n">config</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;A100-80G&quot;</span><span class="p">:</span> <span class="p">{</span>
        <span class="s2">&quot;micro_batch_size&quot;</span><span class="p">:</span> <span class="mi">8</span><span class="p">,</span>  <span class="c1"># å……åˆ†åˆ©ç”¨æ˜¾å­˜</span>
        <span class="s2">&quot;gradient_accumulation_steps&quot;</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span>
        <span class="s2">&quot;effective_batch_per_gpu&quot;</span><span class="p">:</span> <span class="mi">16</span>
    <span class="p">},</span>
    <span class="s2">&quot;V100-32G&quot;</span><span class="p">:</span> <span class="p">{</span>
        <span class="s2">&quot;micro_batch_size&quot;</span><span class="p">:</span> <span class="mi">3</span><span class="p">,</span>  <span class="c1"># æ˜¾å­˜é™åˆ¶</span>
        <span class="s2">&quot;gradient_accumulation_steps&quot;</span><span class="p">:</span> <span class="mi">5</span><span class="p">,</span>  <span class="c1"># è¡¥å¿å°æ‰¹æ¬¡</span>
        <span class="s2">&quot;effective_batch_per_gpu&quot;</span><span class="p">:</span> <span class="mi">15</span>
    <span class="p">}</span>
<span class="p">}</span>

<span class="c1"># éªŒè¯ï¼š</span>
<span class="c1"># 2 * 16 (A100) + 2 * 15 (V100) = 62 â‰ˆ 64</span>
<span class="c1"># å¯ä»¥é€šè¿‡è°ƒæ•´æœ€åä¸€ä¸ª batch æ¥ç²¾ç¡®è¾¾åˆ° 64</span>
</code></pre></div>

<p>è¿™ç§é…ç½®å¹³è¡¡äº†æ˜¾å­˜ä½¿ç”¨å’Œè®¡ç®—æ•ˆç‡ï¼Œé¿å…äº†æœ¨æ¡¶æ•ˆåº”ã€‚</p>
</details>
<h3 id="_4">æŒ‘æˆ˜é¢˜</h3>
<p><strong>ç»ƒä¹  12.4ï¼šFSDP å†…å­˜ä¼˜åŒ–</strong></p>
<p>ä½ çš„ LLaVA-34B æ¨¡å‹åœ¨ 8Ã—A100-40G ä¸Šç”¨ FSDP è®­ç»ƒæ—¶ OOMã€‚è¯·æä¾›å®Œæ•´çš„ä¼˜åŒ–æ–¹æ¡ˆï¼ŒåŒ…æ‹¬é…ç½®å’Œä»£ç ã€‚</p>
<p>ğŸ’¡ <strong>æç¤º</strong>ï¼šè€ƒè™‘ CPU offloadã€æ¿€æ´»æ£€æŸ¥ç‚¹ã€åˆ†ç‰‡ç­–ç•¥ç­‰ã€‚</p>
<details>
<summary>ğŸ“ å‚è€ƒç­”æ¡ˆ</summary>
<div class="codehilite"><pre><span></span><code><span class="kn">from</span> <span class="nn">torch.distributed.fsdp</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">FullyShardedDataParallel</span> <span class="k">as</span> <span class="n">FSDP</span><span class="p">,</span>
    <span class="n">MixedPrecision</span><span class="p">,</span>
    <span class="n">BackwardPrefetch</span><span class="p">,</span>
    <span class="n">ShardingStrategy</span><span class="p">,</span>
    <span class="n">CPUOffload</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">torch.distributed.algorithms._checkpoint.checkpoint_wrapper</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">checkpoint_wrapper</span><span class="p">,</span>
    <span class="n">CheckpointImpl</span><span class="p">,</span>
    <span class="n">apply_activation_checkpointing</span>
<span class="p">)</span>

<span class="k">def</span> <span class="nf">optimize_fsdp_for_large_vlm</span><span class="p">(</span><span class="n">model</span><span class="p">):</span>
    <span class="c1"># 1. æ¿€æ´»æ£€æŸ¥ç‚¹</span>
    <span class="n">check_fn</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">m</span><span class="p">:</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">TransformerBlock</span><span class="p">)</span>
    <span class="n">apply_activation_checkpointing</span><span class="p">(</span>
        <span class="n">model</span><span class="p">,</span>
        <span class="n">checkpoint_wrapper_fn</span><span class="o">=</span><span class="n">functools</span><span class="o">.</span><span class="n">partial</span><span class="p">(</span>
            <span class="n">checkpoint_wrapper</span><span class="p">,</span>
            <span class="n">checkpoint_impl</span><span class="o">=</span><span class="n">CheckpointImpl</span><span class="o">.</span><span class="n">NO_REENTRANT</span>
        <span class="p">),</span>
        <span class="n">check_fn</span><span class="o">=</span><span class="n">check_fn</span>
    <span class="p">)</span>

    <span class="c1"># 2. æ··åˆç²¾åº¦ - ä½¿ç”¨ BF16</span>
    <span class="n">mp_policy</span> <span class="o">=</span> <span class="n">MixedPrecision</span><span class="p">(</span>
        <span class="n">param_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">,</span>
        <span class="n">reduce_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>  <span class="c1"># æ¢¯åº¦ç”¨ FP32 æ›´ç¨³å®š</span>
        <span class="n">buffer_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span>
    <span class="p">)</span>

    <span class="c1"># 3. CPU Offload</span>
    <span class="n">cpu_offload</span> <span class="o">=</span> <span class="n">CPUOffload</span><span class="p">(</span><span class="n">offload_params</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="c1"># 4. ä¼˜åŒ–çš„åˆ†ç‰‡ç­–ç•¥</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">FSDP</span><span class="p">(</span>
        <span class="n">model</span><span class="p">,</span>
        <span class="n">auto_wrap_policy</span><span class="o">=</span><span class="n">functools</span><span class="o">.</span><span class="n">partial</span><span class="p">(</span>
            <span class="n">transformer_auto_wrap_policy</span><span class="p">,</span>
            <span class="n">transformer_layer_cls</span><span class="o">=</span><span class="p">{</span><span class="n">TransformerBlock</span><span class="p">},</span>
        <span class="p">),</span>
        <span class="n">mixed_precision</span><span class="o">=</span><span class="n">mp_policy</span><span class="p">,</span>
        <span class="n">sharding_strategy</span><span class="o">=</span><span class="n">ShardingStrategy</span><span class="o">.</span><span class="n">HYBRID_SHARD</span><span class="p">,</span>  <span class="c1"># æ··åˆåˆ†ç‰‡</span>
        <span class="n">cpu_offload</span><span class="o">=</span><span class="n">cpu_offload</span><span class="p">,</span>
        <span class="n">backward_prefetch</span><span class="o">=</span><span class="n">BackwardPrefetch</span><span class="o">.</span><span class="n">BACKWARD_PRE</span><span class="p">,</span>
        <span class="n">limit_all_gathers</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">use_orig_params</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">sync_module_states</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">forward_prefetch</span><span class="o">=</span><span class="kc">True</span>
    <span class="p">)</span>

    <span class="c1"># 5. æ¢¯åº¦ç´¯ç§¯ + å° batch</span>
    <span class="n">micro_batch_size</span> <span class="o">=</span> <span class="mi">1</span>  <span class="c1"># æå°æ‰¹æ¬¡</span>
    <span class="n">gradient_accumulation_steps</span> <span class="o">=</span> <span class="mi">32</span>

    <span class="k">return</span> <span class="n">model</span><span class="p">,</span> <span class="n">micro_batch_size</span><span class="p">,</span> <span class="n">gradient_accumulation_steps</span>
</code></pre></div>

<p>è¿™ä¸ªæ–¹æ¡ˆé€šè¿‡æ¿€æ´»æ£€æŸ¥ç‚¹å‡å°‘ 50% æ¿€æ´»å€¼å†…å­˜ï¼ŒCPU offload èŠ‚çœå‚æ•°å†…å­˜ï¼Œæ··åˆåˆ†ç‰‡ä¼˜åŒ–é€šä¿¡ï¼Œå¯ä»¥æˆåŠŸè®­ç»ƒ 34B æ¨¡å‹ã€‚</p>
</details>
<p><strong>ç»ƒä¹  12.5ï¼šåˆ†å¸ƒå¼è°ƒè¯•å·¥å…·è®¾è®¡</strong></p>
<p>è®¾è®¡ä¸€ä¸ªè°ƒè¯•å·¥å…·ï¼Œèƒ½å¤Ÿå®æ—¶ç›‘æ§å¤šæœºè®­ç»ƒçš„è¿›ç¨‹çŠ¶æ€ã€é€šä¿¡æ—¶é—´å’Œæ½œåœ¨æ­»é”ã€‚</p>
<p>ğŸ’¡ <strong>æç¤º</strong>ï¼šè€ƒè™‘å¿ƒè·³æœºåˆ¶ã€é€šä¿¡ hook å’Œå¼‚å¸¸æ£€æµ‹ã€‚</p>
<details>
<summary>ğŸ“ å‚è€ƒç­”æ¡ˆ</summary>
<div class="codehilite"><pre><span></span><code><span class="kn">import</span> <span class="nn">threading</span>
<span class="kn">import</span> <span class="nn">time</span>
<span class="kn">import</span> <span class="nn">torch.distributed</span> <span class="k">as</span> <span class="nn">dist</span>
<span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">deque</span>
<span class="kn">from</span> <span class="nn">datetime</span> <span class="kn">import</span> <span class="n">datetime</span>

<span class="k">class</span> <span class="nc">DistributedDebugger</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">rank</span><span class="p">,</span> <span class="n">world_size</span><span class="p">,</span> <span class="n">check_interval</span><span class="o">=</span><span class="mi">10</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">rank</span> <span class="o">=</span> <span class="n">rank</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">world_size</span> <span class="o">=</span> <span class="n">world_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">check_interval</span> <span class="o">=</span> <span class="n">check_interval</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">comm_times</span> <span class="o">=</span> <span class="n">deque</span><span class="p">(</span><span class="n">maxlen</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">last_heartbeat</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">deadlock_threshold</span> <span class="o">=</span> <span class="mi">300</span>  <span class="c1"># 5åˆ†é’Ÿ</span>

        <span class="c1"># æ³¨å†Œé€šä¿¡ hook</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_register_comm_hooks</span><span class="p">()</span>

        <span class="c1"># å¯åŠ¨ç›‘æ§çº¿ç¨‹</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_start_monitor</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">_register_comm_hooks</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;æ³¨å†Œé€šä¿¡é’©å­æ¥æµ‹é‡æ—¶é—´&quot;&quot;&quot;</span>
        <span class="n">original_all_reduce</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">all_reduce</span>

        <span class="k">def</span> <span class="nf">timed_all_reduce</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
            <span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
            <span class="n">result</span> <span class="o">=</span> <span class="n">original_all_reduce</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
            <span class="n">elapsed</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">start</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">comm_times</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">elapsed</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">elapsed</span> <span class="o">&gt;</span> <span class="mi">10</span><span class="p">:</span>  <span class="c1"># è¶…è¿‡10ç§’è­¦å‘Š</span>
                <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;[Rank </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">rank</span><span class="si">}</span><span class="s2">] WARNING: all_reduce took </span><span class="si">{</span><span class="n">elapsed</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">s&quot;</span><span class="p">)</span>

            <span class="k">return</span> <span class="n">result</span>

        <span class="n">dist</span><span class="o">.</span><span class="n">all_reduce</span> <span class="o">=</span> <span class="n">timed_all_reduce</span>

    <span class="k">def</span> <span class="nf">_start_monitor</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;å¯åŠ¨ç›‘æ§çº¿ç¨‹&quot;&quot;&quot;</span>
        <span class="k">def</span> <span class="nf">monitor</span><span class="p">():</span>
            <span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
                <span class="n">time</span><span class="o">.</span><span class="n">sleep</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">check_interval</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_check_health</span><span class="p">()</span>

        <span class="n">thread</span> <span class="o">=</span> <span class="n">threading</span><span class="o">.</span><span class="n">Thread</span><span class="p">(</span><span class="n">target</span><span class="o">=</span><span class="n">monitor</span><span class="p">,</span> <span class="n">daemon</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">thread</span><span class="o">.</span><span class="n">start</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">_check_health</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;å¥åº·æ£€æŸ¥&quot;&quot;&quot;</span>
        <span class="n">current_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>

        <span class="c1"># 1. æ£€æŸ¥å¿ƒè·³</span>
        <span class="k">if</span> <span class="n">current_time</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">last_heartbeat</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">deadlock_threshold</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_report_deadlock</span><span class="p">()</span>

        <span class="c1"># 2. ç»Ÿè®¡é€šä¿¡æ—¶é—´</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">comm_times</span><span class="p">:</span>
            <span class="n">avg_comm</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">comm_times</span><span class="p">)</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">comm_times</span><span class="p">)</span>
            <span class="n">max_comm</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">comm_times</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;[Rank </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">rank</span><span class="si">}</span><span class="s2">] Comm stats: avg=</span><span class="si">{</span><span class="n">avg_comm</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">s, max=</span><span class="si">{</span><span class="n">max_comm</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">s&quot;</span><span class="p">)</span>

        <span class="c1"># 3. å†…å­˜çŠ¶æ€</span>
        <span class="n">mem_alloc</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">memory_allocated</span><span class="p">()</span> <span class="o">/</span> <span class="mf">1e9</span>
        <span class="n">mem_reserved</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">memory_reserved</span><span class="p">()</span> <span class="o">/</span> <span class="mf">1e9</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;[Rank </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">rank</span><span class="si">}</span><span class="s2">] Memory: </span><span class="si">{</span><span class="n">mem_alloc</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">mem_reserved</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> GB&quot;</span><span class="p">)</span>

        <span class="c1"># 4. åŒæ­¥æ£€æŸ¥ï¼ˆå¯é€‰ï¼‰</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_sync_check</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">_sync_check</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;æ£€æŸ¥æ‰€æœ‰è¿›ç¨‹æ˜¯å¦åŒæ­¥&quot;&quot;&quot;</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">check_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">rank</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;cuda&#39;</span><span class="p">)</span>
            <span class="n">check_list</span> <span class="o">=</span> <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">check_tensor</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">world_size</span><span class="p">)]</span>
            <span class="n">dist</span><span class="o">.</span><span class="n">all_gather</span><span class="p">(</span><span class="n">check_list</span><span class="p">,</span> <span class="n">check_tensor</span><span class="p">,</span> <span class="n">timeout</span><span class="o">=</span><span class="n">datetime</span><span class="o">.</span><span class="n">timedelta</span><span class="p">(</span><span class="n">seconds</span><span class="o">=</span><span class="mi">30</span><span class="p">))</span>

            <span class="c1"># éªŒè¯æ‰€æœ‰è¿›ç¨‹éƒ½å“åº”</span>
            <span class="n">ranks</span> <span class="o">=</span> <span class="p">[</span><span class="n">t</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">check_list</span><span class="p">]</span>
            <span class="k">if</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">ranks</span><span class="p">)</span> <span class="o">!=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">world_size</span><span class="p">)):</span>
                <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;[Rank </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">rank</span><span class="si">}</span><span class="s2">] ERROR: Missing ranks in sync check: </span><span class="si">{</span><span class="n">ranks</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;[Rank </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">rank</span><span class="si">}</span><span class="s2">] Sync check failed: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_report_deadlock</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;æŠ¥å‘Šå¯èƒ½çš„æ­»é”&quot;&quot;&quot;</span>
        <span class="kn">import</span> <span class="nn">traceback</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;[Rank </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">rank</span><span class="si">}</span><span class="s2">] DEADLOCK WARNING!&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Stack trace:&quot;</span><span class="p">)</span>
        <span class="n">traceback</span><span class="o">.</span><span class="n">print_stack</span><span class="p">()</span>

        <span class="c1"># å¯é€‰ï¼šè§¦å‘ core dump</span>
        <span class="kn">import</span> <span class="nn">signal</span>
        <span class="n">os</span><span class="o">.</span><span class="n">kill</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">getpid</span><span class="p">(),</span> <span class="n">signal</span><span class="o">.</span><span class="n">SIGABRT</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">update_heartbeat</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;æ›´æ–°å¿ƒè·³æ—¶é—´ï¼ˆåœ¨è®­ç»ƒå¾ªç¯ä¸­è°ƒç”¨ï¼‰&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">last_heartbeat</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>

<span class="c1"># ä½¿ç”¨ç¤ºä¾‹</span>
<span class="n">debugger</span> <span class="o">=</span> <span class="n">DistributedDebugger</span><span class="p">(</span><span class="n">rank</span><span class="p">,</span> <span class="n">world_size</span><span class="p">)</span>

<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">dataloader</span><span class="p">:</span>
        <span class="n">debugger</span><span class="o">.</span><span class="n">update_heartbeat</span><span class="p">()</span>  <span class="c1"># æ›´æ–°å¿ƒè·³</span>
        <span class="c1"># è®­ç»ƒä»£ç ...</span>
</code></pre></div>

<p>è¿™ä¸ªè°ƒè¯•å™¨æä¾›äº†å®æ—¶ç›‘æ§ã€æ­»é”æ£€æµ‹ã€é€šä¿¡æ€§èƒ½åˆ†æç­‰åŠŸèƒ½ï¼Œèƒ½å¤Ÿå¿«é€Ÿå®šä½åˆ†å¸ƒå¼è®­ç»ƒé—®é¢˜ã€‚</p>
</details>
<p><strong>ç»ƒä¹  12.6ï¼šæ··åˆå¹¶è¡Œç­–ç•¥è®¾è®¡</strong></p>
<p>ä¸º VLM-65B æ¨¡å‹è®¾è®¡ä¸€ä¸ªç»“åˆ FSDPã€Pipeline å¹¶è¡Œå’Œ Tensor å¹¶è¡Œçš„è®­ç»ƒæ–¹æ¡ˆï¼Œç¡¬ä»¶æ˜¯ 16Ã—A100-80Gï¼ˆ2 èŠ‚ç‚¹ï¼‰ã€‚</p>
<p>ğŸ’¡ <strong>æç¤º</strong>ï¼šè€ƒè™‘ä¸åŒå¹¶è¡Œç­–ç•¥çš„é€šä¿¡æ¨¡å¼å’Œå†…å­˜å ç”¨ã€‚</p>
<details>
<summary>ğŸ“ å‚è€ƒç­”æ¡ˆ</summary>
<div class="codehilite"><pre><span></span><code><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">æ··åˆå¹¶è¡Œç­–ç•¥è®¾è®¡ï¼š</span>

<span class="sd">- Tensor Parallel (TP): 4-way (èŠ‚ç‚¹å†…)</span>
<span class="sd">- Pipeline Parallel (PP): 2-way (è·¨èŠ‚ç‚¹)</span>
<span class="sd">- Data Parallel with FSDP: 2-way</span>

<span class="sd">æ€»å¹¶è¡Œåº¦: 4 Ã— 2 Ã— 2 = 16</span>
<span class="sd">&quot;&quot;&quot;</span>

<span class="k">class</span> <span class="nc">HybridParallelVLM</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model_config</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">world_size</span> <span class="o">=</span> <span class="mi">16</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tp_size</span> <span class="o">=</span> <span class="mi">4</span>  <span class="c1"># èŠ‚ç‚¹å†… tensor parallel</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pp_size</span> <span class="o">=</span> <span class="mi">2</span>  <span class="c1"># pipeline stages</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dp_size</span> <span class="o">=</span> <span class="mi">2</span>  <span class="c1"># data parallel groups</span>

        <span class="c1"># åˆå§‹åŒ–è¿›ç¨‹ç»„</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_init_process_groups</span><span class="p">()</span>

        <span class="c1"># æ„å»ºæ¨¡å‹</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_build_model</span><span class="p">(</span><span class="n">model_config</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_init_process_groups</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;åˆ›å»ºä¸åŒçš„è¿›ç¨‹ç»„&quot;&quot;&quot;</span>
        <span class="n">rank</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">get_rank</span><span class="p">()</span>

        <span class="c1"># Tensor Parallel ç»„ (åŒèŠ‚ç‚¹å†…)</span>
        <span class="n">tp_ranks</span> <span class="o">=</span> <span class="p">[</span><span class="n">rank</span> <span class="o">//</span> <span class="mi">4</span> <span class="o">*</span> <span class="mi">4</span> <span class="o">+</span> <span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">4</span><span class="p">)]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tp_group</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">new_group</span><span class="p">(</span><span class="n">tp_ranks</span><span class="p">)</span>

        <span class="c1"># Pipeline Parallel ç»„ (è·¨èŠ‚ç‚¹)</span>
        <span class="n">pp_ranks</span> <span class="o">=</span> <span class="p">[</span><span class="n">rank</span> <span class="o">%</span> <span class="mi">4</span> <span class="o">+</span> <span class="n">i</span> <span class="o">*</span> <span class="mi">8</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">)]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pp_group</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">new_group</span><span class="p">(</span><span class="n">pp_ranks</span><span class="p">)</span>

        <span class="c1"># Data Parallel ç»„</span>
        <span class="n">dp_ranks</span> <span class="o">=</span> <span class="p">[</span><span class="n">rank</span> <span class="o">//</span> <span class="mi">8</span> <span class="o">*</span> <span class="mi">8</span> <span class="o">+</span> <span class="n">rank</span> <span class="o">%</span> <span class="mi">4</span> <span class="o">+</span> <span class="n">i</span> <span class="o">*</span> <span class="mi">4</span> 
                   <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">)]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dp_group</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">new_group</span><span class="p">(</span><span class="n">dp_ranks</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_build_model</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;æ„å»ºæ··åˆå¹¶è¡Œæ¨¡å‹&quot;&quot;&quot;</span>
        <span class="n">rank</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">get_rank</span><span class="p">()</span>

        <span class="c1"># 1. æ¨¡å‹åˆ†å±‚ (Pipeline)</span>
        <span class="k">if</span> <span class="n">rank</span> <span class="o">&lt;</span> <span class="mi">8</span><span class="p">:</span>  <span class="c1"># ç¬¬ä¸€ä¸ª pipeline stage</span>
            <span class="n">layers</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_first_stage_layers</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>  <span class="c1"># ç¬¬äºŒä¸ª pipeline stage</span>
            <span class="n">layers</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_second_stage_layers</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>

        <span class="c1"># 2. Tensor Parallel</span>
        <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="n">layers</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">layer</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">):</span>
                <span class="n">layer</span> <span class="o">=</span> <span class="n">ColumnParallelLinear</span><span class="p">(</span><span class="n">layer</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">tp_group</span><span class="p">)</span>
            <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">layer</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">):</span>
                <span class="n">layer</span> <span class="o">=</span> <span class="n">ParallelEmbedding</span><span class="p">(</span><span class="n">layer</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">tp_group</span><span class="p">)</span>

        <span class="c1"># 3. FSDP åŒ…è£…</span>
        <span class="n">model</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="o">*</span><span class="n">layers</span><span class="p">)</span>
        <span class="n">model</span> <span class="o">=</span> <span class="n">FSDP</span><span class="p">(</span>
            <span class="n">model</span><span class="p">,</span>
            <span class="n">process_group</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dp_group</span><span class="p">,</span>
            <span class="n">sharding_strategy</span><span class="o">=</span><span class="n">ShardingStrategy</span><span class="o">.</span><span class="n">FULL_SHARD</span><span class="p">,</span>
            <span class="n">mixed_precision</span><span class="o">=</span><span class="n">MixedPrecision</span><span class="p">(</span>
                <span class="n">param_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">,</span>
                <span class="n">reduce_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span>
            <span class="p">),</span>
            <span class="n">limit_all_gathers</span><span class="o">=</span><span class="kc">True</span>
        <span class="p">)</span>

        <span class="k">return</span> <span class="n">model</span>

    <span class="k">def</span> <span class="nf">train_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;æ··åˆå¹¶è¡Œè®­ç»ƒæ­¥éª¤&quot;&quot;&quot;</span>
        <span class="c1"># Pipeline parallel çš„ micro-batching</span>
        <span class="n">micro_batches</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_split_batch</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">pp_size</span><span class="p">)</span>

        <span class="n">losses</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">micro_batch</span> <span class="ow">in</span> <span class="n">micro_batches</span><span class="p">:</span>
            <span class="c1"># Forward (with pipeline)</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_first_stage</span><span class="p">():</span>
                <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">(</span><span class="n">micro_batch</span><span class="p">)</span>
                <span class="c1"># å‘é€åˆ°ä¸‹ä¸€ä¸ª stage</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_send_activations</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target_stage</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="c1"># æ¥æ”¶å‰ä¸€ä¸ª stage çš„æ¿€æ´»</span>
                <span class="n">activations</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_recv_activations</span><span class="p">(</span><span class="n">source_stage</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
                <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">(</span><span class="n">activations</span><span class="p">)</span>
                <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">compute_loss</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">micro_batch</span><span class="p">[</span><span class="s1">&#39;labels&#39;</span><span class="p">])</span>
                <span class="n">losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>

            <span class="c1"># Backward (reverse pipeline)</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_last_stage</span><span class="p">():</span>
                <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
                <span class="c1"># å‘é€æ¢¯åº¦åˆ°å‰ä¸€ä¸ª stage</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_send_gradients</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="c1"># æ¥æ”¶æ¢¯åº¦</span>
                <span class="n">gradients</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_recv_gradients</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
                <span class="n">output</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">gradients</span><span class="p">)</span>

        <span class="c1"># FSDP ä¼šè‡ªåŠ¨å¤„ç†æ¢¯åº¦åŒæ­¥</span>
        <span class="k">return</span> <span class="nb">sum</span><span class="p">(</span><span class="n">losses</span><span class="p">)</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">losses</span><span class="p">)</span>

<span class="c1"># é…ç½®ç¤ºä¾‹</span>
<span class="n">config</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;model&quot;</span><span class="p">:</span> <span class="p">{</span>
        <span class="s2">&quot;hidden_size&quot;</span><span class="p">:</span> <span class="mi">8192</span><span class="p">,</span>
        <span class="s2">&quot;num_layers&quot;</span><span class="p">:</span> <span class="mi">80</span><span class="p">,</span>
        <span class="s2">&quot;num_heads&quot;</span><span class="p">:</span> <span class="mi">64</span><span class="p">,</span>
        <span class="s2">&quot;vocab_size&quot;</span><span class="p">:</span> <span class="mi">32000</span>
    <span class="p">},</span>
    <span class="s2">&quot;training&quot;</span><span class="p">:</span> <span class="p">{</span>
        <span class="s2">&quot;micro_batch_size&quot;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
        <span class="s2">&quot;gradient_accumulation&quot;</span><span class="p">:</span> <span class="mi">8</span><span class="p">,</span>
        <span class="s2">&quot;optimizer&quot;</span><span class="p">:</span> <span class="s2">&quot;AdamW&quot;</span><span class="p">,</span>
        <span class="s2">&quot;lr&quot;</span><span class="p">:</span> <span class="mf">1e-4</span>
    <span class="p">}</span>
<span class="p">}</span>

<span class="c1"># å†…å­˜ä¼°ç®—</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">æ¯ä¸ª GPU çš„æ¨¡å‹å‚æ•°ï¼š65B / 16 = 4B å‚æ•°</span>
<span class="sd">FP16 å­˜å‚¨ï¼š4B * 2 bytes = 8GB</span>
<span class="sd">ä¼˜åŒ–å™¨çŠ¶æ€ (AdamW)ï¼š8GB * 2 = 16GB</span>
<span class="sd">æ¿€æ´»å€¼ (with checkpointing)ï¼š~20GB</span>
<span class="sd">æ€»è®¡ï¼š~44GB &lt; 80GB (å®‰å…¨)</span>
<span class="sd">&quot;&quot;&quot;</span>
</code></pre></div>

<p>è¿™ä¸ªæ–¹æ¡ˆé€šè¿‡ä¸‰ç§å¹¶è¡Œç­–ç•¥çš„ç»„åˆï¼Œå®ç°äº† 65B æ¨¡å‹åœ¨ 16 å¡ä¸Šçš„é«˜æ•ˆè®­ç»ƒï¼Œæ¯ç§å¹¶è¡Œç­–ç•¥éƒ½é’ˆå¯¹å…¶æœ€é€‚åˆçš„ç»´åº¦è¿›è¡Œä¼˜åŒ–ã€‚</p>
</details>
<p><strong>ç»ƒä¹  12.7ï¼šé€šä¿¡ç“¶é¢ˆåˆ†æ</strong></p>
<p>ä½ çš„è®­ç»ƒåœ¨ scaling åˆ° 32 å¡åï¼Œæ•ˆç‡ä» 8 å¡çš„ 90% ä¸‹é™åˆ° 60%ã€‚è¯·åˆ†æå¯èƒ½çš„åŸå› å¹¶æä¾›ä¼˜åŒ–æ–¹æ¡ˆã€‚</p>
<p>ğŸ’¡ <strong>æç¤º</strong>ï¼šè€ƒè™‘é€šä¿¡æ‹“æ‰‘ã€æ¢¯åº¦åŒæ­¥ç­–ç•¥å’Œæ•°æ®åŠ è½½ã€‚</p>
<details>
<summary>ğŸ“ å‚è€ƒç­”æ¡ˆ</summary>
<p>å¯èƒ½åŸå› åˆ†æï¼š</p>
<ol>
<li>
<p><strong>é€šä¿¡ç“¶é¢ˆå¢åŠ </strong>
   - All-Reduce æ—¶é—´ âˆ log(N) Ã— æ•°æ®é‡
   - 32å¡çš„é€šä¿¡è½®æ•°æ¯”8å¡å¤š</p>
</li>
<li>
<p><strong>ç½‘ç»œæ‹“æ‰‘ä¸ä¼˜åŒ–</strong>
   - è·¨èŠ‚ç‚¹é€šä¿¡å¸¦å®½å—é™
   - PCIe/NVLink æ‹“æ‰‘ä¸å‡è¡¡</p>
</li>
<li>
<p><strong>åŒæ­¥å¼€é”€</strong>
   - Barrier ç­‰å¾…æ—¶é—´å¢åŠ 
   - æ•°æ®åŠ è½½ä¸å‡è¡¡åŠ å‰§</p>
</li>
</ol>
<p>ä¼˜åŒ–æ–¹æ¡ˆï¼š</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># 1. æ¢¯åº¦å‹ç¼©</span>
<span class="kn">from</span> <span class="nn">torch.distributed.algorithms.ddp_comm_hooks</span> <span class="kn">import</span> <span class="n">default_hooks</span>

<span class="n">model</span><span class="o">.</span><span class="n">register_comm_hook</span><span class="p">(</span>
    <span class="n">state</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">hook</span><span class="o">=</span><span class="n">default_hooks</span><span class="o">.</span><span class="n">fp16_compress_hook</span>
<span class="p">)</span>

<span class="c1"># 2. åˆ†å±‚ All-Reduce</span>
<span class="k">def</span> <span class="nf">hierarchical_allreduce</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">groups</span><span class="p">):</span>
    <span class="c1"># èŠ‚ç‚¹å†…å…ˆåŒæ­¥</span>
    <span class="n">dist</span><span class="o">.</span><span class="n">all_reduce</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="n">groups</span><span class="p">[</span><span class="s1">&#39;intra_node&#39;</span><span class="p">])</span>

    <span class="c1"># èŠ‚ç‚¹é—´åŒæ­¥ï¼ˆä»… masterï¼‰</span>
    <span class="k">if</span> <span class="n">is_node_master</span><span class="p">():</span>
        <span class="n">dist</span><span class="o">.</span><span class="n">all_reduce</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="n">groups</span><span class="p">[</span><span class="s1">&#39;inter_node&#39;</span><span class="p">])</span>

    <span class="c1"># èŠ‚ç‚¹å†…å¹¿æ’­</span>
    <span class="n">dist</span><span class="o">.</span><span class="n">broadcast</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">src</span><span class="o">=</span><span class="n">node_master_rank</span><span class="p">,</span> 
                  <span class="n">group</span><span class="o">=</span><span class="n">groups</span><span class="p">[</span><span class="s1">&#39;intra_node&#39;</span><span class="p">])</span>

<span class="c1"># 3. æ¢¯åº¦ç´¯ç§¯å¢åŠ </span>
<span class="n">gradient_accumulation_steps</span> <span class="o">=</span> <span class="mi">4</span>  <span class="c1"># 8å¡</span>
<span class="n">gradient_accumulation_steps</span> <span class="o">=</span> <span class="mi">8</span>  <span class="c1"># 32å¡ï¼Œå‡å°‘åŒæ­¥é¢‘ç‡</span>

<span class="c1"># 4. å¼‚æ­¥æ•°æ®é¢„å–</span>
<span class="k">class</span> <span class="nc">AsyncDataLoader</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dataset</span><span class="p">,</span> <span class="n">num_workers</span><span class="o">=</span><span class="mi">8</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">queue</span> <span class="o">=</span> <span class="n">Queue</span><span class="p">(</span><span class="n">maxsize</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">workers</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_workers</span><span class="p">):</span>
            <span class="n">w</span> <span class="o">=</span> <span class="n">Process</span><span class="p">(</span><span class="n">target</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_worker</span><span class="p">,</span> <span class="n">args</span><span class="o">=</span><span class="p">(</span><span class="n">dataset</span><span class="p">,))</span>
            <span class="n">w</span><span class="o">.</span><span class="n">start</span><span class="p">()</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">workers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_worker</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dataset</span><span class="p">):</span>
        <span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
            <span class="n">batch</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">get_next_batch</span><span class="p">()</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">queue</span><span class="o">.</span><span class="n">put</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__iter__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
            <span class="k">yield</span> <span class="bp">self</span><span class="o">.</span><span class="n">queue</span><span class="o">.</span><span class="n">get</span><span class="p">()</span>

<span class="c1"># 5. é€šä¿¡ä¸è®¡ç®—é‡å </span>
<span class="k">class</span> <span class="nc">OverlappedOptimizer</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">step</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># å¯åŠ¨å¼‚æ­¥ all-reduce</span>
        <span class="n">handles</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span>
            <span class="n">handle</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">all_reduce</span><span class="p">(</span><span class="n">param</span><span class="o">.</span><span class="n">grad</span><span class="p">,</span> <span class="n">async_op</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="n">handles</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">handle</span><span class="p">)</span>

        <span class="c1"># åŒæ—¶è¿›è¡Œå…¶ä»–è®¡ç®—</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">update_metrics</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">log_progress</span><span class="p">()</span>

        <span class="c1"># ç­‰å¾…é€šä¿¡å®Œæˆ</span>
        <span class="k">for</span> <span class="n">handle</span> <span class="ow">in</span> <span class="n">handles</span><span class="p">:</span>
            <span class="n">handle</span><span class="o">.</span><span class="n">wait</span><span class="p">()</span>

        <span class="c1"># åº”ç”¨æ¢¯åº¦</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

<span class="c1"># 6. NCCL ä¼˜åŒ–</span>
<span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s1">&#39;NCCL_TREE_THRESHOLD&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;0&#39;</span>
<span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s1">&#39;NCCL_ALGO&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;Ring,Tree&#39;</span>
<span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s1">&#39;NCCL_CROSS_NIC&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;1&#39;</span>
<span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s1">&#39;NCCL_NET_GDR_LEVEL&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;5&#39;</span>
</code></pre></div>

<p>é¢„æœŸæ•ˆæœï¼šä¼˜åŒ–å 32 å¡æ•ˆç‡å¯æå‡åˆ° 75-80%ã€‚</p>
</details>
<p><strong>ç»ƒä¹  12.8ï¼šç”Ÿäº§ç¯å¢ƒæ•…éšœæ¢å¤</strong></p>
<p>è®¾è®¡ä¸€ä¸ªå®Œæ•´çš„æ•…éšœæ¢å¤ç³»ç»Ÿï¼Œèƒ½å¤Ÿå¤„ç†èŠ‚ç‚¹æ•…éšœã€ç½‘ç»œä¸­æ–­å’Œ GPU é”™è¯¯ï¼Œç¡®ä¿è®­ç»ƒèƒ½å¤Ÿè‡ªåŠ¨æ¢å¤ã€‚</p>
<p>ğŸ’¡ <strong>æç¤º</strong>ï¼šè€ƒè™‘æ£€æŸ¥ç‚¹ã€å¥åº·æ£€æŸ¥ã€è‡ªåŠ¨é‡å¯å’Œå¼¹æ€§è®­ç»ƒã€‚</p>
<details>
<summary>ğŸ“ å‚è€ƒç­”æ¡ˆ</summary>
<div class="codehilite"><pre><span></span><code><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">time</span>
<span class="kn">import</span> <span class="nn">signal</span>
<span class="kn">import</span> <span class="nn">subprocess</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Optional</span><span class="p">,</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">Any</span>
<span class="kn">from</span> <span class="nn">dataclasses</span> <span class="kn">import</span> <span class="n">dataclass</span>
<span class="kn">from</span> <span class="nn">datetime</span> <span class="kn">import</span> <span class="n">datetime</span><span class="p">,</span> <span class="n">timedelta</span>

<span class="nd">@dataclass</span>
<span class="k">class</span> <span class="nc">TrainingState</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;è®­ç»ƒçŠ¶æ€&quot;&quot;&quot;</span>
    <span class="n">epoch</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">step</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">best_loss</span><span class="p">:</span> <span class="nb">float</span>
    <span class="n">checkpoint_path</span><span class="p">:</span> <span class="nb">str</span>
    <span class="n">failure_count</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">last_failure_time</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">datetime</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>

<span class="k">class</span> <span class="nc">ResilientTrainer</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;å¼¹æ€§è®­ç»ƒå™¨ - è‡ªåŠ¨æ•…éšœæ¢å¤&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">config</span> <span class="o">=</span> <span class="n">config</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_load_state</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">health_checker</span> <span class="o">=</span> <span class="n">HealthChecker</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">checkpoint_manager</span> <span class="o">=</span> <span class="n">CheckpointManager</span><span class="p">(</span><span class="n">config</span><span class="p">[</span><span class="s1">&#39;checkpoint_dir&#39;</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_failures</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;max_failures&#39;</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">failure_window</span> <span class="o">=</span> <span class="n">timedelta</span><span class="p">(</span><span class="n">hours</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

        <span class="c1"># æ³¨å†Œä¿¡å·å¤„ç†</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_register_signal_handlers</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">_register_signal_handlers</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;æ³¨å†Œä¼˜é›…é€€å‡ºçš„ä¿¡å·å¤„ç†&quot;&quot;&quot;</span>
        <span class="k">def</span> <span class="nf">graceful_exit</span><span class="p">(</span><span class="n">signum</span><span class="p">,</span> <span class="n">frame</span><span class="p">):</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Received signal </span><span class="si">{</span><span class="n">signum</span><span class="si">}</span><span class="s2">, saving checkpoint...&quot;</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">save_checkpoint</span><span class="p">(</span><span class="n">emergency</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="n">sys</span><span class="o">.</span><span class="n">exit</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

        <span class="n">signal</span><span class="o">.</span><span class="n">signal</span><span class="p">(</span><span class="n">signal</span><span class="o">.</span><span class="n">SIGTERM</span><span class="p">,</span> <span class="n">graceful_exit</span><span class="p">)</span>
        <span class="n">signal</span><span class="o">.</span><span class="n">signal</span><span class="p">(</span><span class="n">signal</span><span class="o">.</span><span class="n">SIGINT</span><span class="p">,</span> <span class="n">graceful_exit</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;ä¸»è®­ç»ƒå¾ªç¯ - å¸¦æ•…éšœæ¢å¤&quot;&quot;&quot;</span>
        <span class="k">while</span> <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">epoch</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s1">&#39;max_epochs&#39;</span><span class="p">]:</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="c1"># å¥åº·æ£€æŸ¥</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">health_checker</span><span class="o">.</span><span class="n">check_all</span><span class="p">():</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_handle_unhealthy_state</span><span class="p">()</span>
                    <span class="k">continue</span>

                <span class="c1"># è®­ç»ƒä¸€ä¸ª epoch</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_train_epoch</span><span class="p">()</span>

                <span class="c1"># å®šæœŸä¿å­˜æ£€æŸ¥ç‚¹</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">step</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s1">&#39;checkpoint_interval&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">save_checkpoint</span><span class="p">()</span>

                <span class="c1"># é‡ç½®æ•…éšœè®¡æ•°ï¼ˆæˆåŠŸå®Œæˆ epochï¼‰</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">failure_count</span> <span class="o">=</span> <span class="mi">0</span>

            <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_handle_training_failure</span><span class="p">(</span><span class="n">e</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_train_epoch</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;è®­ç»ƒä¸€ä¸ª epoch&quot;&quot;&quot;</span>
        <span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">dataloader</span><span class="p">:</span>
            <span class="c1"># æ•…éšœæ³¨å…¥æµ‹è¯•ï¼ˆå¯é€‰ï¼‰</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;fault_injection&#39;</span><span class="p">):</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_inject_random_fault</span><span class="p">()</span>

            <span class="c1"># è®­ç»ƒæ­¥éª¤</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">train_step</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>

            <span class="c1"># å¼‚å¸¸æ£€æµ‹</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_detect_anomaly</span><span class="p">(</span><span class="n">loss</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Anomaly detected: loss=</span><span class="si">{</span><span class="n">loss</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">step</span> <span class="o">+=</span> <span class="mi">1</span>

    <span class="k">def</span> <span class="nf">_handle_training_failure</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">error</span><span class="p">:</span> <span class="ne">Exception</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;å¤„ç†è®­ç»ƒæ•…éšœ&quot;&quot;&quot;</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Training failed: </span><span class="si">{</span><span class="n">error</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

        <span class="c1"># æ›´æ–°æ•…éšœç»Ÿè®¡</span>
        <span class="n">current_time</span> <span class="o">=</span> <span class="n">datetime</span><span class="o">.</span><span class="n">now</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">failure_count</span> <span class="o">+=</span> <span class="mi">1</span>

        <span class="c1"># æ£€æŸ¥æ˜¯å¦è¶…è¿‡æœ€å¤§æ•…éšœæ¬¡æ•°</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">last_failure_time</span><span class="p">:</span>
            <span class="n">time_since_last</span> <span class="o">=</span> <span class="n">current_time</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">last_failure_time</span>
            <span class="k">if</span> <span class="n">time_since_last</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">failure_window</span><span class="p">:</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">failure_count</span> <span class="o">&gt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_failures</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_escalate_failure</span><span class="p">(</span><span class="s2">&quot;Too many failures in short time&quot;</span><span class="p">)</span>
                    <span class="k">return</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">last_failure_time</span> <span class="o">=</span> <span class="n">current_time</span>

        <span class="c1"># å°è¯•æ¢å¤</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_attempt_recovery</span><span class="p">(</span><span class="n">error</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_attempt_recovery</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">error</span><span class="p">:</span> <span class="ne">Exception</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;å°è¯•ä»æ•…éšœä¸­æ¢å¤&quot;&quot;&quot;</span>
        <span class="n">recovery_strategies</span> <span class="o">=</span> <span class="p">[</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_recover_from_oom</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_recover_from_nccl_error</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_recover_from_checkpoint</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_restart_workers</span>
        <span class="p">]</span>

        <span class="k">for</span> <span class="n">strategy</span> <span class="ow">in</span> <span class="n">recovery_strategies</span><span class="p">:</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">strategy</span><span class="p">(</span><span class="n">error</span><span class="p">):</span>
                    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Recovery successful using </span><span class="si">{</span><span class="n">strategy</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
                    <span class="k">return</span>
            <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Recovery strategy </span><span class="si">{</span><span class="n">strategy</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2"> failed: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

        <span class="c1"># æ‰€æœ‰ç­–ç•¥éƒ½å¤±è´¥</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_escalate_failure</span><span class="p">(</span><span class="s2">&quot;All recovery strategies failed&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_recover_from_oom</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">error</span><span class="p">:</span> <span class="ne">Exception</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;ä» OOM é”™è¯¯æ¢å¤&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="s2">&quot;out of memory&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="nb">str</span><span class="p">(</span><span class="n">error</span><span class="p">)</span><span class="o">.</span><span class="n">lower</span><span class="p">():</span>
            <span class="k">return</span> <span class="kc">False</span>

        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Attempting OOM recovery...&quot;</span><span class="p">)</span>

        <span class="c1"># 1. æ¸…ç†ç¼“å­˜</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">empty_cache</span><span class="p">()</span>

        <span class="c1"># 2. å‡å°æ‰¹å¤§å°</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s1">&#39;batch_size&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s1">&#39;batch_size&#39;</span><span class="p">]</span> <span class="o">//</span> <span class="mi">2</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Reduced batch size to </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s1">&#39;batch_size&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

        <span class="c1"># 3. å¯ç”¨æ›´æ¿€è¿›çš„å†…å­˜ä¼˜åŒ–</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s1">&#39;gradient_checkpointing&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s1">&#39;cpu_offload&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>

        <span class="c1"># 4. é‡æ–°åˆå§‹åŒ–æ¨¡å‹</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_reinitialize_model</span><span class="p">()</span>

        <span class="k">return</span> <span class="kc">True</span>

    <span class="k">def</span> <span class="nf">_recover_from_nccl_error</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">error</span><span class="p">:</span> <span class="ne">Exception</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;ä» NCCL é”™è¯¯æ¢å¤&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="s2">&quot;nccl&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="nb">str</span><span class="p">(</span><span class="n">error</span><span class="p">)</span><span class="o">.</span><span class="n">lower</span><span class="p">():</span>
            <span class="k">return</span> <span class="kc">False</span>

        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Attempting NCCL recovery...&quot;</span><span class="p">)</span>

        <span class="c1"># 1. é”€æ¯è¿›ç¨‹ç»„</span>
        <span class="k">if</span> <span class="n">dist</span><span class="o">.</span><span class="n">is_initialized</span><span class="p">():</span>
            <span class="n">dist</span><span class="o">.</span><span class="n">destroy_process_group</span><span class="p">()</span>

        <span class="c1"># 2. ç­‰å¾…æ‰€æœ‰è¿›ç¨‹</span>
        <span class="n">time</span><span class="o">.</span><span class="n">sleep</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>

        <span class="c1"># 3. é‡æ–°åˆå§‹åŒ–</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_init_distributed</span><span class="p">()</span>

        <span class="c1"># 4. ä»æ£€æŸ¥ç‚¹æ¢å¤</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">load_checkpoint</span><span class="p">()</span>

        <span class="k">return</span> <span class="kc">True</span>

    <span class="k">def</span> <span class="nf">_restart_workers</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">error</span><span class="p">:</span> <span class="ne">Exception</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;é‡å¯å·¥ä½œè¿›ç¨‹&quot;&quot;&quot;</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Restarting all workers...&quot;</span><span class="p">)</span>

        <span class="c1"># ä¿å­˜å½“å‰çŠ¶æ€</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">save_checkpoint</span><span class="p">(</span><span class="n">emergency</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="c1"># æ„å»ºé‡å¯å‘½ä»¤</span>
        <span class="n">restart_cmd</span> <span class="o">=</span> <span class="p">[</span>
            <span class="s2">&quot;python&quot;</span><span class="p">,</span> <span class="s2">&quot;-m&quot;</span><span class="p">,</span> <span class="s2">&quot;torch.distributed.launch&quot;</span><span class="p">,</span>
            <span class="s2">&quot;--nproc_per_node&quot;</span><span class="p">,</span> <span class="nb">str</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s1">&#39;gpus_per_node&#39;</span><span class="p">]),</span>
            <span class="s2">&quot;--nnodes&quot;</span><span class="p">,</span> <span class="nb">str</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s1">&#39;num_nodes&#39;</span><span class="p">]),</span>
            <span class="s2">&quot;--node_rank&quot;</span><span class="p">,</span> <span class="nb">str</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s1">&#39;node_rank&#39;</span><span class="p">]),</span>
            <span class="s2">&quot;--master_addr&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s1">&#39;master_addr&#39;</span><span class="p">],</span>
            <span class="s2">&quot;--master_port&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s1">&#39;master_port&#39;</span><span class="p">],</span>
            <span class="s2">&quot;train.py&quot;</span><span class="p">,</span>
            <span class="s2">&quot;--resume&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">checkpoint_path</span>
        <span class="p">]</span>

        <span class="c1"># æ‰§è¡Œé‡å¯</span>
        <span class="n">subprocess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">restart_cmd</span><span class="p">)</span>
        <span class="k">return</span> <span class="kc">True</span>

    <span class="k">def</span> <span class="nf">save_checkpoint</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">emergency</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;ä¿å­˜æ£€æŸ¥ç‚¹&quot;&quot;&quot;</span>
        <span class="n">checkpoint</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s1">&#39;model_state&#39;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span>
            <span class="s1">&#39;optimizer_state&#39;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span>
            <span class="s1">&#39;training_state&#39;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="p">,</span>
            <span class="s1">&#39;config&#39;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">,</span>
            <span class="s1">&#39;timestamp&#39;</span><span class="p">:</span> <span class="n">datetime</span><span class="o">.</span><span class="n">now</span><span class="p">()</span><span class="o">.</span><span class="n">isoformat</span><span class="p">(),</span>
            <span class="s1">&#39;emergency&#39;</span><span class="p">:</span> <span class="n">emergency</span>
        <span class="p">}</span>

        <span class="c1"># ä¿å­˜å¤šä¸ªå‰¯æœ¬é˜²æ­¢æŸå</span>
        <span class="n">paths</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">checkpoint_manager</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">,</span> <span class="n">emergency</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">checkpoint_path</span> <span class="o">=</span> <span class="n">paths</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

        <span class="c1"># å¼‚æ­¥ä¸Šä¼ åˆ°äº‘å­˜å‚¨</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;cloud_backup&#39;</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_async_cloud_backup</span><span class="p">(</span><span class="n">paths</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">_escalate_failure</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">reason</span><span class="p">:</span> <span class="nb">str</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;å‡çº§æ•…éšœå¤„ç†&quot;&quot;&quot;</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;CRITICAL: </span><span class="si">{</span><span class="n">reason</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

        <span class="c1"># 1. å‘é€å‘Šè­¦</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_send_alert</span><span class="p">(</span><span class="n">reason</span><span class="p">)</span>

        <span class="c1"># 2. ä¿å­˜è°ƒè¯•ä¿¡æ¯</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_save_debug_info</span><span class="p">()</span>

        <span class="c1"># 3. ä¼˜é›…é€€å‡º</span>
        <span class="n">sys</span><span class="o">.</span><span class="n">exit</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

<span class="k">class</span> <span class="nc">HealthChecker</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;å¥åº·æ£€æŸ¥å™¨&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">check_all</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;æ‰§è¡Œæ‰€æœ‰å¥åº·æ£€æŸ¥&quot;&quot;&quot;</span>
        <span class="n">checks</span> <span class="o">=</span> <span class="p">[</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">check_gpu_health</span><span class="p">(),</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">check_network_health</span><span class="p">(),</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">check_memory_health</span><span class="p">(),</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">check_disk_space</span><span class="p">()</span>
        <span class="p">]</span>
        <span class="k">return</span> <span class="nb">all</span><span class="p">(</span><span class="n">checks</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">check_gpu_health</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;æ£€æŸ¥ GPU å¥åº·çŠ¶æ€&quot;&quot;&quot;</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="c1"># æ£€æŸ¥ GPU æ˜¯å¦å¯ç”¨</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">device_count</span><span class="p">()):</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">set_device</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">synchronize</span><span class="p">()</span>

                <span class="c1"># æ£€æŸ¥æ¸©åº¦</span>
                <span class="n">temp</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_gpu_temperature</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">temp</span> <span class="o">&gt;</span> <span class="mi">85</span><span class="p">:</span>
                    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;WARNING: GPU </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2"> temperature </span><span class="si">{</span><span class="n">temp</span><span class="si">}</span><span class="s2">Â°C&quot;</span><span class="p">)</span>
                    <span class="k">return</span> <span class="kc">False</span>

                <span class="c1"># æ£€æŸ¥ ECC é”™è¯¯</span>
                <span class="n">ecc_errors</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_ecc_errors</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">ecc_errors</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;WARNING: GPU </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2"> has </span><span class="si">{</span><span class="n">ecc_errors</span><span class="si">}</span><span class="s2"> ECC errors&quot;</span><span class="p">)</span>
                    <span class="k">return</span> <span class="kc">False</span>

            <span class="k">return</span> <span class="kc">True</span>
        <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;GPU health check failed: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="k">return</span> <span class="kc">False</span>

    <span class="k">def</span> <span class="nf">check_network_health</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;æ£€æŸ¥ç½‘ç»œå¥åº·çŠ¶æ€&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">dist</span><span class="o">.</span><span class="n">is_initialized</span><span class="p">():</span>
            <span class="k">return</span> <span class="kc">True</span>

        <span class="k">try</span><span class="p">:</span>
            <span class="c1"># ç®€å•çš„ all-reduce æµ‹è¯•</span>
            <span class="n">test_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>
            <span class="n">dist</span><span class="o">.</span><span class="n">all_reduce</span><span class="p">(</span><span class="n">test_tensor</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">test_tensor</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">==</span> <span class="n">dist</span><span class="o">.</span><span class="n">get_world_size</span><span class="p">()</span>
        <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Network health check failed: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="k">return</span> <span class="kc">False</span>

<span class="c1"># ä½¿ç”¨ç¤ºä¾‹</span>
<span class="n">config</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;max_epochs&#39;</span><span class="p">:</span> <span class="mi">100</span><span class="p">,</span>
    <span class="s1">&#39;checkpoint_dir&#39;</span><span class="p">:</span> <span class="s1">&#39;./checkpoints&#39;</span><span class="p">,</span>
    <span class="s1">&#39;checkpoint_interval&#39;</span><span class="p">:</span> <span class="mi">1000</span><span class="p">,</span>
    <span class="s1">&#39;max_failures&#39;</span><span class="p">:</span> <span class="mi">3</span><span class="p">,</span>
    <span class="s1">&#39;batch_size&#39;</span><span class="p">:</span> <span class="mi">32</span><span class="p">,</span>
    <span class="s1">&#39;gpus_per_node&#39;</span><span class="p">:</span> <span class="mi">8</span><span class="p">,</span>
    <span class="s1">&#39;num_nodes&#39;</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span>
    <span class="s1">&#39;node_rank&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
    <span class="s1">&#39;master_addr&#39;</span><span class="p">:</span> <span class="s1">&#39;192.168.1.1&#39;</span><span class="p">,</span>
    <span class="s1">&#39;master_port&#39;</span><span class="p">:</span> <span class="s1">&#39;29500&#39;</span><span class="p">,</span>
    <span class="s1">&#39;cloud_backup&#39;</span><span class="p">:</span> <span class="kc">True</span>
<span class="p">}</span>

<span class="n">trainer</span> <span class="o">=</span> <span class="n">ResilientTrainer</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
<span class="n">trainer</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
</code></pre></div>

<p>è¿™ä¸ªç³»ç»Ÿæä¾›äº†å®Œæ•´çš„æ•…éšœæ¢å¤èƒ½åŠ›ï¼ŒåŒ…æ‹¬è‡ªåŠ¨é‡è¯•ã€é™çº§ç­–ç•¥ã€å¥åº·æ£€æŸ¥å’Œäº‘å¤‡ä»½ï¼Œèƒ½å¤Ÿå¤„ç†ç”Ÿäº§ç¯å¢ƒä¸­çš„å„ç§æ•…éšœåœºæ™¯ã€‚</p>
</details>
<h2 id="gotchas">å¸¸è§é™·é˜±ä¸é”™è¯¯ (Gotchas)</h2>
<h3 id="1-nccl">1. NCCL ç‰ˆæœ¬ä¸åŒ¹é…</h3>
<p><strong>é™·é˜±</strong>ï¼šä¸åŒèŠ‚ç‚¹çš„ NCCL ç‰ˆæœ¬ä¸ä¸€è‡´å¯¼è‡´é€šä¿¡å¤±è´¥ã€‚
<strong>è§£å†³</strong>ï¼šç»Ÿä¸€æ‰€æœ‰èŠ‚ç‚¹çš„ PyTorch å’Œ NCCL ç‰ˆæœ¬ã€‚</p>
<h3 id="2-hanging-without-error">2. Hanging Without Error</h3>
<p><strong>é™·é˜±</strong>ï¼šè®­ç»ƒæŒ‚èµ·ä½†æ²¡æœ‰ä»»ä½•é”™è¯¯è¾“å‡ºã€‚
<strong>è§£å†³</strong>ï¼šå¯ç”¨ NCCL_DEBUG=INFO å’Œè®¾ç½®åˆç†çš„è¶…æ—¶æ—¶é—´ã€‚</p>
<h3 id="3">3. éšå¼åŒæ­¥ç‚¹</h3>
<p><strong>é™·é˜±</strong>ï¼šprintã€æ—¥å¿—ç­‰æ“ä½œå¯èƒ½å¼•å…¥éšå¼åŒæ­¥ã€‚
<strong>è§£å†³</strong>ï¼šåªåœ¨ rank 0 è¿›è¡Œ I/O æ“ä½œï¼Œæˆ–ä½¿ç”¨å¼‚æ­¥ I/Oã€‚</p>
<h3 id="4-gpu">4. GPU äº²å’Œæ€§è®¾ç½®é”™è¯¯</h3>
<p><strong>é™·é˜±</strong>ï¼šCUDA_VISIBLE_DEVICES è®¾ç½®ä¸å½“å¯¼è‡´è¿›ç¨‹çœ‹åˆ°é”™è¯¯çš„ GPUã€‚
<strong>è§£å†³</strong>ï¼šä½¿ç”¨ torchrun æˆ–æ­£ç¡®è®¾ç½®æ¯ä¸ªè¿›ç¨‹çš„ GPU æ˜ å°„ã€‚</p>
<h3 id="5">5. æ··åˆç²¾åº¦ä¸å…¼å®¹</h3>
<p><strong>é™·é˜±</strong>ï¼šä¸åŒ GPU æ”¯æŒçš„ç²¾åº¦ä¸åŒï¼ˆFP16 vs BF16ï¼‰ã€‚
<strong>è§£å†³</strong>ï¼šæ£€æµ‹ç¡¬ä»¶èƒ½åŠ›ï¼Œé™çº§åˆ°æ‰€æœ‰ GPU éƒ½æ”¯æŒçš„ç²¾åº¦ã€‚</p>
<h3 id="6-checkpoint">6. Checkpoint è…è´¥</h3>
<p><strong>é™·é˜±</strong>ï¼šä¿å­˜ checkpoint æ—¶è¿›ç¨‹è¢«ä¸­æ–­å¯¼è‡´æ–‡ä»¶æŸåã€‚
<strong>è§£å†³</strong>ï¼šå…ˆä¿å­˜åˆ°ä¸´æ—¶æ–‡ä»¶ï¼ŒæˆåŠŸåå†é‡å‘½åã€‚</p>
<h2 id="_5">æœ€ä½³å®è·µæ£€æŸ¥æ¸…å•</h2>
<h3 id="_6">å¯åŠ¨å‰æ£€æŸ¥</h3>
<ul>
<li>[ ] æ‰€æœ‰èŠ‚ç‚¹çš„ç¯å¢ƒä¸€è‡´ï¼ˆPythonã€PyTorchã€CUDA ç‰ˆæœ¬ï¼‰</li>
<li>[ ] ç½‘ç»œè¿é€šæ€§æµ‹è¯•é€šè¿‡</li>
<li>[ ] GPU å¥åº·æ£€æŸ¥é€šè¿‡ï¼ˆæ¸©åº¦ã€ECC é”™è¯¯ï¼‰</li>
<li>[ ] ç£ç›˜ç©ºé—´å……è¶³ï¼ˆcheckpoint éœ€è¦å¤§é‡ç©ºé—´ï¼‰</li>
<li>[ ] NCCL ç¯å¢ƒå˜é‡æ­£ç¡®è®¾ç½®</li>
</ul>
<h3 id="_7">è®­ç»ƒä¸­ç›‘æ§</h3>
<ul>
<li>[ ] GPU åˆ©ç”¨ç‡ &gt; 85%</li>
<li>[ ] ç½‘ç»œå¸¦å®½åˆ©ç”¨åˆç†</li>
<li>[ ] æ— è¿›ç¨‹æ˜æ˜¾è½åï¼ˆé€šè¿‡ progress barï¼‰</li>
<li>[ ] å†…å­˜ä½¿ç”¨ç¨³å®šï¼ˆæ— æ³„æ¼ï¼‰</li>
<li>[ ] Loss æ›²çº¿æ­£å¸¸ï¼ˆæ—  NaNã€æ— å¼‚å¸¸è·³å˜ï¼‰</li>
</ul>
<h3 id="_8">æ•…éšœæ¢å¤å‡†å¤‡</h3>
<ul>
<li>[ ] Checkpoint å®šæœŸä¿å­˜ï¼ˆè‡³å°‘æ¯å°æ—¶ï¼‰</li>
<li>[ ] æœ‰å¤šä¸ª checkpoint å‰¯æœ¬</li>
<li>[ ] æ•…éšœæ¢å¤è„šæœ¬å·²æµ‹è¯•</li>
<li>[ ] ç›‘æ§å‘Šè­¦å·²é…ç½®</li>
<li>[ ] æœ‰å›æ»šè®¡åˆ’</li>
</ul>
<h3 id="_9">æ€§èƒ½ä¼˜åŒ–</h3>
<ul>
<li>[ ] é€šä¿¡ä¸è®¡ç®—é‡å </li>
<li>[ ] æ¢¯åº¦ç´¯ç§¯åˆç†è®¾ç½®</li>
<li>[ ] æ•°æ®åŠ è½½ä¸æ˜¯ç“¶é¢ˆ</li>
<li>[ ] ä½¿ç”¨äº†åˆé€‚çš„ NCCL ç®—æ³•</li>
<li>[ ] æ··åˆç²¾åº¦è®­ç»ƒå·²å¯ç”¨</li>
</ul>
<h3 id="_10">è°ƒè¯•å·¥å…·</h3>
<ul>
<li>[ ] NCCL æ—¥å¿—å·²å¯ç”¨ï¼ˆé—®é¢˜æ’æŸ¥æ—¶ï¼‰</li>
<li>[ ] è¿›ç¨‹ç›‘æ§è„šæœ¬è¿è¡Œä¸­</li>
<li>[ ] æ€§èƒ½ profiling å·¥å…·å°±ç»ª</li>
<li>[ ] æœ‰ core dump ç”Ÿæˆé…ç½®</li>
</ul>
            </article>
            
            <nav class="page-nav"><a href="chapter11.html" class="nav-link prev">â† ç¬¬ 11 ç« ï¼šè®­ç»ƒé€Ÿåº¦ä¼˜åŒ–å®æˆ˜</a><a href="CLAUDE.html" class="nav-link next">Untitled â†’</a></nav>
        </main>
    </div>
</body>
</html>