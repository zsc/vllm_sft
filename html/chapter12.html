<!DOCTYPE html>
<html lang="zh">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <base href="./">
    <title>第 12 章：多机多卡调试地狱</title>
    <link rel="stylesheet" href="assets/style.css">
    <link rel="stylesheet" href="assets/highlight.css">
    <script src="assets/script.js" defer></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>
        window.MathJax = {
            tex: {
                inlineMath: [['$', '$']],
                displayMath: [['$$', '$$']],
                processEscapes: false,
                packages: {'[+]': ['noerrors', 'ams']}
            },
            options: {
                ignoreHtmlClass: 'tex2jax_ignore',
                processHtmlClass: 'tex2jax_process'
            },
            loader: {
                load: ['[tex]/noerrors', '[tex]/ams']
            }
        };
    </script>
</head>
<body>
    <div class="container">
        <nav id="sidebar" class="sidebar">
            <div class="sidebar-header">
                <h3>目录</h3>
                <button id="sidebar-toggle" class="sidebar-toggle">
                    <span></span>
                    <span></span>
                    <span></span>
                </button>
            </div>
            <div class="sidebar-search">
                <input type="text" id="sidebar-search-input" placeholder="搜索..." autocomplete="off">
            </div>
            <div id="tree-container">
                <nav class="tree-nav" role="tree">
                    <div class="tree-item " >
                        <a href="index.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">视觉语言模型（VLM）的监督微调与强化学习实战教程</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter1.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第 1 章：VLM 架构与原理</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter2.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第 2 章：数据准备与预处理</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter3.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第 3 章：SFT 训练策略</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter4.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第 4 章：分布式训练与优化</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter5.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第 5 章：RLHF 基础与实现</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter6.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第 6 章：直接偏好优化（DPO）</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter7.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第 7 章：评估体系设计</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter8.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第 8 章：模型部署与服务化</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter9.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第 9 章：CUDA OOM 调试完全指南</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter10.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第 10 章：训练崩溃与 NaN 问题</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter11.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第 11 章：训练速度优化实战</span>
                        </a>
                    </div>
                
                    <div class="tree-item active" >
                        <a href="chapter12.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第 12 章：多机多卡调试地狱</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="CLAUDE.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">Untitled</span>
                        </a>
                    </div>
                </nav>
            </div>
        </nav>
        
        <main class="content">
            <article>
                <h1 id="12">第 12 章：多机多卡调试地狱</h1>
<p>当你的 VLM 训练扩展到多机多卡时，你将进入一个充满挑战的调试世界。本章将系统地介绍分布式训练中最常见的问题和解决方案，从 NCCL 通信错误到进程死锁，从异构 GPU 混训到框架选择，帮助你快速定位和解决多机训练中的各种"地狱"级问题。无论是凌晨三点的 NCCL timeout，还是莫名其妙的进程挂起，本章都能让你在 5 分钟内找到解决思路。</p>
<h2 id="121-nccl">12.1 NCCL 错误的常见原因与解决</h2>
<p>NCCL（NVIDIA Collective Communications Library）是多 GPU 训练的核心通信库。当你看到 "NCCL Error" 时，不要慌张，按照以下流程系统排查。</p>
<h3 id="1211">12.1.1 快速诊断流程</h3>
<p>当遇到 NCCL 错误时，首先执行以下诊断步骤：</p>
<p><strong>第一步：启用详细日志</strong></p>
<div class="codehilite"><pre><span></span><code><span class="nb">export</span><span class="w"> </span><span class="nv">NCCL_DEBUG</span><span class="o">=</span>INFO
<span class="nb">export</span><span class="w"> </span><span class="nv">NCCL_DEBUG_SUBSYS</span><span class="o">=</span>ALL
<span class="nb">export</span><span class="w"> </span><span class="nv">TORCH_DISTRIBUTED_DEBUG</span><span class="o">=</span>DETAIL
</code></pre></div>

<p><strong>第二步：检查基础连通性</strong></p>
<div class="codehilite"><pre><span></span><code><span class="c1"># 检查节点间网络连通性</span>
ping<span class="w"> </span>&lt;other_node_ip&gt;
nc<span class="w"> </span>-zv<span class="w"> </span>&lt;other_node_ip&gt;<span class="w"> </span><span class="m">29500</span><span class="w">  </span><span class="c1"># 检查端口是否开放</span>

<span class="c1"># 检查 GPU 可见性</span>
nvidia-smi<span class="w"> </span>-L
<span class="nb">echo</span><span class="w"> </span><span class="nv">$CUDA_VISIBLE_DEVICES</span>
</code></pre></div>

<p><strong>第三步：运行 NCCL 测试</strong></p>
<div class="codehilite"><pre><span></span><code><span class="c1"># 单机测试</span>
python<span class="w"> </span>-m<span class="w"> </span>torch.distributed.run<span class="w"> </span>--nproc_per_node<span class="o">=</span><span class="m">8</span><span class="w"> </span>--nnodes<span class="o">=</span><span class="m">1</span><span class="w"> </span>test_nccl.py

<span class="c1"># 多机测试（在主节点运行）</span>
python<span class="w"> </span>-m<span class="w"> </span>torch.distributed.run<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--nproc_per_node<span class="o">=</span><span class="m">8</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--nnodes<span class="o">=</span><span class="m">2</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--node_rank<span class="o">=</span><span class="m">0</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--master_addr<span class="o">=</span>&lt;master_ip&gt;<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--master_port<span class="o">=</span><span class="m">29500</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>test_nccl.py
</code></pre></div>

<h3 id="1212">12.1.2 常见错误类型与解决方案</h3>
<p><strong>错误 1：NCCL Init Failed</strong></p>
<p>症状：</p>
<div class="codehilite"><pre><span></span><code><span class="n">RuntimeError</span><span class="o">:</span><span class="w"> </span><span class="n">NCCL</span><span class="w"> </span><span class="n">error</span><span class="w"> </span><span class="k">in</span><span class="o">:</span><span class="w"> </span><span class="o">../</span><span class="n">torch</span><span class="sr">/csrc/distributed/c10d/</span><span class="n">ProcessGroupNCCL</span><span class="o">.</span><span class="na">cpp</span><span class="o">:</span><span class="mi">1191</span>
<span class="n">unhandled</span><span class="w"> </span><span class="n">system</span><span class="w"> </span><span class="n">error</span><span class="o">,</span><span class="w"> </span><span class="n">NCCL</span><span class="w"> </span><span class="n">version</span><span class="w"> </span><span class="mf">2.14</span><span class="o">.</span><span class="mi">3</span>
</code></pre></div>

<p>原因分析：</p>
<ol>
<li>GPU 不可见或编号错误</li>
<li>NCCL 版本不兼容</li>
<li>共享内存不足</li>
</ol>
<p>解决方案：</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># 1. 确保 GPU 编号正确</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">CUDA_VISIBLE_DEVICES</span><span class="o">=</span><span class="m">0</span>,1,2,3,4,5,6,7

<span class="c1"># 2. 增加共享内存</span>
docker<span class="w"> </span>run<span class="w"> </span>--shm-size<span class="o">=</span>32gb<span class="w"> </span>...<span class="w">  </span><span class="c1"># Docker 环境</span>
<span class="c1"># 或修改系统配置</span>
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;kernel.shmmax = 68719476736&quot;</span><span class="w"> </span>&gt;&gt;<span class="w"> </span>/etc/sysctl.conf
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;kernel.shmall = 4294967296&quot;</span><span class="w"> </span>&gt;&gt;<span class="w"> </span>/etc/sysctl.conf
sysctl<span class="w"> </span>-p

<span class="c1"># 3. 降级或升级 NCCL</span>
pip<span class="w"> </span>install<span class="w"> </span><span class="nv">torch</span><span class="o">==</span><span class="m">2</span>.0.1+cu118<span class="w">  </span><span class="c1"># 使用兼容版本</span>
</code></pre></div>

<p><strong>错误 2：NCCL Timeout</strong></p>
<p>症状：</p>
<div class="codehilite"><pre><span></span><code>torch.distributed.DistBackendError: NCCL timeout after 1800 seconds
</code></pre></div>

<p>原因分析：</p>
<ol>
<li>网络带宽不足或延迟过高</li>
<li>某个进程卡住或崩溃</li>
<li>数据不均衡导致等待</li>
</ol>
<p>解决方案：</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># 1. 增加超时时间</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">NCCL_TIMEOUT</span><span class="o">=</span><span class="m">3600</span><span class="w">  </span><span class="c1"># 单位：秒</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">NCCL_ASYNC_ERROR_HANDLING</span><span class="o">=</span><span class="m">1</span>

<span class="c1"># 2. 使用更快的网络协议</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">NCCL_IB_DISABLE</span><span class="o">=</span><span class="m">0</span><span class="w">  </span><span class="c1"># 启用 InfiniBand</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">NCCL_SOCKET_IFNAME</span><span class="o">=</span>eth0<span class="w">  </span><span class="c1"># 指定网络接口</span>

<span class="c1"># 3. 检查进程状态</span>
ps<span class="w"> </span>aux<span class="w"> </span><span class="p">|</span><span class="w"> </span>grep<span class="w"> </span>python<span class="w">  </span><span class="c1"># 查看是否有僵尸进程</span>
htop<span class="w">  </span><span class="c1"># 查看 CPU/内存使用情况</span>
</code></pre></div>

<p><strong>错误 3：NCCL AllReduce Failed</strong></p>
<p>症状：</p>
<div class="codehilite"><pre><span></span><code><span class="nv">NCCL</span><span class="w"> </span><span class="nv">WARN</span><span class="w"> </span><span class="nv">Unhandled</span><span class="w"> </span><span class="nv">System</span><span class="w"> </span><span class="nv">Error</span><span class="w"> </span><span class="k">while</span><span class="w"> </span><span class="nv">waiting</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="nv">event</span>
</code></pre></div>

<p>原因分析：</p>
<ol>
<li>PCIe 通信问题</li>
<li>NVLink 配置错误</li>
<li>拓扑结构不优化</li>
</ol>
<p>解决方案：</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># 1. 检查 PCIe 和 NVLink 状态</span>
nvidia-smi<span class="w"> </span>topo<span class="w"> </span>-m

<span class="c1"># 2. 优化 P2P 通信</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">NCCL_P2P_DISABLE</span><span class="o">=</span><span class="m">0</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">NCCL_P2P_LEVEL</span><span class="o">=</span>NVL<span class="w">  </span><span class="c1"># 使用 NVLink</span>

<span class="c1"># 3. 设置合理的 NCCL 树形拓扑</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">NCCL_TREE_THRESHOLD</span><span class="o">=</span><span class="m">0</span><span class="w">  </span><span class="c1"># 总是使用树形算法</span>
</code></pre></div>

<h3 id="1213">12.1.3 网络配置优化</h3>
<p><strong>InfiniBand 配置</strong></p>
<p>InfiniBand 是高性能计算的首选网络：</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># 检查 IB 状态</span>
ibstat
ibping<span class="w"> </span>-S<span class="w">  </span><span class="c1"># 在一个节点启动服务器</span>
ibping<span class="w"> </span>-c<span class="w"> </span>&lt;server_guid&gt;<span class="w">  </span><span class="c1"># 在另一个节点测试</span>

<span class="c1"># NCCL IB 配置</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">NCCL_IB_HCA</span><span class="o">=</span>mlx5_0,mlx5_1<span class="w">  </span><span class="c1"># 指定 HCA</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">NCCL_IB_GID_INDEX</span><span class="o">=</span><span class="m">3</span><span class="w">  </span><span class="c1"># RoCE 环境需要</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">NCCL_IB_TC</span><span class="o">=</span><span class="m">106</span><span class="w">  </span><span class="c1"># Traffic Class</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">NCCL_IB_SL</span><span class="o">=</span><span class="m">0</span><span class="w">  </span><span class="c1"># Service Level</span>
</code></pre></div>

<p><strong>TCP 网络优化</strong></p>
<p>当只有以太网时的优化策略：</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># 1. 选择正确的网络接口</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">NCCL_SOCKET_IFNAME</span><span class="o">=</span>eth0,eth1<span class="w">  </span><span class="c1"># 可以指定多个</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">NCCL_SOCKET_NTHREADS</span><span class="o">=</span><span class="m">8</span><span class="w">  </span><span class="c1"># 增加 socket 线程数</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">NCCL_NSOCKS_PERTHREAD</span><span class="o">=</span><span class="m">4</span><span class="w">  </span><span class="c1"># 每线程 socket 数</span>

<span class="c1"># 2. TCP 缓冲区优化</span>
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;net.core.rmem_max = 134217728&quot;</span><span class="w"> </span>&gt;&gt;<span class="w"> </span>/etc/sysctl.conf
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;net.core.wmem_max = 134217728&quot;</span><span class="w"> </span>&gt;&gt;<span class="w"> </span>/etc/sysctl.conf
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;net.ipv4.tcp_rmem = 4096 87380 134217728&quot;</span><span class="w"> </span>&gt;&gt;<span class="w"> </span>/etc/sysctl.conf
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;net.ipv4.tcp_wmem = 4096 65536 134217728&quot;</span><span class="w"> </span>&gt;&gt;<span class="w"> </span>/etc/sysctl.conf
sysctl<span class="w"> </span>-p
</code></pre></div>

<h3 id="1214">12.1.4 环境变量速查表</h3>
<p>关键 NCCL 环境变量及其作用：</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># 调试相关</span>
<span class="nv">NCCL_DEBUG</span><span class="o">=</span>INFO/WARN/ERROR<span class="w">  </span><span class="c1"># 日志级别</span>
<span class="nv">NCCL_DEBUG_FILE</span><span class="o">=</span>/path/to/log<span class="w">  </span><span class="c1"># 日志输出文件</span>

<span class="c1"># 性能调优</span>
<span class="nv">NCCL_BUFFSIZE</span><span class="o">=</span><span class="m">8388608</span><span class="w">  </span><span class="c1"># 缓冲区大小（默认 4MB）</span>
<span class="nv">NCCL_NTHREADS</span><span class="o">=</span><span class="m">512</span><span class="w">  </span><span class="c1"># NCCL 线程数</span>
<span class="nv">NCCL_MAX_NCHANNELS</span><span class="o">=</span><span class="m">16</span><span class="w">  </span><span class="c1"># 最大通道数</span>

<span class="c1"># 网络选择</span>
<span class="nv">NCCL_NET_GDR_LEVEL</span><span class="o">=</span><span class="m">5</span><span class="w">  </span><span class="c1"># GPUDirect RDMA 级别</span>
<span class="nv">NCCL_CROSS_NIC</span><span class="o">=</span><span class="m">1</span><span class="w">  </span><span class="c1"># 允许跨 NIC 通信</span>

<span class="c1"># 算法选择</span>
<span class="nv">NCCL_ALGO</span><span class="o">=</span>Ring/Tree/CollNet<span class="w">  </span><span class="c1"># 指定算法</span>
<span class="nv">NCCL_PROTO</span><span class="o">=</span>LL/LL128/Simple<span class="w">  </span><span class="c1"># 协议选择</span>
</code></pre></div>

<h3 id="1215-8a100">12.1.5 实战案例：8×A100 集群调试</h3>
<p>真实案例：某团队在 2 节点 8×A100 集群上训练 VLM，遇到间歇性 NCCL 错误。</p>
<p><strong>问题表现</strong>：</p>
<ul>
<li>训练进行到 30% 时随机出现 NCCL timeout</li>
<li>错误日志显示 <code>unhandled cuda error</code></li>
<li>重启后能继续训练一段时间</li>
</ul>
<p><strong>排查过程</strong>：</p>
<ol>
<li>启用 NCCL_DEBUG=INFO，发现特定 GPU 对通信超时</li>
<li>nvidia-smi topo -m 显示 GPU 6-7 之间是 PIX 连接（最慢）</li>
<li>检查温度日志，发现 GPU 6 经常触发温度保护（throttling）</li>
</ol>
<p><strong>解决方案</strong>：</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># 1. 调整 GPU 映射，避免使用问题 GPU 对</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">CUDA_VISIBLE_DEVICES</span><span class="o">=</span><span class="m">0</span>,1,2,3,4,5<span class="w">  </span><span class="c1"># 跳过 6,7</span>

<span class="c1"># 2. 降低功率上限防止过热</span>
nvidia-smi<span class="w"> </span>-pl<span class="w"> </span><span class="m">300</span><span class="w">  </span><span class="c1"># 设置功率上限 300W</span>

<span class="c1"># 3. 优化通信模式</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">NCCL_P2P_LEVEL</span><span class="o">=</span>PHB<span class="w">  </span><span class="c1"># 只使用同一 PCIe 桥下的 P2P</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">NCCL_ALGO</span><span class="o">=</span>Tree<span class="w">  </span><span class="c1"># 使用树形算法减少 P2P 依赖</span>

<span class="c1"># 4. 监控脚本</span>
watch<span class="w"> </span>-n<span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="s1">&#39;nvidia-smi --query-gpu=index,name,temperature.gpu,power.draw --format=csv&#39;</span>
</code></pre></div>

<p><strong>最终效果</strong>：</p>
<ul>
<li>训练稳定性提升，未再出现 timeout</li>
<li>虽然使用 6 个 GPU，但整体吞吐量反而提升 15%（避免了通信瓶颈）</li>
</ul>
<h2 id="122">12.2 进程同步与死锁排查</h2>
<p>分布式训练中，进程同步问题是仅次于 NCCL 错误的第二大"杀手"。本节将深入剖析死锁的成因和快速定位方法。</p>
<h3 id="1221">12.2.1 分布式训练的同步机制</h3>
<p>理解同步点是排查死锁的基础。VLM 训练中的主要同步点：</p>
<p><strong>显式同步点</strong>：</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># 1. Barrier 同步</span>
<span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">barrier</span><span class="p">()</span>  <span class="c1"># 所有进程必须到达</span>

<span class="c1"># 2. All-Reduce 操作</span>
<span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">all_reduce</span><span class="p">(</span><span class="n">tensor</span><span class="p">)</span>  <span class="c1"># 梯度同步</span>

<span class="c1"># 3. Broadcast 操作  </span>
<span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">broadcast</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">src</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># 参数广播</span>
</code></pre></div>

<p><strong>隐式同步点</strong>：</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># 1. 优化器步进</span>
<span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>  <span class="c1"># DDP 会自动同步梯度</span>

<span class="c1"># 2. 模型保存</span>
<span class="k">if</span> <span class="n">rank</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span> <span class="n">path</span><span class="p">)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">barrier</span><span class="p">()</span>  <span class="c1"># 等待保存完成</span>

<span class="c1"># 3. 数据加载</span>
<span class="n">dataloader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">sampler</span><span class="o">=</span><span class="n">DistributedSampler</span><span class="p">(</span><span class="n">dataset</span><span class="p">))</span>
<span class="c1"># DistributedSampler 确保各进程数据不重复</span>
</code></pre></div>

<h3 id="1222">12.2.2 死锁的典型场景</h3>
<p><strong>场景 1：条件分支不一致</strong></p>
<p>错误代码：</p>
<div class="codehilite"><pre><span></span><code><span class="k">if</span> <span class="n">rank</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">epoch</span> <span class="o">%</span> <span class="mi">10</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
    <span class="c1"># 只有 rank 0 执行验证</span>
    <span class="n">val_loss</span> <span class="o">=</span> <span class="n">validate</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">val_loader</span><span class="p">)</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">broadcast</span><span class="p">(</span><span class="n">val_loss</span><span class="p">,</span> <span class="n">src</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># 死锁！其他进程未执行</span>
</code></pre></div>

<p>正确做法：</p>
<div class="codehilite"><pre><span></span><code><span class="k">if</span> <span class="n">epoch</span> <span class="o">%</span> <span class="mi">10</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">rank</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">val_loss</span> <span class="o">=</span> <span class="n">validate</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">val_loader</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">val_loss</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">broadcast</span><span class="p">(</span><span class="n">val_loss</span><span class="p">,</span> <span class="n">src</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># 所有进程都参与</span>
</code></pre></div>

<p><strong>场景 2：数据不均衡导致的死锁</strong></p>
<p>问题代码：</p>
<div class="codehilite"><pre><span></span><code><span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">dataloader</span><span class="p">:</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>  <span class="c1"># 某些进程数据提前结束，未参与同步</span>
</code></pre></div>

<p>解决方案：</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># 方案 1：使用 drop_last</span>
<span class="n">dataloader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">drop_last</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="o">...</span><span class="p">)</span>

<span class="c1"># 方案 2：填充数据</span>
<span class="n">total_samples</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>
<span class="n">samples_per_rank</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="n">total_samples</span> <span class="o">/</span> <span class="n">world_size</span><span class="p">)</span>
<span class="c1"># 确保每个进程有相同数量的批次</span>
</code></pre></div>

<p><strong>场景 3：异常处理不当</strong></p>
<p>危险代码：</p>
<div class="codehilite"><pre><span></span><code><span class="k">try</span><span class="p">:</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Error on rank </span><span class="si">{</span><span class="n">rank</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="k">continue</span>  <span class="c1"># 跳过这个批次，但其他进程还在等待同步！</span>
</code></pre></div>

<p>安全处理：</p>
<div class="codehilite"><pre><span></span><code><span class="k">try</span><span class="p">:</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Error on rank </span><span class="si">{</span><span class="n">rank</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="c1"># 通知所有进程出现异常</span>
    <span class="n">error_flag</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.0</span><span class="p">])</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">all_reduce</span><span class="p">(</span><span class="n">error_flag</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">error_flag</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="c1"># 所有进程一起退出</span>
        <span class="n">cleanup_distributed</span><span class="p">()</span>
        <span class="n">sys</span><span class="o">.</span><span class="n">exit</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div>

<h3 id="1223">12.2.3 死锁快速诊断方法</h3>
<p><strong>方法 1：添加超时和日志</strong></p>
<div class="codehilite"><pre><span></span><code><span class="kn">import</span> <span class="nn">functools</span>
<span class="kn">import</span> <span class="nn">torch.distributed</span> <span class="k">as</span> <span class="nn">dist</span>

<span class="k">def</span> <span class="nf">timeout_wrapper</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="n">timeout</span><span class="o">=</span><span class="mi">300</span><span class="p">):</span>
    <span class="nd">@functools</span><span class="o">.</span><span class="n">wraps</span><span class="p">(</span><span class="n">func</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">wrapper</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="kn">import</span> <span class="nn">signal</span>

        <span class="k">def</span> <span class="nf">timeout_handler</span><span class="p">(</span><span class="n">signum</span><span class="p">,</span> <span class="n">frame</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">TimeoutError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">func</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2"> timeout after </span><span class="si">{</span><span class="n">timeout</span><span class="si">}</span><span class="s2">s&quot;</span><span class="p">)</span>

        <span class="n">signal</span><span class="o">.</span><span class="n">signal</span><span class="p">(</span><span class="n">signal</span><span class="o">.</span><span class="n">SIGALRM</span><span class="p">,</span> <span class="n">timeout_handler</span><span class="p">)</span>
        <span class="n">signal</span><span class="o">.</span><span class="n">alarm</span><span class="p">(</span><span class="n">timeout</span><span class="p">)</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">result</span> <span class="o">=</span> <span class="n">func</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="k">finally</span><span class="p">:</span>
            <span class="n">signal</span><span class="o">.</span><span class="n">alarm</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">result</span>
    <span class="k">return</span> <span class="n">wrapper</span>

<span class="c1"># 使用</span>
<span class="nd">@timeout_wrapper</span>
<span class="k">def</span> <span class="nf">training_step</span><span class="p">(</span><span class="n">batch</span><span class="p">):</span>
    <span class="c1"># 训练代码</span>
    <span class="k">pass</span>
</code></pre></div>

<p><strong>方法 2：进程状态监控</strong></p>
<div class="codehilite"><pre><span></span><code><span class="kn">import</span> <span class="nn">threading</span>
<span class="kn">import</span> <span class="nn">time</span>

<span class="k">def</span> <span class="nf">monitor_thread</span><span class="p">(</span><span class="n">rank</span><span class="p">,</span> <span class="n">interval</span><span class="o">=</span><span class="mi">30</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;监控线程，定期打印进程状态&quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="nf">run</span><span class="p">():</span>
        <span class="n">step</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
            <span class="n">time</span><span class="o">.</span><span class="n">sleep</span><span class="p">(</span><span class="n">interval</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;[Rank </span><span class="si">{</span><span class="n">rank</span><span class="si">}</span><span class="s2">] Heartbeat at step </span><span class="si">{</span><span class="n">step</span><span class="si">}</span><span class="s2">, &quot;</span>
                  <span class="sa">f</span><span class="s2">&quot;Memory: </span><span class="si">{</span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">memory_allocated</span><span class="p">()</span><span class="o">/</span><span class="mf">1e9</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">GB&quot;</span><span class="p">)</span>
            <span class="n">step</span> <span class="o">+=</span> <span class="mi">1</span>

    <span class="n">thread</span> <span class="o">=</span> <span class="n">threading</span><span class="o">.</span><span class="n">Thread</span><span class="p">(</span><span class="n">target</span><span class="o">=</span><span class="n">run</span><span class="p">,</span> <span class="n">daemon</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">thread</span><span class="o">.</span><span class="n">start</span><span class="p">()</span>

<span class="c1"># 在训练开始时启动</span>
<span class="n">monitor_thread</span><span class="p">(</span><span class="n">rank</span><span class="p">)</span>
</code></pre></div>

<p><strong>方法 3：使用 py-spy 分析</strong></p>
<div class="codehilite"><pre><span></span><code><span class="c1"># 安装 py-spy</span>
pip<span class="w"> </span>install<span class="w"> </span>py-spy

<span class="c1"># 分析挂起的进程</span>
py-spy<span class="w"> </span>dump<span class="w"> </span>--pid<span class="w"> </span>&lt;process_id&gt;

<span class="c1"># 生成火焰图</span>
py-spy<span class="w"> </span>record<span class="w"> </span>-d<span class="w"> </span><span class="m">30</span><span class="w"> </span>-o<span class="w"> </span>profile.svg<span class="w"> </span>--pid<span class="w"> </span>&lt;process_id&gt;
</code></pre></div>

<h3 id="1224-barrier">12.2.4 Barrier 超时问题</h3>
<p><strong>常见原因</strong>：</p>
<ol>
<li><strong>不均匀的计算负载</strong></li>
</ol>
<div class="codehilite"><pre><span></span><code><span class="c1"># 问题：rank 0 做额外的日志记录</span>
<span class="k">if</span> <span class="n">rank</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
    <span class="c1"># 复杂的日志计算</span>
    <span class="n">log_metrics</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">epoch</span><span class="p">,</span> <span class="n">step</span><span class="p">)</span>

<span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">barrier</span><span class="p">()</span>  <span class="c1"># rank 0 太慢，其他进程超时</span>
</code></pre></div>

<ol start="2">
<li><strong>I/O 操作不当</strong></li>
</ol>
<div class="codehilite"><pre><span></span><code><span class="c1"># 问题：所有进程同时写入</span>
<span class="k">for</span> <span class="n">rank</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">world_size</span><span class="p">):</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;log_</span><span class="si">{</span><span class="n">rank</span><span class="si">}</span><span class="s2">.txt&quot;</span><span class="p">,</span> <span class="s2">&quot;a&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
        <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">metrics</span><span class="p">)</span>  <span class="c1"># 文件系统压力导致某些进程阻塞</span>
</code></pre></div>

<p><strong>解决策略</strong>：</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># 1. 使用异步 I/O</span>
<span class="kn">import</span> <span class="nn">asyncio</span>

<span class="k">async</span> <span class="k">def</span> <span class="nf">async_log</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">filename</span><span class="p">):</span>
    <span class="k">async</span> <span class="k">with</span> <span class="n">aiofiles</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="n">filename</span><span class="p">,</span> <span class="s1">&#39;a&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
        <span class="k">await</span> <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

<span class="c1"># 2. 错开 I/O 时机</span>
<span class="k">if</span> <span class="n">step</span> <span class="o">%</span> <span class="mi">100</span> <span class="o">==</span> <span class="n">rank</span><span class="p">:</span>  <span class="c1"># 不同 rank 在不同步数记录</span>
    <span class="n">save_checkpoint</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;ckpt_</span><span class="si">{</span><span class="n">rank</span><span class="si">}</span><span class="s2">_</span><span class="si">{</span><span class="n">step</span><span class="si">}</span><span class="s2">.pt&quot;</span><span class="p">)</span>

<span class="c1"># 3. 设置合理的超时</span>
<span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s1">&#39;TORCH_DISTRIBUTED_TIMEOUT&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;3600&#39;</span>  <span class="c1"># 1 小时</span>
</code></pre></div>

<h3 id="1225-64-gpu">12.2.5 实战案例：64 GPU 训练死锁排查</h3>
<p><strong>背景</strong>：某团队使用 8 节点 × 8 V100 训练 13B VLM，在第 1000 步突然挂起。</p>
<p><strong>症状</strong>：</p>
<ul>
<li>所有 GPU 利用率降为 0%</li>
<li>CPU 占用率正常</li>
<li>无错误日志输出</li>
</ul>
<p><strong>排查步骤</strong>：</p>
<ol>
<li><strong>确认挂起位置</strong></li>
</ol>
<div class="codehilite"><pre><span></span><code><span class="c1"># 使用 gdb 附加到进程</span>
gdb<span class="w"> </span>-p<span class="w"> </span>&lt;pid&gt;
<span class="o">(</span>gdb<span class="o">)</span><span class="w"> </span>py-bt<span class="w">  </span><span class="c1"># 查看 Python 调用栈</span>

<span class="c1"># 发现卡在：</span>
File<span class="w"> </span><span class="s2">&quot;torch/distributed/distributed_c10d.py&quot;</span>,<span class="w"> </span>line<span class="w"> </span><span class="m">2838</span>,<span class="w"> </span><span class="k">in</span><span class="w"> </span>barrier
<span class="w">    </span>work.wait<span class="o">()</span>
</code></pre></div>

<ol start="2">
<li><strong>检查各进程状态</strong></li>
</ol>
<div class="codehilite"><pre><span></span><code><span class="c1"># 添加调试代码</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;[Rank </span><span class="si">{</span><span class="n">rank</span><span class="si">}</span><span class="s2">] Entering barrier at step </span><span class="si">{</span><span class="n">step</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">barrier</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;[Rank </span><span class="si">{</span><span class="n">rank</span><span class="si">}</span><span class="s2">] Exited barrier&quot;</span><span class="p">)</span>

<span class="c1"># 发现 rank 43 未进入 barrier</span>
</code></pre></div>

<ol start="3">
<li><strong>定位问题代码</strong></li>
</ol>
<div class="codehilite"><pre><span></span><code><span class="c1"># 原代码</span>
<span class="k">if</span> <span class="n">batch_idx</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">batch_idx</span> <span class="o">%</span> <span class="n">gradient_accumulation_steps</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

<span class="c1"># 问题：rank 43 的数据少一个 batch，未执行最后一次 optimizer.step()</span>
</code></pre></div>

<p><strong>解决方案</strong>：</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># 1. 确保所有 rank 执行相同次数的优化步骤</span>
<span class="n">total_steps</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">dataloader</span><span class="p">)</span> <span class="o">//</span> <span class="n">gradient_accumulation_steps</span>
<span class="k">for</span> <span class="n">step</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">total_steps</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">gradient_accumulation_steps</span><span class="p">):</span>
        <span class="n">batch</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">dataloader</span><span class="p">),</span> <span class="kc">None</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">batch</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>
            <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

    <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

<span class="c1"># 2. 添加同步检查点</span>
<span class="k">if</span> <span class="n">step</span> <span class="o">%</span> <span class="mi">100</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
    <span class="c1"># 同步检查，确保所有进程进度一致</span>
    <span class="n">progress_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">step</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;cuda&#39;</span><span class="p">)</span>
    <span class="n">progress_list</span> <span class="o">=</span> <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">progress_tensor</span><span class="p">)</span> 
                    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">world_size</span><span class="p">)]</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">all_gather</span><span class="p">(</span><span class="n">progress_list</span><span class="p">,</span> <span class="n">progress_tensor</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">rank</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">progress</span> <span class="o">=</span> <span class="p">[</span><span class="n">p</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">progress_list</span><span class="p">]</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">progress</span><span class="p">))</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Warning: Progress mismatch: </span><span class="si">{</span><span class="n">progress</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</code></pre></div>

<h2 id="123-gpu">12.3 不同 GPU 型号混合训练的坑</h2>
<p>在实际生产环境中，你可能面临 A100 和 V100 混用、3090 和 4090 并存的情况。异构 GPU 训练充满挑战，本节将揭示所有隐藏的陷阱。</p>
<h3 id="1231-gpu">12.3.1 异构 GPU 的主要挑战</h3>
<p><strong>硬件差异带来的问题</strong>：</p>
<p>| 差异维度 | 影响 | 典型场景 |</p>
<table>
<thead>
<tr>
<th>差异维度</th>
<th>影响</th>
<th>典型场景</th>
</tr>
</thead>
<tbody>
<tr>
<td>显存大小</td>
<td>OOM 风险</td>
<td>A100-80G vs A100-40G</td>
</tr>
<tr>
<td>计算能力</td>
<td>速度瓶颈</td>
<td>V100 vs A100 (2.5x 差距)</td>
</tr>
<tr>
<td>精度支持</td>
<td>训练不稳定</td>
<td>3090 (FP16) vs A100 (BF16)</td>
</tr>
<tr>
<td>互联带宽</td>
<td>通信瓶颈</td>
<td>NVLink vs PCIe</td>
</tr>
<tr>
<td>架构差异</td>
<td>功能不兼容</td>
<td>Ampere vs Volta</td>
</tr>
</tbody>
</table>
<h3 id="1232">12.3.2 性能瓶颈与负载均衡</h3>
<p><strong>问题 1：木桶效应</strong></p>
<p>最慢的 GPU 决定整体训练速度：</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># 诊断代码</span>
<span class="kn">import</span> <span class="nn">time</span>
<span class="kn">import</span> <span class="nn">torch.distributed</span> <span class="k">as</span> <span class="nn">dist</span>

<span class="k">def</span> <span class="nf">measure_gpu_speed</span><span class="p">(</span><span class="n">rank</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">dummy_batch</span><span class="p">):</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">synchronize</span><span class="p">()</span>
    <span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>

    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">100</span><span class="p">):</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">dummy_batch</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

    <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">synchronize</span><span class="p">()</span>
    <span class="n">elapsed</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">start</span>

    <span class="c1"># 收集所有 GPU 的时间</span>
    <span class="n">times</span> <span class="o">=</span> <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">dist</span><span class="o">.</span><span class="n">get_world_size</span><span class="p">())]</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">all_gather</span><span class="p">(</span><span class="n">times</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">elapsed</span><span class="p">])</span><span class="o">.</span><span class="n">cuda</span><span class="p">())</span>

    <span class="k">if</span> <span class="n">rank</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">times</span> <span class="o">=</span> <span class="p">[</span><span class="n">t</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">times</span><span class="p">]</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;GPU speeds: </span><span class="si">{</span><span class="n">times</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Slowest/Fastest ratio: </span><span class="si">{</span><span class="nb">max</span><span class="p">(</span><span class="n">times</span><span class="p">)</span><span class="o">/</span><span class="nb">min</span><span class="p">(</span><span class="n">times</span><span class="p">)</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">x&quot;</span><span class="p">)</span>
</code></pre></div>

<p><strong>解决方案：动态批大小</strong></p>
<div class="codehilite"><pre><span></span><code><span class="k">class</span> <span class="nc">HeterogeneousDataLoader</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dataset</span><span class="p">,</span> <span class="n">gpu_configs</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        gpu_configs: {</span>
<span class="sd">            0: {&#39;type&#39;: &#39;A100&#39;, &#39;memory&#39;: 80, &#39;batch_size&#39;: 8},</span>
<span class="sd">            1: {&#39;type&#39;: &#39;V100&#39;, &#39;memory&#39;: 32, &#39;batch_size&#39;: 4},</span>
<span class="sd">        }</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gpu_configs</span> <span class="o">=</span> <span class="n">gpu_configs</span>

    <span class="k">def</span> <span class="nf">get_batch_size</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">rank</span><span class="p">):</span>
        <span class="c1"># 根据 GPU 能力分配不同批大小</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">gpu_configs</span><span class="p">[</span><span class="n">rank</span><span class="p">][</span><span class="s1">&#39;batch_size&#39;</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">create_loader</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">rank</span><span class="p">):</span>
        <span class="n">batch_size</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_batch_size</span><span class="p">(</span><span class="n">rank</span><span class="p">)</span>
        <span class="n">sampler</span> <span class="o">=</span> <span class="n">DistributedSampler</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">dataset</span><span class="p">,</span>
            <span class="n">num_replicas</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">gpu_configs</span><span class="p">),</span>
            <span class="n">rank</span><span class="o">=</span><span class="n">rank</span><span class="p">,</span>
            <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">DataLoader</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">dataset</span><span class="p">,</span>
            <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
            <span class="n">sampler</span><span class="o">=</span><span class="n">sampler</span>
        <span class="p">)</span>
</code></pre></div>

<p><strong>问题 2：显存不均衡</strong></p>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">adaptive_gradient_accumulation</span><span class="p">(</span><span class="n">rank</span><span class="p">,</span> <span class="n">base_batch_size</span><span class="o">=</span><span class="mi">8</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;根据 GPU 显存动态调整梯度累积&quot;&quot;&quot;</span>
    <span class="n">gpu_memory</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">get_device_properties</span><span class="p">(</span><span class="n">rank</span><span class="p">)</span><span class="o">.</span><span class="n">total_memory</span>

    <span class="k">if</span> <span class="n">gpu_memory</span> <span class="o">&gt;</span> <span class="mi">80</span> <span class="o">*</span> <span class="mi">1024</span><span class="o">**</span><span class="mi">3</span><span class="p">:</span>  <span class="c1"># 80GB</span>
        <span class="n">micro_batch_size</span> <span class="o">=</span> <span class="n">base_batch_size</span>
        <span class="n">accumulation_steps</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="k">elif</span> <span class="n">gpu_memory</span> <span class="o">&gt;</span> <span class="mi">40</span> <span class="o">*</span> <span class="mi">1024</span><span class="o">**</span><span class="mi">3</span><span class="p">:</span>  <span class="c1"># 40GB</span>
        <span class="n">micro_batch_size</span> <span class="o">=</span> <span class="n">base_batch_size</span> <span class="o">//</span> <span class="mi">2</span>
        <span class="n">accumulation_steps</span> <span class="o">=</span> <span class="mi">2</span>
    <span class="k">elif</span> <span class="n">gpu_memory</span> <span class="o">&gt;</span> <span class="mi">24</span> <span class="o">*</span> <span class="mi">1024</span><span class="o">**</span><span class="mi">3</span><span class="p">:</span>  <span class="c1"># 24GB</span>
        <span class="n">micro_batch_size</span> <span class="o">=</span> <span class="n">base_batch_size</span> <span class="o">//</span> <span class="mi">4</span>
        <span class="n">accumulation_steps</span> <span class="o">=</span> <span class="mi">4</span>
    <span class="k">else</span><span class="p">:</span>  <span class="c1"># 16GB or less</span>
        <span class="n">micro_batch_size</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="n">accumulation_steps</span> <span class="o">=</span> <span class="n">base_batch_size</span>

    <span class="k">return</span> <span class="n">micro_batch_size</span><span class="p">,</span> <span class="n">accumulation_steps</span>
</code></pre></div>

<h3 id="1233">12.3.3 混合精度的兼容性问题</h3>
<p><strong>BF16 vs FP16 混用陷阱</strong>：</p>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">setup_mixed_precision</span><span class="p">(</span><span class="n">rank</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;处理不同 GPU 的精度差异&quot;&quot;&quot;</span>
    <span class="n">gpu_name</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">get_device_name</span><span class="p">(</span><span class="n">rank</span><span class="p">)</span>

    <span class="k">if</span> <span class="s1">&#39;A100&#39;</span> <span class="ow">in</span> <span class="n">gpu_name</span> <span class="ow">or</span> <span class="s1">&#39;H100&#39;</span> <span class="ow">in</span> <span class="n">gpu_name</span><span class="p">:</span>
        <span class="c1"># 支持 BF16</span>
        <span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span>
        <span class="n">use_bf16</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># 只支持 FP16</span>
        <span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float16</span>
        <span class="n">use_bf16</span> <span class="o">=</span> <span class="kc">False</span>

    <span class="c1"># 统一精度设置</span>
    <span class="n">all_use_bf16</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">use_bf16</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bool</span><span class="p">)</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>
    <span class="n">dist</span><span class="o">.</span><span class="n">all_reduce</span><span class="p">(</span><span class="n">all_use_bf16</span><span class="p">,</span> <span class="n">op</span><span class="o">=</span><span class="n">dist</span><span class="o">.</span><span class="n">ReduceOp</span><span class="o">.</span><span class="n">MIN</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">all_use_bf16</span><span class="o">.</span><span class="n">item</span><span class="p">():</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># 降级到 FP16</span>
        <span class="k">if</span> <span class="n">rank</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Warning: Falling back to FP16 due to hardware limitations&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">float16</span>
</code></pre></div>

<p><strong>梯度同步精度问题</strong>：</p>
<div class="codehilite"><pre><span></span><code><span class="k">class</span> <span class="nc">MixedPrecisionOptimizer</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">dtype</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">optimizer</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">model</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">dtype</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">grad_scaler</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">amp</span><span class="o">.</span><span class="n">GradScaler</span><span class="p">(</span><span class="n">enabled</span><span class="o">=</span><span class="p">(</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">step</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># 梯度转换到统一精度</span>
        <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span>
            <span class="k">if</span> <span class="n">param</span><span class="o">.</span><span class="n">grad</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="c1"># 确保梯度精度一致</span>
                <span class="n">param</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="n">param</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

        <span class="c1"># 同步前转换</span>
        <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span>
            <span class="k">if</span> <span class="n">param</span><span class="o">.</span><span class="n">grad</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">dist</span><span class="o">.</span><span class="n">all_reduce</span><span class="p">(</span><span class="n">param</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span>
                <span class="n">param</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="n">param</span><span class="o">.</span><span class="n">grad</span> <span class="o">/</span> <span class="n">dist</span><span class="o">.</span><span class="n">get_world_size</span><span class="p">()</span>

        <span class="c1"># 优化器步进</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">grad_scaler</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">grad_scaler</span><span class="o">.</span><span class="n">update</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</code></pre></div>

<h3 id="1234">12.3.4 实战配置示例</h3>
<p><strong>场景：4×A100-80G + 4×V100-32G 混合集群</strong></p>
<div class="codehilite"><pre><span></span><code><span class="c1"># 配置文件 heterogeneous_config.yaml</span>
<span class="n">gpu_groups</span><span class="p">:</span>
  <span class="n">high_tier</span><span class="p">:</span>  <span class="c1"># A100-80G</span>
    <span class="n">ranks</span><span class="p">:</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]</span>
    <span class="n">batch_size</span><span class="p">:</span> <span class="mi">8</span>
    <span class="n">gradient_accumulation</span><span class="p">:</span> <span class="mi">1</span>
    <span class="n">precision</span><span class="p">:</span> <span class="n">bfloat16</span>

  <span class="n">low_tier</span><span class="p">:</span>  <span class="c1"># V100-32G  </span>
    <span class="n">ranks</span><span class="p">:</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">7</span><span class="p">]</span>
    <span class="n">batch_size</span><span class="p">:</span> <span class="mi">3</span>
    <span class="n">gradient_accumulation</span><span class="p">:</span> <span class="mi">3</span>
    <span class="n">precision</span><span class="p">:</span> <span class="n">float16</span>

<span class="n">communication</span><span class="p">:</span>
  <span class="c1"># 分组通信策略</span>
  <span class="n">allreduce_groups</span><span class="p">:</span>

    <span class="o">-</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]</span>  <span class="c1"># A100 内部先同步</span>
    <span class="o">-</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">7</span><span class="p">]</span>  <span class="c1"># V100 内部先同步</span>

  <span class="c1"># 跨组同步使用异步模式</span>
  <span class="n">cross_group_async</span><span class="p">:</span> <span class="n">true</span>

<span class="n">training</span><span class="p">:</span>
  <span class="c1"># 使用 pipeline 并行缓解不均衡</span>
  <span class="n">pipeline_parallel</span><span class="p">:</span> <span class="n">true</span>
  <span class="n">pipeline_stages</span><span class="p">:</span>

    <span class="o">-</span> <span class="n">ranks</span><span class="p">:</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>  <span class="c1"># A100 处理前面层</span>
    <span class="o">-</span> <span class="n">ranks</span><span class="p">:</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]</span>  <span class="c1"># A100 处理中间层</span>
    <span class="o">-</span> <span class="n">ranks</span><span class="p">:</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">7</span><span class="p">]</span>  <span class="c1"># V100 处理后面层（计算量较小）</span>
</code></pre></div>

<p>实现代码：</p>
<div class="codehilite"><pre><span></span><code><span class="k">class</span> <span class="nc">HeterogeneousTrainer</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config_path</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">config</span> <span class="o">=</span> <span class="n">yaml</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="nb">open</span><span class="p">(</span><span class="n">config_path</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">rank</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">get_rank</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">setup_gpu_group</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">setup_gpu_group</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># 确定当前 GPU 所属组</span>
        <span class="k">for</span> <span class="n">group_name</span><span class="p">,</span> <span class="n">group_config</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s1">&#39;gpu_groups&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">rank</span> <span class="ow">in</span> <span class="n">group_config</span><span class="p">[</span><span class="s1">&#39;ranks&#39;</span><span class="p">]:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">group_name</span> <span class="o">=</span> <span class="n">group_name</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">group_config</span> <span class="o">=</span> <span class="n">group_config</span>
                <span class="k">break</span>

        <span class="c1"># 创建通信组</span>
        <span class="k">for</span> <span class="n">group_ranks</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s1">&#39;communication&#39;</span><span class="p">][</span><span class="s1">&#39;allreduce_groups&#39;</span><span class="p">]:</span>
            <span class="n">group</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">new_group</span><span class="p">(</span><span class="n">ranks</span><span class="o">=</span><span class="n">group_ranks</span><span class="p">)</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">rank</span> <span class="ow">in</span> <span class="n">group_ranks</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">local_group</span> <span class="o">=</span> <span class="n">group</span>

    <span class="k">def</span> <span class="nf">train_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">):</span>
        <span class="c1"># 根据组配置调整批大小</span>
        <span class="n">micro_batch</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_split_batch</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">group_config</span><span class="p">[</span><span class="s1">&#39;batch_size&#39;</span><span class="p">])</span>

        <span class="n">total_loss</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">step</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">group_config</span><span class="p">[</span><span class="s1">&#39;gradient_accumulation&#39;</span><span class="p">]):</span>
            <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">amp</span><span class="o">.</span><span class="n">autocast</span><span class="p">(</span>
                <span class="n">dtype</span><span class="o">=</span><span class="nb">getattr</span><span class="p">(</span><span class="n">torch</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">group_config</span><span class="p">[</span><span class="s1">&#39;precision&#39;</span><span class="p">])</span>
            <span class="p">):</span>
                <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">(</span><span class="n">micro_batch</span><span class="p">[</span><span class="n">step</span><span class="p">])</span>
                <span class="n">loss</span> <span class="o">=</span> <span class="n">loss</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">group_config</span><span class="p">[</span><span class="s1">&#39;gradient_accumulation&#39;</span><span class="p">]</span>

            <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
            <span class="n">total_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>

        <span class="c1"># 分层同步策略</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_hierarchical_allreduce</span><span class="p">()</span>

        <span class="k">return</span> <span class="n">total_loss</span>

    <span class="k">def</span> <span class="nf">_hierarchical_allreduce</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># 第一步：组内同步</span>
        <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span>
            <span class="k">if</span> <span class="n">param</span><span class="o">.</span><span class="n">grad</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">dist</span><span class="o">.</span><span class="n">all_reduce</span><span class="p">(</span><span class="n">param</span><span class="o">.</span><span class="n">grad</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">local_group</span><span class="p">)</span>

        <span class="c1"># 第二步：跨组同步（仅组长参与）</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">rank</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">group_config</span><span class="p">[</span><span class="s1">&#39;ranks&#39;</span><span class="p">][</span><span class="mi">0</span><span class="p">]:</span>
            <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span>
                <span class="k">if</span> <span class="n">param</span><span class="o">.</span><span class="n">grad</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="n">dist</span><span class="o">.</span><span class="n">all_reduce</span><span class="p">(</span><span class="n">param</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span>  <span class="c1"># 全局同步</span>

        <span class="c1"># 第三步：组内广播</span>
        <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span>
            <span class="k">if</span> <span class="n">param</span><span class="o">.</span><span class="n">grad</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">dist</span><span class="o">.</span><span class="n">broadcast</span><span class="p">(</span><span class="n">param</span><span class="o">.</span><span class="n">grad</span><span class="p">,</span> 
                             <span class="n">src</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">group_config</span><span class="p">[</span><span class="s1">&#39;ranks&#39;</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span>
                             <span class="n">group</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">local_group</span><span class="p">)</span>
</code></pre></div>

<h3 id="1235">12.3.5 调试技巧与监控</h3>
<p><strong>异构集群监控脚本</strong>：</p>
<div class="codehilite"><pre><span></span><code><span class="ch">#!/bin/bash</span>
<span class="c1"># monitor_heterogeneous.sh</span>

<span class="k">while</span><span class="w"> </span>true<span class="p">;</span><span class="w"> </span><span class="k">do</span>
<span class="w">    </span><span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;=== GPU Status at </span><span class="k">$(</span>date<span class="k">)</span><span class="s2"> ===&quot;</span>

<span class="w">    </span><span class="c1"># 收集所有节点的 GPU 信息</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span>node<span class="w"> </span><span class="k">in</span><span class="w"> </span>node1<span class="w"> </span>node2<span class="p">;</span><span class="w"> </span><span class="k">do</span>
<span class="w">        </span><span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;Node: </span><span class="nv">$node</span><span class="s2">&quot;</span>
<span class="w">        </span>ssh<span class="w"> </span><span class="nv">$node</span><span class="w"> </span><span class="s2">&quot;nvidia-smi --query-gpu=index,name,memory.used,memory.total,utilization.gpu,temperature.gpu,power.draw --format=csv&quot;</span>
<span class="w">    </span><span class="k">done</span>

<span class="w">    </span><span class="c1"># 检查速度差异</span>
<span class="w">    </span><span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;=== Training Speed ===&quot;</span>
<span class="w">    </span>tail<span class="w"> </span>-n<span class="w"> </span><span class="m">8</span><span class="w"> </span>training.log<span class="w"> </span><span class="p">|</span><span class="w"> </span>grep<span class="w"> </span><span class="s2">&quot;step_time&quot;</span>

<span class="w">    </span><span class="c1"># 检查是否有 OOM</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span>grep<span class="w"> </span>-q<span class="w"> </span><span class="s2">&quot;out of memory&quot;</span><span class="w"> </span>training.log<span class="p">;</span><span class="w"> </span><span class="k">then</span>
<span class="w">        </span><span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;WARNING: OOM detected!&quot;</span>
<span class="w">        </span>grep<span class="w"> </span><span class="s2">&quot;out of memory&quot;</span><span class="w"> </span>training.log<span class="w"> </span><span class="p">|</span><span class="w"> </span>tail<span class="w"> </span>-n<span class="w"> </span><span class="m">5</span>
<span class="w">    </span><span class="k">fi</span>

<span class="w">    </span>sleep<span class="w"> </span><span class="m">30</span>
<span class="k">done</span>
</code></pre></div>

<h2 id="124-fsdp-vs-deepspeed">12.4 FSDP vs DeepSpeed 实战对比</h2>
<p>选择 FSDP 还是 DeepSpeed？这是每个大模型训练者都会面临的问题。本节通过实战对比，帮你做出最佳选择。</p>
<h3 id="1241">12.4.1 架构差异与设计理念</h3>
<p>| 特性 | FSDP | DeepSpeed |</p>
<table>
<thead>
<tr>
<th>特性</th>
<th>FSDP</th>
<th>DeepSpeed</th>
</tr>
</thead>
<tbody>
<tr>
<td>开发者</td>
<td>Meta/PyTorch 原生</td>
<td>Microsoft</td>
</tr>
<tr>
<td>集成度</td>
<td>PyTorch 内置</td>
<td>需要额外安装</td>
</tr>
<tr>
<td>学习曲线</td>
<td>相对简单</td>
<td>功能丰富但复杂</td>
</tr>
<tr>
<td>优化器状态分片</td>
<td>✅</td>
<td>✅ (ZeRO-2/3)</td>
</tr>
<tr>
<td>参数分片</td>
<td>✅</td>
<td>✅ (ZeRO-3)</td>
</tr>
<tr>
<td>激活值分片</td>
<td>部分支持</td>
<td>✅ (ZeRO-R)</td>
</tr>
<tr>
<td>CPU Offload</td>
<td>✅</td>
<td>✅ 更成熟</td>
</tr>
<tr>
<td>混合精度</td>
<td>原生支持</td>
<td>需要配置</td>
</tr>
<tr>
<td>Pipeline 并行</td>
<td>❌</td>
<td>✅</td>
</tr>
</tbody>
</table>
<h3 id="1242-vlm">12.4.2 VLM 训练配置对比</h3>
<p><strong>FSDP 配置示例</strong>：</p>
<div class="codehilite"><pre><span></span><code><span class="kn">from</span> <span class="nn">torch.distributed.fsdp</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">FullyShardedDataParallel</span> <span class="k">as</span> <span class="n">FSDP</span><span class="p">,</span>
    <span class="n">MixedPrecision</span><span class="p">,</span>
    <span class="n">BackwardPrefetch</span><span class="p">,</span>
    <span class="n">ShardingStrategy</span><span class="p">,</span>
    <span class="n">CPUOffload</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">torch.distributed.fsdp.wrap</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">transformer_auto_wrap_policy</span><span class="p">,</span>
    <span class="n">size_based_auto_wrap_policy</span>
<span class="p">)</span>

<span class="k">def</span> <span class="nf">setup_fsdp_for_vlm</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">vision_encoder</span><span class="p">,</span> <span class="n">language_model</span><span class="p">):</span>
    <span class="c1"># 混合精度配置</span>
    <span class="n">mp_policy</span> <span class="o">=</span> <span class="n">MixedPrecision</span><span class="p">(</span>
        <span class="n">param_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">,</span>
        <span class="n">reduce_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">,</span>
        <span class="n">buffer_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">,</span>
        <span class="n">cast_forward_inputs</span><span class="o">=</span><span class="kc">True</span>
    <span class="p">)</span>

    <span class="c1"># 自动包装策略 - 关键！</span>
    <span class="n">auto_wrap_policy</span> <span class="o">=</span> <span class="n">functools</span><span class="o">.</span><span class="n">partial</span><span class="p">(</span>
        <span class="n">transformer_auto_wrap_policy</span><span class="p">,</span>
        <span class="n">transformer_layer_cls</span><span class="o">=</span><span class="p">{</span>
            <span class="c1"># 视觉编码器层</span>
            <span class="n">vision_encoder</span><span class="o">.</span><span class="vm">__class__</span><span class="p">,</span>
            <span class="c1"># 语言模型层  </span>
            <span class="nb">type</span><span class="p">(</span><span class="n">language_model</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span>
        <span class="p">}</span>
    <span class="p">)</span>

    <span class="c1"># CPU Offload（显存不足时启用）</span>
    <span class="n">cpu_offload</span> <span class="o">=</span> <span class="n">CPUOffload</span><span class="p">(</span><span class="n">offload_params</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="c1"># FSDP 配置</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">FSDP</span><span class="p">(</span>
        <span class="n">model</span><span class="p">,</span>
        <span class="n">auto_wrap_policy</span><span class="o">=</span><span class="n">auto_wrap_policy</span><span class="p">,</span>
        <span class="n">mixed_precision</span><span class="o">=</span><span class="n">mp_policy</span><span class="p">,</span>
        <span class="n">sharding_strategy</span><span class="o">=</span><span class="n">ShardingStrategy</span><span class="o">.</span><span class="n">FULL_SHARD</span><span class="p">,</span>  <span class="c1"># 完全分片</span>
        <span class="n">cpu_offload</span><span class="o">=</span><span class="n">cpu_offload</span><span class="p">,</span>
        <span class="n">backward_prefetch</span><span class="o">=</span><span class="n">BackwardPrefetch</span><span class="o">.</span><span class="n">BACKWARD_PRE</span><span class="p">,</span>  <span class="c1"># 预取优化</span>
        <span class="n">limit_all_gathers</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>  <span class="c1"># 限制 all-gather 防止 OOM</span>
        <span class="n">use_orig_params</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>  <span class="c1"># 保持原始参数（重要！）</span>
    <span class="p">)</span>

    <span class="k">return</span> <span class="n">model</span>
</code></pre></div>

<p><strong>DeepSpeed 配置示例</strong>：</p>
<div class="codehilite"><pre><span></span><code><span class="p">{</span>
<span class="w">    </span><span class="nt">&quot;train_batch_size&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">64</span><span class="p">,</span>
<span class="w">    </span><span class="nt">&quot;gradient_accumulation_steps&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">8</span><span class="p">,</span>
<span class="w">    </span><span class="nt">&quot;train_micro_batch_size_per_gpu&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">2</span><span class="p">,</span>

<span class="w">    </span><span class="nt">&quot;bf16&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="nt">&quot;enabled&quot;</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span>
<span class="w">    </span><span class="p">},</span>

<span class="w">    </span><span class="nt">&quot;zero_optimization&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="nt">&quot;stage&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">3</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;offload_optimizer&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">            </span><span class="nt">&quot;device&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;cpu&quot;</span><span class="p">,</span>
<span class="w">            </span><span class="nt">&quot;pin_memory&quot;</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span>
<span class="w">        </span><span class="p">},</span>
<span class="w">        </span><span class="nt">&quot;offload_param&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">            </span><span class="nt">&quot;device&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;cpu&quot;</span><span class="p">,</span>
<span class="w">            </span><span class="nt">&quot;pin_memory&quot;</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span>
<span class="w">        </span><span class="p">},</span>
<span class="w">        </span><span class="nt">&quot;overlap_comm&quot;</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;contiguous_gradients&quot;</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;sub_group_size&quot;</span><span class="p">:</span><span class="w"> </span><span class="mf">1e9</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;reduce_bucket_size&quot;</span><span class="p">:</span><span class="w"> </span><span class="mf">1e9</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;stage3_prefetch_bucket_size&quot;</span><span class="p">:</span><span class="w"> </span><span class="mf">1e8</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;stage3_param_persistence_threshold&quot;</span><span class="p">:</span><span class="w"> </span><span class="mf">1e6</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;stage3_max_live_parameters&quot;</span><span class="p">:</span><span class="w"> </span><span class="mf">1e9</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;stage3_max_reuse_distance&quot;</span><span class="p">:</span><span class="w"> </span><span class="mf">1e9</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;stage3_gather_16bit_weights_on_model_save&quot;</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span>
<span class="w">    </span><span class="p">},</span>

<span class="w">    </span><span class="nt">&quot;gradient_clipping&quot;</span><span class="p">:</span><span class="w"> </span><span class="mf">1.0</span><span class="p">,</span>

<span class="w">    </span><span class="nt">&quot;optimizer&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="nt">&quot;type&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;AdamW&quot;</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;params&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">            </span><span class="nt">&quot;lr&quot;</span><span class="p">:</span><span class="w"> </span><span class="mf">2e-5</span><span class="p">,</span>
<span class="w">            </span><span class="nt">&quot;betas&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="mf">0.9</span><span class="p">,</span><span class="w"> </span><span class="mf">0.999</span><span class="p">],</span>
<span class="w">            </span><span class="nt">&quot;eps&quot;</span><span class="p">:</span><span class="w"> </span><span class="mf">1e-8</span><span class="p">,</span>
<span class="w">            </span><span class="nt">&quot;weight_decay&quot;</span><span class="p">:</span><span class="w"> </span><span class="mf">0.01</span>
<span class="w">        </span><span class="p">}</span>
<span class="w">    </span><span class="p">},</span>

<span class="w">    </span><span class="nt">&quot;scheduler&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="nt">&quot;type&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;WarmupCosineLR&quot;</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;params&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">            </span><span class="nt">&quot;warmup_min_lr&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span>
<span class="w">            </span><span class="nt">&quot;warmup_max_lr&quot;</span><span class="p">:</span><span class="w"> </span><span class="mf">2e-5</span><span class="p">,</span>
<span class="w">            </span><span class="nt">&quot;warmup_num_steps&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">1000</span><span class="p">,</span>
<span class="w">            </span><span class="nt">&quot;total_num_steps&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">10000</span>
<span class="w">        </span><span class="p">}</span>
<span class="w">    </span><span class="p">},</span>

<span class="w">    </span><span class="nt">&quot;activation_checkpointing&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="nt">&quot;partition_activations&quot;</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;cpu_checkpointing&quot;</span><span class="p">:</span><span class="w"> </span><span class="kc">false</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;contiguous_memory_optimization&quot;</span><span class="p">:</span><span class="w"> </span><span class="kc">false</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;number_checkpoints&quot;</span><span class="p">:</span><span class="w"> </span><span class="kc">null</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;synchronize_checkpoint_boundary&quot;</span><span class="p">:</span><span class="w"> </span><span class="kc">false</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;profile&quot;</span><span class="p">:</span><span class="w"> </span><span class="kc">false</span>
<span class="w">    </span><span class="p">}</span>
<span class="p">}</span>
</code></pre></div>

<h3 id="1243">12.4.3 性能基准测试</h3>
<p><strong>测试环境</strong>：</p>
<ul>
<li>模型：LLaVA-13B</li>
<li>硬件：8×A100-40G</li>
<li>数据：图文对，批大小 64</li>
</ul>
<p><strong>测试结果</strong>：</p>
<p>| 指标 | FSDP | DeepSpeed ZeRO-2 | DeepSpeed ZeRO-3 |</p>
<table>
<thead>
<tr>
<th>指标</th>
<th>FSDP</th>
<th>DeepSpeed ZeRO-2</th>
<th>DeepSpeed ZeRO-3</th>
</tr>
</thead>
<tbody>
<tr>
<td>吞吐量 (samples/s)</td>
<td>28.5</td>
<td>31.2</td>
<td>26.8</td>
</tr>
<tr>
<td>显存占用 (GB)</td>
<td>35.2</td>
<td>32.1</td>
<td>28.9</td>
</tr>
<tr>
<td>通信开销 (%)</td>
<td>18%</td>
<td>15%</td>
<td>22%</td>
</tr>
<tr>
<td>启动时间 (s)</td>
<td>45</td>
<td>62</td>
<td>78</td>
</tr>
<tr>
<td>Checkpoint 大小 (GB)</td>
<td>26</td>
<td>26</td>
<td>52 (分片)</td>
</tr>
</tbody>
</table>
<p><strong>性能分析代码</strong>：</p>
<div class="codehilite"><pre><span></span><code><span class="kn">import</span> <span class="nn">time</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">contextlib</span> <span class="kn">import</span> <span class="n">contextmanager</span>

<span class="nd">@contextmanager</span>
<span class="k">def</span> <span class="nf">profile_time</span><span class="p">(</span><span class="n">name</span><span class="p">):</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">synchronize</span><span class="p">()</span>
    <span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
    <span class="k">yield</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">synchronize</span><span class="p">()</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">start</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">s&quot;</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">benchmark_training_step</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">use_fsdp</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="n">metrics</span> <span class="o">=</span> <span class="p">{}</span>

    <span class="c1"># Forward</span>
    <span class="k">with</span> <span class="n">profile_time</span><span class="p">(</span><span class="s2">&quot;Forward&quot;</span><span class="p">):</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">loss</span>
    <span class="n">metrics</span><span class="p">[</span><span class="s1">&#39;loss&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>

    <span class="c1"># Backward</span>
    <span class="k">with</span> <span class="n">profile_time</span><span class="p">(</span><span class="s2">&quot;Backward&quot;</span><span class="p">):</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

    <span class="c1"># Optimizer step</span>
    <span class="k">with</span> <span class="n">profile_time</span><span class="p">(</span><span class="s2">&quot;Optimizer&quot;</span><span class="p">):</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

    <span class="c1"># 内存统计</span>
    <span class="n">metrics</span><span class="p">[</span><span class="s1">&#39;memory_allocated&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">memory_allocated</span><span class="p">()</span> <span class="o">/</span> <span class="mf">1e9</span>
    <span class="n">metrics</span><span class="p">[</span><span class="s1">&#39;memory_reserved&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">memory_reserved</span><span class="p">()</span> <span class="o">/</span> <span class="mf">1e9</span>

    <span class="k">if</span> <span class="n">use_fsdp</span><span class="p">:</span>
        <span class="c1"># FSDP 特定指标</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="s1">&#39;_fsdp_wrapped_module&#39;</span><span class="p">):</span>
            <span class="n">metrics</span><span class="p">[</span><span class="s1">&#39;communication_time&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">_communication_time</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># DeepSpeed 特定指标</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="s1">&#39;timer_names&#39;</span><span class="p">):</span>
            <span class="k">for</span> <span class="n">name</span> <span class="ow">in</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">timer_names</span><span class="p">:</span>
                <span class="n">metrics</span><span class="p">[</span><span class="sa">f</span><span class="s1">&#39;ds_</span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">timers</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">metrics</span>
</code></pre></div>

<h3 id="1244">12.4.4 选择建议与迁移指南</h3>
<p><strong>何时选择 FSDP</strong>：</p>
<ol>
<li>PyTorch 原生项目，不想引入额外依赖</li>
<li>模型相对简单，不需要复杂的并行策略</li>
<li>团队熟悉 PyTorch，学习成本低</li>
<li>需要与其他 PyTorch 生态工具集成</li>
</ol>
<p><strong>何时选择 DeepSpeed</strong>：</p>
<ol>
<li>超大模型（&gt;30B），需要极致优化</li>
<li>需要 Pipeline 并行等高级特性</li>
<li>混合训练环境，需要更细粒度控制</li>
<li>已有 DeepSpeed 经验积累</li>
</ol>
<p><strong>从 FSDP 迁移到 DeepSpeed</strong>：</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># 迁移检查清单</span>
<span class="n">migration_checklist</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;模型包装&quot;</span><span class="p">:</span> <span class="s2">&quot;FSDP(...) -&gt; deepspeed.initialize(...)&quot;</span><span class="p">,</span>
    <span class="s2">&quot;优化器&quot;</span><span class="p">:</span> <span class="s2">&quot;需要在 config 中配置，不能直接传入&quot;</span><span class="p">,</span>
    <span class="s2">&quot;梯度累积&quot;</span><span class="p">:</span> <span class="s2">&quot;自动处理 -&gt; 需要显式配置&quot;</span><span class="p">,</span>
    <span class="s2">&quot;Checkpoint&quot;</span><span class="p">:</span> <span class="s2">&quot;torch.save -&gt; model.save_checkpoint&quot;</span><span class="p">,</span>
    <span class="s2">&quot;混合精度&quot;</span><span class="p">:</span> <span class="s2">&quot;MixedPrecision -&gt; fp16/bf16 config&quot;</span><span class="p">,</span>
    <span class="s2">&quot;学习率调度&quot;</span><span class="p">:</span> <span class="s2">&quot;手动 -&gt; 配置文件&quot;</span><span class="p">,</span>
<span class="p">}</span>

<span class="c1"># 迁移示例</span>
<span class="k">def</span> <span class="nf">migrate_fsdp_to_deepspeed</span><span class="p">(</span><span class="n">fsdp_model</span><span class="p">,</span> <span class="n">fsdp_optimizer</span><span class="p">):</span>
    <span class="c1"># 1. 提取原始模型</span>
    <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">fsdp_model</span><span class="p">,</span> <span class="s1">&#39;module&#39;</span><span class="p">):</span>
        <span class="n">base_model</span> <span class="o">=</span> <span class="n">fsdp_model</span><span class="o">.</span><span class="n">module</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">base_model</span> <span class="o">=</span> <span class="n">fsdp_model</span>

    <span class="c1"># 2. 创建 DeepSpeed 配置</span>
    <span class="n">ds_config</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">&quot;train_batch_size&quot;</span><span class="p">:</span> <span class="n">world_size</span> <span class="o">*</span> <span class="n">batch_size</span><span class="p">,</span>
        <span class="s2">&quot;gradient_accumulation_steps&quot;</span><span class="p">:</span> <span class="n">grad_acc_steps</span><span class="p">,</span>
        <span class="s2">&quot;zero_optimization&quot;</span><span class="p">:</span> <span class="p">{</span>
            <span class="s2">&quot;stage&quot;</span><span class="p">:</span> <span class="mi">3</span> <span class="k">if</span> <span class="n">was_full_shard</span> <span class="k">else</span> <span class="mi">2</span><span class="p">,</span>
            <span class="c1"># 其他配置...</span>
        <span class="p">}</span>
    <span class="p">}</span>

    <span class="c1"># 3. 初始化 DeepSpeed</span>
    <span class="n">model</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">deepspeed</span><span class="o">.</span><span class="n">initialize</span><span class="p">(</span>
        <span class="n">model</span><span class="o">=</span><span class="n">base_model</span><span class="p">,</span>
        <span class="n">config</span><span class="o">=</span><span class="n">ds_config</span><span class="p">,</span>
        <span class="n">model_parameters</span><span class="o">=</span><span class="n">base_model</span><span class="o">.</span><span class="n">parameters</span><span class="p">()</span>
    <span class="p">)</span>

    <span class="k">return</span> <span class="n">model</span><span class="p">,</span> <span class="n">optimizer</span>
</code></pre></div>

<h3 id="1245">12.4.5 常见问题与解决方案</h3>
<p><strong>问题 1：FSDP OOM 但 DeepSpeed 正常</strong></p>
<p>原因：FSDP 的 all-gather 操作可能导致瞬时显存峰值。</p>
<p>解决方案：</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># FSDP 限制 all-gather</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">FSDP</span><span class="p">(</span>
    <span class="n">model</span><span class="p">,</span>
    <span class="n">limit_all_gathers</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">forward_prefetch</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>  <span class="c1"># 前向预取</span>
    <span class="n">backward_prefetch</span><span class="o">=</span><span class="n">BackwardPrefetch</span><span class="o">.</span><span class="n">BACKWARD_PRE</span><span class="p">,</span>
<span class="p">)</span>
</code></pre></div>

<p><strong>问题 2：DeepSpeed 训练速度慢</strong></p>
<p>原因：ZeRO-3 的参数收集开销。</p>
<p>解决方案：</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># 优化 ZeRO-3 配置</span>
<span class="s2">&quot;zero_optimization&quot;</span><span class="p">:</span> <span class="p">{</span>
    <span class="s2">&quot;stage&quot;</span><span class="p">:</span> <span class="mi">3</span><span class="p">,</span>
    <span class="s2">&quot;stage3_max_live_parameters&quot;</span><span class="p">:</span> <span class="mf">2e9</span><span class="p">,</span>  <span class="c1"># 增加缓存</span>
    <span class="s2">&quot;stage3_max_reuse_distance&quot;</span><span class="p">:</span> <span class="mf">2e9</span><span class="p">,</span>
    <span class="s2">&quot;stage3_prefetch_bucket_size&quot;</span><span class="p">:</span> <span class="mf">2e8</span><span class="p">,</span>  <span class="c1"># 增加预取</span>
<span class="p">}</span>
</code></pre></div>

<p><strong>问题 3：Checkpoint 不兼容</strong></p>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">convert_checkpoint</span><span class="p">(</span><span class="n">checkpoint_path</span><span class="p">,</span> <span class="n">from_format</span><span class="o">=</span><span class="s2">&quot;fsdp&quot;</span><span class="p">,</span> <span class="n">to_format</span><span class="o">=</span><span class="s2">&quot;deepspeed&quot;</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;转换不同格式的 checkpoint&quot;&quot;&quot;</span>

    <span class="k">if</span> <span class="n">from_format</span> <span class="o">==</span> <span class="s2">&quot;fsdp&quot;</span> <span class="ow">and</span> <span class="n">to_format</span> <span class="o">==</span> <span class="s2">&quot;deepspeed&quot;</span><span class="p">:</span>
        <span class="c1"># FSDP -&gt; DeepSpeed</span>
        <span class="n">state_dict</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">checkpoint_path</span><span class="p">)</span>

        <span class="c1"># FSDP 可能有 _fsdp 前缀</span>
        <span class="n">new_state_dict</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="n">new_key</span> <span class="o">=</span> <span class="n">key</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot;_fsdp_wrapped_module.&quot;</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">)</span>
            <span class="n">new_key</span> <span class="o">=</span> <span class="n">new_key</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot;_fpw_module.&quot;</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">)</span>
            <span class="n">new_state_dict</span><span class="p">[</span><span class="n">new_key</span><span class="p">]</span> <span class="o">=</span> <span class="n">value</span>

        <span class="c1"># DeepSpeed 期望的格式</span>
        <span class="n">ds_checkpoint</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">&quot;module&quot;</span><span class="p">:</span> <span class="n">new_state_dict</span><span class="p">,</span>
            <span class="s2">&quot;epoch&quot;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
            <span class="s2">&quot;global_step&quot;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
        <span class="p">}</span>

        <span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">ds_checkpoint</span><span class="p">,</span> <span class="n">checkpoint_path</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot;.pt&quot;</span><span class="p">,</span> <span class="s2">&quot;_ds.pt&quot;</span><span class="p">))</span>

    <span class="k">elif</span> <span class="n">from_format</span> <span class="o">==</span> <span class="s2">&quot;deepspeed&quot;</span> <span class="ow">and</span> <span class="n">to_format</span> <span class="o">==</span> <span class="s2">&quot;fsdp&quot;</span><span class="p">:</span>
        <span class="c1"># DeepSpeed -&gt; FSDP</span>
        <span class="c1"># DeepSpeed ZeRO-3 需要先收集分片</span>
        <span class="kn">from</span> <span class="nn">deepspeed.utils.zero_to_fp32</span> <span class="kn">import</span> <span class="n">convert_zero_checkpoint_to_fp32_state_dict</span>

        <span class="n">convert_zero_checkpoint_to_fp32_state_dict</span><span class="p">(</span>
            <span class="n">checkpoint_path</span><span class="p">,</span>
            <span class="n">checkpoint_path</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot;ds&quot;</span><span class="p">,</span> <span class="s2">&quot;fsdp.pt&quot;</span><span class="p">)</span>
        <span class="p">)</span>
</code></pre></div>

<h2 id="_1">本章小结</h2>
<p>多机多卡训练是 VLM 扩展的必经之路，但也充满挑战。本章系统介绍了分布式训练中最常见的四类问题：</p>
<ol>
<li>
<p><strong>NCCL 通信错误</strong>：掌握了快速诊断流程、环境变量配置和网络优化策略。记住，大部分 NCCL 错误都可以通过正确的环境变量和网络配置解决。</p>
</li>
<li>
<p><strong>进程同步与死锁</strong>：理解了分布式训练的同步机制，学会了识别和避免死锁的典型场景。关键是确保所有进程执行相同的集合通信操作。</p>
</li>
<li>
<p><strong>异构 GPU 训练</strong>：了解了混合 GPU 训练的挑战和解决方案。核心思想是根据硬件能力动态调整批大小和梯度累积策略。</p>
</li>
<li>
<p><strong>FSDP vs DeepSpeed</strong>：通过实战对比，明确了两种框架的优劣和适用场景。FSDP 更简单直接，DeepSpeed 功能更丰富。</p>
</li>
</ol>
<p><strong>关键公式回顾</strong>：</p>
<p>有效批大小计算：
$$\text{Effective Batch Size} = \text{World Size} \times \text{Micro Batch Size} \times \text{Gradient Accumulation Steps}$$
通信时间估算：
$$T_{\text{comm}} = \frac{\text{Data Size}}{\text{Bandwidth}} + \text{Latency} \times \text{Num Operations}$$
显存占用（ZeRO-3）：
$$M_{\text{per GPU}} = \frac{M_{\text{model}} + M_{\text{optimizer}} + M_{\text{gradients}}}{\text{World Size}} + M_{\text{activations}}$$</p>
<h2 id="_2">练习题</h2>
<h3 id="_3">基础题</h3>
<p><strong>练习 12.1：NCCL 环境变量配置</strong></p>
<p>你的 8 卡 V100 服务器训练时经常出现 NCCL timeout，请写出完整的环境变量配置来优化通信。</p>
<p>💡 <strong>提示</strong>：考虑超时时间、日志级别、P2P 通信和网络接口选择。</p>
<details>
<summary>📝 参考答案</summary>
<div class="codehilite"><pre><span></span><code><span class="c1"># 增加超时时间</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">NCCL_TIMEOUT</span><span class="o">=</span><span class="m">7200</span><span class="w">  </span><span class="c1"># 2小时</span>

<span class="c1"># 启用调试日志</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">NCCL_DEBUG</span><span class="o">=</span>INFO
<span class="nb">export</span><span class="w"> </span><span class="nv">NCCL_DEBUG_SUBSYS</span><span class="o">=</span>INIT,GRAPH

<span class="c1"># 优化 P2P 通信</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">NCCL_P2P_DISABLE</span><span class="o">=</span><span class="m">0</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">NCCL_P2P_LEVEL</span><span class="o">=</span>NVL

<span class="c1"># 指定网络接口</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">NCCL_SOCKET_IFNAME</span><span class="o">=</span>eth0
<span class="nb">export</span><span class="w"> </span><span class="nv">NCCL_IB_DISABLE</span><span class="o">=</span><span class="m">1</span><span class="w">  </span><span class="c1"># 如果没有 InfiniBand</span>

<span class="c1"># 优化缓冲区</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">NCCL_BUFFSIZE</span><span class="o">=</span><span class="m">8388608</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">NCCL_NTHREADS</span><span class="o">=</span><span class="m">256</span>

<span class="c1"># 树形算法优化</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">NCCL_TREE_THRESHOLD</span><span class="o">=</span><span class="m">0</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">NCCL_ALGO</span><span class="o">=</span>Tree
</code></pre></div>

<p>这套配置增加了超时容忍度，启用了详细日志便于调试，优化了 P2P 和网络通信，适合大多数 V100 集群。</p>
</details>
<p><strong>练习 12.2：死锁诊断</strong></p>
<p>以下代码在 4 卡训练时会死锁，请找出原因并修复：</p>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">validation_step</span><span class="p">(</span><span class="n">rank</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">val_loader</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">rank</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
        <span class="n">total_loss</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">val_loader</span><span class="p">:</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span><span class="o">.</span><span class="n">loss</span>
            <span class="n">total_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        <span class="n">avg_loss</span> <span class="o">=</span> <span class="n">total_loss</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">val_loader</span><span class="p">)</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">broadcast</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">avg_loss</span><span class="p">])</span><span class="o">.</span><span class="n">cuda</span><span class="p">(),</span> <span class="n">src</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">avg_loss</span>
</code></pre></div>

<p>💡 <strong>提示</strong>：考虑所有进程的执行路径。</p>
<details>
<summary>📝 参考答案</summary>
<p>问题：只有 rank 0 执行 broadcast，其他进程没有对应的 broadcast 调用，导致死锁。</p>
<p>修复方案：</p>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">validation_step</span><span class="p">(</span><span class="n">rank</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">val_loader</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">rank</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
        <span class="n">total_loss</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">val_loader</span><span class="p">:</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span><span class="o">.</span><span class="n">loss</span>
            <span class="n">total_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        <span class="n">avg_loss</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">total_loss</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">val_loader</span><span class="p">)])</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">avg_loss</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>

    <span class="c1"># 所有进程都参与 broadcast</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">broadcast</span><span class="p">(</span><span class="n">avg_loss</span><span class="p">,</span> <span class="n">src</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">avg_loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
</code></pre></div>

</details>
<p><strong>练习 12.3：异构 GPU 批大小计算</strong></p>
<p>你有 2 张 A100-80G 和 2 张 V100-32G，目标是总批大小 64。请设计每个 GPU 的 micro batch size 和梯度累积步数。</p>
<p>💡 <strong>提示</strong>：A100 的计算能力约是 V100 的 2.5 倍。</p>
<details>
<summary>📝 参考答案</summary>
<p>根据显存和计算能力分配：</p>
<div class="codehilite"><pre><span></span><code><span class="n">config</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;A100-80G&quot;</span><span class="p">:</span> <span class="p">{</span>
        <span class="s2">&quot;micro_batch_size&quot;</span><span class="p">:</span> <span class="mi">8</span><span class="p">,</span>  <span class="c1"># 充分利用显存</span>
        <span class="s2">&quot;gradient_accumulation_steps&quot;</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span>
        <span class="s2">&quot;effective_batch_per_gpu&quot;</span><span class="p">:</span> <span class="mi">16</span>
    <span class="p">},</span>
    <span class="s2">&quot;V100-32G&quot;</span><span class="p">:</span> <span class="p">{</span>
        <span class="s2">&quot;micro_batch_size&quot;</span><span class="p">:</span> <span class="mi">3</span><span class="p">,</span>  <span class="c1"># 显存限制</span>
        <span class="s2">&quot;gradient_accumulation_steps&quot;</span><span class="p">:</span> <span class="mi">5</span><span class="p">,</span>  <span class="c1"># 补偿小批次</span>
        <span class="s2">&quot;effective_batch_per_gpu&quot;</span><span class="p">:</span> <span class="mi">15</span>
    <span class="p">}</span>
<span class="p">}</span>

<span class="c1"># 验证：</span>
<span class="c1"># 2 * 16 (A100) + 2 * 15 (V100) = 62 ≈ 64</span>
<span class="c1"># 可以通过调整最后一个 batch 来精确达到 64</span>
</code></pre></div>

<p>这种配置平衡了显存使用和计算效率，避免了木桶效应。</p>
</details>
<h3 id="_4">挑战题</h3>
<p><strong>练习 12.4：FSDP 内存优化</strong></p>
<p>你的 LLaVA-34B 模型在 8×A100-40G 上用 FSDP 训练时 OOM。请提供完整的优化方案，包括配置和代码。</p>
<p>💡 <strong>提示</strong>：考虑 CPU offload、激活检查点、分片策略等。</p>
<details>
<summary>📝 参考答案</summary>
<div class="codehilite"><pre><span></span><code><span class="kn">from</span> <span class="nn">torch.distributed.fsdp</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">FullyShardedDataParallel</span> <span class="k">as</span> <span class="n">FSDP</span><span class="p">,</span>
    <span class="n">MixedPrecision</span><span class="p">,</span>
    <span class="n">BackwardPrefetch</span><span class="p">,</span>
    <span class="n">ShardingStrategy</span><span class="p">,</span>
    <span class="n">CPUOffload</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">torch.distributed.algorithms._checkpoint.checkpoint_wrapper</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">checkpoint_wrapper</span><span class="p">,</span>
    <span class="n">CheckpointImpl</span><span class="p">,</span>
    <span class="n">apply_activation_checkpointing</span>
<span class="p">)</span>

<span class="k">def</span> <span class="nf">optimize_fsdp_for_large_vlm</span><span class="p">(</span><span class="n">model</span><span class="p">):</span>
    <span class="c1"># 1. 激活检查点</span>
    <span class="n">check_fn</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">m</span><span class="p">:</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">TransformerBlock</span><span class="p">)</span>
    <span class="n">apply_activation_checkpointing</span><span class="p">(</span>
        <span class="n">model</span><span class="p">,</span>
        <span class="n">checkpoint_wrapper_fn</span><span class="o">=</span><span class="n">functools</span><span class="o">.</span><span class="n">partial</span><span class="p">(</span>
            <span class="n">checkpoint_wrapper</span><span class="p">,</span>
            <span class="n">checkpoint_impl</span><span class="o">=</span><span class="n">CheckpointImpl</span><span class="o">.</span><span class="n">NO_REENTRANT</span>
        <span class="p">),</span>
        <span class="n">check_fn</span><span class="o">=</span><span class="n">check_fn</span>
    <span class="p">)</span>

    <span class="c1"># 2. 混合精度 - 使用 BF16</span>
    <span class="n">mp_policy</span> <span class="o">=</span> <span class="n">MixedPrecision</span><span class="p">(</span>
        <span class="n">param_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">,</span>
        <span class="n">reduce_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>  <span class="c1"># 梯度用 FP32 更稳定</span>
        <span class="n">buffer_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span>
    <span class="p">)</span>

    <span class="c1"># 3. CPU Offload</span>
    <span class="n">cpu_offload</span> <span class="o">=</span> <span class="n">CPUOffload</span><span class="p">(</span><span class="n">offload_params</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="c1"># 4. 优化的分片策略</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">FSDP</span><span class="p">(</span>
        <span class="n">model</span><span class="p">,</span>
        <span class="n">auto_wrap_policy</span><span class="o">=</span><span class="n">functools</span><span class="o">.</span><span class="n">partial</span><span class="p">(</span>
            <span class="n">transformer_auto_wrap_policy</span><span class="p">,</span>
            <span class="n">transformer_layer_cls</span><span class="o">=</span><span class="p">{</span><span class="n">TransformerBlock</span><span class="p">},</span>
        <span class="p">),</span>
        <span class="n">mixed_precision</span><span class="o">=</span><span class="n">mp_policy</span><span class="p">,</span>
        <span class="n">sharding_strategy</span><span class="o">=</span><span class="n">ShardingStrategy</span><span class="o">.</span><span class="n">HYBRID_SHARD</span><span class="p">,</span>  <span class="c1"># 混合分片</span>
        <span class="n">cpu_offload</span><span class="o">=</span><span class="n">cpu_offload</span><span class="p">,</span>
        <span class="n">backward_prefetch</span><span class="o">=</span><span class="n">BackwardPrefetch</span><span class="o">.</span><span class="n">BACKWARD_PRE</span><span class="p">,</span>
        <span class="n">limit_all_gathers</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">use_orig_params</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">sync_module_states</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">forward_prefetch</span><span class="o">=</span><span class="kc">True</span>
    <span class="p">)</span>

    <span class="c1"># 5. 梯度累积 + 小 batch</span>
    <span class="n">micro_batch_size</span> <span class="o">=</span> <span class="mi">1</span>  <span class="c1"># 极小批次</span>
    <span class="n">gradient_accumulation_steps</span> <span class="o">=</span> <span class="mi">32</span>

    <span class="k">return</span> <span class="n">model</span><span class="p">,</span> <span class="n">micro_batch_size</span><span class="p">,</span> <span class="n">gradient_accumulation_steps</span>
</code></pre></div>

<p>这个方案通过激活检查点减少 50% 激活值内存，CPU offload 节省参数内存，混合分片优化通信，可以成功训练 34B 模型。</p>
</details>
<p><strong>练习 12.5：分布式调试工具设计</strong></p>
<p>设计一个调试工具，能够实时监控多机训练的进程状态、通信时间和潜在死锁。</p>
<p>💡 <strong>提示</strong>：考虑心跳机制、通信 hook 和异常检测。</p>
<details>
<summary>📝 参考答案</summary>
<div class="codehilite"><pre><span></span><code><span class="kn">import</span> <span class="nn">threading</span>
<span class="kn">import</span> <span class="nn">time</span>
<span class="kn">import</span> <span class="nn">torch.distributed</span> <span class="k">as</span> <span class="nn">dist</span>
<span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">deque</span>
<span class="kn">from</span> <span class="nn">datetime</span> <span class="kn">import</span> <span class="n">datetime</span>

<span class="k">class</span> <span class="nc">DistributedDebugger</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">rank</span><span class="p">,</span> <span class="n">world_size</span><span class="p">,</span> <span class="n">check_interval</span><span class="o">=</span><span class="mi">10</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">rank</span> <span class="o">=</span> <span class="n">rank</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">world_size</span> <span class="o">=</span> <span class="n">world_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">check_interval</span> <span class="o">=</span> <span class="n">check_interval</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">comm_times</span> <span class="o">=</span> <span class="n">deque</span><span class="p">(</span><span class="n">maxlen</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">last_heartbeat</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">deadlock_threshold</span> <span class="o">=</span> <span class="mi">300</span>  <span class="c1"># 5分钟</span>

        <span class="c1"># 注册通信 hook</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_register_comm_hooks</span><span class="p">()</span>

        <span class="c1"># 启动监控线程</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_start_monitor</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">_register_comm_hooks</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;注册通信钩子来测量时间&quot;&quot;&quot;</span>
        <span class="n">original_all_reduce</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">all_reduce</span>

        <span class="k">def</span> <span class="nf">timed_all_reduce</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
            <span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
            <span class="n">result</span> <span class="o">=</span> <span class="n">original_all_reduce</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
            <span class="n">elapsed</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">start</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">comm_times</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">elapsed</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">elapsed</span> <span class="o">&gt;</span> <span class="mi">10</span><span class="p">:</span>  <span class="c1"># 超过10秒警告</span>
                <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;[Rank </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">rank</span><span class="si">}</span><span class="s2">] WARNING: all_reduce took </span><span class="si">{</span><span class="n">elapsed</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">s&quot;</span><span class="p">)</span>

            <span class="k">return</span> <span class="n">result</span>

        <span class="n">dist</span><span class="o">.</span><span class="n">all_reduce</span> <span class="o">=</span> <span class="n">timed_all_reduce</span>

    <span class="k">def</span> <span class="nf">_start_monitor</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;启动监控线程&quot;&quot;&quot;</span>
        <span class="k">def</span> <span class="nf">monitor</span><span class="p">():</span>
            <span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
                <span class="n">time</span><span class="o">.</span><span class="n">sleep</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">check_interval</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_check_health</span><span class="p">()</span>

        <span class="n">thread</span> <span class="o">=</span> <span class="n">threading</span><span class="o">.</span><span class="n">Thread</span><span class="p">(</span><span class="n">target</span><span class="o">=</span><span class="n">monitor</span><span class="p">,</span> <span class="n">daemon</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">thread</span><span class="o">.</span><span class="n">start</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">_check_health</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;健康检查&quot;&quot;&quot;</span>
        <span class="n">current_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>

        <span class="c1"># 1. 检查心跳</span>
        <span class="k">if</span> <span class="n">current_time</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">last_heartbeat</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">deadlock_threshold</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_report_deadlock</span><span class="p">()</span>

        <span class="c1"># 2. 统计通信时间</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">comm_times</span><span class="p">:</span>
            <span class="n">avg_comm</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">comm_times</span><span class="p">)</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">comm_times</span><span class="p">)</span>
            <span class="n">max_comm</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">comm_times</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;[Rank </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">rank</span><span class="si">}</span><span class="s2">] Comm stats: avg=</span><span class="si">{</span><span class="n">avg_comm</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">s, max=</span><span class="si">{</span><span class="n">max_comm</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">s&quot;</span><span class="p">)</span>

        <span class="c1"># 3. 内存状态</span>
        <span class="n">mem_alloc</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">memory_allocated</span><span class="p">()</span> <span class="o">/</span> <span class="mf">1e9</span>
        <span class="n">mem_reserved</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">memory_reserved</span><span class="p">()</span> <span class="o">/</span> <span class="mf">1e9</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;[Rank </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">rank</span><span class="si">}</span><span class="s2">] Memory: </span><span class="si">{</span><span class="n">mem_alloc</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">mem_reserved</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> GB&quot;</span><span class="p">)</span>

        <span class="c1"># 4. 同步检查（可选）</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_sync_check</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">_sync_check</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;检查所有进程是否同步&quot;&quot;&quot;</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">check_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">rank</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;cuda&#39;</span><span class="p">)</span>
            <span class="n">check_list</span> <span class="o">=</span> <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">check_tensor</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">world_size</span><span class="p">)]</span>
            <span class="n">dist</span><span class="o">.</span><span class="n">all_gather</span><span class="p">(</span><span class="n">check_list</span><span class="p">,</span> <span class="n">check_tensor</span><span class="p">,</span> <span class="n">timeout</span><span class="o">=</span><span class="n">datetime</span><span class="o">.</span><span class="n">timedelta</span><span class="p">(</span><span class="n">seconds</span><span class="o">=</span><span class="mi">30</span><span class="p">))</span>

            <span class="c1"># 验证所有进程都响应</span>
            <span class="n">ranks</span> <span class="o">=</span> <span class="p">[</span><span class="n">t</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">check_list</span><span class="p">]</span>
            <span class="k">if</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">ranks</span><span class="p">)</span> <span class="o">!=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">world_size</span><span class="p">)):</span>
                <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;[Rank </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">rank</span><span class="si">}</span><span class="s2">] ERROR: Missing ranks in sync check: </span><span class="si">{</span><span class="n">ranks</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;[Rank </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">rank</span><span class="si">}</span><span class="s2">] Sync check failed: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_report_deadlock</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;报告可能的死锁&quot;&quot;&quot;</span>
        <span class="kn">import</span> <span class="nn">traceback</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;[Rank </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">rank</span><span class="si">}</span><span class="s2">] DEADLOCK WARNING!&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Stack trace:&quot;</span><span class="p">)</span>
        <span class="n">traceback</span><span class="o">.</span><span class="n">print_stack</span><span class="p">()</span>

        <span class="c1"># 可选：触发 core dump</span>
        <span class="kn">import</span> <span class="nn">signal</span>
        <span class="n">os</span><span class="o">.</span><span class="n">kill</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">getpid</span><span class="p">(),</span> <span class="n">signal</span><span class="o">.</span><span class="n">SIGABRT</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">update_heartbeat</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;更新心跳时间（在训练循环中调用）&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">last_heartbeat</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>

<span class="c1"># 使用示例</span>
<span class="n">debugger</span> <span class="o">=</span> <span class="n">DistributedDebugger</span><span class="p">(</span><span class="n">rank</span><span class="p">,</span> <span class="n">world_size</span><span class="p">)</span>

<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">dataloader</span><span class="p">:</span>
        <span class="n">debugger</span><span class="o">.</span><span class="n">update_heartbeat</span><span class="p">()</span>  <span class="c1"># 更新心跳</span>
        <span class="c1"># 训练代码...</span>
</code></pre></div>

<p>这个调试器提供了实时监控、死锁检测、通信性能分析等功能，能够快速定位分布式训练问题。</p>
</details>
<p><strong>练习 12.6：混合并行策略设计</strong></p>
<p>为 VLM-65B 模型设计一个结合 FSDP、Pipeline 并行和 Tensor 并行的训练方案，硬件是 16×A100-80G（2 节点）。</p>
<p>💡 <strong>提示</strong>：考虑不同并行策略的通信模式和内存占用。</p>
<details>
<summary>📝 参考答案</summary>
<div class="codehilite"><pre><span></span><code><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">混合并行策略设计：</span>

<span class="sd">- Tensor Parallel (TP): 4-way (节点内)</span>
<span class="sd">- Pipeline Parallel (PP): 2-way (跨节点)</span>
<span class="sd">- Data Parallel with FSDP: 2-way</span>

<span class="sd">总并行度: 4 × 2 × 2 = 16</span>
<span class="sd">&quot;&quot;&quot;</span>

<span class="k">class</span> <span class="nc">HybridParallelVLM</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model_config</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">world_size</span> <span class="o">=</span> <span class="mi">16</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tp_size</span> <span class="o">=</span> <span class="mi">4</span>  <span class="c1"># 节点内 tensor parallel</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pp_size</span> <span class="o">=</span> <span class="mi">2</span>  <span class="c1"># pipeline stages</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dp_size</span> <span class="o">=</span> <span class="mi">2</span>  <span class="c1"># data parallel groups</span>

        <span class="c1"># 初始化进程组</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_init_process_groups</span><span class="p">()</span>

        <span class="c1"># 构建模型</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_build_model</span><span class="p">(</span><span class="n">model_config</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_init_process_groups</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;创建不同的进程组&quot;&quot;&quot;</span>
        <span class="n">rank</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">get_rank</span><span class="p">()</span>

        <span class="c1"># Tensor Parallel 组 (同节点内)</span>
        <span class="n">tp_ranks</span> <span class="o">=</span> <span class="p">[</span><span class="n">rank</span> <span class="o">//</span> <span class="mi">4</span> <span class="o">*</span> <span class="mi">4</span> <span class="o">+</span> <span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">4</span><span class="p">)]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tp_group</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">new_group</span><span class="p">(</span><span class="n">tp_ranks</span><span class="p">)</span>

        <span class="c1"># Pipeline Parallel 组 (跨节点)</span>
        <span class="n">pp_ranks</span> <span class="o">=</span> <span class="p">[</span><span class="n">rank</span> <span class="o">%</span> <span class="mi">4</span> <span class="o">+</span> <span class="n">i</span> <span class="o">*</span> <span class="mi">8</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">)]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pp_group</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">new_group</span><span class="p">(</span><span class="n">pp_ranks</span><span class="p">)</span>

        <span class="c1"># Data Parallel 组</span>
        <span class="n">dp_ranks</span> <span class="o">=</span> <span class="p">[</span><span class="n">rank</span> <span class="o">//</span> <span class="mi">8</span> <span class="o">*</span> <span class="mi">8</span> <span class="o">+</span> <span class="n">rank</span> <span class="o">%</span> <span class="mi">4</span> <span class="o">+</span> <span class="n">i</span> <span class="o">*</span> <span class="mi">4</span> 
                   <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">)]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dp_group</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">new_group</span><span class="p">(</span><span class="n">dp_ranks</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_build_model</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;构建混合并行模型&quot;&quot;&quot;</span>
        <span class="n">rank</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">get_rank</span><span class="p">()</span>

        <span class="c1"># 1. 模型分层 (Pipeline)</span>
        <span class="k">if</span> <span class="n">rank</span> <span class="o">&lt;</span> <span class="mi">8</span><span class="p">:</span>  <span class="c1"># 第一个 pipeline stage</span>
            <span class="n">layers</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_first_stage_layers</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>  <span class="c1"># 第二个 pipeline stage</span>
            <span class="n">layers</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_second_stage_layers</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>

        <span class="c1"># 2. Tensor Parallel</span>
        <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="n">layers</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">layer</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">):</span>
                <span class="n">layer</span> <span class="o">=</span> <span class="n">ColumnParallelLinear</span><span class="p">(</span><span class="n">layer</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">tp_group</span><span class="p">)</span>
            <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">layer</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">):</span>
                <span class="n">layer</span> <span class="o">=</span> <span class="n">ParallelEmbedding</span><span class="p">(</span><span class="n">layer</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">tp_group</span><span class="p">)</span>

        <span class="c1"># 3. FSDP 包装</span>
        <span class="n">model</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="o">*</span><span class="n">layers</span><span class="p">)</span>
        <span class="n">model</span> <span class="o">=</span> <span class="n">FSDP</span><span class="p">(</span>
            <span class="n">model</span><span class="p">,</span>
            <span class="n">process_group</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dp_group</span><span class="p">,</span>
            <span class="n">sharding_strategy</span><span class="o">=</span><span class="n">ShardingStrategy</span><span class="o">.</span><span class="n">FULL_SHARD</span><span class="p">,</span>
            <span class="n">mixed_precision</span><span class="o">=</span><span class="n">MixedPrecision</span><span class="p">(</span>
                <span class="n">param_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">,</span>
                <span class="n">reduce_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span>
            <span class="p">),</span>
            <span class="n">limit_all_gathers</span><span class="o">=</span><span class="kc">True</span>
        <span class="p">)</span>

        <span class="k">return</span> <span class="n">model</span>

    <span class="k">def</span> <span class="nf">train_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;混合并行训练步骤&quot;&quot;&quot;</span>
        <span class="c1"># Pipeline parallel 的 micro-batching</span>
        <span class="n">micro_batches</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_split_batch</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">pp_size</span><span class="p">)</span>

        <span class="n">losses</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">micro_batch</span> <span class="ow">in</span> <span class="n">micro_batches</span><span class="p">:</span>
            <span class="c1"># Forward (with pipeline)</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_first_stage</span><span class="p">():</span>
                <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">(</span><span class="n">micro_batch</span><span class="p">)</span>
                <span class="c1"># 发送到下一个 stage</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_send_activations</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target_stage</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="c1"># 接收前一个 stage 的激活</span>
                <span class="n">activations</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_recv_activations</span><span class="p">(</span><span class="n">source_stage</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
                <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">(</span><span class="n">activations</span><span class="p">)</span>
                <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">compute_loss</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">micro_batch</span><span class="p">[</span><span class="s1">&#39;labels&#39;</span><span class="p">])</span>
                <span class="n">losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>

            <span class="c1"># Backward (reverse pipeline)</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_last_stage</span><span class="p">():</span>
                <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
                <span class="c1"># 发送梯度到前一个 stage</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_send_gradients</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="c1"># 接收梯度</span>
                <span class="n">gradients</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_recv_gradients</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
                <span class="n">output</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">gradients</span><span class="p">)</span>

        <span class="c1"># FSDP 会自动处理梯度同步</span>
        <span class="k">return</span> <span class="nb">sum</span><span class="p">(</span><span class="n">losses</span><span class="p">)</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">losses</span><span class="p">)</span>

<span class="c1"># 配置示例</span>
<span class="n">config</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;model&quot;</span><span class="p">:</span> <span class="p">{</span>
        <span class="s2">&quot;hidden_size&quot;</span><span class="p">:</span> <span class="mi">8192</span><span class="p">,</span>
        <span class="s2">&quot;num_layers&quot;</span><span class="p">:</span> <span class="mi">80</span><span class="p">,</span>
        <span class="s2">&quot;num_heads&quot;</span><span class="p">:</span> <span class="mi">64</span><span class="p">,</span>
        <span class="s2">&quot;vocab_size&quot;</span><span class="p">:</span> <span class="mi">32000</span>
    <span class="p">},</span>
    <span class="s2">&quot;training&quot;</span><span class="p">:</span> <span class="p">{</span>
        <span class="s2">&quot;micro_batch_size&quot;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
        <span class="s2">&quot;gradient_accumulation&quot;</span><span class="p">:</span> <span class="mi">8</span><span class="p">,</span>
        <span class="s2">&quot;optimizer&quot;</span><span class="p">:</span> <span class="s2">&quot;AdamW&quot;</span><span class="p">,</span>
        <span class="s2">&quot;lr&quot;</span><span class="p">:</span> <span class="mf">1e-4</span>
    <span class="p">}</span>
<span class="p">}</span>

<span class="c1"># 内存估算</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">每个 GPU 的模型参数：65B / 16 = 4B 参数</span>
<span class="sd">FP16 存储：4B * 2 bytes = 8GB</span>
<span class="sd">优化器状态 (AdamW)：8GB * 2 = 16GB</span>
<span class="sd">激活值 (with checkpointing)：~20GB</span>
<span class="sd">总计：~44GB &lt; 80GB (安全)</span>
<span class="sd">&quot;&quot;&quot;</span>
</code></pre></div>

<p>这个方案通过三种并行策略的组合，实现了 65B 模型在 16 卡上的高效训练，每种并行策略都针对其最适合的维度进行优化。</p>
</details>
<p><strong>练习 12.7：通信瓶颈分析</strong></p>
<p>你的训练在 scaling 到 32 卡后，效率从 8 卡的 90% 下降到 60%。请分析可能的原因并提供优化方案。</p>
<p>💡 <strong>提示</strong>：考虑通信拓扑、梯度同步策略和数据加载。</p>
<details>
<summary>📝 参考答案</summary>
<p>可能原因分析：</p>
<ol>
<li>
<p><strong>通信瓶颈增加</strong>
   - All-Reduce 时间 ∝ log(N) × 数据量
   - 32卡的通信轮数比8卡多</p>
</li>
<li>
<p><strong>网络拓扑不优化</strong>
   - 跨节点通信带宽受限
   - PCIe/NVLink 拓扑不均衡</p>
</li>
<li>
<p><strong>同步开销</strong>
   - Barrier 等待时间增加
   - 数据加载不均衡加剧</p>
</li>
</ol>
<p>优化方案：</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># 1. 梯度压缩</span>
<span class="kn">from</span> <span class="nn">torch.distributed.algorithms.ddp_comm_hooks</span> <span class="kn">import</span> <span class="n">default_hooks</span>

<span class="n">model</span><span class="o">.</span><span class="n">register_comm_hook</span><span class="p">(</span>
    <span class="n">state</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">hook</span><span class="o">=</span><span class="n">default_hooks</span><span class="o">.</span><span class="n">fp16_compress_hook</span>
<span class="p">)</span>

<span class="c1"># 2. 分层 All-Reduce</span>
<span class="k">def</span> <span class="nf">hierarchical_allreduce</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">groups</span><span class="p">):</span>
    <span class="c1"># 节点内先同步</span>
    <span class="n">dist</span><span class="o">.</span><span class="n">all_reduce</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="n">groups</span><span class="p">[</span><span class="s1">&#39;intra_node&#39;</span><span class="p">])</span>

    <span class="c1"># 节点间同步（仅 master）</span>
    <span class="k">if</span> <span class="n">is_node_master</span><span class="p">():</span>
        <span class="n">dist</span><span class="o">.</span><span class="n">all_reduce</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="n">groups</span><span class="p">[</span><span class="s1">&#39;inter_node&#39;</span><span class="p">])</span>

    <span class="c1"># 节点内广播</span>
    <span class="n">dist</span><span class="o">.</span><span class="n">broadcast</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">src</span><span class="o">=</span><span class="n">node_master_rank</span><span class="p">,</span> 
                  <span class="n">group</span><span class="o">=</span><span class="n">groups</span><span class="p">[</span><span class="s1">&#39;intra_node&#39;</span><span class="p">])</span>

<span class="c1"># 3. 梯度累积增加</span>
<span class="n">gradient_accumulation_steps</span> <span class="o">=</span> <span class="mi">4</span>  <span class="c1"># 8卡</span>
<span class="n">gradient_accumulation_steps</span> <span class="o">=</span> <span class="mi">8</span>  <span class="c1"># 32卡，减少同步频率</span>

<span class="c1"># 4. 异步数据预取</span>
<span class="k">class</span> <span class="nc">AsyncDataLoader</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dataset</span><span class="p">,</span> <span class="n">num_workers</span><span class="o">=</span><span class="mi">8</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">queue</span> <span class="o">=</span> <span class="n">Queue</span><span class="p">(</span><span class="n">maxsize</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">workers</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_workers</span><span class="p">):</span>
            <span class="n">w</span> <span class="o">=</span> <span class="n">Process</span><span class="p">(</span><span class="n">target</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_worker</span><span class="p">,</span> <span class="n">args</span><span class="o">=</span><span class="p">(</span><span class="n">dataset</span><span class="p">,))</span>
            <span class="n">w</span><span class="o">.</span><span class="n">start</span><span class="p">()</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">workers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_worker</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dataset</span><span class="p">):</span>
        <span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
            <span class="n">batch</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">get_next_batch</span><span class="p">()</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">queue</span><span class="o">.</span><span class="n">put</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__iter__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
            <span class="k">yield</span> <span class="bp">self</span><span class="o">.</span><span class="n">queue</span><span class="o">.</span><span class="n">get</span><span class="p">()</span>

<span class="c1"># 5. 通信与计算重叠</span>
<span class="k">class</span> <span class="nc">OverlappedOptimizer</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">step</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># 启动异步 all-reduce</span>
        <span class="n">handles</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span>
            <span class="n">handle</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">all_reduce</span><span class="p">(</span><span class="n">param</span><span class="o">.</span><span class="n">grad</span><span class="p">,</span> <span class="n">async_op</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="n">handles</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">handle</span><span class="p">)</span>

        <span class="c1"># 同时进行其他计算</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">update_metrics</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">log_progress</span><span class="p">()</span>

        <span class="c1"># 等待通信完成</span>
        <span class="k">for</span> <span class="n">handle</span> <span class="ow">in</span> <span class="n">handles</span><span class="p">:</span>
            <span class="n">handle</span><span class="o">.</span><span class="n">wait</span><span class="p">()</span>

        <span class="c1"># 应用梯度</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

<span class="c1"># 6. NCCL 优化</span>
<span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s1">&#39;NCCL_TREE_THRESHOLD&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;0&#39;</span>
<span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s1">&#39;NCCL_ALGO&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;Ring,Tree&#39;</span>
<span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s1">&#39;NCCL_CROSS_NIC&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;1&#39;</span>
<span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s1">&#39;NCCL_NET_GDR_LEVEL&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;5&#39;</span>
</code></pre></div>

<p>预期效果：优化后 32 卡效率可提升到 75-80%。</p>
</details>
<p><strong>练习 12.8：生产环境故障恢复</strong></p>
<p>设计一个完整的故障恢复系统，能够处理节点故障、网络中断和 GPU 错误，确保训练能够自动恢复。</p>
<p>💡 <strong>提示</strong>：考虑检查点、健康检查、自动重启和弹性训练。</p>
<details>
<summary>📝 参考答案</summary>
<div class="codehilite"><pre><span></span><code><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">time</span>
<span class="kn">import</span> <span class="nn">signal</span>
<span class="kn">import</span> <span class="nn">subprocess</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Optional</span><span class="p">,</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">Any</span>
<span class="kn">from</span> <span class="nn">dataclasses</span> <span class="kn">import</span> <span class="n">dataclass</span>
<span class="kn">from</span> <span class="nn">datetime</span> <span class="kn">import</span> <span class="n">datetime</span><span class="p">,</span> <span class="n">timedelta</span>

<span class="nd">@dataclass</span>
<span class="k">class</span> <span class="nc">TrainingState</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;训练状态&quot;&quot;&quot;</span>
    <span class="n">epoch</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">step</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">best_loss</span><span class="p">:</span> <span class="nb">float</span>
    <span class="n">checkpoint_path</span><span class="p">:</span> <span class="nb">str</span>
    <span class="n">failure_count</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">last_failure_time</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">datetime</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>

<span class="k">class</span> <span class="nc">ResilientTrainer</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;弹性训练器 - 自动故障恢复&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">config</span> <span class="o">=</span> <span class="n">config</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_load_state</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">health_checker</span> <span class="o">=</span> <span class="n">HealthChecker</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">checkpoint_manager</span> <span class="o">=</span> <span class="n">CheckpointManager</span><span class="p">(</span><span class="n">config</span><span class="p">[</span><span class="s1">&#39;checkpoint_dir&#39;</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_failures</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;max_failures&#39;</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">failure_window</span> <span class="o">=</span> <span class="n">timedelta</span><span class="p">(</span><span class="n">hours</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

        <span class="c1"># 注册信号处理</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_register_signal_handlers</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">_register_signal_handlers</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;注册优雅退出的信号处理&quot;&quot;&quot;</span>
        <span class="k">def</span> <span class="nf">graceful_exit</span><span class="p">(</span><span class="n">signum</span><span class="p">,</span> <span class="n">frame</span><span class="p">):</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Received signal </span><span class="si">{</span><span class="n">signum</span><span class="si">}</span><span class="s2">, saving checkpoint...&quot;</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">save_checkpoint</span><span class="p">(</span><span class="n">emergency</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="n">sys</span><span class="o">.</span><span class="n">exit</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

        <span class="n">signal</span><span class="o">.</span><span class="n">signal</span><span class="p">(</span><span class="n">signal</span><span class="o">.</span><span class="n">SIGTERM</span><span class="p">,</span> <span class="n">graceful_exit</span><span class="p">)</span>
        <span class="n">signal</span><span class="o">.</span><span class="n">signal</span><span class="p">(</span><span class="n">signal</span><span class="o">.</span><span class="n">SIGINT</span><span class="p">,</span> <span class="n">graceful_exit</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;主训练循环 - 带故障恢复&quot;&quot;&quot;</span>
        <span class="k">while</span> <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">epoch</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s1">&#39;max_epochs&#39;</span><span class="p">]:</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="c1"># 健康检查</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">health_checker</span><span class="o">.</span><span class="n">check_all</span><span class="p">():</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_handle_unhealthy_state</span><span class="p">()</span>
                    <span class="k">continue</span>

                <span class="c1"># 训练一个 epoch</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_train_epoch</span><span class="p">()</span>

                <span class="c1"># 定期保存检查点</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">step</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s1">&#39;checkpoint_interval&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">save_checkpoint</span><span class="p">()</span>

                <span class="c1"># 重置故障计数（成功完成 epoch）</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">failure_count</span> <span class="o">=</span> <span class="mi">0</span>

            <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_handle_training_failure</span><span class="p">(</span><span class="n">e</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_train_epoch</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;训练一个 epoch&quot;&quot;&quot;</span>
        <span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">dataloader</span><span class="p">:</span>
            <span class="c1"># 故障注入测试（可选）</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;fault_injection&#39;</span><span class="p">):</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_inject_random_fault</span><span class="p">()</span>

            <span class="c1"># 训练步骤</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">train_step</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>

            <span class="c1"># 异常检测</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_detect_anomaly</span><span class="p">(</span><span class="n">loss</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Anomaly detected: loss=</span><span class="si">{</span><span class="n">loss</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">step</span> <span class="o">+=</span> <span class="mi">1</span>

    <span class="k">def</span> <span class="nf">_handle_training_failure</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">error</span><span class="p">:</span> <span class="ne">Exception</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;处理训练故障&quot;&quot;&quot;</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Training failed: </span><span class="si">{</span><span class="n">error</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

        <span class="c1"># 更新故障统计</span>
        <span class="n">current_time</span> <span class="o">=</span> <span class="n">datetime</span><span class="o">.</span><span class="n">now</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">failure_count</span> <span class="o">+=</span> <span class="mi">1</span>

        <span class="c1"># 检查是否超过最大故障次数</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">last_failure_time</span><span class="p">:</span>
            <span class="n">time_since_last</span> <span class="o">=</span> <span class="n">current_time</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">last_failure_time</span>
            <span class="k">if</span> <span class="n">time_since_last</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">failure_window</span><span class="p">:</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">failure_count</span> <span class="o">&gt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_failures</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_escalate_failure</span><span class="p">(</span><span class="s2">&quot;Too many failures in short time&quot;</span><span class="p">)</span>
                    <span class="k">return</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">last_failure_time</span> <span class="o">=</span> <span class="n">current_time</span>

        <span class="c1"># 尝试恢复</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_attempt_recovery</span><span class="p">(</span><span class="n">error</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_attempt_recovery</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">error</span><span class="p">:</span> <span class="ne">Exception</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;尝试从故障中恢复&quot;&quot;&quot;</span>
        <span class="n">recovery_strategies</span> <span class="o">=</span> <span class="p">[</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_recover_from_oom</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_recover_from_nccl_error</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_recover_from_checkpoint</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_restart_workers</span>
        <span class="p">]</span>

        <span class="k">for</span> <span class="n">strategy</span> <span class="ow">in</span> <span class="n">recovery_strategies</span><span class="p">:</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">strategy</span><span class="p">(</span><span class="n">error</span><span class="p">):</span>
                    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Recovery successful using </span><span class="si">{</span><span class="n">strategy</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
                    <span class="k">return</span>
            <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Recovery strategy </span><span class="si">{</span><span class="n">strategy</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2"> failed: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

        <span class="c1"># 所有策略都失败</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_escalate_failure</span><span class="p">(</span><span class="s2">&quot;All recovery strategies failed&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_recover_from_oom</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">error</span><span class="p">:</span> <span class="ne">Exception</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;从 OOM 错误恢复&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="s2">&quot;out of memory&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="nb">str</span><span class="p">(</span><span class="n">error</span><span class="p">)</span><span class="o">.</span><span class="n">lower</span><span class="p">():</span>
            <span class="k">return</span> <span class="kc">False</span>

        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Attempting OOM recovery...&quot;</span><span class="p">)</span>

        <span class="c1"># 1. 清理缓存</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">empty_cache</span><span class="p">()</span>

        <span class="c1"># 2. 减小批大小</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s1">&#39;batch_size&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s1">&#39;batch_size&#39;</span><span class="p">]</span> <span class="o">//</span> <span class="mi">2</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Reduced batch size to </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s1">&#39;batch_size&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

        <span class="c1"># 3. 启用更激进的内存优化</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s1">&#39;gradient_checkpointing&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s1">&#39;cpu_offload&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>

        <span class="c1"># 4. 重新初始化模型</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_reinitialize_model</span><span class="p">()</span>

        <span class="k">return</span> <span class="kc">True</span>

    <span class="k">def</span> <span class="nf">_recover_from_nccl_error</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">error</span><span class="p">:</span> <span class="ne">Exception</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;从 NCCL 错误恢复&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="s2">&quot;nccl&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="nb">str</span><span class="p">(</span><span class="n">error</span><span class="p">)</span><span class="o">.</span><span class="n">lower</span><span class="p">():</span>
            <span class="k">return</span> <span class="kc">False</span>

        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Attempting NCCL recovery...&quot;</span><span class="p">)</span>

        <span class="c1"># 1. 销毁进程组</span>
        <span class="k">if</span> <span class="n">dist</span><span class="o">.</span><span class="n">is_initialized</span><span class="p">():</span>
            <span class="n">dist</span><span class="o">.</span><span class="n">destroy_process_group</span><span class="p">()</span>

        <span class="c1"># 2. 等待所有进程</span>
        <span class="n">time</span><span class="o">.</span><span class="n">sleep</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>

        <span class="c1"># 3. 重新初始化</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_init_distributed</span><span class="p">()</span>

        <span class="c1"># 4. 从检查点恢复</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">load_checkpoint</span><span class="p">()</span>

        <span class="k">return</span> <span class="kc">True</span>

    <span class="k">def</span> <span class="nf">_restart_workers</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">error</span><span class="p">:</span> <span class="ne">Exception</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;重启工作进程&quot;&quot;&quot;</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Restarting all workers...&quot;</span><span class="p">)</span>

        <span class="c1"># 保存当前状态</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">save_checkpoint</span><span class="p">(</span><span class="n">emergency</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="c1"># 构建重启命令</span>
        <span class="n">restart_cmd</span> <span class="o">=</span> <span class="p">[</span>
            <span class="s2">&quot;python&quot;</span><span class="p">,</span> <span class="s2">&quot;-m&quot;</span><span class="p">,</span> <span class="s2">&quot;torch.distributed.launch&quot;</span><span class="p">,</span>
            <span class="s2">&quot;--nproc_per_node&quot;</span><span class="p">,</span> <span class="nb">str</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s1">&#39;gpus_per_node&#39;</span><span class="p">]),</span>
            <span class="s2">&quot;--nnodes&quot;</span><span class="p">,</span> <span class="nb">str</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s1">&#39;num_nodes&#39;</span><span class="p">]),</span>
            <span class="s2">&quot;--node_rank&quot;</span><span class="p">,</span> <span class="nb">str</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s1">&#39;node_rank&#39;</span><span class="p">]),</span>
            <span class="s2">&quot;--master_addr&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s1">&#39;master_addr&#39;</span><span class="p">],</span>
            <span class="s2">&quot;--master_port&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s1">&#39;master_port&#39;</span><span class="p">],</span>
            <span class="s2">&quot;train.py&quot;</span><span class="p">,</span>
            <span class="s2">&quot;--resume&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">checkpoint_path</span>
        <span class="p">]</span>

        <span class="c1"># 执行重启</span>
        <span class="n">subprocess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">restart_cmd</span><span class="p">)</span>
        <span class="k">return</span> <span class="kc">True</span>

    <span class="k">def</span> <span class="nf">save_checkpoint</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">emergency</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;保存检查点&quot;&quot;&quot;</span>
        <span class="n">checkpoint</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s1">&#39;model_state&#39;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span>
            <span class="s1">&#39;optimizer_state&#39;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span>
            <span class="s1">&#39;training_state&#39;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="p">,</span>
            <span class="s1">&#39;config&#39;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">,</span>
            <span class="s1">&#39;timestamp&#39;</span><span class="p">:</span> <span class="n">datetime</span><span class="o">.</span><span class="n">now</span><span class="p">()</span><span class="o">.</span><span class="n">isoformat</span><span class="p">(),</span>
            <span class="s1">&#39;emergency&#39;</span><span class="p">:</span> <span class="n">emergency</span>
        <span class="p">}</span>

        <span class="c1"># 保存多个副本防止损坏</span>
        <span class="n">paths</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">checkpoint_manager</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">,</span> <span class="n">emergency</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">checkpoint_path</span> <span class="o">=</span> <span class="n">paths</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

        <span class="c1"># 异步上传到云存储</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;cloud_backup&#39;</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_async_cloud_backup</span><span class="p">(</span><span class="n">paths</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">_escalate_failure</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">reason</span><span class="p">:</span> <span class="nb">str</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;升级故障处理&quot;&quot;&quot;</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;CRITICAL: </span><span class="si">{</span><span class="n">reason</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

        <span class="c1"># 1. 发送告警</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_send_alert</span><span class="p">(</span><span class="n">reason</span><span class="p">)</span>

        <span class="c1"># 2. 保存调试信息</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_save_debug_info</span><span class="p">()</span>

        <span class="c1"># 3. 优雅退出</span>
        <span class="n">sys</span><span class="o">.</span><span class="n">exit</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

<span class="k">class</span> <span class="nc">HealthChecker</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;健康检查器&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">check_all</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;执行所有健康检查&quot;&quot;&quot;</span>
        <span class="n">checks</span> <span class="o">=</span> <span class="p">[</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">check_gpu_health</span><span class="p">(),</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">check_network_health</span><span class="p">(),</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">check_memory_health</span><span class="p">(),</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">check_disk_space</span><span class="p">()</span>
        <span class="p">]</span>
        <span class="k">return</span> <span class="nb">all</span><span class="p">(</span><span class="n">checks</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">check_gpu_health</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;检查 GPU 健康状态&quot;&quot;&quot;</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="c1"># 检查 GPU 是否可用</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">device_count</span><span class="p">()):</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">set_device</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">synchronize</span><span class="p">()</span>

                <span class="c1"># 检查温度</span>
                <span class="n">temp</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_gpu_temperature</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">temp</span> <span class="o">&gt;</span> <span class="mi">85</span><span class="p">:</span>
                    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;WARNING: GPU </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2"> temperature </span><span class="si">{</span><span class="n">temp</span><span class="si">}</span><span class="s2">°C&quot;</span><span class="p">)</span>
                    <span class="k">return</span> <span class="kc">False</span>

                <span class="c1"># 检查 ECC 错误</span>
                <span class="n">ecc_errors</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_ecc_errors</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">ecc_errors</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;WARNING: GPU </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2"> has </span><span class="si">{</span><span class="n">ecc_errors</span><span class="si">}</span><span class="s2"> ECC errors&quot;</span><span class="p">)</span>
                    <span class="k">return</span> <span class="kc">False</span>

            <span class="k">return</span> <span class="kc">True</span>
        <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;GPU health check failed: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="k">return</span> <span class="kc">False</span>

    <span class="k">def</span> <span class="nf">check_network_health</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;检查网络健康状态&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">dist</span><span class="o">.</span><span class="n">is_initialized</span><span class="p">():</span>
            <span class="k">return</span> <span class="kc">True</span>

        <span class="k">try</span><span class="p">:</span>
            <span class="c1"># 简单的 all-reduce 测试</span>
            <span class="n">test_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>
            <span class="n">dist</span><span class="o">.</span><span class="n">all_reduce</span><span class="p">(</span><span class="n">test_tensor</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">test_tensor</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">==</span> <span class="n">dist</span><span class="o">.</span><span class="n">get_world_size</span><span class="p">()</span>
        <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Network health check failed: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="k">return</span> <span class="kc">False</span>

<span class="c1"># 使用示例</span>
<span class="n">config</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;max_epochs&#39;</span><span class="p">:</span> <span class="mi">100</span><span class="p">,</span>
    <span class="s1">&#39;checkpoint_dir&#39;</span><span class="p">:</span> <span class="s1">&#39;./checkpoints&#39;</span><span class="p">,</span>
    <span class="s1">&#39;checkpoint_interval&#39;</span><span class="p">:</span> <span class="mi">1000</span><span class="p">,</span>
    <span class="s1">&#39;max_failures&#39;</span><span class="p">:</span> <span class="mi">3</span><span class="p">,</span>
    <span class="s1">&#39;batch_size&#39;</span><span class="p">:</span> <span class="mi">32</span><span class="p">,</span>
    <span class="s1">&#39;gpus_per_node&#39;</span><span class="p">:</span> <span class="mi">8</span><span class="p">,</span>
    <span class="s1">&#39;num_nodes&#39;</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span>
    <span class="s1">&#39;node_rank&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
    <span class="s1">&#39;master_addr&#39;</span><span class="p">:</span> <span class="s1">&#39;192.168.1.1&#39;</span><span class="p">,</span>
    <span class="s1">&#39;master_port&#39;</span><span class="p">:</span> <span class="s1">&#39;29500&#39;</span><span class="p">,</span>
    <span class="s1">&#39;cloud_backup&#39;</span><span class="p">:</span> <span class="kc">True</span>
<span class="p">}</span>

<span class="n">trainer</span> <span class="o">=</span> <span class="n">ResilientTrainer</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
<span class="n">trainer</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
</code></pre></div>

<p>这个系统提供了完整的故障恢复能力，包括自动重试、降级策略、健康检查和云备份，能够处理生产环境中的各种故障场景。</p>
</details>
<h2 id="gotchas">常见陷阱与错误 (Gotchas)</h2>
<h3 id="1-nccl">1. NCCL 版本不匹配</h3>
<p><strong>陷阱</strong>：不同节点的 NCCL 版本不一致导致通信失败。
<strong>解决</strong>：统一所有节点的 PyTorch 和 NCCL 版本。</p>
<h3 id="2-hanging-without-error">2. Hanging Without Error</h3>
<p><strong>陷阱</strong>：训练挂起但没有任何错误输出。
<strong>解决</strong>：启用 NCCL_DEBUG=INFO 和设置合理的超时时间。</p>
<h3 id="3">3. 隐式同步点</h3>
<p><strong>陷阱</strong>：print、日志等操作可能引入隐式同步。
<strong>解决</strong>：只在 rank 0 进行 I/O 操作，或使用异步 I/O。</p>
<h3 id="4-gpu">4. GPU 亲和性设置错误</h3>
<p><strong>陷阱</strong>：CUDA_VISIBLE_DEVICES 设置不当导致进程看到错误的 GPU。
<strong>解决</strong>：使用 torchrun 或正确设置每个进程的 GPU 映射。</p>
<h3 id="5">5. 混合精度不兼容</h3>
<p><strong>陷阱</strong>：不同 GPU 支持的精度不同（FP16 vs BF16）。
<strong>解决</strong>：检测硬件能力，降级到所有 GPU 都支持的精度。</p>
<h3 id="6-checkpoint">6. Checkpoint 腐败</h3>
<p><strong>陷阱</strong>：保存 checkpoint 时进程被中断导致文件损坏。
<strong>解决</strong>：先保存到临时文件，成功后再重命名。</p>
<h2 id="_5">最佳实践检查清单</h2>
<h3 id="_6">启动前检查</h3>
<ul>
<li>[ ] 所有节点的环境一致（Python、PyTorch、CUDA 版本）</li>
<li>[ ] 网络连通性测试通过</li>
<li>[ ] GPU 健康检查通过（温度、ECC 错误）</li>
<li>[ ] 磁盘空间充足（checkpoint 需要大量空间）</li>
<li>[ ] NCCL 环境变量正确设置</li>
</ul>
<h3 id="_7">训练中监控</h3>
<ul>
<li>[ ] GPU 利用率 &gt; 85%</li>
<li>[ ] 网络带宽利用合理</li>
<li>[ ] 无进程明显落后（通过 progress bar）</li>
<li>[ ] 内存使用稳定（无泄漏）</li>
<li>[ ] Loss 曲线正常（无 NaN、无异常跳变）</li>
</ul>
<h3 id="_8">故障恢复准备</h3>
<ul>
<li>[ ] Checkpoint 定期保存（至少每小时）</li>
<li>[ ] 有多个 checkpoint 副本</li>
<li>[ ] 故障恢复脚本已测试</li>
<li>[ ] 监控告警已配置</li>
<li>[ ] 有回滚计划</li>
</ul>
<h3 id="_9">性能优化</h3>
<ul>
<li>[ ] 通信与计算重叠</li>
<li>[ ] 梯度累积合理设置</li>
<li>[ ] 数据加载不是瓶颈</li>
<li>[ ] 使用了合适的 NCCL 算法</li>
<li>[ ] 混合精度训练已启用</li>
</ul>
<h3 id="_10">调试工具</h3>
<ul>
<li>[ ] NCCL 日志已启用（问题排查时）</li>
<li>[ ] 进程监控脚本运行中</li>
<li>[ ] 性能 profiling 工具就绪</li>
<li>[ ] 有 core dump 生成配置</li>
</ul>
            </article>
            
            <nav class="page-nav"><a href="chapter11.html" class="nav-link prev">← 第 11 章：训练速度优化实战</a><a href="CLAUDE.html" class="nav-link next">Untitled →</a></nav>
        </main>
    </div>
</body>
</html>