<!DOCTYPE html>
<html lang="zh">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <base href="./">
    <title>ç¬¬ 8 ç« ï¼šæ¨¡å‹éƒ¨ç½²ä¸æœåŠ¡åŒ–</title>
    <link rel="stylesheet" href="assets/style.css">
    <link rel="stylesheet" href="assets/highlight.css">
    <script src="assets/script.js" defer></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>
        window.MathJax = {
            tex: {
                inlineMath: [['$', '$']],
                displayMath: [['$$', '$$']],
                processEscapes: false,
                packages: {'[+]': ['noerrors', 'ams']}
            },
            options: {
                ignoreHtmlClass: 'tex2jax_ignore',
                processHtmlClass: 'tex2jax_process'
            },
            loader: {
                load: ['[tex]/noerrors', '[tex]/ams']
            }
        };
    </script>
</head>
<body>
    <div class="container">
        <nav id="sidebar" class="sidebar">
            <div class="sidebar-header">
                <h3>ç›®å½•</h3>
                <button id="sidebar-toggle" class="sidebar-toggle">
                    <span></span>
                    <span></span>
                    <span></span>
                </button>
            </div>
            <div class="sidebar-search">
                <input type="text" id="sidebar-search-input" placeholder="æœç´¢..." autocomplete="off">
            </div>
            <div id="tree-container">
                <nav class="tree-nav" role="tree">
                    <div class="tree-item " >
                        <a href="index.html" class="tree-link">
                            <span class="tree-icon">ğŸ“„</span>
                            <span class="tree-title">è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰çš„ç›‘ç£å¾®è°ƒä¸å¼ºåŒ–å­¦ä¹ å®æˆ˜æ•™ç¨‹</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter1.html" class="tree-link">
                            <span class="tree-icon">ğŸ“„</span>
                            <span class="tree-title">ç¬¬ 1 ç« ï¼šVLM æ¶æ„ä¸åŸç†</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter2.html" class="tree-link">
                            <span class="tree-icon">ğŸ“„</span>
                            <span class="tree-title">ç¬¬ 2 ç« ï¼šæ•°æ®å‡†å¤‡ä¸é¢„å¤„ç†</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter3.html" class="tree-link">
                            <span class="tree-icon">ğŸ“„</span>
                            <span class="tree-title">ç¬¬ 3 ç« ï¼šSFT è®­ç»ƒç­–ç•¥</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter4.html" class="tree-link">
                            <span class="tree-icon">ğŸ“„</span>
                            <span class="tree-title">ç¬¬ 4 ç« ï¼šåˆ†å¸ƒå¼è®­ç»ƒä¸ä¼˜åŒ–</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter5.html" class="tree-link">
                            <span class="tree-icon">ğŸ“„</span>
                            <span class="tree-title">ç¬¬ 5 ç« ï¼šRLHF åŸºç¡€ä¸å®ç°</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter6.html" class="tree-link">
                            <span class="tree-icon">ğŸ“„</span>
                            <span class="tree-title">ç¬¬ 6 ç« ï¼šç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter7.html" class="tree-link">
                            <span class="tree-icon">ğŸ“„</span>
                            <span class="tree-title">ç¬¬ 7 ç« ï¼šè¯„ä¼°ä½“ç³»è®¾è®¡</span>
                        </a>
                    </div>
                
                    <div class="tree-item active" >
                        <a href="chapter8.html" class="tree-link">
                            <span class="tree-icon">ğŸ“„</span>
                            <span class="tree-title">ç¬¬ 8 ç« ï¼šæ¨¡å‹éƒ¨ç½²ä¸æœåŠ¡åŒ–</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter9.html" class="tree-link">
                            <span class="tree-icon">ğŸ“„</span>
                            <span class="tree-title">ç¬¬ 9 ç« ï¼šCUDA OOM è°ƒè¯•å®Œå…¨æŒ‡å—</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter10.html" class="tree-link">
                            <span class="tree-icon">ğŸ“„</span>
                            <span class="tree-title">ç¬¬ 10 ç« ï¼šè®­ç»ƒå´©æºƒä¸ NaN é—®é¢˜</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter11.html" class="tree-link">
                            <span class="tree-icon">ğŸ“„</span>
                            <span class="tree-title">ç¬¬ 11 ç« ï¼šè®­ç»ƒé€Ÿåº¦ä¼˜åŒ–å®æˆ˜</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter12.html" class="tree-link">
                            <span class="tree-icon">ğŸ“„</span>
                            <span class="tree-title">ç¬¬ 12 ç« ï¼šå¤šæœºå¤šå¡è°ƒè¯•åœ°ç‹±</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="CLAUDE.html" class="tree-link">
                            <span class="tree-icon">ğŸ“„</span>
                            <span class="tree-title">Untitled</span>
                        </a>
                    </div>
                </nav>
            </div>
        </nav>
        
        <main class="content">
            <article>
                <h1 id="8">ç¬¬ 8 ç« ï¼šæ¨¡å‹éƒ¨ç½²ä¸æœåŠ¡åŒ–</h1>
<p>å°†è®­ç»ƒå¥½çš„ VLM æ¨¡å‹é«˜æ•ˆéƒ¨ç½²åˆ°ç”Ÿäº§ç¯å¢ƒæ˜¯æ•´ä¸ªé¡¹ç›®è½åœ°çš„å…³é”®ç¯èŠ‚ã€‚æœ¬ç« å°†ç³»ç»Ÿä»‹ç»ä»æ¨¡å‹ä¼˜åŒ–åˆ°æœåŠ¡åŒ–éƒ¨ç½²çš„å®Œæ•´æµç¨‹ï¼Œé‡ç‚¹å…³æ³¨å¦‚ä½•åœ¨ä¿è¯æ¨ç†ç²¾åº¦çš„å‰æä¸‹ï¼Œæœ€å¤§ç¨‹åº¦æå‡æ¨ç†é€Ÿåº¦å’Œé™ä½èµ„æºæ¶ˆè€—ã€‚æˆ‘ä»¬å°†æ·±å…¥æ¢è®¨é‡åŒ–æŠ€æœ¯ã€æ¨ç†ä¼˜åŒ–ã€æœåŠ¡æ¶æ„è®¾è®¡ä»¥åŠç”Ÿäº§ç¯å¢ƒçš„ç›‘æ§ä¸è¿­ä»£ç­–ç•¥ã€‚</p>
<h2 id="81">8.1 æ¨¡å‹é‡åŒ–ä¸å‹ç¼©</h2>
<h3 id="811">8.1.1 é‡åŒ–åŸºç¡€ç†è®º</h3>
<p>æ¨¡å‹é‡åŒ–é€šè¿‡é™ä½æƒé‡å’Œæ¿€æ´»å€¼çš„æ•°å€¼ç²¾åº¦æ¥å‡å°‘æ¨¡å‹å¤§å°å’Œè®¡ç®—å¼€é”€ã€‚å¯¹äº VLM æ¨¡å‹ï¼Œé‡åŒ–ç­–ç•¥éœ€è¦åŒæ—¶è€ƒè™‘è§†è§‰ç¼–ç å™¨å’Œè¯­è¨€æ¨¡å‹ä¸¤éƒ¨åˆ†çš„ç‰¹æ€§ã€‚</p>
<p><strong>é‡åŒ–çš„æ•°å­¦è¡¨ç¤º</strong>ï¼š</p>
<p>å¯¹äºæƒé‡ $W \in \mathbb{R}^{m \times n}$ï¼Œé‡åŒ–è¿‡ç¨‹å¯è¡¨ç¤ºä¸ºï¼š</p>
<p>$$W_q = \text{round}\left(\frac{W - Z}{S}\right)$$
å…¶ä¸­ $S$ æ˜¯ç¼©æ”¾å› å­ï¼ˆscaleï¼‰ï¼Œ$Z$ æ˜¯é›¶ç‚¹ï¼ˆzero pointï¼‰ï¼Œ$W_q$ æ˜¯é‡åŒ–åçš„æ•´æ•°æƒé‡ã€‚</p>
<p>åé‡åŒ–è¿‡ç¨‹ï¼š
$$W_{dq} = S \cdot W_q + Z$$</p>
<h3 id="812-int8">8.1.2 INT8 é‡åŒ–å®è·µ</h3>
<p>INT8 é‡åŒ–æ˜¯æœ€å¸¸ç”¨çš„é‡åŒ–æ–¹æ¡ˆï¼Œå¯ä»¥å°†æ¨¡å‹å¤§å°å‡å°‘ 75%ï¼Œæ¨ç†é€Ÿåº¦æå‡ 2-4 å€ã€‚</p>
<p><strong>å¯¹ç§°é‡åŒ– vs éå¯¹ç§°é‡åŒ–</strong>ï¼š</p>
<div class="codehilite"><pre><span></span><code>å¯¹ç§°é‡åŒ–ï¼ˆSymmetricï¼‰:
    èŒƒå›´: [-127, 127]
    é›¶ç‚¹ Z = 0
    é€‚ç”¨: æƒé‡é‡åŒ–

éå¯¹ç§°é‡åŒ–ï¼ˆAsymmetricï¼‰:
    èŒƒå›´: [0, 255]  
    é›¶ç‚¹ Z â‰  0
    é€‚ç”¨: æ¿€æ´»å€¼é‡åŒ–
</code></pre></div>

<p><strong>VLM ç‰¹æœ‰çš„é‡åŒ–æŒ‘æˆ˜</strong>ï¼š</p>
<ol>
<li>
<p><strong>è§†è§‰ç¼–ç å™¨çš„é‡åŒ–æ•æ„Ÿæ€§</strong>ï¼š
   - ViT çš„è‡ªæ³¨æ„åŠ›å±‚å¯¹é‡åŒ–æ›´æ•æ„Ÿ
   - Patch embedding å±‚é€šå¸¸ä¿æŒ FP16
   - å»ºè®®: è§†è§‰ç¼–ç å™¨ä½¿ç”¨ INT8 åŠ¨æ€é‡åŒ–</p>
</li>
<li>
<p><strong>è·¨æ¨¡æ€æŠ•å½±å±‚çš„å¤„ç†</strong>ï¼š
   - MLP projector æ˜¯ç²¾åº¦ç“¶é¢ˆ
   - å»ºè®®ä¿æŒ FP16 æˆ–ä½¿ç”¨æ›´é«˜æ¯”ç‰¹é‡åŒ–</p>
</li>
<li>
<p><strong>æ··åˆç²¾åº¦ç­–ç•¥</strong>ï¼š</p>
</li>
</ol>
<div class="codehilite"><pre><span></span><code>æ¨¡å‹ç»„ä»¶é‡åŒ–é…ç½®:
â”œâ”€â”€ è§†è§‰ç¼–ç å™¨: INT8 åŠ¨æ€é‡åŒ–
â”œâ”€â”€ æŠ•å½±å±‚: FP16 ä¿æŒ
â”œâ”€â”€ è¯­è¨€æ¨¡å‹
â”‚   â”œâ”€â”€ Embedding: INT8
â”‚   â”œâ”€â”€ Attention: INT8 + FP16 (QKè®¡ç®—)
â”‚   â””â”€â”€ FFN: INT8
â””â”€â”€ LM Head: FP16 (å…³é”®å±‚ä¿æŠ¤)
</code></pre></div>

<h3 id="813-gptq">8.1.3 GPTQ é‡åŒ–æŠ€æœ¯</h3>
<p>GPTQï¼ˆGradient-based Post-training Quantizationï¼‰é€šè¿‡ä¼˜åŒ–é‡æ„è¯¯å·®å®ç°é«˜è´¨é‡çš„ 4-bit é‡åŒ–ã€‚</p>
<p><strong>GPTQ æ ¸å¿ƒç®—æ³•</strong>ï¼š</p>
<p>ä¼˜åŒ–ç›®æ ‡ï¼š
$$\min_{W_q} ||WX - W_qX||_2^2$$
å…¶ä¸­ $X$ æ˜¯æ ¡å‡†æ•°æ®ï¼Œé€šè¿‡é€å±‚ä¼˜åŒ–æœ€å°åŒ–é‡æ„è¯¯å·®ã€‚</p>
<p><strong>å®æ–½æ­¥éª¤</strong>ï¼š</p>
<ol>
<li>
<p><strong>å‡†å¤‡æ ¡å‡†æ•°æ®é›†</strong>ï¼ˆå…³é”®ï¼‰ï¼š
   - ä½¿ç”¨ 100-200 ä¸ªä»£è¡¨æ€§æ ·æœ¬
   - å¿…é¡»åŒ…å«å›¾åƒ-æ–‡æœ¬å¯¹
   - è¦†ç›–ä¸åŒä»»åŠ¡ç±»å‹</p>
</li>
<li>
<p><strong>é€å±‚é‡åŒ–æµç¨‹</strong>ï¼š</p>
</li>
</ol>
<div class="codehilite"><pre><span></span><code>for layer in model.layers:
    # æ”¶é›†è¯¥å±‚è¾“å…¥æ¿€æ´»å€¼
    X = collect_activations(layer, calibration_data)

    # è®¡ç®— Hessian çŸ©é˜µ
    H = 2 * X @ X.T

    # é€åˆ—é‡åŒ–æƒé‡
    for col in range(W.shape[1]):
        w_q = quantize_column(W[:, col], H)
        # æ›´æ–°å‰©ä½™åˆ—ä»¥è¡¥å¿é‡åŒ–è¯¯å·®
        update_remaining_columns(W, w_q, col)
</code></pre></div>

<ol start="3">
<li><strong>Group-wise é‡åŒ–</strong>ï¼š
   - å°†æƒé‡åˆ†ç»„ï¼ˆé€šå¸¸ 128 ä¸ªæƒé‡ä¸€ç»„ï¼‰
   - æ¯ç»„ç‹¬ç«‹è®¡ç®— scale å’Œ zero point
   - å¹³è¡¡å‹ç¼©ç‡å’Œç²¾åº¦</li>
</ol>
<h3 id="814-awq">8.1.4 AWQ é‡åŒ–æŠ€æœ¯</h3>
<p>AWQï¼ˆActivation-aware Weight Quantizationï¼‰é€šè¿‡æ¿€æ´»å€¼æ„ŸçŸ¥çš„æƒé‡ç¼©æ”¾æå‡é‡åŒ–è´¨é‡ã€‚</p>
<p><strong>AWQ æ ¸å¿ƒåˆ›æ–°</strong>ï¼š</p>
<p>åŸºäºè§‚å¯Ÿï¼šæƒé‡çš„é‡è¦æ€§ä¸å¯¹åº”æ¿€æ´»å€¼çš„å¤§å°ç›¸å…³ã€‚</p>
<p>ç¼©æ”¾ç­–ç•¥ï¼š
$$W_{scaled} = W \cdot \text{diag}(s)$$
$$X_{scaled} = X \cdot \text{diag}(s^{-1})$$
å…¶ä¸­ $s$ æ˜¯æ ¹æ®æ¿€æ´»å€¼ç»Ÿè®¡è®¡ç®—çš„ç¼©æ”¾å› å­ã€‚</p>
<p><strong>AWQ vs GPTQ å¯¹æ¯”</strong>ï¼š</p>
<p>| ç‰¹æ€§ | AWQ | GPTQ |</p>
<table>
<thead>
<tr>
<th>ç‰¹æ€§</th>
<th>AWQ</th>
<th>GPTQ</th>
</tr>
</thead>
<tbody>
<tr>
<td>é‡åŒ–é€Ÿåº¦</td>
<td>å¿«ï¼ˆ10-20åˆ†é’Ÿï¼‰</td>
<td>æ…¢ï¼ˆ1-2å°æ—¶ï¼‰</td>
</tr>
<tr>
<td>æ¨ç†é€Ÿåº¦</td>
<td>æ›´å¿«ï¼ˆç¡¬ä»¶å‹å¥½ï¼‰</td>
<td>è¾ƒå¿«</td>
</tr>
<tr>
<td>ç²¾åº¦ä¿æŒ</td>
<td>ä¼˜ç§€ï¼ˆ4-bitï¼‰</td>
<td>ä¼˜ç§€ï¼ˆ4-bitï¼‰</td>
</tr>
<tr>
<td>æ˜¾å­˜å ç”¨</td>
<td>æ›´ä½</td>
<td>è¾ƒä½</td>
</tr>
<tr>
<td>å®ç°å¤æ‚åº¦</td>
<td>ä¸­ç­‰</td>
<td>è¾ƒé«˜</td>
</tr>
</tbody>
</table>
<h3 id="815">8.1.5 é‡åŒ–æ–¹æ¡ˆé€‰æ‹©æŒ‡å—</h3>
<div class="codehilite"><pre><span></span><code>å†³ç­–æ ‘ï¼š
æ˜¾å­˜å……è¶³ï¼Ÿ
â”œâ”€â”€ æ˜¯ â†’ FP16/BF16 æ¨ç†
â””â”€â”€ å¦ â†’ éœ€è¦é‡åŒ–
    â”œâ”€â”€ å»¶è¿Ÿæ•æ„Ÿï¼Ÿ
    â”‚   â”œâ”€â”€ æ˜¯ â†’ INT8 é‡åŒ–ï¼ˆæœ€å¿«ï¼‰
    â”‚   â””â”€â”€ å¦ â†’ ç»§ç»­è¯„ä¼°
    â””â”€â”€ ç²¾åº¦è¦æ±‚ï¼Ÿ
        â”œâ”€â”€ é«˜ â†’ GPTQ 4-bit
        â””â”€â”€ ä¸­ â†’ AWQ 4-bitï¼ˆæ¨èï¼‰
</code></pre></div>

<h2 id="82">8.2 æ¨ç†ä¼˜åŒ–æŠ€æœ¯</h2>
<h3 id="821-kv-cache">8.2.1 KV Cache ä¼˜åŒ–</h3>
<p>KV Cache æ˜¯ Transformer æ¨ç†çš„æ ¸å¿ƒä¼˜åŒ–ï¼Œå¯¹ VLM å°¤å…¶é‡è¦ã€‚</p>
<p><strong>å†…å­˜å ç”¨è®¡ç®—</strong>ï¼š
$$M_{kv} = 2 \times L \times H \times D \times (N_{text} + N_{image}) \times B \times P$$
å…¶ä¸­ï¼š</p>
<ul>
<li>$L$: å±‚æ•°</li>
<li>$H$: æ³¨æ„åŠ›å¤´æ•°  </li>
<li>$D$: æ¯ä¸ªå¤´çš„ç»´åº¦</li>
<li>$N_{text}$, $N_{image}$: æ–‡æœ¬å’Œå›¾åƒ token æ•°</li>
<li>$B$: batch size</li>
<li>$P$: ç²¾åº¦å­—èŠ‚æ•°</li>
</ul>
<p><strong>ä¼˜åŒ–ç­–ç•¥</strong>ï¼š</p>
<ol>
<li><strong>PagedAttention</strong>ï¼ˆvLLM æ ¸å¿ƒï¼‰ï¼š</li>
</ol>
<div class="codehilite"><pre><span></span><code>ä¼ ç»Ÿ KV Cache:
[è¿ç»­å†…å­˜å—] â†’ æµªè´¹ä¸¥é‡

PagedAttention:
[é¡µè¡¨ç®¡ç†] â†’ [æŒ‰éœ€åˆ†é…] â†’ [å†…å­˜å…±äº«]
ä¼˜åŠ¿: å‡å°‘ 50-80% å†…å­˜æµªè´¹
</code></pre></div>

<ol start="2">
<li>
<p><strong>Multi-Query Attention (MQA)</strong>ï¼š
   - æ‰€æœ‰æŸ¥è¯¢å¤´å…±äº«ä¸€ç»„ KV
   - å†…å­˜å‡å°‘ $H$ å€
   - é€Ÿåº¦æå‡ 30-50%</p>
</li>
<li>
<p><strong>Grouped-Query Attention (GQA)</strong>ï¼š
   - æŠ˜ä¸­æ–¹æ¡ˆï¼š$G$ ç»„å…±äº« KV
   - å¹³è¡¡é€Ÿåº¦å’Œè´¨é‡</p>
</li>
</ol>
<h3 id="822-flash-attention">8.2.2 Flash Attention é›†æˆ</h3>
<p>Flash Attention é€šè¿‡ IO ä¼˜åŒ–å¤§å¹…æå‡æ³¨æ„åŠ›è®¡ç®—æ•ˆç‡ã€‚</p>
<p><strong>æ ¸å¿ƒä¼˜åŒ–</strong>ï¼š</p>
<ol>
<li><strong>åˆ†å—è®¡ç®—</strong>ï¼š</li>
</ol>
<div class="codehilite"><pre><span></span><code><span class="c1"># ä¼ªä»£ç å±•ç¤ºåŸç†</span>
<span class="k">def</span> <span class="nf">flash_attention</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">V</span><span class="p">,</span> <span class="n">block_size</span><span class="o">=</span><span class="mi">64</span><span class="p">):</span>
    <span class="c1"># åˆ†å—éå†ï¼Œå‡å°‘ HBM è®¿é—®</span>
    <span class="k">for</span> <span class="n">q_block</span> <span class="ow">in</span> <span class="n">split</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">block_size</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">kv_block</span> <span class="ow">in</span> <span class="n">split</span><span class="p">(</span><span class="n">K</span><span class="p">,</span> <span class="n">V</span><span class="p">,</span> <span class="n">block_size</span><span class="p">):</span>
            <span class="c1"># åœ¨ SRAM ä¸­è®¡ç®—</span>
            <span class="n">attn_block</span> <span class="o">=</span> <span class="n">softmax</span><span class="p">(</span><span class="n">q_block</span> <span class="o">@</span> <span class="n">kv_block</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
            <span class="n">out_block</span> <span class="o">=</span> <span class="n">attn_block</span> <span class="o">@</span> <span class="n">v_block</span>
            <span class="c1"># å¢é‡æ›´æ–°ç»“æœ</span>
            <span class="n">update_output</span><span class="p">(</span><span class="n">out_block</span><span class="p">)</span>
</code></pre></div>

<ol start="2">
<li><strong>VLM ç‰¹æ®Šè€ƒè™‘</strong>ï¼š
   - å›¾åƒ token é€šå¸¸è¿ç»­ä¸”æ•°é‡å›ºå®š
   - å¯ä»¥é¢„è®¡ç®—å›¾åƒéƒ¨åˆ†çš„æ³¨æ„åŠ›
   - æ–‡æœ¬ç”Ÿæˆæ—¶åªæ›´æ–°æ–‡æœ¬éƒ¨åˆ†</li>
</ol>
<p><strong>æ€§èƒ½æå‡</strong>ï¼š</p>
<ul>
<li>é€Ÿåº¦: 2-4Ã— æå‡</li>
<li>æ˜¾å­˜: çº¿æ€§è€ŒéäºŒæ¬¡å¢é•¿</li>
<li>é•¿åºåˆ—: æ”¯æŒ 32K+ token</li>
</ul>
<h3 id="823-batching">8.2.3 åŠ¨æ€ Batching ä¼˜åŒ–</h3>
<p>åŠ¨æ€ batching æ˜¯æé«˜ååé‡çš„å…³é”®æŠ€æœ¯ã€‚</p>
<p><strong>å®ç°ç­–ç•¥</strong>ï¼š</p>
<ol>
<li><strong>Continuous Batching</strong>ï¼š</li>
</ol>
<div class="codehilite"><pre><span></span><code>ä¼ ç»Ÿ Static Batching:
[ç­‰å¾…æ‰€æœ‰è¯·æ±‚å®Œæˆ] â†’ GPU åˆ©ç”¨ç‡ä½

Continuous Batching:
[æŒç»­åŠ å…¥æ–°è¯·æ±‚] â†’ [åŠ¨æ€è°ƒåº¦] â†’ GPU åˆ©ç”¨ç‡é«˜
</code></pre></div>

<ol start="2">
<li>
<p><strong>VLM ç‰¹æœ‰æŒ‘æˆ˜</strong>ï¼š
   - å›¾åƒé¢„å¤„ç†æ—¶é—´ä¸ä¸€è‡´
   - å›¾åƒ token æ•°é‡å¯å˜ï¼ˆåŠ¨æ€åˆ†è¾¨ç‡ï¼‰
   - éœ€è¦å¹³è¡¡è§†è§‰ç¼–ç å’Œæ–‡æœ¬ç”Ÿæˆ</p>
</li>
<li>
<p><strong>ä¼˜åŒ–æ–¹æ¡ˆ</strong>ï¼š</p>
</li>
</ol>
<div class="codehilite"><pre><span></span><code><span class="k">class</span> <span class="nc">VLMBatchScheduler</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">schedule</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">requests</span><span class="p">):</span>
        <span class="c1"># æŒ‰å›¾åƒå¤§å°åˆ†ç»„</span>
        <span class="n">groups</span> <span class="o">=</span> <span class="n">group_by_image_size</span><span class="p">(</span><span class="n">requests</span><span class="p">)</span>

        <span class="c1"># è§†è§‰ç¼–ç æ‰¹å¤„ç†</span>
        <span class="k">for</span> <span class="n">group</span> <span class="ow">in</span> <span class="n">groups</span><span class="p">:</span>
            <span class="n">vision_features</span> <span class="o">=</span> <span class="n">batch_encode_images</span><span class="p">(</span><span class="n">group</span><span class="p">)</span>
            <span class="n">cache_features</span><span class="p">(</span><span class="n">vision_features</span><span class="p">)</span>

        <span class="c1"># æ–‡æœ¬ç”ŸæˆåŠ¨æ€batching</span>
        <span class="k">while</span> <span class="n">active_requests</span><span class="p">:</span>
            <span class="n">batch</span> <span class="o">=</span> <span class="n">select_compatible_requests</span><span class="p">()</span>
            <span class="n">tokens</span> <span class="o">=</span> <span class="n">generate_batch</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>
            <span class="n">update_requests</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">tokens</span><span class="p">)</span>
</code></pre></div>

<h3 id="824-speculative-decoding">8.2.4 æŠ•æœºè§£ç ï¼ˆSpeculative Decodingï¼‰</h3>
<p>ä½¿ç”¨å°æ¨¡å‹åŠ é€Ÿå¤§æ¨¡å‹æ¨ç†ã€‚</p>
<p><strong>åŸç†</strong>ï¼š</p>
<ol>
<li>å°æ¨¡å‹å¿«é€Ÿç”Ÿæˆå€™é€‰ token</li>
<li>å¤§æ¨¡å‹å¹¶è¡ŒéªŒè¯</li>
<li>æ¥å—/æ‹’ç»å€™é€‰ç»“æœ</li>
</ol>
<p><strong>VLM é€‚é…</strong>ï¼š</p>
<ul>
<li>è§†è§‰ç¼–ç å™¨å¯ä»¥å…±äº«</li>
<li>ä»…è¯­è¨€æ¨¡å‹éƒ¨åˆ†ä½¿ç”¨æŠ•æœºè§£ç </li>
<li>å…¸å‹åŠ é€Ÿ: 2-3Ã—</li>
</ul>
<h2 id="83">8.3 æœåŠ¡åŒ–æ¶æ„è®¾è®¡</h2>
<h3 id="831">8.3.1 æ•´ä½“æ¶æ„</h3>
<div class="codehilite"><pre><span></span><code>â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Load Balancer               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                 â”‚
â”Œâ”€â”€â”€â–¼â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”
â”‚ API   â”‚       â”‚  API    â”‚
â”‚Server â”‚       â”‚ Server  â”‚
â””â”€â”€â”€â”¬â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
    â”‚                â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Request Queue  â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Inference Engine   â”‚
    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
    â”‚  â”‚ Vision       â”‚  â”‚
    â”‚  â”‚ Encoder Pool â”‚  â”‚
    â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
    â”‚         â”‚          â”‚
    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”  â”‚
    â”‚  â”‚   Language   â”‚  â”‚
    â”‚  â”‚  Model Pool  â”‚  â”‚
    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
</code></pre></div>

<h3 id="832">8.3.2 å…³é”®ç»„ä»¶è®¾è®¡</h3>
<p><strong>1. è¯·æ±‚è·¯ç”±å±‚</strong>ï¼š</p>
<div class="codehilite"><pre><span></span><code><span class="k">class</span> <span class="nc">RequestRouter</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">route</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">request</span><span class="p">):</span>
        <span class="c1"># æ ¹æ®æ¨¡å‹ç‰ˆæœ¬è·¯ç”±</span>
        <span class="k">if</span> <span class="n">request</span><span class="o">.</span><span class="n">model_version</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">version_pools</span><span class="p">[</span><span class="n">request</span><span class="o">.</span><span class="n">model_version</span><span class="p">]</span>

        <span class="c1"># æ ¹æ®è´Ÿè½½å‡è¡¡</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">select_least_loaded</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">health_check</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># å®šæœŸæ£€æŸ¥åç«¯å¥åº·çŠ¶æ€</span>
        <span class="k">for</span> <span class="n">backend</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">backends</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">backend</span><span class="o">.</span><span class="n">is_healthy</span><span class="p">():</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">remove_backend</span><span class="p">(</span><span class="n">backend</span><span class="p">)</span>
</code></pre></div>

<p><strong>2. ç¼“å­˜ç­–ç•¥</strong>ï¼š</p>
<div class="codehilite"><pre><span></span><code><span class="k">class</span> <span class="nc">VLMCache</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># å›¾åƒç‰¹å¾ç¼“å­˜</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">vision_cache</span> <span class="o">=</span> <span class="n">LRUCache</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="mi">10000</span><span class="p">)</span>
        <span class="c1"># Prompt ç¼“å­˜</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">prompt_cache</span> <span class="o">=</span> <span class="n">LRUCache</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="mi">5000</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">get_vision_features</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">image_hash</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">image_hash</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">vision_cache</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">vision_cache</span><span class="p">[</span><span class="n">image_hash</span><span class="p">]</span>
        <span class="k">return</span> <span class="kc">None</span>

    <span class="k">def</span> <span class="nf">cache_vision_features</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">image_hash</span><span class="p">,</span> <span class="n">features</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">vision_cache</span><span class="p">[</span><span class="n">image_hash</span><span class="p">]</span> <span class="o">=</span> <span class="n">features</span>
</code></pre></div>

<p><strong>3. èµ„æºç®¡ç†</strong>ï¼š</p>
<div class="codehilite"><pre><span></span><code><span class="k">class</span> <span class="nc">ResourceManager</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">allocate_request</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">request</span><span class="p">):</span>
        <span class="n">required_memory</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">estimate_memory</span><span class="p">(</span><span class="n">request</span><span class="p">)</span>

        <span class="c1"># ç­‰å¾…èµ„æºå¯ç”¨</span>
        <span class="k">while</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">has_available_memory</span><span class="p">(</span><span class="n">required_memory</span><span class="p">):</span>
            <span class="n">time</span><span class="o">.</span><span class="n">sleep</span><span class="p">(</span><span class="mf">0.1</span><span class="p">)</span>

        <span class="c1"># åˆ†é…èµ„æº</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">current_memory</span> <span class="o">+=</span> <span class="n">required_memory</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">process_request</span><span class="p">(</span><span class="n">request</span><span class="p">)</span>
</code></pre></div>

<h3 id="833">8.3.3 é«˜å¯ç”¨è®¾è®¡</h3>
<p><strong>1. æ¨¡å‹çƒ­æ›´æ–°</strong>ï¼š</p>
<div class="codehilite"><pre><span></span><code><span class="k">class</span> <span class="nc">ModelManager</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">update_model</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">new_model_path</span><span class="p">):</span>
        <span class="c1"># åŠ è½½æ–°æ¨¡å‹</span>
        <span class="n">new_model</span> <span class="o">=</span> <span class="n">load_model</span><span class="p">(</span><span class="n">new_model_path</span><span class="p">)</span>

        <span class="c1"># é€æ­¥åˆ‡æ¢æµé‡</span>
        <span class="k">for</span> <span class="n">ratio</span> <span class="ow">in</span> <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.7</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">]:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">traffic_ratio</span> <span class="o">=</span> <span class="n">ratio</span>
            <span class="n">time</span><span class="o">.</span><span class="n">sleep</span><span class="p">(</span><span class="mi">60</span><span class="p">)</span>  <span class="c1"># è§‚å¯ŸæŒ‡æ ‡</span>

            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">has_errors</span><span class="p">():</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">rollback</span><span class="p">()</span>
                <span class="k">break</span>
</code></pre></div>

<p><strong>2. æ•…éšœæ¢å¤</strong>ï¼š
- è¯·æ±‚é‡è¯•æœºåˆ¶
- é™çº§ç­–ç•¥ï¼ˆä½¿ç”¨æ›´å°æ¨¡å‹ï¼‰
- ç†”æ–­ä¿æŠ¤</p>
<h3 id="834-api">8.3.4 API è®¾è®¡</h3>
<p><strong>RESTful API ç¤ºä¾‹</strong>ï¼š</p>
<div class="codehilite"><pre><span></span><code><span class="nd">@app</span><span class="o">.</span><span class="n">post</span><span class="p">(</span><span class="s2">&quot;/v1/chat/completions&quot;</span><span class="p">)</span>
<span class="k">async</span> <span class="k">def</span> <span class="nf">chat_completion</span><span class="p">(</span><span class="n">request</span><span class="p">:</span> <span class="n">ChatRequest</span><span class="p">):</span>
    <span class="c1"># è¯·æ±‚éªŒè¯</span>
    <span class="n">validate_request</span><span class="p">(</span><span class="n">request</span><span class="p">)</span>

    <span class="c1"># å›¾åƒé¢„å¤„ç†</span>
    <span class="k">if</span> <span class="n">request</span><span class="o">.</span><span class="n">images</span><span class="p">:</span>
        <span class="n">vision_features</span> <span class="o">=</span> <span class="k">await</span> <span class="n">encode_images</span><span class="p">(</span><span class="n">request</span><span class="o">.</span><span class="n">images</span><span class="p">)</span>

    <span class="c1"># ç”Ÿæˆå“åº”</span>
    <span class="n">response</span> <span class="o">=</span> <span class="k">await</span> <span class="n">generate_response</span><span class="p">(</span>
        <span class="n">prompt</span><span class="o">=</span><span class="n">request</span><span class="o">.</span><span class="n">messages</span><span class="p">,</span>
        <span class="n">vision_features</span><span class="o">=</span><span class="n">vision_features</span><span class="p">,</span>
        <span class="o">**</span><span class="n">request</span><span class="o">.</span><span class="n">parameters</span>
    <span class="p">)</span>

    <span class="k">return</span> <span class="n">response</span>
</code></pre></div>

<p><strong>æµå¼å“åº”</strong>ï¼š</p>
<div class="codehilite"><pre><span></span><code><span class="nd">@app</span><span class="o">.</span><span class="n">post</span><span class="p">(</span><span class="s2">&quot;/v1/chat/completions/stream&quot;</span><span class="p">)</span>
<span class="k">async</span> <span class="k">def</span> <span class="nf">stream_chat_completion</span><span class="p">(</span><span class="n">request</span><span class="p">:</span> <span class="n">ChatRequest</span><span class="p">):</span>
    <span class="k">async</span> <span class="k">def</span> <span class="nf">generate</span><span class="p">():</span>
        <span class="k">async</span> <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">generate_tokens</span><span class="p">(</span><span class="n">request</span><span class="p">):</span>
            <span class="k">yield</span> <span class="sa">f</span><span class="s2">&quot;data: </span><span class="si">{</span><span class="n">json</span><span class="o">.</span><span class="n">dumps</span><span class="p">({</span><span class="s1">&#39;token&#39;</span><span class="p">:</span><span class="w"> </span><span class="n">token</span><span class="p">})</span><span class="si">}</span><span class="se">\n\n</span><span class="s2">&quot;</span>

    <span class="k">return</span> <span class="n">StreamingResponse</span><span class="p">(</span><span class="n">generate</span><span class="p">(),</span> <span class="n">media_type</span><span class="o">=</span><span class="s2">&quot;text/event-stream&quot;</span><span class="p">)</span>
</code></pre></div>

<h2 id="84">8.4 ç›‘æ§ä¸è¿­ä»£ä¼˜åŒ–</h2>
<h3 id="841">8.4.1 å…³é”®æŒ‡æ ‡ç›‘æ§</h3>
<p><strong>æ€§èƒ½æŒ‡æ ‡</strong>ï¼š</p>
<ol>
<li>
<p><strong>å»¶è¿ŸæŒ‡æ ‡</strong>ï¼š
   - TTFT (Time To First Token): é¦–ä¸ª token å»¶è¿Ÿ
   - TPS (Tokens Per Second): ç”Ÿæˆé€Ÿåº¦
   - E2E Latency: ç«¯åˆ°ç«¯å»¶è¿Ÿ</p>
</li>
<li>
<p><strong>ååé‡æŒ‡æ ‡</strong>ï¼š
   - QPS (Queries Per Second)
   - GPU åˆ©ç”¨ç‡
   - å†…å­˜ä½¿ç”¨ç‡</p>
</li>
<li>
<p><strong>è´¨é‡æŒ‡æ ‡</strong>ï¼š
   - ç”Ÿæˆè´¨é‡è¯„åˆ†
   - é”™è¯¯ç‡
   - ç”¨æˆ·æ»¡æ„åº¦</p>
</li>
</ol>
<p><strong>ç›‘æ§å®ç°</strong>ï¼š</p>
<div class="codehilite"><pre><span></span><code><span class="k">class</span> <span class="nc">MetricsCollector</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">metrics</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s1">&#39;ttft&#39;</span><span class="p">:</span> <span class="p">[],</span>
            <span class="s1">&#39;tps&#39;</span><span class="p">:</span> <span class="p">[],</span>
            <span class="s1">&#39;gpu_util&#39;</span><span class="p">:</span> <span class="p">[],</span>
            <span class="s1">&#39;memory_usage&#39;</span><span class="p">:</span> <span class="p">[]</span>
        <span class="p">}</span>

    <span class="k">def</span> <span class="nf">record_inference</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">request_id</span><span class="p">,</span> <span class="n">start_time</span><span class="p">,</span> <span class="n">tokens</span><span class="p">):</span>
        <span class="n">ttft</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">start_time</span>
        <span class="n">tps</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">start_time</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">metrics</span><span class="p">[</span><span class="s1">&#39;ttft&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ttft</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">metrics</span><span class="p">[</span><span class="s1">&#39;tps&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">tps</span><span class="p">)</span>

        <span class="c1"># è®°å½•åˆ° Prometheus</span>
        <span class="n">TTFT_HISTOGRAM</span><span class="o">.</span><span class="n">observe</span><span class="p">(</span><span class="n">ttft</span><span class="p">)</span>
        <span class="n">TPS_GAUGE</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">tps</span><span class="p">)</span>
</code></pre></div>

<h3 id="842">8.4.2 æ€§èƒ½åˆ†æå·¥å…·</h3>
<p><strong>1. GPU æ€§èƒ½åˆ†æ</strong>ï¼š</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># ä½¿ç”¨ nsys è¿›è¡Œæ€§èƒ½åˆ†æ</span>
nsys<span class="w"> </span>profile<span class="w"> </span>-o<span class="w"> </span>model_profile<span class="w"> </span>python<span class="w"> </span>inference_server.py

<span class="c1"># ä½¿ç”¨ nvprof åˆ†æ kernel æ‰§è¡Œ</span>
nvprof<span class="w"> </span>--print-gpu-trace<span class="w"> </span>python<span class="w"> </span>benchmark.py
</code></pre></div>

<p><strong>2. å†…å­˜åˆ†æ</strong>ï¼š</p>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">analyze_memory</span><span class="p">():</span>
    <span class="c1"># æ˜¾å­˜å¿«ç…§</span>
    <span class="n">snapshot</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">memory_snapshot</span><span class="p">()</span>

    <span class="c1"># åˆ†æå†…å­˜åˆ†é…</span>
    <span class="k">for</span> <span class="n">block</span> <span class="ow">in</span> <span class="n">snapshot</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">block</span><span class="p">[</span><span class="s1">&#39;allocated&#39;</span><span class="p">]:</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Size: </span><span class="si">{</span><span class="n">block</span><span class="p">[</span><span class="s1">&#39;size&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2">, Stream: </span><span class="si">{</span><span class="n">block</span><span class="p">[</span><span class="s1">&#39;stream&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="c1"># å†…å­˜ç»Ÿè®¡</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Allocated: </span><span class="si">{</span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">memory_allocated</span><span class="p">()</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="mf">1e9</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> GB&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Reserved: </span><span class="si">{</span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">memory_reserved</span><span class="p">()</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="mf">1e9</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> GB&quot;</span><span class="p">)</span>
</code></pre></div>

<h3 id="843-ab">8.4.3 A/B æµ‹è¯•æ¡†æ¶</h3>
<div class="codehilite"><pre><span></span><code><span class="k">class</span> <span class="nc">ABTestManager</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">experiments</span> <span class="o">=</span> <span class="p">{}</span>

    <span class="k">def</span> <span class="nf">create_experiment</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">variants</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">experiments</span><span class="p">[</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s1">&#39;variants&#39;</span><span class="p">:</span> <span class="n">variants</span><span class="p">,</span>
            <span class="s1">&#39;metrics&#39;</span><span class="p">:</span> <span class="n">defaultdict</span><span class="p">(</span><span class="nb">list</span><span class="p">)</span>
        <span class="p">}</span>

    <span class="k">def</span> <span class="nf">route_request</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">request</span><span class="p">,</span> <span class="n">experiment_name</span><span class="p">):</span>
        <span class="c1"># åŸºäºç”¨æˆ· ID çš„ä¸€è‡´æ€§å“ˆå¸Œ</span>
        <span class="n">user_hash</span> <span class="o">=</span> <span class="nb">hash</span><span class="p">(</span><span class="n">request</span><span class="o">.</span><span class="n">user_id</span><span class="p">)</span>
        <span class="n">variant_index</span> <span class="o">=</span> <span class="n">user_hash</span> <span class="o">%</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">experiments</span><span class="p">[</span><span class="n">experiment_name</span><span class="p">][</span><span class="s1">&#39;variants&#39;</span><span class="p">])</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">experiments</span><span class="p">[</span><span class="n">experiment_name</span><span class="p">][</span><span class="s1">&#39;variants&#39;</span><span class="p">][</span><span class="n">variant_index</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">record_metric</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">experiment_name</span><span class="p">,</span> <span class="n">variant</span><span class="p">,</span> <span class="n">metric_name</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">experiments</span><span class="p">[</span><span class="n">experiment_name</span><span class="p">][</span><span class="s1">&#39;metrics&#39;</span><span class="p">][</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">variant</span><span class="si">}</span><span class="s2">_</span><span class="si">{</span><span class="n">metric_name</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>
</code></pre></div>

<h3 id="844">8.4.4 è‡ªåŠ¨ä¼˜åŒ–ç­–ç•¥</h3>
<p><strong>1. åŠ¨æ€æ‰¹å¤§å°è°ƒæ•´</strong>ï¼š</p>
<div class="codehilite"><pre><span></span><code><span class="k">class</span> <span class="nc">DynamicBatchSizer</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">current_batch_size</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">latency_history</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">def</span> <span class="nf">adjust_batch_size</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">avg_latency</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">latency_history</span><span class="p">[</span><span class="o">-</span><span class="mi">100</span><span class="p">:])</span>

        <span class="k">if</span> <span class="n">avg_latency</span> <span class="o">&lt;</span> <span class="n">TARGET_LATENCY</span> <span class="o">*</span> <span class="mf">0.8</span><span class="p">:</span>
            <span class="c1"># å»¶è¿Ÿå……è£•ï¼Œå¢åŠ æ‰¹å¤§å°</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">current_batch_size</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">current_batch_size</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">MAX_BATCH</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">avg_latency</span> <span class="o">&gt;</span> <span class="n">TARGET_LATENCY</span><span class="p">:</span>
            <span class="c1"># å»¶è¿Ÿè¶…æ ‡ï¼Œå‡å°æ‰¹å¤§å°</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">current_batch_size</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">current_batch_size</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</code></pre></div>

<p><strong>2. æ¨¡å‹å‰¯æœ¬è‡ªåŠ¨æ‰©ç¼©å®¹</strong>ï¼š</p>
<div class="codehilite"><pre><span></span><code><span class="k">class</span> <span class="nc">AutoScaler</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">scale_decision</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">metrics</span><span class="p">):</span>
        <span class="c1"># åŸºäºé˜Ÿåˆ—é•¿åº¦å’Œå»¶è¿Ÿå†³ç­–</span>
        <span class="k">if</span> <span class="n">metrics</span><span class="p">[</span><span class="s1">&#39;queue_length&#39;</span><span class="p">]</span> <span class="o">&gt;</span> <span class="n">QUEUE_THRESHOLD</span><span class="p">:</span>
            <span class="k">return</span> <span class="s1">&#39;scale_up&#39;</span>
        <span class="k">elif</span> <span class="n">metrics</span><span class="p">[</span><span class="s1">&#39;avg_gpu_util&#39;</span><span class="p">]</span> <span class="o">&lt;</span> <span class="mf">0.3</span><span class="p">:</span>
            <span class="k">return</span> <span class="s1">&#39;scale_down&#39;</span>
        <span class="k">return</span> <span class="s1">&#39;maintain&#39;</span>
</code></pre></div>

<h2 id="case-study-vllm-vlm">Case Study: vLLM éƒ¨ç½² VLM çš„æœ€ä½³å®è·µ</h2>
<h3 id="_1">èƒŒæ™¯ä»‹ç»</h3>
<p>vLLM æ˜¯ç›®å‰æœ€æµè¡Œçš„ LLM æ¨ç†æ¡†æ¶ä¹‹ä¸€ï¼Œé€šè¿‡ PagedAttention ç­‰åˆ›æ–°æ˜¾è‘—æå‡äº†æ¨ç†æ•ˆç‡ã€‚æœ¬æ¡ˆä¾‹å°†è¯¦ç»†ä»‹ç»å¦‚ä½•ä½¿ç”¨ vLLM éƒ¨ç½² LLaVA-NeXT æ¨¡å‹ã€‚</p>
<h3 id="_2">ç¯å¢ƒå‡†å¤‡</h3>
<div class="codehilite"><pre><span></span><code><span class="c1"># å®‰è£… vLLM (æ”¯æŒ VLM)</span>
pip<span class="w"> </span>install<span class="w"> </span>vllm&gt;<span class="o">=</span><span class="m">0</span>.3.0

<span class="c1"># éªŒè¯ GPU æ”¯æŒ</span>
python<span class="w"> </span>-c<span class="w"> </span><span class="s2">&quot;import torch; print(torch.cuda.get_device_capability())&quot;</span>
<span class="c1"># éœ€è¦ compute capability &gt;= 7.0</span>
</code></pre></div>

<h3 id="_3">æ¨¡å‹éƒ¨ç½²é…ç½®</h3>
<div class="codehilite"><pre><span></span><code><span class="kn">from</span> <span class="nn">vllm</span> <span class="kn">import</span> <span class="n">LLM</span><span class="p">,</span> <span class="n">SamplingParams</span>
<span class="kn">from</span> <span class="nn">vllm.multimodal</span> <span class="kn">import</span> <span class="n">MultiModalData</span>

<span class="k">class</span> <span class="nc">VLMDeployment</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model_path</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">llm</span> <span class="o">=</span> <span class="n">LLM</span><span class="p">(</span>
            <span class="n">model</span><span class="o">=</span><span class="n">model_path</span><span class="p">,</span>
            <span class="c1"># å…³é”®å‚æ•°é…ç½®</span>
            <span class="n">tensor_parallel_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>  <span class="c1"># TP å¹¶è¡Œåº¦</span>
            <span class="n">max_model_len</span><span class="o">=</span><span class="mi">4096</span><span class="p">,</span>      <span class="c1"># æœ€å¤§åºåˆ—é•¿åº¦</span>
            <span class="n">gpu_memory_utilization</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span>  <span class="c1"># GPU å†…å­˜åˆ©ç”¨ç‡</span>

            <span class="c1"># VLM ç‰¹å®šé…ç½®</span>
            <span class="n">image_input_type</span><span class="o">=</span><span class="s2">&quot;pixel_values&quot;</span><span class="p">,</span>
            <span class="n">image_token_id</span><span class="o">=</span><span class="mi">32000</span><span class="p">,</span>
            <span class="n">image_input_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">336</span><span class="p">,</span> <span class="mi">336</span><span class="p">),</span>
            <span class="n">image_feature_size</span><span class="o">=</span><span class="mi">576</span><span class="p">,</span>  <span class="c1"># 24*24 patches</span>

            <span class="c1"># ä¼˜åŒ–å‚æ•°</span>
            <span class="n">enable_prefix_caching</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>  <span class="c1"># å¯ç”¨å‰ç¼€ç¼“å­˜</span>
            <span class="n">enable_chunked_prefill</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>  <span class="c1"># åˆ†å—é¢„å¡«å……</span>
            <span class="n">max_num_batched_tokens</span><span class="o">=</span><span class="mi">8192</span><span class="p">,</span>
            <span class="n">max_num_seqs</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span>

            <span class="c1"># é‡åŒ–é…ç½®ï¼ˆå¯é€‰ï¼‰</span>
            <span class="n">quantization</span><span class="o">=</span><span class="s2">&quot;awq&quot;</span><span class="p">,</span>  <span class="c1"># ä½¿ç”¨ AWQ 4-bit é‡åŒ–</span>
        <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">sampling_params</span> <span class="o">=</span> <span class="n">SamplingParams</span><span class="p">(</span>
            <span class="n">temperature</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span>
            <span class="n">top_p</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span>
            <span class="n">max_tokens</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span>
        <span class="p">)</span>
</code></pre></div>

<h3 id="_4">æ¨ç†ä¼˜åŒ–é…ç½®</h3>
<div class="codehilite"><pre><span></span><code><span class="c1"># 1. å¯ç”¨ Flash Attention</span>
<span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;VLLM_USE_FLASH_ATTN&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;1&quot;</span>

<span class="c1"># 2. é…ç½® CUDA Graph</span>
<span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;VLLM_USE_CUDA_GRAPH&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;1&quot;</span>
<span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;VLLM_CUDA_GRAPH_MAX_SEQS&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;32&quot;</span>

<span class="c1"># 3. è°ƒæ•´è°ƒåº¦ç­–ç•¥</span>
<span class="n">engine_args</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;scheduler_config&quot;</span><span class="p">:</span> <span class="p">{</span>
        <span class="s2">&quot;max_num_batched_tokens&quot;</span><span class="p">:</span> <span class="mi">8192</span><span class="p">,</span>
        <span class="s2">&quot;max_num_seqs&quot;</span><span class="p">:</span> <span class="mi">256</span><span class="p">,</span>
        <span class="s2">&quot;max_paddings&quot;</span><span class="p">:</span> <span class="mi">512</span><span class="p">,</span>
        <span class="s2">&quot;delay_factor&quot;</span><span class="p">:</span> <span class="mf">0.1</span><span class="p">,</span>  <span class="c1"># æ§åˆ¶æ‰¹å¤„ç†ç­‰å¾…æ—¶é—´</span>
    <span class="p">}</span>
<span class="p">}</span>
</code></pre></div>

<h3 id="_5">æ€§èƒ½è°ƒä¼˜å®æˆ˜</h3>
<p><strong>1. æ‰¹å¤„ç†ä¼˜åŒ–</strong>ï¼š</p>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">optimized_batch_inference</span><span class="p">(</span><span class="n">requests</span><span class="p">):</span>
    <span class="c1"># æŒ‰å›¾åƒå¤§å°åˆ†ç»„</span>
    <span class="n">grouped</span> <span class="o">=</span> <span class="n">defaultdict</span><span class="p">(</span><span class="nb">list</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">req</span> <span class="ow">in</span> <span class="n">requests</span><span class="p">:</span>
        <span class="n">img_size</span> <span class="o">=</span> <span class="n">req</span><span class="o">.</span><span class="n">image</span><span class="o">.</span><span class="n">shape</span>
        <span class="n">grouped</span><span class="p">[</span><span class="n">img_size</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">req</span><span class="p">)</span>

    <span class="n">results</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">size</span><span class="p">,</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">grouped</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="c1"># åŒå°ºå¯¸å›¾åƒæ‰¹å¤„ç†</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="n">llm</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span>
            <span class="n">prompts</span><span class="o">=</span><span class="p">[</span><span class="n">r</span><span class="o">.</span><span class="n">prompt</span> <span class="k">for</span> <span class="n">r</span> <span class="ow">in</span> <span class="n">batch</span><span class="p">],</span>
            <span class="n">multi_modal_data</span><span class="o">=</span><span class="p">[</span><span class="n">r</span><span class="o">.</span><span class="n">image</span> <span class="k">for</span> <span class="n">r</span> <span class="ow">in</span> <span class="n">batch</span><span class="p">],</span>
            <span class="n">sampling_params</span><span class="o">=</span><span class="n">sampling_params</span>
        <span class="p">)</span>
        <span class="n">results</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">outputs</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">results</span>
</code></pre></div>

<p><strong>2. å†…å­˜ä¼˜åŒ–</strong>ï¼š</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># ç›‘æ§å†…å­˜ä½¿ç”¨</span>
<span class="k">def</span> <span class="nf">monitor_memory</span><span class="p">():</span>
    <span class="n">stats</span> <span class="o">=</span> <span class="n">llm</span><span class="o">.</span><span class="n">get_model_memory_usage</span><span class="p">()</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;KV Cache: </span><span class="si">{</span><span class="n">stats</span><span class="p">[</span><span class="s1">&#39;kv_cache_usage&#39;</span><span class="p">]</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="mf">1e9</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> GB&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Model Weights: </span><span class="si">{</span><span class="n">stats</span><span class="p">[</span><span class="s1">&#39;model_weights&#39;</span><span class="p">]</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="mf">1e9</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> GB&quot;</span><span class="p">)</span>

    <span class="c1"># åŠ¨æ€è°ƒæ•´ KV cache å¤§å°</span>
    <span class="k">if</span> <span class="n">stats</span><span class="p">[</span><span class="s1">&#39;kv_cache_usage&#39;</span><span class="p">]</span> <span class="o">&gt;</span> <span class="n">MEMORY_THRESHOLD</span><span class="p">:</span>
        <span class="n">llm</span><span class="o">.</span><span class="n">reduce_max_num_seqs</span><span class="p">(</span><span class="n">factor</span><span class="o">=</span><span class="mf">0.8</span><span class="p">)</span>
</code></pre></div>

<h3 id="_6">ç”Ÿäº§éƒ¨ç½²æ£€æŸ¥æ¸…å•</h3>
<ul>
<li>[x] é…ç½®å¥åº·æ£€æŸ¥ç«¯ç‚¹</li>
<li>[x] å®ç°ä¼˜é›…å…³é—­æœºåˆ¶</li>
<li>[x] è®¾ç½®è¯·æ±‚è¶…æ—¶</li>
<li>[x] é…ç½®æ—¥å¿—å’Œç›‘æ§</li>
<li>[x] å®ç°é™çº§ç­–ç•¥</li>
<li>[x] å‡†å¤‡å›æ»šæ–¹æ¡ˆ</li>
</ul>
<h3 id="_7">æ€§èƒ½åŸºå‡†æµ‹è¯•ç»“æœ</h3>
<p>| é…ç½® | TTFT (ms) | TPS | QPS | GPU åˆ©ç”¨ç‡ |</p>
<table>
<thead>
<tr>
<th>é…ç½®</th>
<th>TTFT (ms)</th>
<th>TPS</th>
<th>QPS</th>
<th>GPU åˆ©ç”¨ç‡</th>
</tr>
</thead>
<tbody>
<tr>
<td>åŸºç¡€é…ç½®</td>
<td>450</td>
<td>42</td>
<td>8</td>
<td>65%</td>
</tr>
<tr>
<td>+ PagedAttention</td>
<td>380</td>
<td>48</td>
<td>12</td>
<td>75%</td>
</tr>
<tr>
<td>+ Flash Attention</td>
<td>320</td>
<td>56</td>
<td>15</td>
<td>82%</td>
</tr>
<tr>
<td>+ AWQ é‡åŒ–</td>
<td>280</td>
<td>68</td>
<td>20</td>
<td>88%</td>
</tr>
<tr>
<td>+ Dynamic Batching</td>
<td>250</td>
<td>72</td>
<td>28</td>
<td>92%</td>
</tr>
</tbody>
</table>
<h2 id="_8">é«˜çº§è¯é¢˜</h2>
<h3 id="awq-vs-gptq">AWQ vs GPTQ æ·±åº¦å¯¹æ¯”</h3>
<p><strong>é‡åŒ–ç²¾åº¦å¯¹æ¯”å®éªŒ</strong>ï¼š</p>
<p>æµ‹è¯•æ¨¡å‹ï¼šLLaVA-NeXT-13B
æµ‹è¯•æ•°æ®é›†ï¼šCOCO Captions Validation</p>
<p>| é‡åŒ–æ–¹æ³• | Perplexity | BLEU-4 | æ¨ç†é€Ÿåº¦ | æ˜¾å­˜å ç”¨ |</p>
<table>
<thead>
<tr>
<th>é‡åŒ–æ–¹æ³•</th>
<th>Perplexity</th>
<th>BLEU-4</th>
<th>æ¨ç†é€Ÿåº¦</th>
<th>æ˜¾å­˜å ç”¨</th>
</tr>
</thead>
<tbody>
<tr>
<td>FP16 (åŸºå‡†)</td>
<td>8.32</td>
<td>35.2</td>
<td>1.0x</td>
<td>26GB</td>
</tr>
<tr>
<td>INT8</td>
<td>8.45</td>
<td>34.8</td>
<td>2.1x</td>
<td>13GB</td>
</tr>
<tr>
<td>GPTQ 4-bit</td>
<td>8.68</td>
<td>34.1</td>
<td>3.2x</td>
<td>8.5GB</td>
</tr>
<tr>
<td>AWQ 4-bit</td>
<td>8.59</td>
<td>34.4</td>
<td>3.8x</td>
<td>8.2GB</td>
</tr>
</tbody>
</table>
<p><strong>å…³é”®å‘ç°</strong>ï¼š</p>
<ol>
<li>
<p><strong>AWQ åœ¨æ¨ç†é€Ÿåº¦ä¸Šä¼˜åŠ¿æ˜æ˜¾</strong>ï¼š
   - åŸå› ï¼šæƒé‡å¸ƒå±€æ›´é€‚åˆç¡¬ä»¶åŠ é€Ÿ
   - kernel å®ç°æ›´é«˜æ•ˆ</p>
</li>
<li>
<p><strong>GPTQ åœ¨æŸäº›ä»»åŠ¡ä¸Šç²¾åº¦ç•¥é«˜</strong>ï¼š
   - ç‰¹åˆ«æ˜¯éœ€è¦ç²¾ç¡®æ•°å€¼è®¡ç®—çš„ä»»åŠ¡
   - ä½†å·®å¼‚é€šå¸¸ &lt; 1%</p>
</li>
<li>
<p><strong>æ··åˆç­–ç•¥</strong>ï¼š</p>
</li>
</ol>
<div class="codehilite"><pre><span></span><code><span class="c1"># å¯¹ä¸åŒå±‚ä½¿ç”¨ä¸åŒé‡åŒ–</span>
<span class="n">quantization_config</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;vision_encoder&quot;</span><span class="p">:</span> <span class="s2">&quot;int8&quot;</span><span class="p">,</span>      <span class="c1"># è§†è§‰ç¼–ç å™¨ç”¨ INT8</span>
    <span class="s2">&quot;projection&quot;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>             <span class="c1"># æŠ•å½±å±‚ä¸é‡åŒ–</span>
    <span class="s2">&quot;llm_layers_0_15&quot;</span><span class="p">:</span> <span class="s2">&quot;awq_4bit&quot;</span><span class="p">,</span> <span class="c1"># å‰åŠéƒ¨åˆ†ç”¨ AWQ</span>
    <span class="s2">&quot;llm_layers_16_31&quot;</span><span class="p">:</span> <span class="s2">&quot;gptq_4bit&quot;</span><span class="p">,</span> <span class="c1"># ååŠéƒ¨åˆ†ç”¨ GPTQ</span>
    <span class="s2">&quot;lm_head&quot;</span><span class="p">:</span> <span class="kc">None</span>                <span class="c1"># è¾“å‡ºå±‚ä¸é‡åŒ–</span>
<span class="p">}</span>
</code></pre></div>

<h3 id="batching">åŠ¨æ€ Batching é«˜çº§ä¼˜åŒ–</h3>
<p><strong>1. è¯·æ±‚ä¼˜å…ˆçº§è°ƒåº¦</strong>ï¼š</p>
<div class="codehilite"><pre><span></span><code><span class="k">class</span> <span class="nc">PriorityBatchScheduler</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">queues</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s1">&#39;high&#39;</span><span class="p">:</span> <span class="n">PriorityQueue</span><span class="p">(),</span>
            <span class="s1">&#39;normal&#39;</span><span class="p">:</span> <span class="n">Queue</span><span class="p">(),</span>
            <span class="s1">&#39;low&#39;</span><span class="p">:</span> <span class="n">Queue</span><span class="p">()</span>
        <span class="p">}</span>

    <span class="k">def</span> <span class="nf">schedule_next_batch</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">max_batch_size</span><span class="p">):</span>
        <span class="n">batch</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="c1"># ä¼˜å…ˆå¤„ç†é«˜ä¼˜å…ˆçº§è¯·æ±‚</span>
        <span class="k">for</span> <span class="n">priority</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;high&#39;</span><span class="p">,</span> <span class="s1">&#39;normal&#39;</span><span class="p">,</span> <span class="s1">&#39;low&#39;</span><span class="p">]:</span>
            <span class="k">while</span> <span class="nb">len</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">max_batch_size</span> <span class="ow">and</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">queues</span><span class="p">[</span><span class="n">priority</span><span class="p">]</span><span class="o">.</span><span class="n">empty</span><span class="p">():</span>
                <span class="n">batch</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">queues</span><span class="p">[</span><span class="n">priority</span><span class="p">]</span><span class="o">.</span><span class="n">get</span><span class="p">())</span>

        <span class="k">return</span> <span class="n">batch</span>
</code></pre></div>

<p><strong>2. è‡ªé€‚åº” Padding ç­–ç•¥</strong>ï¼š</p>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">adaptive_padding</span><span class="p">(</span><span class="n">sequences</span><span class="p">):</span>
    <span class="n">lengths</span> <span class="o">=</span> <span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="n">seq</span><span class="p">)</span> <span class="k">for</span> <span class="n">seq</span> <span class="ow">in</span> <span class="n">sequences</span><span class="p">]</span>

    <span class="c1"># è®¡ç®—æœ€ä¼˜ padding é•¿åº¦</span>
    <span class="c1"># è€ƒè™‘ç¡¬ä»¶ç‰¹æ€§ï¼ˆå¦‚ tensor core éœ€è¦ 8 çš„å€æ•°ï¼‰</span>
    <span class="n">max_len</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">lengths</span><span class="p">)</span>
    <span class="n">optimal_len</span> <span class="o">=</span> <span class="p">((</span><span class="n">max_len</span> <span class="o">+</span> <span class="mi">7</span><span class="p">)</span> <span class="o">//</span> <span class="mi">8</span><span class="p">)</span> <span class="o">*</span> <span class="mi">8</span>

    <span class="c1"># å¦‚æœæµªè´¹è¶…è¿‡é˜ˆå€¼ï¼Œè€ƒè™‘åˆ†æ‰¹</span>
    <span class="n">waste_ratio</span> <span class="o">=</span> <span class="p">(</span><span class="n">optimal_len</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">sequences</span><span class="p">)</span> <span class="o">-</span> <span class="nb">sum</span><span class="p">(</span><span class="n">lengths</span><span class="p">))</span> <span class="o">/</span> <span class="p">(</span><span class="n">optimal_len</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">sequences</span><span class="p">))</span>

    <span class="k">if</span> <span class="n">waste_ratio</span> <span class="o">&gt;</span> <span class="mf">0.3</span><span class="p">:</span>  <span class="c1"># 30% æµªè´¹é˜ˆå€¼</span>
        <span class="c1"># åˆ†æˆä¸¤æ‰¹å¤„ç†</span>
        <span class="k">return</span> <span class="n">split_by_length</span><span class="p">(</span><span class="n">sequences</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">pad_sequences</span><span class="p">(</span><span class="n">sequences</span><span class="p">,</span> <span class="n">optimal_len</span><span class="p">)</span>
</code></pre></div>

<p><strong>3. é¢„æµ‹æ€§æ‰¹å¤„ç†</strong>ï¼š</p>
<div class="codehilite"><pre><span></span><code><span class="k">class</span> <span class="nc">PredictiveBatcher</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">arrival_predictor</span> <span class="o">=</span> <span class="n">ArrivalRatePredictor</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">should_wait_for_batch</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">current_batch_size</span><span class="p">):</span>
        <span class="c1"># é¢„æµ‹æœªæ¥è¯·æ±‚åˆ°è¾¾</span>
        <span class="n">expected_arrivals</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">arrival_predictor</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">window</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>  <span class="c1"># 100ms</span>

        <span class="c1"># è®¡ç®—ç­‰å¾…æ”¶ç›Š</span>
        <span class="n">current_efficiency</span> <span class="o">=</span> <span class="n">batch_efficiency</span><span class="p">(</span><span class="n">current_batch_size</span><span class="p">)</span>
        <span class="n">future_efficiency</span> <span class="o">=</span> <span class="n">batch_efficiency</span><span class="p">(</span><span class="n">current_batch_size</span> <span class="o">+</span> <span class="n">expected_arrivals</span><span class="p">)</span>

        <span class="c1"># å†³ç­–ï¼šç­‰å¾… vs ç«‹å³å¤„ç†</span>
        <span class="k">if</span> <span class="n">future_efficiency</span> <span class="o">/</span> <span class="n">current_efficiency</span> <span class="o">&gt;</span> <span class="mf">1.2</span><span class="p">:</span>  <span class="c1"># 20% æå‡é˜ˆå€¼</span>
            <span class="k">return</span> <span class="kc">True</span><span class="p">,</span> <span class="mi">100</span>  <span class="c1"># ç­‰å¾… 100ms</span>
        <span class="k">return</span> <span class="kc">False</span><span class="p">,</span> <span class="mi">0</span>
</code></pre></div>

<h2 id="_9">æœ¬ç« å°ç»“</h2>
<p>æœ¬ç« ç³»ç»Ÿä»‹ç»äº† VLM æ¨¡å‹ä»ä¼˜åŒ–åˆ°éƒ¨ç½²çš„å®Œæ•´æµç¨‹ã€‚æˆ‘ä»¬æ·±å…¥æ¢è®¨äº†ä»¥ä¸‹å…³é”®æŠ€æœ¯ï¼š</p>
<h3 id="_10">æ ¸å¿ƒè¦ç‚¹å›é¡¾</h3>
<ol>
<li>
<p><strong>æ¨¡å‹é‡åŒ–æŠ€æœ¯</strong>ï¼š
   - INT8 é‡åŒ–å¯å®ç° 2-4Ã— åŠ é€Ÿï¼Œé€‚åˆå»¶è¿Ÿæ•æ„Ÿåœºæ™¯
   - GPTQ å’Œ AWQ 4-bit é‡åŒ–å¯å‡å°‘ 75% æ˜¾å­˜ï¼Œç²¾åº¦æŸå¤± &lt; 2%
   - æ··åˆç²¾åº¦ç­–ç•¥ï¼šè§†è§‰ç¼–ç å™¨ INT8ï¼ŒæŠ•å½±å±‚ FP16ï¼Œè¯­è¨€æ¨¡å‹ 4-bit</p>
</li>
<li>
<p><strong>æ¨ç†ä¼˜åŒ–</strong>ï¼š
   - PagedAttention å‡å°‘ 50-80% KV cache æµªè´¹
   - Flash Attention å®ç° 2-4Ã— é€Ÿåº¦æå‡
   - åŠ¨æ€ batching æå‡ GPU åˆ©ç”¨ç‡è‡³ 90%+</p>
</li>
<li>
<p><strong>æœåŠ¡åŒ–æ¶æ„</strong>ï¼š
   - åˆ†ç¦»è§†è§‰ç¼–ç å’Œæ–‡æœ¬ç”Ÿæˆï¼Œç‹¬ç«‹æ‰©å±•
   - å®æ–½å¤šçº§ç¼“å­˜ç­–ç•¥ï¼ˆå›¾åƒç‰¹å¾ã€promptï¼‰
   - æ”¯æŒæµå¼å“åº”å’Œæ‰¹å¤„ç† API</p>
</li>
<li>
<p><strong>ç›‘æ§ä¸ä¼˜åŒ–</strong>ï¼š
   - å…³æ³¨ TTFTã€TPSã€QPS ä¸‰å¤§æ ¸å¿ƒæŒ‡æ ‡
   - å®æ–½ A/B æµ‹è¯•éªŒè¯ä¼˜åŒ–æ•ˆæœ
   - è‡ªåŠ¨è°ƒæ•´æ‰¹å¤§å°å’Œæ¨¡å‹å‰¯æœ¬æ•°</p>
</li>
</ol>
<h3 id="_11">å…³é”®å…¬å¼æ±‡æ€»</h3>
<p><strong>é‡åŒ–è¯¯å·®</strong>ï¼š
$$\epsilon = ||W - W_q||_F \approx \frac{\sigma_W \cdot n}{\sqrt{12} \cdot 2^b}$$
<strong>KV Cache å†…å­˜</strong>ï¼š
$$M_{kv} = 2LHD(N_{text} + N_{image})BP$$
<strong>æ‰¹å¤„ç†æ•ˆç‡</strong>ï¼š
$$\eta = \frac{\sum_{i=1}^B l_i}{B \cdot \max(l_i)}$$
<strong>æ¨ç†å»¶è¿Ÿæ¨¡å‹</strong>ï¼š
$$T_{total} = T_{encode} + N_{tokens} \cdot T_{decode} + T_{overhead}$$</p>
<h2 id="_12">ç»ƒä¹ é¢˜</h2>
<h3 id="_13">åŸºç¡€é¢˜</h3>
<p><strong>ç»ƒä¹  8.1</strong>: è®¡ç®— KV Cache å†…å­˜éœ€æ±‚</p>
<p>ä¸€ä¸ª 13B å‚æ•°çš„ VLM æ¨¡å‹ï¼Œ40 å±‚ï¼Œ40 ä¸ªæ³¨æ„åŠ›å¤´ï¼Œæ¯å¤´ç»´åº¦ 128ï¼Œå¤„ç†æ‰¹å¤§å°ä¸º 8ï¼Œæ¯ä¸ªæ ·æœ¬åŒ…å« 576 ä¸ªå›¾åƒ token å’Œå¹³å‡ 512 ä¸ªæ–‡æœ¬ tokenã€‚ä½¿ç”¨ FP16 ç²¾åº¦ï¼Œè®¡ç®— KV cache çš„å†…å­˜éœ€æ±‚ã€‚</p>
<details>
<summary>ğŸ’¡ æç¤º</summary>
<p>ä½¿ç”¨ KV cache å†…å­˜å…¬å¼ï¼Œæ³¨æ„å•ä½è½¬æ¢ï¼ˆGBï¼‰ã€‚</p>
</details>
<details>
<summary>ğŸ“ å‚è€ƒç­”æ¡ˆ</summary>
<p>$$M_{kv} = 2 \times 40 \times 40 \times 128 \times (512 + 576) \times 8 \times 2$$
$$= 2 \times 40 \times 40 \times 128 \times 1088 \times 8 \times 2$$
$$= 3,565,158,400 \text{ bytes} \approx 3.32 \text{ GB}$$</p>
<p>è¿™è§£é‡Šäº†ä¸ºä»€ä¹ˆ KV cache ä¼˜åŒ–å¦‚æ­¤é‡è¦ã€‚</p>
</details>
<p><strong>ç»ƒä¹  8.2</strong>: AWQ é‡åŒ–å‹ç¼©ç‡è®¡ç®—</p>
<p>å°†ä¸€ä¸ª FP16 çš„ 7B æ¨¡å‹é‡åŒ–ä¸º AWQ 4-bitï¼Œå‡è®¾æ¨¡å‹æƒé‡å  14GBï¼Œè®¡ç®—ï¼š</p>
<ol>
<li>é‡åŒ–åçš„æ¨¡å‹å¤§å°</li>
<li>ç†è®ºå‹ç¼©ç‡</li>
<li>è€ƒè™‘é¢å¤–çš„ scale/zero point å¼€é”€ï¼ˆgroup size = 128ï¼‰ï¼Œå®é™…æ¨¡å‹å¤§å°</li>
</ol>
<details>
<summary>ğŸ’¡ æç¤º</summary>
<p>4-bit é‡åŒ–ç†è®ºä¸Šå‹ç¼© 4 å€ï¼Œä½†éœ€è¦å­˜å‚¨é¢å¤–çš„é‡åŒ–å‚æ•°ã€‚</p>
</details>
<details>
<summary>ğŸ“ å‚è€ƒç­”æ¡ˆ</summary>
<ol>
<li>
<p>ç†è®ºé‡åŒ–åå¤§å°ï¼š14GB Ã· 4 = 3.5GB</p>
</li>
<li>
<p>ç†è®ºå‹ç¼©ç‡ï¼š16 bits / 4 bits = 4Ã—</p>
</li>
<li>
<p>å®é™…å¤§å°è®¡ç®—ï¼š
   - æ¯ 128 ä¸ªæƒé‡éœ€è¦é¢å¤– 32 bits (FP16 scale + zero)
   - å¼€é”€ç‡ï¼š32 / (128 Ã— 4) = 6.25%
   - å®é™…å¤§å°ï¼š3.5GB Ã— 1.0625 â‰ˆ 3.72GB
   - å®é™…å‹ç¼©ç‡ï¼š14GB / 3.72GB â‰ˆ 3.76Ã—</p>
</li>
</ol>
</details>
<p><strong>ç»ƒä¹  8.3</strong>: Flash Attention å†…å­˜èŠ‚çœ</p>
<p>ä¼ ç»Ÿæ³¨æ„åŠ›è®¡ç®—éœ€è¦å­˜å‚¨ NÃ—N çš„æ³¨æ„åŠ›çŸ©é˜µï¼ŒFlash Attention é€šè¿‡åˆ†å—è®¡ç®—é¿å…è¿™ä¸€å¼€é”€ã€‚å¯¹äºåºåˆ—é•¿åº¦ 4096ï¼Œæ‰¹å¤§å° 8ï¼Œæ³¨æ„åŠ›å¤´æ•° 32ï¼Œè®¡ç®—ä¸¤ç§æ–¹æ³•çš„å³°å€¼å†…å­˜å·®å¼‚ã€‚</p>
<details>
<summary>ğŸ’¡ æç¤º</summary>
<p>ä¼ ç»Ÿæ–¹æ³•éœ€è¦å­˜å‚¨å®Œæ•´æ³¨æ„åŠ›çŸ©é˜µï¼ŒFlash Attention åªéœ€å­˜å‚¨å—å¤§å°çš„çŸ©é˜µã€‚</p>
</details>
<details>
<summary>ğŸ“ å‚è€ƒç­”æ¡ˆ</summary>
<p>ä¼ ç»Ÿæ³¨æ„åŠ›ï¼š</p>
<ul>
<li>æ³¨æ„åŠ›çŸ©é˜µï¼š8 Ã— 32 Ã— 4096 Ã— 4096 Ã— 2 bytes (FP16)</li>
<li>= 8,589,934,592 bytes â‰ˆ 8 GB</li>
</ul>
<p>Flash Attentionï¼ˆå—å¤§å° 64ï¼‰ï¼š</p>
<ul>
<li>å—çŸ©é˜µï¼š8 Ã— 32 Ã— 64 Ã— 64 Ã— 2 bytes</li>
<li>= 2,097,152 bytes â‰ˆ 2 MB</li>
</ul>
<p>å†…å­˜èŠ‚çœï¼š8 GB â†’ 2 MBï¼Œå‡å°‘ 4000 å€ï¼</p>
</details>
<h3 id="_14">æŒ‘æˆ˜é¢˜</h3>
<p><strong>ç»ƒä¹  8.4</strong>: åŠ¨æ€ Batching è°ƒåº¦ç®—æ³•è®¾è®¡</p>
<p>è®¾è®¡ä¸€ä¸ªåŠ¨æ€ batching è°ƒåº¦å™¨ï¼Œéœ€è¦è€ƒè™‘ï¼š</p>
<ul>
<li>ä¸åŒè¯·æ±‚çš„ä¼˜å…ˆçº§ï¼ˆP0/P1/P2ï¼‰</li>
<li>å›¾åƒå¤§å°å·®å¼‚ï¼ˆ224Ã—224, 336Ã—336, 448Ã—448ï¼‰</li>
<li>æœ€å¤§æ‰¹å¤§å°é™åˆ¶ï¼ˆ32ï¼‰</li>
<li>å»¶è¿Ÿ SLA è¦æ±‚ï¼ˆP0 &lt; 100ms, P1 &lt; 500ms, P2 &lt; 2000msï¼‰</li>
</ul>
<p>è¯·ç»™å‡ºè°ƒåº¦ç­–ç•¥çš„ä¼ªä»£ç ã€‚</p>
<details>
<summary>ğŸ’¡ æç¤º</summary>
<p>è€ƒè™‘å¤šé˜Ÿåˆ—è®¾è®¡ï¼ŒæŒ‰ä¼˜å…ˆçº§å’Œå›¾åƒå¤§å°åˆ†ç»„ï¼Œå®æ–½æŠ¢å æœºåˆ¶ã€‚</p>
</details>
<details>
<summary>ğŸ“ å‚è€ƒç­”æ¡ˆ</summary>
<div class="codehilite"><pre><span></span><code><span class="k">class</span> <span class="nc">AdaptiveBatchScheduler</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># å¤šç»´åº¦é˜Ÿåˆ—</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">queues</span> <span class="o">=</span> <span class="p">{</span>
            <span class="p">(</span><span class="n">priority</span><span class="p">,</span> <span class="n">img_size</span><span class="p">):</span> <span class="n">Queue</span><span class="p">()</span>
            <span class="k">for</span> <span class="n">priority</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;P0&#39;</span><span class="p">,</span> <span class="s1">&#39;P1&#39;</span><span class="p">,</span> <span class="s1">&#39;P2&#39;</span><span class="p">]</span>
            <span class="k">for</span> <span class="n">img_size</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">224</span><span class="p">,</span> <span class="mi">336</span><span class="p">,</span> <span class="mi">448</span><span class="p">]</span>
        <span class="p">}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sla_timers</span> <span class="o">=</span> <span class="p">{}</span>

    <span class="k">def</span> <span class="nf">schedule_next_batch</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">batch</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">selected_size</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="c1"># æ­¥éª¤1ï¼šæ£€æŸ¥ P0 ç´§æ€¥è¯·æ±‚</span>
        <span class="k">for</span> <span class="n">size</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">224</span><span class="p">,</span> <span class="mi">336</span><span class="p">,</span> <span class="mi">448</span><span class="p">]:</span>
            <span class="n">queue</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">queues</span><span class="p">[(</span><span class="s1">&#39;P0&#39;</span><span class="p">,</span> <span class="n">size</span><span class="p">)]</span>
            <span class="n">urgent</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">check_sla_violation</span><span class="p">(</span><span class="n">queue</span><span class="p">,</span> <span class="mi">80</span><span class="p">)</span>  <span class="c1"># 80ms è­¦æˆ’çº¿</span>
            <span class="k">if</span> <span class="n">urgent</span><span class="p">:</span>
                <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">create_batch</span><span class="p">(</span><span class="n">urgent</span><span class="p">,</span> <span class="n">size</span><span class="p">)</span>

        <span class="c1"># æ­¥éª¤2ï¼šè´ªå¿ƒé€‰æ‹©æœ€ä¼˜æ‰¹æ¬¡</span>
        <span class="n">best_score</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
        <span class="n">best_config</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="k">for</span> <span class="p">(</span><span class="n">priority</span><span class="p">,</span> <span class="n">size</span><span class="p">),</span> <span class="n">queue</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">queues</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="k">if</span> <span class="n">queue</span><span class="o">.</span><span class="n">empty</span><span class="p">():</span>
                <span class="k">continue</span>

            <span class="c1"># è®¡ç®—å¾—åˆ†ï¼šé˜Ÿåˆ—é•¿åº¦ Ã— ä¼˜å…ˆçº§æƒé‡ / ç­‰å¾…æ—¶é—´</span>
            <span class="n">score</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">queue</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">priority_weight</span><span class="p">[</span><span class="n">priority</span><span class="p">]</span>
            <span class="n">score</span> <span class="o">/=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">avg_wait_time</span><span class="p">(</span><span class="n">queue</span><span class="p">))</span>

            <span class="k">if</span> <span class="n">score</span> <span class="o">&gt;</span> <span class="n">best_score</span><span class="p">:</span>
                <span class="n">best_score</span> <span class="o">=</span> <span class="n">score</span>
                <span class="n">best_config</span> <span class="o">=</span> <span class="p">(</span><span class="n">priority</span><span class="p">,</span> <span class="n">size</span><span class="p">)</span>
                <span class="n">selected_size</span> <span class="o">=</span> <span class="n">size</span>

        <span class="c1"># æ­¥éª¤3ï¼šæ„å»ºæ‰¹æ¬¡</span>
        <span class="k">if</span> <span class="n">best_config</span><span class="p">:</span>
            <span class="c1"># åŒå°ºå¯¸å›¾åƒæ‰“åŒ…</span>
            <span class="n">primary_queue</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">queues</span><span class="p">[</span><span class="n">best_config</span><span class="p">]</span>
            <span class="k">while</span> <span class="nb">len</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mi">32</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">primary_queue</span><span class="o">.</span><span class="n">empty</span><span class="p">():</span>
                <span class="n">batch</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">primary_queue</span><span class="o">.</span><span class="n">get</span><span class="p">())</span>

            <span class="c1"># å¡«å……ç›¸åŒå°ºå¯¸çš„ä½ä¼˜å…ˆçº§è¯·æ±‚</span>
            <span class="k">for</span> <span class="n">priority</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;P0&#39;</span><span class="p">,</span> <span class="s1">&#39;P1&#39;</span><span class="p">,</span> <span class="s1">&#39;P2&#39;</span><span class="p">]:</span>
                <span class="k">if</span> <span class="p">(</span><span class="n">priority</span><span class="p">,</span> <span class="n">selected_size</span><span class="p">)</span> <span class="o">!=</span> <span class="n">best_config</span><span class="p">:</span>
                    <span class="n">queue</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">queues</span><span class="p">[(</span><span class="n">priority</span><span class="p">,</span> <span class="n">selected_size</span><span class="p">)]</span>
                    <span class="k">while</span> <span class="nb">len</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mi">32</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">queue</span><span class="o">.</span><span class="n">empty</span><span class="p">():</span>
                        <span class="n">batch</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">queue</span><span class="o">.</span><span class="n">get</span><span class="p">())</span>

        <span class="k">return</span> <span class="n">batch</span>

    <span class="k">def</span> <span class="nf">check_sla_violation</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">queue</span><span class="p">,</span> <span class="n">threshold_ms</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;æ£€æŸ¥æ˜¯å¦æœ‰æ¥è¿‘ SLA è¿çº¦çš„è¯·æ±‚&quot;&quot;&quot;</span>
        <span class="n">urgent</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">req</span> <span class="ow">in</span> <span class="n">queue</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">req</span><span class="o">.</span><span class="n">arrival_time</span> <span class="o">&gt;</span> <span class="n">threshold_ms</span> <span class="o">/</span> <span class="mi">1000</span><span class="p">:</span>
                <span class="n">urgent</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">req</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">urgent</span>
</code></pre></div>

<p>å…³é”®è®¾è®¡ç‚¹ï¼š</p>
<ol>
<li>å¤šç»´åº¦é˜Ÿåˆ—é¿å…å¤´éƒ¨é˜»å¡</li>
<li>SLA æ„ŸçŸ¥çš„æŠ¢å è°ƒåº¦</li>
<li>åŒå°ºå¯¸å›¾åƒæ‰¹å¤„ç†æå‡æ•ˆç‡</li>
<li>åŠ¨æ€æƒé‡å¹³è¡¡ååé‡å’Œå»¶è¿Ÿ</li>
</ol>
</details>
<p><strong>ç»ƒä¹  8.5</strong>: é‡åŒ–ç­–ç•¥é€‰æ‹©</p>
<p>ä½ éœ€è¦éƒ¨ç½²ä¸€ä¸ª 34B å‚æ•°çš„ VLM æ¨¡å‹åˆ°é…å¤‡ 2Ã—A100 (40GB) çš„æœåŠ¡å™¨ã€‚æ¨¡å‹ FP16 æƒé‡å  68GBï¼Œé¢„æœŸ QPS ä¸º 50ï¼Œå¹³å‡åºåˆ—é•¿åº¦ 2048ã€‚è¯·è®¾è®¡å®Œæ•´çš„é‡åŒ–å’Œä¼˜åŒ–æ–¹æ¡ˆã€‚</p>
<details>
<summary>ğŸ’¡ æç¤º</summary>
<p>éœ€è¦ç»¼åˆè€ƒè™‘æ˜¾å­˜é™åˆ¶ã€æ¨ç†é€Ÿåº¦è¦æ±‚å’Œç²¾åº¦ä¿æŒã€‚</p>
</details>
<details>
<summary>ğŸ“ å‚è€ƒç­”æ¡ˆ</summary>
<p><strong>åˆ†æ</strong>ï¼š</p>
<ul>
<li>æ€»æ˜¾å­˜ï¼š80GB</li>
<li>æ¨¡å‹æƒé‡ï¼š68GB (FP16)</li>
<li>KV Cacheï¼šçº¦ 8-10GB (æ‰¹å¤§å° 16)</li>
<li>æ¿€æ´»å€¼ï¼šçº¦ 4-6GB</li>
</ul>
<p><strong>æ–¹æ¡ˆè®¾è®¡</strong>ï¼š</p>
<ol>
<li><strong>æ··åˆé‡åŒ–ç­–ç•¥</strong>ï¼š</li>
</ol>
<div class="codehilite"><pre><span></span><code><span class="n">config</span> <span class="o">=</span> <span class="p">{</span>
    <span class="c1"># å…³é”®å±‚ä¿æŒé«˜ç²¾åº¦</span>
    <span class="s2">&quot;vision_encoder&quot;</span><span class="p">:</span> <span class="s2">&quot;int8&quot;</span><span class="p">,</span>        <span class="c1"># 14GB â†’ 7GB</span>
    <span class="s2">&quot;projection_layer&quot;</span><span class="p">:</span> <span class="s2">&quot;fp16&quot;</span><span class="p">,</span>      <span class="c1"># 0.5GB (ä¸å˜)</span>
    <span class="s2">&quot;llm.layers[0:8]&quot;</span><span class="p">:</span> <span class="s2">&quot;fp16&quot;</span><span class="p">,</span>      <span class="c1"># 13.5GB (ä¸å˜)</span>
    <span class="s2">&quot;llm.layers[8:32]&quot;</span><span class="p">:</span> <span class="s2">&quot;awq_4bit&quot;</span><span class="p">,</span> <span class="c1"># 40.5GB â†’ 10GB  </span>
    <span class="s2">&quot;lm_head&quot;</span><span class="p">:</span> <span class="s2">&quot;fp16&quot;</span>               <span class="c1"># 0.5GB (ä¸å˜)</span>
<span class="p">}</span>
<span class="c1"># æ€»è®¡ï¼š7 + 0.5 + 13.5 + 10 + 0.5 = 31.5GB</span>
</code></pre></div>

<ol start="2">
<li>
<p><strong>æ¨ç†ä¼˜åŒ–</strong>ï¼š
- å¯ç”¨ PagedAttentionï¼šKV cache 10GB â†’ 6GB
- ä½¿ç”¨ Flash Attention 2
- Continuous batchingï¼Œç»´æŒæ‰¹å¤§å° 12-20</p>
</li>
<li>
<p><strong>éƒ¨ç½²é…ç½®</strong>ï¼š</p>
</li>
</ol>
<div class="codehilite"><pre><span></span><code><span class="n">deployment</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;tensor_parallel&quot;</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span>
    <span class="s2">&quot;max_batch_size&quot;</span><span class="p">:</span> <span class="mi">20</span><span class="p">,</span>
    <span class="s2">&quot;max_seq_length&quot;</span><span class="p">:</span> <span class="mi">2048</span><span class="p">,</span>
    <span class="s2">&quot;gpu_memory_fraction&quot;</span><span class="p">:</span> <span class="mf">0.95</span><span class="p">,</span>
    <span class="s2">&quot;enable_cuda_graph&quot;</span><span class="p">:</span> <span class="kc">True</span>
<span class="p">}</span>
</code></pre></div>

<ol start="4">
<li>
<p><strong>é¢„æœŸæ€§èƒ½</strong>ï¼š
- æ˜¾å­˜ä½¿ç”¨ï¼š31.5GB (æ¨¡å‹) + 6GB (KV) + 4GB (æ¿€æ´») = 41.5GB / 80GB
- TTFTï¼š&lt; 200ms
- TPSï¼š60-80 tokens/s
- æ”¯æŒ QPSï¼š50-60</p>
</li>
<li>
<p><strong>é™çº§æ–¹æ¡ˆ</strong>ï¼š
- é«˜è´Ÿè½½æ—¶ï¼šæ‰¹å¤§å°é™è‡³ 8ï¼Œå…¨æ¨¡å‹ 4-bit
- ç´§æ€¥æƒ…å†µï¼šåˆ‡æ¢è‡³ 13B å¤‡ç”¨æ¨¡å‹</p>
</li>
</ol>
</details>
<p><strong>ç»ƒä¹  8.6</strong>: æ¨ç†æœåŠ¡æ•…éšœè¯Šæ–­</p>
<p>ä½ çš„ VLM æ¨ç†æœåŠ¡å‡ºç°ä»¥ä¸‹ç—‡çŠ¶ï¼š</p>
<ul>
<li>GPU åˆ©ç”¨ç‡åªæœ‰ 40%</li>
<li>P99 å»¶è¿Ÿæ˜¯ P50 çš„ 10 å€</li>
<li>æ¯å°æ—¶æœ‰ 2-3 æ¬¡ OOM é”™è¯¯</li>
<li>ç”¨æˆ·æŠ¥å‘Šå¶å°”ç”Ÿæˆå†…å®¹ä¸å®Œæ•´</li>
</ul>
<p>è¯·åˆ†æå¯èƒ½çš„åŸå› å¹¶ç»™å‡ºè§£å†³æ–¹æ¡ˆã€‚</p>
<details>
<summary>ğŸ’¡ æç¤º</summary>
<p>ä»èµ„æºåˆ©ç”¨ã€è°ƒåº¦ç­–ç•¥ã€å†…å­˜ç®¡ç†ç­‰å¤šä¸ªè§’åº¦åˆ†æã€‚</p>
</details>
<details>
<summary>ğŸ“ å‚è€ƒç­”æ¡ˆ</summary>
<p><strong>é—®é¢˜åˆ†æ</strong>ï¼š</p>
<ol>
<li>
<p><strong>GPU åˆ©ç”¨ç‡ä½ (40%)</strong>ï¼š
   - åŸå› ï¼šIO ç“¶é¢ˆæˆ–æ‰¹å¤„ç†ä¸è¶³
   - è¯Šæ–­ï¼šæ£€æŸ¥æ•°æ®åŠ è½½æ—¶é—´ã€æ‰¹å¤§å°åˆ†å¸ƒ</p>
</li>
<li>
<p><strong>P99 å»¶è¿Ÿå¼‚å¸¸</strong>ï¼š
   - åŸå› ï¼šé•¿å°¾è¯·æ±‚æˆ–èµ„æºç«äº‰
   - è¯Šæ–­ï¼šåˆ†æè¯·æ±‚é•¿åº¦åˆ†å¸ƒã€æ£€æŸ¥æ˜¯å¦æœ‰å·¨å‹è¯·æ±‚</p>
</li>
<li>
<p><strong>é—´æ­‡æ€§ OOM</strong>ï¼š
   - åŸå› ï¼šå†…å­˜æ³„æ¼æˆ–çªå‘å¤§è¯·æ±‚
   - è¯Šæ–­ï¼šç›‘æ§å†…å­˜å¢é•¿æ›²çº¿ã€æ£€æŸ¥ç‰¹å®šè¾“å…¥æ¨¡å¼</p>
</li>
<li>
<p><strong>ç”Ÿæˆä¸å®Œæ•´</strong>ï¼š
   - åŸå› ï¼šè¶…æ—¶æˆªæ–­æˆ– OOM é™é»˜å¤±è´¥
   - è¯Šæ–­ï¼šæ£€æŸ¥è¶…æ—¶é…ç½®ã€é”™è¯¯å¤„ç†é€»è¾‘</p>
</li>
</ol>
<p><strong>è§£å†³æ–¹æ¡ˆ</strong>ï¼š</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># 1. ä¼˜åŒ–æ‰¹å¤„ç†ç­–ç•¥</span>
<span class="k">class</span> <span class="nc">ImprovedScheduler</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_tokens_per_batch</span> <span class="o">=</span> <span class="mi">8192</span>  <span class="c1"># æ€» token é™åˆ¶</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_seq_length</span> <span class="o">=</span> <span class="mi">2048</span>        <span class="c1"># å•è¯·æ±‚é™åˆ¶</span>

    <span class="k">def</span> <span class="nf">create_batch</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">requests</span><span class="p">):</span>
        <span class="c1"># æŒ‰é•¿åº¦æ’åºï¼Œé¿å… padding æµªè´¹</span>
        <span class="n">requests</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">tokens</span><span class="p">))</span>

        <span class="n">batch</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">total_tokens</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="k">for</span> <span class="n">req</span> <span class="ow">in</span> <span class="n">requests</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">req</span><span class="o">.</span><span class="n">tokens</span><span class="p">)</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_seq_length</span><span class="p">:</span>
                <span class="c1"># æ‹’ç»è¶…é•¿è¯·æ±‚</span>
                <span class="n">req</span><span class="o">.</span><span class="n">reject</span><span class="p">(</span><span class="s2">&quot;Sequence too long&quot;</span><span class="p">)</span>
                <span class="k">continue</span>

            <span class="n">req_tokens</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">req</span><span class="o">.</span><span class="n">tokens</span><span class="p">)</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">batch</span> <span class="o">+</span> <span class="p">[</span><span class="n">req</span><span class="p">])</span>
            <span class="k">if</span> <span class="n">total_tokens</span> <span class="o">+</span> <span class="n">req_tokens</span> <span class="o">&lt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_tokens_per_batch</span><span class="p">:</span>
                <span class="n">batch</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">req</span><span class="p">)</span>
                <span class="n">total_tokens</span> <span class="o">+=</span> <span class="n">req_tokens</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">break</span>

        <span class="k">return</span> <span class="n">batch</span>

<span class="c1"># 2. å†…å­˜ä¿æŠ¤æœºåˆ¶</span>
<span class="k">class</span> <span class="nc">MemoryGuard</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">memory_threshold</span> <span class="o">=</span> <span class="mf">0.85</span>

    <span class="k">def</span> <span class="nf">check_memory</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">usage</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">memory_reserved</span><span class="p">()</span> <span class="o">/</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">max_memory_allocated</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">usage</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">memory_threshold</span><span class="p">:</span>
            <span class="c1"># è§¦å‘å†…å­˜æ¸…ç†</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">empty_cache</span><span class="p">()</span>
            <span class="c1"># é™çº§ç­–ç•¥</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">reduce_batch_size</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">estimate_request_memory</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">request</span><span class="p">):</span>
        <span class="c1"># é¢„ä¼°å†…å­˜éœ€æ±‚</span>
        <span class="n">kv_cache</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">layers</span> <span class="o">*</span> <span class="n">heads</span> <span class="o">*</span> <span class="n">dim</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">request</span><span class="o">.</span><span class="n">tokens</span><span class="p">)</span>
        <span class="n">activation</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">request</span><span class="o">.</span><span class="n">tokens</span><span class="p">)</span> <span class="o">*</span> <span class="n">hidden_size</span> <span class="o">*</span> <span class="mi">4</span>
        <span class="k">return</span> <span class="n">kv_cache</span> <span class="o">+</span> <span class="n">activation</span>

<span class="c1"># 3. è¯·æ±‚é¢„å¤„ç†å’ŒéªŒè¯</span>
<span class="k">def</span> <span class="nf">validate_request</span><span class="p">(</span><span class="n">request</span><span class="p">):</span>
    <span class="c1"># æ£€æŸ¥å›¾åƒå¤§å°</span>
    <span class="k">if</span> <span class="n">request</span><span class="o">.</span><span class="n">image</span><span class="o">.</span><span class="n">size</span> <span class="o">&gt;</span> <span class="n">MAX_IMAGE_SIZE</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">resize_image</span><span class="p">(</span><span class="n">request</span><span class="o">.</span><span class="n">image</span><span class="p">)</span>

    <span class="c1"># æ£€æŸ¥ token é•¿åº¦</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">request</span><span class="o">.</span><span class="n">tokens</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">MAX_TOKENS</span><span class="p">:</span>
        <span class="n">request</span><span class="o">.</span><span class="n">tokens</span> <span class="o">=</span> <span class="n">request</span><span class="o">.</span><span class="n">tokens</span><span class="p">[:</span><span class="n">MAX_TOKENS</span><span class="p">]</span>
        <span class="n">request</span><span class="o">.</span><span class="n">add_warning</span><span class="p">(</span><span class="s2">&quot;Truncated to max length&quot;</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">request</span>

<span class="c1"># 4. ç›‘æ§å’Œå‘Šè­¦</span>
<span class="nd">@app</span><span class="o">.</span><span class="n">middleware</span><span class="p">(</span><span class="s2">&quot;http&quot;</span><span class="p">)</span>
<span class="k">async</span> <span class="k">def</span> <span class="nf">monitor_middleware</span><span class="p">(</span><span class="n">request</span><span class="p">,</span> <span class="n">call_next</span><span class="p">):</span>
    <span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>

    <span class="c1"># è®°å½•è¯·æ±‚å‰çŠ¶æ€</span>
    <span class="n">gpu_util_before</span> <span class="o">=</span> <span class="n">get_gpu_utilization</span><span class="p">()</span>
    <span class="n">memory_before</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">memory_allocated</span><span class="p">()</span>

    <span class="n">response</span> <span class="o">=</span> <span class="k">await</span> <span class="n">call_next</span><span class="p">(</span><span class="n">request</span><span class="p">)</span>

    <span class="c1"># è®¡ç®—æŒ‡æ ‡</span>
    <span class="n">latency</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">start_time</span>
    <span class="n">memory_delta</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">memory_allocated</span><span class="p">()</span> <span class="o">-</span> <span class="n">memory_before</span>

    <span class="c1"># å¼‚å¸¸æ£€æµ‹</span>
    <span class="k">if</span> <span class="n">latency</span> <span class="o">&gt;</span> <span class="n">LATENCY_THRESHOLD</span><span class="p">:</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;High latency: </span><span class="si">{</span><span class="n">latency</span><span class="si">}</span><span class="s2">s&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">memory_delta</span> <span class="o">&gt;</span> <span class="n">MEMORY_SPIKE_THRESHOLD</span><span class="p">:</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Memory spike: </span><span class="si">{</span><span class="n">memory_delta</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="mf">1e9</span><span class="si">}</span><span class="s2">GB&quot;</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">response</span>
</code></pre></div>

<p><strong>å…·ä½“æªæ–½</strong>ï¼š</p>
<ol>
<li>å®æ–½è¯·æ±‚å¤§å°é™åˆ¶å’Œé¢„éªŒè¯</li>
<li>åŠ¨æ€è°ƒæ•´æ‰¹å¤§å°åŸºäºå†…å­˜ä½¿ç”¨</li>
<li>åˆ†ç¦»é•¿çŸ­è¯·æ±‚åˆ°ä¸åŒå¤„ç†é˜Ÿåˆ—  </li>
<li>æ·»åŠ è¯¦ç»†ç›‘æ§å’Œè‡ªåŠ¨é™çº§æœºåˆ¶</li>
<li>å®æ–½ä¼˜é›…çš„é”™è¯¯å¤„ç†å’Œé‡è¯•</li>
</ol>
</details>
<h2 id="gotchas">å¸¸è§é™·é˜±ä¸é”™è¯¯ (Gotchas)</h2>
<h3 id="1">1. é‡åŒ–ç›¸å…³é™·é˜±</h3>
<p><strong>é™·é˜±ï¼šç›²ç›®è¿½æ±‚ä½æ¯”ç‰¹é‡åŒ–</strong></p>
<div class="codehilite"><pre><span></span><code><span class="c1"># âŒ é”™è¯¯ï¼šæ‰€æœ‰å±‚éƒ½ç”¨ 2-bit</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">quantize_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">bits</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>  <span class="c1"># ç²¾åº¦ä¸¥é‡ä¸‹é™</span>

<span class="c1"># âœ… æ­£ç¡®ï¼šæ··åˆç²¾åº¦ç­–ç•¥</span>
<span class="n">critical_layers</span> <span class="o">=</span> <span class="n">identify_sensitive_layers</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
<span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">layer</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">named_modules</span><span class="p">():</span>
    <span class="k">if</span> <span class="n">name</span> <span class="ow">in</span> <span class="n">critical_layers</span><span class="p">:</span>
        <span class="n">quantize_layer</span><span class="p">(</span><span class="n">layer</span><span class="p">,</span> <span class="n">bits</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>  <span class="c1"># å…³é”®å±‚ä¿æŒé«˜ç²¾åº¦</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">quantize_layer</span><span class="p">(</span><span class="n">layer</span><span class="p">,</span> <span class="n">bits</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
</code></pre></div>

<p><strong>é™·é˜±ï¼šå¿½è§†æ ¡å‡†æ•°æ®è´¨é‡</strong></p>
<div class="codehilite"><pre><span></span><code><span class="c1"># âŒ é”™è¯¯ï¼šä½¿ç”¨éšæœºæ•°æ®æ ¡å‡†</span>
<span class="n">calibration_data</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">)</span>

<span class="c1"># âœ… æ­£ç¡®ï¼šä½¿ç”¨çœŸå®åˆ†å¸ƒçš„æ•°æ®</span>
<span class="n">calibration_data</span> <span class="o">=</span> <span class="n">load_representative_samples</span><span class="p">(</span>
    <span class="n">dataset</span><span class="p">,</span> 
    <span class="n">n_samples</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span>
    <span class="n">stratified</span><span class="o">=</span><span class="kc">True</span>  <span class="c1"># ç¡®ä¿è¦†ç›–å„ç§æƒ…å†µ</span>
<span class="p">)</span>
</code></pre></div>

<h3 id="2">2. æ¨ç†ä¼˜åŒ–é™·é˜±</h3>
<p><strong>é™·é˜±ï¼šè¿‡åº¦ä¼˜åŒ–å•ä¸€æŒ‡æ ‡</strong></p>
<div class="codehilite"><pre><span></span><code><span class="c1"># âŒ é”™è¯¯ï¼šåªä¼˜åŒ–ååé‡ï¼Œå¿½è§†å»¶è¿Ÿ</span>
<span class="n">config</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;max_batch_size&quot;</span><span class="p">:</span> <span class="mi">128</span><span class="p">}</span>  <span class="c1"># P99 å»¶è¿Ÿçˆ†ç‚¸</span>

<span class="c1"># âœ… æ­£ç¡®ï¼šå¹³è¡¡å¤šä¸ªæŒ‡æ ‡</span>
<span class="n">config</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;max_batch_size&quot;</span><span class="p">:</span> <span class="mi">32</span><span class="p">,</span>
    <span class="s2">&quot;max_wait_time&quot;</span><span class="p">:</span> <span class="mi">50</span><span class="p">,</span>  <span class="c1"># ms</span>
    <span class="s2">&quot;target_latency&quot;</span><span class="p">:</span> <span class="mi">200</span>  <span class="c1"># ms</span>
<span class="p">}</span>
</code></pre></div>

<p><strong>é™·é˜±ï¼šKV Cache å†…å­˜æ³„æ¼</strong></p>
<div class="codehilite"><pre><span></span><code><span class="c1"># âŒ é”™è¯¯ï¼šä¸æ¸…ç†å·²å®Œæˆè¯·æ±‚çš„ cache</span>
<span class="n">kv_cache</span><span class="p">[</span><span class="n">request_id</span><span class="p">]</span> <span class="o">=</span> <span class="n">compute_kv</span><span class="p">(</span><span class="n">request</span><span class="p">)</span>
<span class="c1"># è¯·æ±‚å®Œæˆåæœªåˆ é™¤...</span>

<span class="c1"># âœ… æ­£ç¡®ï¼šåŠæ—¶æ¸…ç†</span>
<span class="k">try</span><span class="p">:</span>
    <span class="n">kv_cache</span><span class="p">[</span><span class="n">request_id</span><span class="p">]</span> <span class="o">=</span> <span class="n">compute_kv</span><span class="p">(</span><span class="n">request</span><span class="p">)</span>
    <span class="n">result</span> <span class="o">=</span> <span class="n">generate</span><span class="p">(</span><span class="n">kv_cache</span><span class="p">[</span><span class="n">request_id</span><span class="p">])</span>
<span class="k">finally</span><span class="p">:</span>
    <span class="k">del</span> <span class="n">kv_cache</span><span class="p">[</span><span class="n">request_id</span><span class="p">]</span>  <span class="c1"># ç¡®ä¿æ¸…ç†</span>
</code></pre></div>

<h3 id="3">3. æœåŠ¡åŒ–é™·é˜±</h3>
<p><strong>é™·é˜±ï¼šå¿½è§†å†·å¯åŠ¨é—®é¢˜</strong></p>
<div class="codehilite"><pre><span></span><code><span class="c1"># âŒ é”™è¯¯ï¼šç›´æ¥å¤„ç†ç¬¬ä¸€ä¸ªè¯·æ±‚</span>
<span class="nd">@app</span><span class="o">.</span><span class="n">on_event</span><span class="p">(</span><span class="s2">&quot;startup&quot;</span><span class="p">)</span>
<span class="k">async</span> <span class="k">def</span> <span class="nf">startup</span><span class="p">():</span>
    <span class="k">global</span> <span class="n">model</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">load_model</span><span class="p">()</span>  <span class="c1"># åŠ è½½å®Œå°±ç»“æŸ</span>

<span class="c1"># âœ… æ­£ç¡®ï¼šé¢„çƒ­æ¨¡å‹</span>
<span class="nd">@app</span><span class="o">.</span><span class="n">on_event</span><span class="p">(</span><span class="s2">&quot;startup&quot;</span><span class="p">)</span> 
<span class="k">async</span> <span class="k">def</span> <span class="nf">startup</span><span class="p">():</span>
    <span class="k">global</span> <span class="n">model</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">load_model</span><span class="p">()</span>
    <span class="c1"># é¢„çƒ­ï¼šè¿è¡Œå‡ ä¸ªæ¨ç†é¿å…é¦–æ¬¡è°ƒç”¨æ…¢</span>
    <span class="n">warmup_inputs</span> <span class="o">=</span> <span class="n">create_dummy_inputs</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">3</span><span class="p">):</span>
        <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">warmup_inputs</span><span class="p">)</span>
</code></pre></div>

<p><strong>é™·é˜±ï¼šåŒæ­¥é˜»å¡æ“ä½œ</strong></p>
<div class="codehilite"><pre><span></span><code><span class="c1"># âŒ é”™è¯¯ï¼šåŒæ­¥å›¾åƒå¤„ç†é˜»å¡äº‹ä»¶å¾ªç¯</span>
<span class="k">def</span> <span class="nf">process_request</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">text</span><span class="p">):</span>
    <span class="n">processed_image</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">resize</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="p">(</span><span class="mi">336</span><span class="p">,</span> <span class="mi">336</span><span class="p">))</span>  <span class="c1"># é˜»å¡</span>
    <span class="k">return</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">processed_image</span><span class="p">,</span> <span class="n">text</span><span class="p">)</span>

<span class="c1"># âœ… æ­£ç¡®ï¼šå¼‚æ­¥å¤„ç†</span>
<span class="k">async</span> <span class="k">def</span> <span class="nf">process_request</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">text</span><span class="p">):</span>
    <span class="n">processed_image</span> <span class="o">=</span> <span class="k">await</span> <span class="n">asyncio</span><span class="o">.</span><span class="n">to_thread</span><span class="p">(</span>
        <span class="n">cv2</span><span class="o">.</span><span class="n">resize</span><span class="p">,</span> <span class="n">image</span><span class="p">,</span> <span class="p">(</span><span class="mi">336</span><span class="p">,</span> <span class="mi">336</span><span class="p">)</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="k">await</span> <span class="n">model</span><span class="o">.</span><span class="n">generate_async</span><span class="p">(</span><span class="n">processed_image</span><span class="p">,</span> <span class="n">text</span><span class="p">)</span>
</code></pre></div>

<h3 id="4">4. ç›‘æ§ç›²åŒº</h3>
<p><strong>é™·é˜±ï¼šåªç›‘æ§å¹³å‡å€¼</strong></p>
<div class="codehilite"><pre><span></span><code><span class="c1"># âŒ é”™è¯¯ï¼šå¹³å‡å»¶è¿Ÿçœ‹èµ·æ¥å¾ˆå¥½</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Avg latency: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">latencies</span><span class="p">)</span><span class="si">}</span><span class="s2">ms&quot;</span><span class="p">)</span>  <span class="c1"># 200ms</span>

<span class="c1"># âœ… æ­£ç¡®ï¼šå…³æ³¨åˆ†ä½æ•°</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;P50: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">percentile</span><span class="p">(</span><span class="n">latencies</span><span class="p">,</span><span class="w"> </span><span class="mi">50</span><span class="p">)</span><span class="si">}</span><span class="s2">ms&quot;</span><span class="p">)</span>  <span class="c1"># 150ms</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;P95: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">percentile</span><span class="p">(</span><span class="n">latencies</span><span class="p">,</span><span class="w"> </span><span class="mi">95</span><span class="p">)</span><span class="si">}</span><span class="s2">ms&quot;</span><span class="p">)</span>  <span class="c1"># 800msï¼</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;P99: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">percentile</span><span class="p">(</span><span class="n">latencies</span><span class="p">,</span><span class="w"> </span><span class="mi">99</span><span class="p">)</span><span class="si">}</span><span class="s2">ms&quot;</span><span class="p">)</span>  <span class="c1"># 2000msï¼ï¼</span>
</code></pre></div>

<h2 id="_15">æœ€ä½³å®è·µæ£€æŸ¥æ¸…å•</h2>
<h3 id="_16">éƒ¨ç½²å‰æ£€æŸ¥</h3>
<p><strong>æ¨¡å‹ä¼˜åŒ–</strong></p>
<ul>
<li>[ ] é€‰æ‹©åˆé€‚çš„é‡åŒ–æ–¹æ¡ˆï¼ˆINT8/4-bitï¼‰</li>
<li>[ ] éªŒè¯é‡åŒ–åç²¾åº¦æŸå¤± &lt; é˜ˆå€¼</li>
<li>[ ] å…³é”®å±‚ä¿æŒé«˜ç²¾åº¦</li>
<li>[ ] ä½¿ç”¨ä»£è¡¨æ€§æ•°æ®æ ¡å‡†</li>
</ul>
<p><strong>æ¨ç†é…ç½®</strong></p>
<ul>
<li>[ ] å¯ç”¨ Flash Attention</li>
<li>[ ] é…ç½® PagedAttention</li>
<li>[ ] è®¾ç½®åˆç†çš„æ‰¹å¤§å°ä¸Šé™</li>
<li>[ ] å®æ–½åŠ¨æ€ batching</li>
</ul>
<p><strong>æœåŠ¡æ¶æ„</strong></p>
<ul>
<li>[ ] å®ç°å¥åº·æ£€æŸ¥æ¥å£</li>
<li>[ ] é…ç½®è´Ÿè½½å‡è¡¡</li>
<li>[ ] è®¾ç½®è¯·æ±‚è¶…æ—¶</li>
<li>[ ] å®ç°ä¼˜é›…å…³é—­</li>
</ul>
<h3 id="_17">éƒ¨ç½²ä¸­ç›‘æ§</h3>
<p><strong>æ€§èƒ½æŒ‡æ ‡</strong></p>
<ul>
<li>[ ] TTFT &lt; ç›®æ ‡å€¼</li>
<li>[ ] TPS æ»¡è¶³éœ€æ±‚</li>
<li>[ ] GPU åˆ©ç”¨ç‡ &gt; 80%</li>
<li>[ ] å†…å­˜ä½¿ç”¨ç¨³å®š</li>
</ul>
<p><strong>è´¨é‡æŒ‡æ ‡</strong></p>
<ul>
<li>[ ] é”™è¯¯ç‡ &lt; 0.1%</li>
<li>[ ] ç”Ÿæˆè´¨é‡è¯„åˆ†è¾¾æ ‡</li>
<li>[ ] æ— å†…å®¹æˆªæ–­é—®é¢˜</li>
</ul>
<p><strong>ç¨³å®šæ€§</strong></p>
<ul>
<li>[ ] æ— å†…å­˜æ³„æ¼</li>
<li>[ ] P99 å»¶è¿Ÿç¨³å®š</li>
<li>[ ] è‡ªåŠ¨æ•…éšœæ¢å¤å·¥ä½œ</li>
</ul>
<h3 id="_18">æŒç»­ä¼˜åŒ–</h3>
<p><strong>A/B æµ‹è¯•</strong></p>
<ul>
<li>[ ] æ–°ä¼˜åŒ–å…ˆå°æµé‡æµ‹è¯•</li>
<li>[ ] æ”¶é›†è¶³å¤Ÿæ ·æœ¬é‡</li>
<li>[ ] å¤šç»´åº¦æŒ‡æ ‡è¯„ä¼°</li>
<li>[ ] æœ‰å›æ»šé¢„æ¡ˆ</li>
</ul>
<p><strong>è¿­ä»£æ”¹è¿›</strong></p>
<ul>
<li>[ ] å®šæœŸ review æ…¢æŸ¥è¯¢</li>
<li>[ ] åˆ†æé”™è¯¯æ—¥å¿—æ¨¡å¼  </li>
<li>[ ] æ”¶é›†ç”¨æˆ·åé¦ˆ</li>
<li>[ ] è·Ÿè¸ªæ–°æŠ€æœ¯è¿›å±•</li>
</ul>
<p><strong>å®¹é‡è§„åˆ’</strong></p>
<ul>
<li>[ ] é¢„æµ‹æµé‡å¢é•¿</li>
<li>[ ] åˆ¶å®šæ‰©å®¹è®¡åˆ’</li>
<li>[ ] ä¼˜åŒ–èµ„æºåˆ©ç”¨ç‡</li>
<li>[ ] æˆæœ¬æ•ˆç›Šåˆ†æ</li>
</ul>
            </article>
            
            <nav class="page-nav"><a href="chapter7.html" class="nav-link prev">â† ç¬¬ 7 ç« ï¼šè¯„ä¼°ä½“ç³»è®¾è®¡</a><a href="chapter9.html" class="nav-link next">ç¬¬ 9 ç« ï¼šCUDA OOM è°ƒè¯•å®Œå…¨æŒ‡å— â†’</a></nav>
        </main>
    </div>
</body>
</html>