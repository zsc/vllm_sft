<!DOCTYPE html>
<html lang="zh">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <base href="./">
    <title>第 9 章：CUDA OOM 调试完全指南</title>
    <link rel="stylesheet" href="assets/style.css">
    <link rel="stylesheet" href="assets/highlight.css">
    <script src="assets/script.js" defer></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>
        window.MathJax = {
            tex: {
                inlineMath: [['$', '$']],
                displayMath: [['$$', '$$']],
                processEscapes: false,
                packages: {'[+]': ['noerrors', 'ams']}
            },
            options: {
                ignoreHtmlClass: 'tex2jax_ignore',
                processHtmlClass: 'tex2jax_process'
            },
            loader: {
                load: ['[tex]/noerrors', '[tex]/ams']
            }
        };
    </script>
</head>
<body>
    <div class="container">
        <nav id="sidebar" class="sidebar">
            <div class="sidebar-header">
                <h3>目录</h3>
                <button id="sidebar-toggle" class="sidebar-toggle">
                    <span></span>
                    <span></span>
                    <span></span>
                </button>
            </div>
            <div class="sidebar-search">
                <input type="text" id="sidebar-search-input" placeholder="搜索..." autocomplete="off">
            </div>
            <div id="tree-container">
                <nav class="tree-nav" role="tree">
                    <div class="tree-item " >
                        <a href="index.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">视觉语言模型（VLM）的监督微调与强化学习实战教程</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter1.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第 1 章：VLM 架构与原理</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter2.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第 2 章：数据准备与预处理</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter3.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第 3 章：SFT 训练策略</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter4.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第 4 章：分布式训练与优化</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter5.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第 5 章：RLHF 基础与实现</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter6.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第 6 章：直接偏好优化（DPO）</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter7.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第 7 章：评估体系设计</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter8.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第 8 章：模型部署与服务化</span>
                        </a>
                    </div>
                
                    <div class="tree-item active" >
                        <a href="chapter9.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第 9 章：CUDA OOM 调试完全指南</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter10.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第 10 章：训练崩溃与 NaN 问题</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter11.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第 11 章：训练速度优化实战</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter12.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第 12 章：多机多卡调试地狱</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="CLAUDE.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">Untitled</span>
                        </a>
                    </div>
                </nav>
            </div>
        </nav>
        
        <main class="content">
            <article>
                <h1 id="9-cuda-oom">第 9 章：CUDA OOM 调试完全指南</h1>
<p>在 VLM 训练过程中，CUDA Out of Memory (OOM) 错误可能是最常见也最令人头疼的问题。当你花费数小时准备数据、配置环境，满怀期待地启动训练，却在第一个 batch 就遭遇 OOM 崩溃时，那种挫败感相信每个 AI 工程师都深有体会。本章将系统介绍 VLM 训练中的内存管理，帮助你快速诊断和解决 OOM 问题，让训练过程更加顺畅。</p>
<h2 id="_1">学习目标</h2>
<p>完成本章学习后，你将能够：</p>
<ul>
<li><strong>30 秒内定位</strong> OOM 的具体原因（模型、梯度、激活值还是优化器）</li>
<li><strong>掌握 5 种紧急处理方案</strong>，让训练立即恢复运行</li>
<li><strong>精确计算</strong>任意 VLM 配置的内存需求，避免盲目试错</li>
<li><strong>识别并规避</strong> VLM 特有的 4 类内存陷阱</li>
<li><strong>建立系统的内存优化流程</strong>，将显存利用率提升至 95% 以上</li>
</ul>
<h2 id="91">9.1 快速诊断内存占用</h2>
<p>当遭遇 OOM 时，首要任务是快速定位内存瓶颈。VLM 训练的内存占用主要分为四个部分：模型参数、梯度、激活值和优化器状态。让我们逐一分析。</p>
<h3 id="911">9.1.1 模型参数内存计算</h3>
<p>VLM 的参数内存包括三个主要组件：</p>
<div class="codehilite"><pre><span></span><code>总参数内存 = 视觉编码器 + 语言模型 + 连接层
</code></pre></div>

<p><strong>快速估算公式</strong>（以 FP16 为例）：</p>
<p>$$M_{params} = 2 \times (N_{vision} + N_{language} + N_{connector}) \text{ bytes}$$
其中：</p>
<ul>
<li>$N_{vision}$：视觉编码器参数量（如 ViT-L/14 约 304M）</li>
<li>$N_{language}$：语言模型参数量（如 Vicuna-7B 约 7B）</li>
<li>$N_{connector}$：连接层参数量（通常 &lt; 100M）</li>
</ul>
<p><strong>实例计算</strong>：LLaVA-1.5-7B</p>
<div class="codehilite"><pre><span></span><code>视觉编码器 (CLIP-ViT-L/14): 304M × 2 bytes = 608 MB
语言模型 (Vicuna-7B): 7B × 2 bytes = 14 GB
MLP 连接层: 20M × 2 bytes = 40 MB
总计: 约 14.6 GB
</code></pre></div>

<h3 id="912">9.1.2 梯度内存计算</h3>
<p>训练时每个参数都需要存储梯度，内存占用与参数相同：
$$M_{gradients} = M_{params}$$
但注意，如果冻结部分模块（如视觉编码器），该部分不产生梯度：</p>
<div class="codehilite"><pre><span></span><code>可训练参数梯度 = 总参数 - 冻结参数
</code></pre></div>

<p><strong>优化技巧</strong>：分阶段解冻</p>
<ul>
<li>Stage 1: 只训练连接层（梯度内存 &lt; 100MB）</li>
<li>Stage 2: 解冻语言模型（梯度内存约 14GB）</li>
<li>Stage 3: 全部解冻（梯度内存约 14.6GB）</li>
</ul>
<h3 id="913">9.1.3 激活值内存分析</h3>
<p>激活值（中间张量）是 OOM 的主要元凶，其大小与 batch size、序列长度成正比：
$$M_{activation} = O(B \times L \times H \times N_{layers})$$
其中：</p>
<ul>
<li>$B$：batch size</li>
<li>$L$：序列长度</li>
<li>$H$：隐藏维度</li>
<li>$N_{layers}$：层数</li>
</ul>
<p><strong>VLM 激活值特点</strong>：</p>
<ol>
<li>
<p><strong>视觉 tokens 爆炸</strong>：
   - 单张图像产生大量 tokens（如 576 个 for ViT-L/14）
   - 多图场景下激活值急剧增长</p>
</li>
<li>
<p><strong>注意力矩阵</strong>：
$$M_{attention} = B \times N_{heads} \times L^2 \times 4 \text{ bytes}$$</p>
</li>
</ol>
<p>当 $L = 2048$ 时，单个注意力层就需要 $B \times 32 \times 4M \times 4 = 512B$ MB！</p>
<h3 id="914">9.1.4 优化器状态内存</h3>
<p>不同优化器的内存占用差异巨大：</p>
<p>| 优化器 | 状态内存 | 计算公式 |</p>
<table>
<thead>
<tr>
<th>优化器</th>
<th>状态内存</th>
<th>计算公式</th>
</tr>
</thead>
<tbody>
<tr>
<td>SGD</td>
<td>0（无动量）或 $M_{params}$（有动量）</td>
<td>$M_{optimizer} = M_{params}$</td>
</tr>
<tr>
<td>Adam</td>
<td>$2 \times M_{params}$</td>
<td>一阶、二阶动量各占一份</td>
</tr>
<tr>
<td>AdamW</td>
<td>$2 \times M_{params}$</td>
<td>同 Adam</td>
</tr>
<tr>
<td>Adafactor</td>
<td>$M_{params} / N$</td>
<td>分解二阶动量，节省内存</td>
</tr>
</tbody>
</table>
<p><strong>示例</strong>：7B 模型使用 Adam</p>
<div class="codehilite"><pre><span></span><code>优化器状态 = 14 GB × 2 = 28 GB
总内存需求 = 14.6 (参数) + 14.6 (梯度) + 28 (优化器) + 激活值
           &gt; 57.2 GB + 激活值
</code></pre></div>

<p>这就是为什么单卡 V100 (32GB) 难以训练 7B 模型！</p>
<h3 id="915">9.1.5 内存占用快速诊断流程</h3>
<div class="codehilite"><pre><span></span><code><span class="kn">import</span> <span class="nn">torch</span>

<span class="k">def</span> <span class="nf">diagnose_memory</span><span class="p">():</span>
    <span class="c1"># 1. 检查当前内存使用</span>
    <span class="n">allocated</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">memory_allocated</span><span class="p">()</span> <span class="o">/</span> <span class="mi">1024</span><span class="o">**</span><span class="mi">3</span>
    <span class="n">reserved</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">memory_reserved</span><span class="p">()</span> <span class="o">/</span> <span class="mi">1024</span><span class="o">**</span><span class="mi">3</span>

    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;已分配: </span><span class="si">{</span><span class="n">allocated</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> GB&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;已预留: </span><span class="si">{</span><span class="n">reserved</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> GB&quot;</span><span class="p">)</span>

    <span class="c1"># 2. 打印详细内存快照</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">memory_summary</span><span class="p">())</span>

    <span class="c1"># 3. 定位大张量</span>
    <span class="k">for</span> <span class="n">obj</span> <span class="ow">in</span> <span class="n">gc</span><span class="o">.</span><span class="n">get_objects</span><span class="p">():</span>
        <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">is_tensor</span><span class="p">(</span><span class="n">obj</span><span class="p">)</span> <span class="ow">and</span> <span class="n">obj</span><span class="o">.</span><span class="n">is_cuda</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">obj</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">obj</span><span class="o">.</span><span class="n">dtype</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">obj</span><span class="o">.</span><span class="n">element_size</span><span class="p">()</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">obj</span><span class="o">.</span><span class="n">nelement</span><span class="p">()</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="mi">1024</span><span class="o">**</span><span class="mi">2</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> MB&quot;</span><span class="p">)</span>
</code></pre></div>

<p><strong>30 秒诊断清单</strong>：</p>
<ol>
<li>运行 <code>nvidia-smi</code> 查看总体占用</li>
<li>调用 <code>diagnose_memory()</code> 定位大张量</li>
<li>检查 batch size 和序列长度</li>
<li>确认优化器类型</li>
<li>验证是否开启混合精度</li>
</ol>
<h2 id="92">9.2 紧急处理方案</h2>
<p>当 OOM 发生时，以下方案可以快速恢复训练，按优先级排序：</p>
<h3 id="921-gradient-checkpointing">9.2.1 Gradient Checkpointing（梯度检查点）</h3>
<p>最有效的内存优化技术，用计算换内存：</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># 开启 gradient checkpointing</span>
<span class="n">model</span><span class="o">.</span><span class="n">gradient_checkpointing_enable</span><span class="p">()</span>

<span class="c1"># 对于 VLM，可以选择性开启</span>
<span class="n">vision_encoder</span><span class="o">.</span><span class="n">gradient_checkpointing_enable</span><span class="p">()</span>  <span class="c1"># 视觉编码器</span>
<span class="n">language_model</span><span class="o">.</span><span class="n">gradient_checkpointing_enable</span><span class="p">()</span>   <span class="c1"># 语言模型</span>
</code></pre></div>

<p><strong>内存节省</strong>：激活值从 $O(N_{layers})$ 降至 $O(\sqrt{N_{layers}})$</p>
<p><strong>性能影响</strong>：训练速度降低 15-30%</p>
<p><strong>最佳实践</strong>：</p>
<ul>
<li>优先在语言模型上开启（层数多，效果明显）</li>
<li>视觉编码器可选（层数少，收益有限）</li>
<li>结合 FlashAttention 使用效果更佳</li>
</ul>
<h3 id="922-batch-size">9.2.2 Batch Size 动态调整</h3>
<p>智能调整 batch size，最大化显存利用：</p>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">find_optimal_batch_size</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">initial_bs</span><span class="o">=</span><span class="mi">32</span><span class="p">):</span>
    <span class="n">batch_size</span> <span class="o">=</span> <span class="n">initial_bs</span>

    <span class="k">while</span> <span class="n">batch_size</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="c1"># 尝试前向传播</span>
            <span class="n">dummy_batch</span> <span class="o">=</span> <span class="n">create_dummy_batch</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">dummy_batch</span><span class="p">)</span>
            <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;最佳 batch size: </span><span class="si">{</span><span class="n">batch_size</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">batch_size</span>

        <span class="k">except</span> <span class="ne">RuntimeError</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
            <span class="k">if</span> <span class="s2">&quot;out of memory&quot;</span> <span class="ow">in</span> <span class="nb">str</span><span class="p">(</span><span class="n">e</span><span class="p">):</span>
                <span class="c1"># 清理缓存</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">empty_cache</span><span class="p">()</span>
                <span class="c1"># 减半重试</span>
                <span class="n">batch_size</span> <span class="o">=</span> <span class="n">batch_size</span> <span class="o">//</span> <span class="mi">2</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="n">e</span>

    <span class="k">return</span> <span class="mi">1</span>  <span class="c1"># 最小 batch size</span>
</code></pre></div>

<p><strong>梯度累积补偿</strong>：</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># 目标：等效 batch size = 32</span>
<span class="n">actual_batch_size</span> <span class="o">=</span> <span class="mi">4</span>  <span class="c1"># 受限于显存</span>
<span class="n">accumulation_steps</span> <span class="o">=</span> <span class="mi">32</span> <span class="o">//</span> <span class="mi">4</span>  <span class="c1"># 累积 8 步</span>

<span class="k">for</span> <span class="n">step</span><span class="p">,</span> <span class="n">batch</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">dataloader</span><span class="p">):</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span> <span class="o">/</span> <span class="n">accumulation_steps</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

    <span class="k">if</span> <span class="p">(</span><span class="n">step</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="n">accumulation_steps</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
</code></pre></div>

<h3 id="923">9.2.3 混合精度训练优化</h3>
<p>FP16/BF16 训练可节省 50% 内存：</p>
<div class="codehilite"><pre><span></span><code><span class="kn">from</span> <span class="nn">torch.cuda.amp</span> <span class="kn">import</span> <span class="n">autocast</span><span class="p">,</span> <span class="n">GradScaler</span>

<span class="n">scaler</span> <span class="o">=</span> <span class="n">GradScaler</span><span class="p">()</span>

<span class="k">with</span> <span class="n">autocast</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">):</span>
    <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">targets</span><span class="p">)</span>

<span class="c1"># 缩放梯度防止下溢</span>
<span class="n">scaler</span><span class="o">.</span><span class="n">scale</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
<span class="n">scaler</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">optimizer</span><span class="p">)</span>
<span class="n">scaler</span><span class="o">.</span><span class="n">update</span><span class="p">()</span>
</code></pre></div>

<p><strong>VLM 特殊考虑</strong>：</p>
<ul>
<li>视觉编码器建议保持 FP32（数值稳定性）</li>
<li>语言模型可以安全使用 FP16</li>
<li>注意力层使用 BF16 更稳定</li>
</ul>
<h3 id="924-cpu-offloading">9.2.4 CPU Offloading</h3>
<p>将部分数据转移到 CPU 内存：</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># DeepSpeed ZeRO-Offload 配置</span>
<span class="n">ds_config</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;zero_optimization&quot;</span><span class="p">:</span> <span class="p">{</span>
        <span class="s2">&quot;stage&quot;</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span>
        <span class="s2">&quot;offload_optimizer&quot;</span><span class="p">:</span> <span class="p">{</span>
            <span class="s2">&quot;device&quot;</span><span class="p">:</span> <span class="s2">&quot;cpu&quot;</span><span class="p">,</span>
            <span class="s2">&quot;pin_memory&quot;</span><span class="p">:</span> <span class="kc">True</span>
        <span class="p">},</span>
        <span class="s2">&quot;offload_param&quot;</span><span class="p">:</span> <span class="p">{</span>
            <span class="s2">&quot;device&quot;</span><span class="p">:</span> <span class="s2">&quot;cpu&quot;</span><span class="p">,</span>
            <span class="s2">&quot;pin_memory&quot;</span><span class="p">:</span> <span class="kc">True</span>
        <span class="p">}</span>
    <span class="p">}</span>
<span class="p">}</span>
</code></pre></div>

<p><strong>权衡</strong>：</p>
<ul>
<li>优点：可训练超大模型（如 65B）</li>
<li>缺点：训练速度降低 2-3 倍</li>
<li>适用：单卡训练大模型的无奈选择</li>
</ul>
<h3 id="925">9.2.5 模型并行策略</h3>
<p>当单卡无法容纳时，考虑模型并行：</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># Pipeline 并行示例</span>
<span class="kn">from</span> <span class="nn">torch.distributed.pipeline.sync</span> <span class="kn">import</span> <span class="n">Pipe</span>

<span class="c1"># 将模型分割为两部分</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
    <span class="n">vision_encoder</span><span class="p">,</span>    <span class="c1"># GPU 0</span>
    <span class="n">language_model</span>     <span class="c1"># GPU 1</span>
<span class="p">)</span>

<span class="c1"># 创建 pipeline</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">Pipe</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">balance</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">devices</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
</code></pre></div>

<p><strong>VLM 并行建议</strong>：</p>
<ul>
<li>视觉编码器和语言模型天然分离，适合 pipeline</li>
<li>Tensor 并行适合单个 Transformer 层</li>
<li>优先使用数据并行，性能最佳</li>
</ul>
<h2 id="93">9.3 内存分析工具使用</h2>
<p>掌握内存分析工具是解决 OOM 问题的关键。本节介绍 4 个必备工具及其高级用法。</p>
<h3 id="931-torchcudamemory_summary">9.3.1 torch.cuda.memory_summary() 深度解析</h3>
<p>PyTorch 内置的最强大内存分析工具：</p>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">analyze_memory_detailed</span><span class="p">():</span>
    <span class="c1"># 获取完整内存报告</span>
    <span class="n">summary</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">memory_summary</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">abbreviated</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">summary</span><span class="p">)</span>

    <span class="c1"># 关键指标解读</span>
    <span class="n">stats</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">memory_stats</span><span class="p">()</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">=== 内存使用细分 ===&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;当前分配: </span><span class="si">{</span><span class="n">stats</span><span class="p">[</span><span class="s1">&#39;allocated_bytes.all.current&#39;</span><span class="p">]</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="mi">1024</span><span class="o">**</span><span class="mi">3</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> GB&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;峰值分配: </span><span class="si">{</span><span class="n">stats</span><span class="p">[</span><span class="s1">&#39;allocated_bytes.all.peak&#39;</span><span class="p">]</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="mi">1024</span><span class="o">**</span><span class="mi">3</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> GB&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;预留内存: </span><span class="si">{</span><span class="n">stats</span><span class="p">[</span><span class="s1">&#39;reserved_bytes.all.current&#39;</span><span class="p">]</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="mi">1024</span><span class="o">**</span><span class="mi">3</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> GB&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;活跃内存块: </span><span class="si">{</span><span class="n">stats</span><span class="p">[</span><span class="s1">&#39;active_bytes.all.current&#39;</span><span class="p">]</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="mi">1024</span><span class="o">**</span><span class="mi">3</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> GB&quot;</span><span class="p">)</span>

    <span class="c1"># 内存碎片分析</span>
    <span class="n">fragmentation</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="p">(</span><span class="n">stats</span><span class="p">[</span><span class="s1">&#39;allocated_bytes.all.current&#39;</span><span class="p">]</span> <span class="o">/</span> 
                        <span class="n">stats</span><span class="p">[</span><span class="s1">&#39;reserved_bytes.all.current&#39;</span><span class="p">])</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;内存碎片率: </span><span class="si">{</span><span class="n">fragmentation</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="mi">100</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2">%&quot;</span><span class="p">)</span>

    <span class="c1"># OOM 次数</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;OOM 重试次数: </span><span class="si">{</span><span class="n">stats</span><span class="p">[</span><span class="s1">&#39;num_ooms&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</code></pre></div>

<p><strong>关键指标解读</strong>：</p>
<ul>
<li><strong>Allocated vs Reserved</strong>：Reserved 是 PyTorch 向 CUDA 申请的总内存，Allocated 是实际使用的</li>
<li><strong>碎片率 &gt; 20%</strong>：需要调用 <code>torch.cuda.empty_cache()</code> 整理内存</li>
<li><strong>num_ooms &gt; 0</strong>：说明发生过 OOM 并自动重试</li>
</ul>
<h3 id="932-nvidia-smi">9.3.2 nvidia-smi 高级用法</h3>
<p>不只是看显存占用，更多高级功能：</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># 1. 持续监控（每 0.1 秒刷新）</span>
nvidia-smi<span class="w"> </span>-l<span class="w"> </span><span class="m">0</span>.1

<span class="c1"># 2. 只显示内存信息</span>
nvidia-smi<span class="w"> </span>--query-gpu<span class="o">=</span>memory.used,memory.free,memory.total<span class="w"> </span><span class="se">\</span>
<span class="w">           </span>--format<span class="o">=</span>csv,noheader,nounits<span class="w"> </span>-l<span class="w"> </span><span class="m">1</span>

<span class="c1"># 3. 监控特定进程</span>
nvidia-smi<span class="w"> </span>pmon<span class="w"> </span>-i<span class="w"> </span><span class="m">0</span>

<span class="c1"># 4. 导出详细日志用于分析</span>
nvidia-smi<span class="w"> </span>--query-gpu<span class="o">=</span>timestamp,name,memory.used,memory.free,utilization.gpu<span class="w"> </span><span class="se">\</span>
<span class="w">           </span>--format<span class="o">=</span>csv<span class="w"> </span>-l<span class="w"> </span><span class="m">1</span><span class="w"> </span>&gt;<span class="w"> </span>gpu_log.csv
</code></pre></div>

<p><strong>Python 集成监控</strong>：</p>
<div class="codehilite"><pre><span></span><code><span class="kn">import</span> <span class="nn">subprocess</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="k">def</span> <span class="nf">monitor_gpu_memory</span><span class="p">():</span>
    <span class="n">result</span> <span class="o">=</span> <span class="n">subprocess</span><span class="o">.</span><span class="n">run</span><span class="p">([</span>
        <span class="s1">&#39;nvidia-smi&#39;</span><span class="p">,</span> 
        <span class="s1">&#39;--query-gpu=memory.used,memory.free,memory.total&#39;</span><span class="p">,</span>
        <span class="s1">&#39;--format=csv,noheader,nounits&#39;</span>
    <span class="p">],</span> <span class="n">capture_output</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">text</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="n">lines</span> <span class="o">=</span> <span class="n">result</span><span class="o">.</span><span class="n">stdout</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">line</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">lines</span><span class="p">):</span>
        <span class="n">used</span><span class="p">,</span> <span class="n">free</span><span class="p">,</span> <span class="n">total</span> <span class="o">=</span> <span class="nb">map</span><span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="n">line</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;, &#39;</span><span class="p">))</span>
        <span class="n">usage_percent</span> <span class="o">=</span> <span class="p">(</span><span class="n">used</span> <span class="o">/</span> <span class="n">total</span><span class="p">)</span> <span class="o">*</span> <span class="mi">100</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;GPU </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">used</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">total</span><span class="si">}</span><span class="s2"> MB (</span><span class="si">{</span><span class="n">usage_percent</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2">%)&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">usage_percent</span> <span class="o">&gt;</span> <span class="mi">90</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;⚠️  GPU </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2"> 内存使用超过 90%！&quot;</span><span class="p">)</span>
</code></pre></div>

<h3 id="933-memory-profiler">9.3.3 Memory Profiler 实战</h3>
<p>使用 PyTorch Profiler 追踪内存分配：</p>
<div class="codehilite"><pre><span></span><code><span class="kn">from</span> <span class="nn">torch.profiler</span> <span class="kn">import</span> <span class="n">profile</span><span class="p">,</span> <span class="n">ProfilerActivity</span><span class="p">,</span> <span class="n">record_function</span>

<span class="k">def</span> <span class="nf">profile_memory_usage</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">dataloader</span><span class="p">):</span>
    <span class="k">with</span> <span class="n">profile</span><span class="p">(</span>
        <span class="n">activities</span><span class="o">=</span><span class="p">[</span><span class="n">ProfilerActivity</span><span class="o">.</span><span class="n">CPU</span><span class="p">,</span> <span class="n">ProfilerActivity</span><span class="o">.</span><span class="n">CUDA</span><span class="p">],</span>
        <span class="n">profile_memory</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">record_shapes</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">with_stack</span><span class="o">=</span><span class="kc">True</span>
    <span class="p">)</span> <span class="k">as</span> <span class="n">prof</span><span class="p">:</span>

        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">batch</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">dataloader</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">i</span> <span class="o">&gt;=</span> <span class="mi">3</span><span class="p">:</span>  <span class="c1"># 只分析前 3 个 batch</span>
                <span class="k">break</span>

            <span class="k">with</span> <span class="n">record_function</span><span class="p">(</span><span class="s2">&quot;forward&quot;</span><span class="p">):</span>
                <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>

            <span class="k">with</span> <span class="n">record_function</span><span class="p">(</span><span class="s2">&quot;loss&quot;</span><span class="p">):</span>
                <span class="n">loss</span> <span class="o">=</span> <span class="n">compute_loss</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">batch</span><span class="p">[</span><span class="s1">&#39;labels&#39;</span><span class="p">])</span>

            <span class="k">with</span> <span class="n">record_function</span><span class="p">(</span><span class="s2">&quot;backward&quot;</span><span class="p">):</span>
                <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

            <span class="k">with</span> <span class="n">record_function</span><span class="p">(</span><span class="s2">&quot;optimizer&quot;</span><span class="p">):</span>
                <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
                <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

    <span class="c1"># 输出分析结果</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">prof</span><span class="o">.</span><span class="n">key_averages</span><span class="p">()</span><span class="o">.</span><span class="n">table</span><span class="p">(</span><span class="n">sort_by</span><span class="o">=</span><span class="s2">&quot;cuda_memory_usage&quot;</span><span class="p">,</span> <span class="n">row_limit</span><span class="o">=</span><span class="mi">10</span><span class="p">))</span>

    <span class="c1"># 生成 Chrome 追踪文件</span>
    <span class="n">prof</span><span class="o">.</span><span class="n">export_chrome_trace</span><span class="p">(</span><span class="s2">&quot;memory_trace.json&quot;</span><span class="p">)</span>

    <span class="c1"># 找出内存热点</span>
    <span class="k">for</span> <span class="n">evt</span> <span class="ow">in</span> <span class="n">prof</span><span class="o">.</span><span class="n">key_averages</span><span class="p">():</span>
        <span class="k">if</span> <span class="n">evt</span><span class="o">.</span><span class="n">cuda_memory_usage</span> <span class="o">&gt;</span> <span class="mi">100</span> <span class="o">*</span> <span class="mi">1024</span> <span class="o">*</span> <span class="mi">1024</span><span class="p">:</span>  <span class="c1"># &gt; 100MB</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;内存热点: </span><span class="si">{</span><span class="n">evt</span><span class="o">.</span><span class="n">key</span><span class="si">}</span><span class="s2">, 使用: </span><span class="si">{</span><span class="n">evt</span><span class="o">.</span><span class="n">cuda_memory_usage</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="mi">1024</span><span class="o">**</span><span class="mi">2</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2"> MB&quot;</span><span class="p">)</span>
</code></pre></div>

<p><strong>分析技巧</strong>：</p>
<ol>
<li>用 Chrome 浏览器打开 <code>chrome://tracing</code>，加载 json 文件</li>
<li>查看内存分配时间线，定位峰值</li>
<li>识别内存泄漏（持续增长的曲线）</li>
</ol>
<h3 id="934">9.3.4 自定义内存监控</h3>
<p>构建实时内存监控系统：</p>
<div class="codehilite"><pre><span></span><code><span class="kn">import</span> <span class="nn">threading</span>
<span class="kn">import</span> <span class="nn">time</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">deque</span>

<span class="k">class</span> <span class="nc">MemoryMonitor</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">interval</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">max_history</span><span class="o">=</span><span class="mi">100</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">interval</span> <span class="o">=</span> <span class="n">interval</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_history</span> <span class="o">=</span> <span class="n">max_history</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">memory_history</span> <span class="o">=</span> <span class="n">deque</span><span class="p">(</span><span class="n">maxlen</span><span class="o">=</span><span class="n">max_history</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">time_history</span> <span class="o">=</span> <span class="n">deque</span><span class="p">(</span><span class="n">maxlen</span><span class="o">=</span><span class="n">max_history</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">running</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">thread</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">def</span> <span class="nf">start</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">running</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">thread</span> <span class="o">=</span> <span class="n">threading</span><span class="o">.</span><span class="n">Thread</span><span class="p">(</span><span class="n">target</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_monitor_loop</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">thread</span><span class="o">.</span><span class="n">start</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">stop</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">running</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">thread</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">thread</span><span class="o">.</span><span class="n">join</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">_monitor_loop</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
        <span class="k">while</span> <span class="bp">self</span><span class="o">.</span><span class="n">running</span><span class="p">:</span>
            <span class="c1"># 记录内存使用</span>
            <span class="n">allocated</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">memory_allocated</span><span class="p">()</span> <span class="o">/</span> <span class="mi">1024</span><span class="o">**</span><span class="mi">3</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">memory_history</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">allocated</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">time_history</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">start_time</span><span class="p">)</span>

            <span class="c1"># 检测异常</span>
            <span class="k">if</span> <span class="n">allocated</span> <span class="o">&gt;</span> <span class="mf">0.9</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">get_device_properties</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">total_memory</span> <span class="o">/</span> <span class="mi">1024</span><span class="o">**</span><span class="mi">3</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;⚠️  内存告警: </span><span class="si">{</span><span class="n">allocated</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> GB&quot;</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_dump_tensors</span><span class="p">()</span>

            <span class="n">time</span><span class="o">.</span><span class="n">sleep</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">interval</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_dump_tensors</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;输出占用内存最大的张量&quot;&quot;&quot;</span>
        <span class="n">tensors</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">obj</span> <span class="ow">in</span> <span class="n">gc</span><span class="o">.</span><span class="n">get_objects</span><span class="p">():</span>
            <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">is_tensor</span><span class="p">(</span><span class="n">obj</span><span class="p">)</span> <span class="ow">and</span> <span class="n">obj</span><span class="o">.</span><span class="n">is_cuda</span><span class="p">:</span>
                <span class="n">tensors</span><span class="o">.</span><span class="n">append</span><span class="p">((</span>
                    <span class="n">obj</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="o">*</span> <span class="n">obj</span><span class="o">.</span><span class="n">element_size</span><span class="p">(),</span>
                    <span class="nb">str</span><span class="p">(</span><span class="n">obj</span><span class="o">.</span><span class="n">shape</span><span class="p">),</span>
                    <span class="nb">str</span><span class="p">(</span><span class="n">obj</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
                <span class="p">))</span>

        <span class="n">tensors</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">reverse</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">=== Top 5 内存占用张量 ===&quot;</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">size</span><span class="p">,</span> <span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span> <span class="ow">in</span> <span class="n">tensors</span><span class="p">[:</span><span class="mi">5</span><span class="p">]:</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">size</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="mi">1024</span><span class="o">**</span><span class="mi">2</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2"> MB: </span><span class="si">{</span><span class="n">shape</span><span class="si">}</span><span class="s2"> (</span><span class="si">{</span><span class="n">dtype</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">plot</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">time_history</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">memory_history</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;时间 (秒)&#39;</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;显存使用 (GB)&#39;</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;训练过程显存监控&#39;</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="c1"># 使用示例</span>
<span class="n">monitor</span> <span class="o">=</span> <span class="n">MemoryMonitor</span><span class="p">(</span><span class="n">interval</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">monitor</span><span class="o">.</span><span class="n">start</span><span class="p">()</span>

<span class="c1"># 训练代码</span>
<span class="n">train_model</span><span class="p">()</span>

<span class="n">monitor</span><span class="o">.</span><span class="n">stop</span><span class="p">()</span>
<span class="n">monitor</span><span class="o">.</span><span class="n">plot</span><span class="p">()</span>
</code></pre></div>

<h3 id="935">9.3.5 内存泄漏检测</h3>
<p>VLM 训练中常见的内存泄漏模式：</p>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">detect_memory_leak</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">dataloader</span><span class="p">,</span> <span class="n">num_iterations</span><span class="o">=</span><span class="mi">50</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;检测训练过程中的内存泄漏&quot;&quot;&quot;</span>

    <span class="n">memory_usage</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">batch</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">dataloader</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">i</span> <span class="o">&gt;=</span> <span class="n">num_iterations</span><span class="p">:</span>
            <span class="k">break</span>

        <span class="c1"># 训练步骤</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">compute_loss</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">batch</span><span class="p">[</span><span class="s1">&#39;labels&#39;</span><span class="p">])</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

        <span class="c1"># 记录内存</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">synchronize</span><span class="p">()</span>
        <span class="n">memory_usage</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">memory_allocated</span><span class="p">())</span>

        <span class="c1"># 每 10 步检查一次</span>
        <span class="k">if</span> <span class="n">i</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">i</span> <span class="o">%</span> <span class="mi">10</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="c1"># 计算内存增长率</span>
            <span class="n">recent_memory</span> <span class="o">=</span> <span class="n">memory_usage</span><span class="p">[</span><span class="o">-</span><span class="mi">10</span><span class="p">:]</span>
            <span class="n">growth_rate</span> <span class="o">=</span> <span class="p">(</span><span class="n">recent_memory</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">recent_memory</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="o">/</span> <span class="n">recent_memory</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

            <span class="k">if</span> <span class="n">growth_rate</span> <span class="o">&gt;</span> <span class="mf">0.05</span><span class="p">:</span>  <span class="c1"># 增长超过 5%</span>
                <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;⚠️  可能存在内存泄漏！步骤 </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">, 增长率: </span><span class="si">{</span><span class="n">growth_rate</span><span class="si">:</span><span class="s2">.2%</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

                <span class="c1"># 尝试定位泄漏源</span>
                <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">():</span>
                    <span class="k">if</span> <span class="n">param</span><span class="o">.</span><span class="n">grad</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">param</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">data_ptr</span><span class="p">()</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
                        <span class="c1"># 检查梯度是否异常累积</span>
                        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">param</span><span class="p">,</span> <span class="s1">&#39;_grad_accumulation_count&#39;</span><span class="p">):</span>
                            <span class="k">if</span> <span class="n">param</span><span class="o">.</span><span class="n">_grad_accumulation_count</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
                                <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  梯度累积异常: </span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">memory_usage</span>

<span class="c1"># 常见泄漏原因及解决方案</span>
<span class="sd">&quot;&quot;&quot;</span>

<span class="sd">1. 保存了计算图：使用 loss.item() 而不是 loss</span>
<span class="sd">2. 列表累积张量：定期清理或使用 .detach()</span>
<span class="sd">3. 自定义 autograd 函数：确保正确实现 backward</span>
<span class="sd">4. hook 未释放：训练结束后调用 handle.remove()</span>
<span class="sd">&quot;&quot;&quot;</span>
</code></pre></div>

<h2 id="94-vlm">9.4 VLM 特有的内存陷阱</h2>
<p>VLM 相比纯语言模型，有其独特的内存挑战。本节深入剖析 4 类常见陷阱及解决方案。</p>
<h3 id="941">9.4.1 视觉编码器内存爆炸</h3>
<p><strong>问题现象</strong>：</p>
<ul>
<li>单张高分辨率图像就 OOM</li>
<li>多图输入时内存指数增长</li>
<li>动态分辨率导致内存不可预测</li>
</ul>
<p><strong>根本原因</strong>：</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># 问题代码示例</span>
<span class="k">def</span> <span class="nf">process_images</span><span class="p">(</span><span class="n">images</span><span class="p">,</span> <span class="n">vision_encoder</span><span class="p">):</span>
    <span class="c1"># 危险！所有图像同时编码</span>
    <span class="n">features</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">img</span> <span class="ow">in</span> <span class="n">images</span><span class="p">:</span>  <span class="c1"># images: [B, N, C, H, W]</span>
        <span class="n">feat</span> <span class="o">=</span> <span class="n">vision_encoder</span><span class="p">(</span><span class="n">img</span><span class="p">)</span>  <span class="c1"># 每次都保留在显存中</span>
        <span class="n">features</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">feat</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">features</span><span class="p">)</span>
</code></pre></div>

<p><strong>内存计算</strong>：</p>
<div class="codehilite"><pre><span></span><code>单张图像 tokens = (H/patch_size) × (W/patch_size)
ViT-L/14: 1024×1024 图像 → 5184 tokens！
内存 = B × N_images × tokens × hidden_dim × 4 bytes
     = 1 × 4 × 5184 × 1024 × 4 = 84.9 MB（仅激活值）
</code></pre></div>

<p><strong>解决方案</strong>：</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># 方案 1：批处理优化</span>
<span class="k">def</span> <span class="nf">process_images_optimized</span><span class="p">(</span><span class="n">images</span><span class="p">,</span> <span class="n">vision_encoder</span><span class="p">,</span> <span class="n">max_batch</span><span class="o">=</span><span class="mi">2</span><span class="p">):</span>
    <span class="n">B</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">W</span> <span class="o">=</span> <span class="n">images</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">features</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="c1"># 分批处理</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">max_batch</span><span class="p">):</span>
        <span class="n">batch_images</span> <span class="o">=</span> <span class="n">images</span><span class="p">[:,</span> <span class="n">i</span><span class="p">:</span><span class="n">i</span><span class="o">+</span><span class="n">max_batch</span><span class="p">]</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">amp</span><span class="o">.</span><span class="n">autocast</span><span class="p">():</span>  <span class="c1"># 使用混合精度</span>
            <span class="n">feat</span> <span class="o">=</span> <span class="n">vision_encoder</span><span class="p">(</span><span class="n">batch_images</span><span class="p">)</span>
        <span class="n">features</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">feat</span><span class="p">)</span>

        <span class="c1"># 及时清理</span>
        <span class="k">if</span> <span class="n">i</span> <span class="o">+</span> <span class="n">max_batch</span> <span class="o">&lt;</span> <span class="n">N</span><span class="p">:</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">empty_cache</span><span class="p">()</span>

    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">features</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># 方案 2：动态分辨率策略</span>
<span class="k">def</span> <span class="nf">adaptive_resolution</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">base_resolution</span><span class="o">=</span><span class="mi">336</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;根据显存动态调整分辨率&quot;&quot;&quot;</span>
    <span class="n">available_memory</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">mem_get_info</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span> <span class="o">/</span> <span class="mi">1024</span><span class="o">**</span><span class="mi">3</span>  <span class="c1"># GB</span>

    <span class="k">if</span> <span class="n">available_memory</span> <span class="o">&lt;</span> <span class="mi">4</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">interpolate</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">base_resolution</span><span class="p">,</span> <span class="n">base_resolution</span><span class="p">))</span>
    <span class="k">elif</span> <span class="n">available_memory</span> <span class="o">&lt;</span> <span class="mi">8</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">interpolate</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">base_resolution</span><span class="o">*</span><span class="mi">2</span><span class="p">,</span> <span class="n">base_resolution</span><span class="o">*</span><span class="mi">2</span><span class="p">))</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">image</span>  <span class="c1"># 原始分辨率</span>
</code></pre></div>

<h3 id="942">9.4.2 注意力矩阵内存问题</h3>
<p><strong>问题现象</strong>：</p>
<ul>
<li>长序列（&gt;2048 tokens）直接 OOM</li>
<li>多模态 token 混合导致内存激增</li>
<li>Cross-attention 内存开销巨大</li>
</ul>
<p><strong>内存分析</strong>：</p>
<p>标准注意力内存复杂度：$O(L^2)$</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># 注意力矩阵大小计算</span>
<span class="k">def</span> <span class="nf">attention_memory</span><span class="p">(</span><span class="n">seq_len</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">):</span>
    <span class="c1"># Q @ K^T 的大小</span>
    <span class="n">memory_bytes</span> <span class="o">=</span> <span class="n">batch_size</span> <span class="o">*</span> <span class="n">num_heads</span> <span class="o">*</span> <span class="n">seq_len</span> <span class="o">*</span> <span class="n">seq_len</span> <span class="o">*</span> <span class="mi">4</span>
    <span class="k">return</span> <span class="n">memory_bytes</span> <span class="o">/</span> <span class="mi">1024</span><span class="o">**</span><span class="mi">3</span>  <span class="c1"># GB</span>

<span class="c1"># 示例：2048 tokens, 32 heads, batch_size=1</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;注意力矩阵: </span><span class="si">{</span><span class="n">attention_memory</span><span class="p">(</span><span class="mi">2048</span><span class="p">,</span><span class="w"> </span><span class="mi">32</span><span class="p">,</span><span class="w"> </span><span class="mi">1</span><span class="p">)</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> GB&quot;</span><span class="p">)</span>
<span class="c1"># 输出: 0.50 GB（单层！）</span>
</code></pre></div>

<p><strong>解决方案</strong>：</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># 方案 1：Flash Attention</span>
<span class="kn">from</span> <span class="nn">flash_attn</span> <span class="kn">import</span> <span class="n">flash_attn_func</span>

<span class="k">class</span> <span class="nc">FlashAttentionVLM</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">):</span>
        <span class="c1"># Flash Attention：内存从 O(L^2) 降至 O(L)</span>
        <span class="k">return</span> <span class="n">flash_attn_func</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">causal</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="c1"># 方案 2：滑动窗口注意力</span>
<span class="k">def</span> <span class="nf">sliding_window_attention</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">window_size</span><span class="o">=</span><span class="mi">512</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;只计算局部窗口内的注意力&quot;&quot;&quot;</span>
    <span class="n">B</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">L</span><span class="p">,</span> <span class="n">D</span> <span class="o">=</span> <span class="n">q</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">attention_scores</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">L</span><span class="p">,</span> <span class="n">window_size</span> <span class="o">//</span> <span class="mi">2</span><span class="p">):</span>  <span class="c1"># 50% 重叠</span>
        <span class="n">start</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">i</span> <span class="o">-</span> <span class="n">window_size</span> <span class="o">//</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">end</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">L</span><span class="p">,</span> <span class="n">i</span> <span class="o">+</span> <span class="n">window_size</span><span class="p">)</span>

        <span class="n">q_window</span> <span class="o">=</span> <span class="n">q</span><span class="p">[:,</span> <span class="p">:,</span> <span class="n">start</span><span class="p">:</span><span class="n">end</span><span class="p">]</span>
        <span class="n">k_window</span> <span class="o">=</span> <span class="n">k</span><span class="p">[:,</span> <span class="p">:,</span> <span class="n">start</span><span class="p">:</span><span class="n">end</span><span class="p">]</span>
        <span class="n">v_window</span> <span class="o">=</span> <span class="n">v</span><span class="p">[:,</span> <span class="p">:,</span> <span class="n">start</span><span class="p">:</span><span class="n">end</span><span class="p">]</span>

        <span class="n">scores</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">q_window</span><span class="p">,</span> <span class="n">k_window</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>
        <span class="n">scores</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">scores</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">D</span><span class="p">),</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">v_window</span><span class="p">)</span>
        <span class="n">attention_scores</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">combine_windows</span><span class="p">(</span><span class="n">attention_scores</span><span class="p">)</span>

<span class="c1"># 方案 3：稀疏注意力</span>
<span class="k">class</span> <span class="nc">SparseAttentionVLM</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sparsity_ratio</span><span class="o">=</span><span class="mf">0.9</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sparsity_ratio</span> <span class="o">=</span> <span class="n">sparsity_ratio</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">):</span>
        <span class="c1"># 只保留 top-k 注意力权重</span>
        <span class="n">scores</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>

        <span class="c1"># 保留 top 10% 的值</span>
        <span class="n">k_val</span> <span class="o">=</span> <span class="nb">int</span><span class="p">((</span><span class="mi">1</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">sparsity_ratio</span><span class="p">)</span> <span class="o">*</span> <span class="n">scores</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
        <span class="n">topk_scores</span><span class="p">,</span> <span class="n">topk_indices</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">topk</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">k_val</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

        <span class="c1"># 创建稀疏矩阵</span>
        <span class="n">sparse_scores</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">scores</span><span class="p">)</span>
        <span class="n">sparse_scores</span><span class="o">.</span><span class="n">scatter_</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">topk_indices</span><span class="p">,</span> <span class="n">topk_scores</span><span class="p">)</span>

        <span class="n">attn_weights</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">sparse_scores</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">attn_weights</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>
</code></pre></div>

<h3 id="943">9.4.3 多分辨率处理陷阱</h3>
<p><strong>问题现象</strong>：</p>
<ul>
<li>不同分辨率图像导致内存波动</li>
<li>动态 padding 造成内存浪费</li>
<li>批处理时最大分辨率决定整体内存</li>
</ul>
<p><strong>示例问题</strong>：</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># 问题代码</span>
<span class="k">def</span> <span class="nf">batch_images_naive</span><span class="p">(</span><span class="n">image_list</span><span class="p">):</span>
    <span class="c1"># 所有图像 pad 到最大尺寸 → 内存浪费！</span>
    <span class="n">max_h</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">img</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span> <span class="k">for</span> <span class="n">img</span> <span class="ow">in</span> <span class="n">image_list</span><span class="p">)</span>
    <span class="n">max_w</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">img</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">img</span> <span class="ow">in</span> <span class="n">image_list</span><span class="p">)</span>

    <span class="n">padded_images</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">img</span> <span class="ow">in</span> <span class="n">image_list</span><span class="p">:</span>
        <span class="n">pad_h</span> <span class="o">=</span> <span class="n">max_h</span> <span class="o">-</span> <span class="n">img</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span>
        <span class="n">pad_w</span> <span class="o">=</span> <span class="n">max_w</span> <span class="o">-</span> <span class="n">img</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">padded</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">pad_w</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">pad_h</span><span class="p">))</span>
        <span class="n">padded_images</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">padded</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">padded_images</span><span class="p">)</span>
</code></pre></div>

<p><strong>优化方案</strong>：</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># 方案 1：分组批处理</span>
<span class="k">def</span> <span class="nf">group_by_resolution</span><span class="p">(</span><span class="n">images</span><span class="p">,</span> <span class="n">num_groups</span><span class="o">=</span><span class="mi">3</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;按分辨率分组，减少 padding 浪费&quot;&quot;&quot;</span>
    <span class="c1"># 计算每张图像的像素数</span>
    <span class="n">resolutions</span> <span class="o">=</span> <span class="p">[</span><span class="n">img</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span> <span class="o">*</span> <span class="n">img</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">img</span> <span class="ow">in</span> <span class="n">images</span><span class="p">]</span>

    <span class="c1"># K-means 聚类</span>
    <span class="n">groups</span> <span class="o">=</span> <span class="n">defaultdict</span><span class="p">(</span><span class="nb">list</span><span class="p">)</span>
    <span class="n">sorted_indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">resolutions</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">idx</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">sorted_indices</span><span class="p">):</span>
        <span class="n">group_id</span> <span class="o">=</span> <span class="n">i</span> <span class="o">*</span> <span class="n">num_groups</span> <span class="o">//</span> <span class="nb">len</span><span class="p">(</span><span class="n">sorted_indices</span><span class="p">)</span>
        <span class="n">groups</span><span class="p">[</span><span class="n">group_id</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">images</span><span class="p">[</span><span class="n">idx</span><span class="p">])</span>

    <span class="c1"># 每组单独处理</span>
    <span class="n">processed_groups</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">group_images</span> <span class="ow">in</span> <span class="n">groups</span><span class="o">.</span><span class="n">values</span><span class="p">():</span>
        <span class="n">batch</span> <span class="o">=</span> <span class="n">batch_images_naive</span><span class="p">(</span><span class="n">group_images</span><span class="p">)</span>  <span class="c1"># 组内 padding</span>
        <span class="n">processed_groups</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">processed_groups</span>

<span class="c1"># 方案 2：动态分块处理</span>
<span class="k">class</span> <span class="nc">DynamicPatchProcessor</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">base_size</span><span class="o">=</span><span class="mi">224</span><span class="p">,</span> <span class="n">max_patches</span><span class="o">=</span><span class="mi">16</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">base_size</span> <span class="o">=</span> <span class="n">base_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_patches</span> <span class="o">=</span> <span class="n">max_patches</span>

    <span class="k">def</span> <span class="nf">process</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">image</span><span class="p">):</span>
        <span class="n">H</span><span class="p">,</span> <span class="n">W</span> <span class="o">=</span> <span class="n">image</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">:]</span>

        <span class="c1"># 计算需要的 patch 数量</span>
        <span class="n">n_h</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="n">H</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">base_size</span><span class="p">)</span>
        <span class="n">n_w</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="n">W</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">base_size</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">n_h</span> <span class="o">*</span> <span class="n">n_w</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_patches</span><span class="p">:</span>
            <span class="c1"># 降采样以满足内存限制</span>
            <span class="n">scale</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">max_patches</span> <span class="o">/</span> <span class="p">(</span><span class="n">n_h</span> <span class="o">*</span> <span class="n">n_w</span><span class="p">))</span>
            <span class="n">new_h</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">H</span> <span class="o">*</span> <span class="n">scale</span><span class="p">)</span>
            <span class="n">new_w</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">W</span> <span class="o">*</span> <span class="n">scale</span><span class="p">)</span>
            <span class="n">image</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">interpolate</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">new_h</span><span class="p">,</span> <span class="n">new_w</span><span class="p">))</span>
            <span class="n">n_h</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="n">new_h</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">base_size</span><span class="p">)</span>
            <span class="n">n_w</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="n">new_w</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">base_size</span><span class="p">)</span>

        <span class="c1"># 分块处理</span>
        <span class="n">patches</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_h</span><span class="p">):</span>
            <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_w</span><span class="p">):</span>
                <span class="n">patch</span> <span class="o">=</span> <span class="n">image</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> 
                             <span class="n">i</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">base_size</span><span class="p">:(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">base_size</span><span class="p">,</span>
                             <span class="n">j</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">base_size</span><span class="p">:(</span><span class="n">j</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">base_size</span><span class="p">]</span>
                <span class="n">patches</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">patch</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">patches</span><span class="p">,</span> <span class="p">(</span><span class="n">n_h</span><span class="p">,</span> <span class="n">n_w</span><span class="p">)</span>
</code></pre></div>

<h3 id="944">9.4.4 交叉注意力内存优化</h3>
<p><strong>问题现象</strong>：</p>
<ul>
<li>视觉-语言交叉注意力内存开销巨大</li>
<li>多层交叉注意力累积导致 OOM</li>
<li>Cache 机制失效</li>
</ul>
<p><strong>内存分析</strong>：</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># 交叉注意力内存计算</span>
<span class="k">def</span> <span class="nf">cross_attention_memory</span><span class="p">(</span><span class="n">text_len</span><span class="p">,</span> <span class="n">image_tokens</span><span class="p">,</span> <span class="n">num_layers</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">):</span>
    <span class="c1"># 每层都需要存储 K, V</span>
    <span class="n">kv_memory</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">image_tokens</span> <span class="o">*</span> <span class="n">hidden_dim</span> <span class="o">*</span> <span class="mi">4</span>  <span class="c1"># bytes</span>

    <span class="c1"># 注意力矩阵</span>
    <span class="n">attn_memory</span> <span class="o">=</span> <span class="n">text_len</span> <span class="o">*</span> <span class="n">image_tokens</span> <span class="o">*</span> <span class="mi">4</span>  <span class="c1"># bytes</span>

    <span class="n">total</span> <span class="o">=</span> <span class="n">num_layers</span> <span class="o">*</span> <span class="p">(</span><span class="n">kv_memory</span> <span class="o">+</span> <span class="n">attn_memory</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">total</span> <span class="o">/</span> <span class="mi">1024</span><span class="o">**</span><span class="mi">3</span>  <span class="c1"># GB</span>

<span class="c1"># 示例：1024 text tokens, 576 image tokens, 24 layers</span>
<span class="n">memory</span> <span class="o">=</span> <span class="n">cross_attention_memory</span><span class="p">(</span><span class="mi">1024</span><span class="p">,</span> <span class="mi">576</span><span class="p">,</span> <span class="mi">24</span><span class="p">,</span> <span class="mi">4096</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;交叉注意力内存: </span><span class="si">{</span><span class="n">memory</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> GB&quot;</span><span class="p">)</span>
</code></pre></div>

<p><strong>优化策略</strong>：</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># 方案 1：共享 KV cache</span>
<span class="k">class</span> <span class="nc">SharedCrossAttention</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_layers</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="c1"># 只在第一层计算 image KV，后续层复用</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">image_proj_k</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">image_proj_v</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layers</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">([</span>
            <span class="n">CrossAttentionLayer</span><span class="p">(</span><span class="n">hidden_dim</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_layers</span><span class="p">)</span>
        <span class="p">])</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">text_hidden</span><span class="p">,</span> <span class="n">image_features</span><span class="p">):</span>
        <span class="c1"># 一次性计算所有层的 KV</span>
        <span class="n">image_k</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">image_proj_k</span><span class="p">(</span><span class="n">image_features</span><span class="p">)</span>
        <span class="n">image_v</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">image_proj_v</span><span class="p">(</span><span class="n">image_features</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">:</span>
            <span class="n">text_hidden</span> <span class="o">=</span> <span class="n">layer</span><span class="p">(</span><span class="n">text_hidden</span><span class="p">,</span> <span class="n">image_k</span><span class="p">,</span> <span class="n">image_v</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">text_hidden</span>

<span class="c1"># 方案 2：门控交叉注意力</span>
<span class="k">class</span> <span class="nc">GatedCrossAttention</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;只在必要时进行交叉注意力&quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">,</span> <span class="n">threshold</span><span class="o">=</span><span class="mf">0.5</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gate</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">threshold</span> <span class="o">=</span> <span class="n">threshold</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cross_attn</span> <span class="o">=</span> <span class="n">CrossAttentionLayer</span><span class="p">(</span><span class="n">hidden_dim</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">text_hidden</span><span class="p">,</span> <span class="n">image_features</span><span class="p">):</span>
        <span class="c1"># 计算门控值</span>
        <span class="n">gate_scores</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">gate</span><span class="p">(</span><span class="n">text_hidden</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)))</span>

        <span class="k">if</span> <span class="n">gate_scores</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">threshold</span><span class="p">:</span>
            <span class="c1"># 执行交叉注意力</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">cross_attn</span><span class="p">(</span><span class="n">text_hidden</span><span class="p">,</span> <span class="n">image_features</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># 跳过，节省内存</span>
            <span class="k">return</span> <span class="n">text_hidden</span>

<span class="c1"># 方案 3：低秩分解</span>
<span class="k">class</span> <span class="nc">LowRankCrossAttention</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;使用低秩分解减少参数和内存&quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">,</span> <span class="n">rank</span><span class="o">=</span><span class="mi">64</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">rank</span> <span class="o">=</span> <span class="n">rank</span>

        <span class="c1"># 分解 W_q, W_k, W_v</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">q_down</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="n">rank</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">q_up</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">rank</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">kv_down</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="n">rank</span> <span class="o">*</span> <span class="mi">2</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">kv_up</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">rank</span> <span class="o">*</span> <span class="mi">2</span><span class="p">,</span> <span class="n">hidden_dim</span> <span class="o">*</span> <span class="mi">2</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">text_hidden</span><span class="p">,</span> <span class="n">image_features</span><span class="p">):</span>
        <span class="c1"># 低秩投影</span>
        <span class="n">q</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">q_up</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">q_down</span><span class="p">(</span><span class="n">text_hidden</span><span class="p">))</span>
        <span class="n">kv</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">kv_up</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">kv_down</span><span class="p">(</span><span class="n">image_features</span><span class="p">))</span>
        <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="o">=</span> <span class="n">kv</span><span class="o">.</span><span class="n">chunk</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

        <span class="c1"># 标准注意力计算</span>
        <span class="n">scores</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">q</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>
        <span class="n">attn</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">attn</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>
</code></pre></div>

<h3 id="945">9.4.5 内存优化最佳实践汇总</h3>
<div class="codehilite"><pre><span></span><code><span class="k">class</span> <span class="nc">MemoryOptimizedVLM</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;集成所有内存优化技术的 VLM&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">config</span> <span class="o">=</span> <span class="n">config</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">setup_memory_optimization</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">setup_memory_optimization</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># 1. 启用梯度检查点</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">gradient_checkpointing</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">gradient_checkpointing_enable</span><span class="p">()</span>

        <span class="c1"># 2. 使用 Flash Attention</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">use_flash_attention</span><span class="p">:</span>
            <span class="n">replace_attention_with_flash_attention</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">)</span>

        <span class="c1"># 3. 混合精度训练</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">mixed_precision</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">scaler</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">amp</span><span class="o">.</span><span class="n">GradScaler</span><span class="p">()</span>

        <span class="c1"># 4. 内存监控</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">memory_monitor</span> <span class="o">=</span> <span class="n">MemoryMonitor</span><span class="p">(</span><span class="n">interval</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">train_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">):</span>
        <span class="c1"># 动态调整 batch size</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">should_reduce_batch_size</span><span class="p">():</span>
            <span class="n">batch</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">split_batch</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>

        <span class="c1"># 分组处理多分辨率图像</span>
        <span class="n">image_groups</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">group_images_by_resolution</span><span class="p">(</span><span class="n">batch</span><span class="p">[</span><span class="s1">&#39;images&#39;</span><span class="p">])</span>

        <span class="n">total_loss</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">images</span> <span class="ow">in</span> <span class="n">image_groups</span><span class="p">:</span>
            <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">amp</span><span class="o">.</span><span class="n">autocast</span><span class="p">(</span><span class="n">enabled</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">mixed_precision</span><span class="p">):</span>
                <span class="c1"># 前向传播</span>
                <span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">(</span><span class="n">images</span><span class="p">,</span> <span class="n">batch</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">])</span>
                <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">criterion</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">batch</span><span class="p">[</span><span class="s1">&#39;labels&#39;</span><span class="p">])</span>

            <span class="c1"># 反向传播</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">mixed_precision</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">scaler</span><span class="o">.</span><span class="n">scale</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

            <span class="n">total_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>

            <span class="c1"># 及时清理</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">image_groups</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">empty_cache</span><span class="p">()</span>

        <span class="k">return</span> <span class="n">total_loss</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">image_groups</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">should_reduce_batch_size</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;动态检测是否需要减小 batch size&quot;&quot;&quot;</span>
        <span class="n">memory_usage</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">memory_allocated</span><span class="p">()</span> <span class="o">/</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">max_memory_allocated</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">memory_usage</span> <span class="o">&gt;</span> <span class="mf">0.9</span>
</code></pre></div>

<h2 id="_2">本章小结</h2>
<p>本章系统介绍了 VLM 训练中 CUDA OOM 问题的诊断和解决方法。我们学习了：</p>
<p><strong>核心概念</strong>：</p>
<ul>
<li>VLM 内存占用的四大组成：模型参数、梯度、激活值、优化器状态</li>
<li>内存计算公式：精确预估任意配置的内存需求</li>
<li>VLM 特有挑战：视觉 tokens 爆炸、注意力二次复杂度、多分辨率处理</li>
</ul>
<p><strong>关键技术</strong>：</p>
<ul>
<li><strong>Gradient Checkpointing</strong>：用计算换内存，激活值从 $O(N)$ 降至 $O(\sqrt{N})$</li>
<li><strong>Flash Attention</strong>：注意力内存从 $O(L^2)$ 降至 $O(L)$</li>
<li><strong>动态批处理</strong>：根据显存实时调整 batch size</li>
<li><strong>混合精度训练</strong>：FP16/BF16 节省 50% 内存</li>
</ul>
<p><strong>实用工具</strong>：</p>
<ul>
<li><code>torch.cuda.memory_summary()</code>：深度内存分析</li>
<li><code>nvidia-smi</code> 高级用法：持续监控和日志导出</li>
<li>PyTorch Profiler：内存热点定位</li>
<li>自定义监控系统：实时预警和自动调整</li>
</ul>
<p><strong>VLM 优化策略</strong>：</p>
<ol>
<li>视觉编码器：分批处理、动态分辨率</li>
<li>注意力优化：Flash/稀疏/滑动窗口注意力</li>
<li>多分辨率：分组批处理、动态分块</li>
<li>交叉注意力：KV 共享、门控机制、低秩分解</li>
</ol>
<p>记住：<strong>OOM 不是无解的</strong>。通过系统的分析和合理的优化，即使在有限的硬件上也能训练大规模 VLM。关键是理解内存分配机制，选择合适的优化策略，并建立完善的监控体系。</p>
<h2 id="_3">练习题</h2>
<h3 id="_4">基础题</h3>
<p><strong>练习 9.1</strong>：计算 LLaVA-1.5-13B 在以下配置下的最小显存需求：</p>
<ul>
<li>视觉编码器：CLIP-ViT-L/14（304M 参数）</li>
<li>语言模型：Vicuna-13B</li>
<li>优化器：AdamW</li>
<li>批大小：1</li>
<li>序列长度：2048</li>
<li>混合精度：FP16</li>
</ul>
<p>💡 <strong>提示</strong>：分别计算参数、梯度、优化器状态的内存，激活值可按经验估算为参数的 2-3 倍。</p>
<details>
<summary>📝 参考答案</summary>
<p>内存计算：</p>
<ol>
<li>
<p>参数内存（FP16）：
   - 视觉编码器：304M × 2 = 0.61 GB
   - 语言模型：13B × 2 = 26 GB
   - 连接层：约 50M × 2 = 0.1 GB
   - 总计：26.71 GB</p>
</li>
<li>
<p>梯度内存：等于参数内存 = 26.71 GB</p>
</li>
<li>
<p>优化器状态（AdamW）：
   - 一阶动量：26.71 GB
   - 二阶动量：26.71 GB
   - 总计：53.42 GB</p>
</li>
<li>
<p>激活值（经验估算）：
   - 约参数的 2.5 倍 = 66.78 GB</p>
</li>
</ol>
<p>最小显存需求：26.71 + 26.71 + 53.42 + 66.78 = <strong>173.62 GB</strong></p>
<p>这就是为什么需要多卡训练或使用内存优化技术！</p>
</details>
<p><strong>练习 9.2</strong>：给定一个 OOM 错误信息，识别问题原因并提出解决方案：</p>
<div class="codehilite"><pre><span></span><code><span class="n">RuntimeError</span><span class="o">:</span><span class="w"> </span><span class="n">CUDA</span><span class="w"> </span><span class="n">out</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">memory</span><span class="o">.</span><span class="w"> </span><span class="n">Tried</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">allocate</span><span class="w"> </span><span class="mf">2.50</span><span class="w"> </span><span class="n">GiB</span><span class="w"> </span>
<span class="o">(</span><span class="n">GPU</span><span class="w"> </span><span class="mi">0</span><span class="o">;</span><span class="w"> </span><span class="mf">23.69</span><span class="w"> </span><span class="n">GiB</span><span class="w"> </span><span class="n">total</span><span class="w"> </span><span class="n">capacity</span><span class="o">;</span><span class="w"> </span><span class="mf">21.45</span><span class="w"> </span><span class="n">GiB</span><span class="w"> </span><span class="n">already</span><span class="w"> </span><span class="n">allocated</span><span class="o">;</span><span class="w"> </span>
<span class="mf">1.89</span><span class="w"> </span><span class="n">GiB</span><span class="w"> </span><span class="n">free</span><span class="o">;</span><span class="w"> </span><span class="mf">21.50</span><span class="w"> </span><span class="n">GiB</span><span class="w"> </span><span class="n">reserved</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="n">total</span><span class="w"> </span><span class="n">by</span><span class="w"> </span><span class="n">PyTorch</span><span class="o">)</span>
</code></pre></div>

<p>💡 <strong>提示</strong>：注意 allocated vs reserved 的差异，以及请求分配的大小。</p>
<details>
<summary>📝 参考答案</summary>
<p>问题分析：</p>
<ol>
<li>已分配：21.45 GB，已预留：21.50 GB</li>
<li>碎片率很低：(21.50 - 21.45) / 21.50 = 0.23%</li>
<li>剩余空间：1.89 GB &lt; 2.50 GB（请求）</li>
</ol>
<p>原因：内存已基本用尽，无法满足新的大块分配请求（可能是注意力矩阵）。</p>
<p>解决方案：</p>
<ol>
<li>
<p>立即措施：
   - 减小 batch size（如果 &gt; 1）
   - 启用 gradient checkpointing
   - 调用 torch.cuda.empty_cache()</p>
</li>
<li>
<p>优化措施：
   - 使用 Flash Attention（2.50 GB 暗示是注意力矩阵）
   - 启用混合精度训练
   - 考虑模型并行或 CPU offloading</p>
</li>
</ol>
</details>
<p><strong>练习 9.3</strong>：编写代码，实现一个函数自动找到最大可用 batch size：</p>
<p>💡 <strong>提示</strong>：使用二分搜索，处理 OOM 异常。</p>
<details>
<summary>📝 参考答案</summary>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">find_max_batch_size</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">create_batch_fn</span><span class="p">,</span> <span class="n">min_bs</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">max_bs</span><span class="o">=</span><span class="mi">128</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;二分搜索找到最大 batch size&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">can_run</span><span class="p">(</span><span class="n">batch_size</span><span class="p">):</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">batch</span> <span class="o">=</span> <span class="n">create_batch_fn</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)</span>
            <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">loss</span>
            <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

            <span class="c1"># 清理</span>
            <span class="k">del</span> <span class="n">output</span><span class="p">,</span> <span class="n">loss</span><span class="p">,</span> <span class="n">batch</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">empty_cache</span><span class="p">()</span>
            <span class="k">return</span> <span class="kc">True</span>

        <span class="k">except</span> <span class="ne">RuntimeError</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
            <span class="k">if</span> <span class="s2">&quot;out of memory&quot;</span> <span class="ow">in</span> <span class="nb">str</span><span class="p">(</span><span class="n">e</span><span class="p">):</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">empty_cache</span><span class="p">()</span>
                <span class="k">return</span> <span class="kc">False</span>
            <span class="k">raise</span> <span class="n">e</span>

    <span class="c1"># 二分搜索</span>
    <span class="n">left</span><span class="p">,</span> <span class="n">right</span> <span class="o">=</span> <span class="n">min_bs</span><span class="p">,</span> <span class="n">max_bs</span>
    <span class="n">best_bs</span> <span class="o">=</span> <span class="n">min_bs</span>

    <span class="k">while</span> <span class="n">left</span> <span class="o">&lt;=</span> <span class="n">right</span><span class="p">:</span>
        <span class="n">mid</span> <span class="o">=</span> <span class="p">(</span><span class="n">left</span> <span class="o">+</span> <span class="n">right</span><span class="p">)</span> <span class="o">//</span> <span class="mi">2</span>

        <span class="k">if</span> <span class="n">can_run</span><span class="p">(</span><span class="n">mid</span><span class="p">):</span>
            <span class="n">best_bs</span> <span class="o">=</span> <span class="n">mid</span>
            <span class="n">left</span> <span class="o">=</span> <span class="n">mid</span> <span class="o">+</span> <span class="mi">1</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">right</span> <span class="o">=</span> <span class="n">mid</span> <span class="o">-</span> <span class="mi">1</span>

    <span class="c1"># 验证最终结果</span>
    <span class="k">if</span> <span class="n">can_run</span><span class="p">(</span><span class="n">best_bs</span><span class="p">):</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;最大 batch size: </span><span class="si">{</span><span class="n">best_bs</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

        <span class="c1"># 留出安全边际</span>
        <span class="n">safe_bs</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">best_bs</span> <span class="o">*</span> <span class="mf">0.9</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;推荐 batch size: </span><span class="si">{</span><span class="n">safe_bs</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">safe_bs</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">best_bs</span> <span class="o">-</span> <span class="mi">1</span>
</code></pre></div>

</details>
<h3 id="_5">挑战题</h3>
<p><strong>练习 9.4</strong>：设计一个自适应内存管理系统，能够：</p>
<ul>
<li>监控内存使用趋势</li>
<li>预测 OOM 风险</li>
<li>自动调整训练参数</li>
</ul>
<p>💡 <strong>提示</strong>：考虑使用滑动窗口和线性回归预测内存增长。</p>
<details>
<summary>📝 参考答案</summary>
<div class="codehilite"><pre><span></span><code><span class="k">class</span> <span class="nc">AdaptiveMemoryManager</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">window_size</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">oom_threshold</span><span class="o">=</span><span class="mf">0.85</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">window_size</span> <span class="o">=</span> <span class="n">window_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">oom_threshold</span> <span class="o">=</span> <span class="n">oom_threshold</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">memory_history</span> <span class="o">=</span> <span class="n">deque</span><span class="p">(</span><span class="n">maxlen</span><span class="o">=</span><span class="n">window_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">step_history</span> <span class="o">=</span> <span class="n">deque</span><span class="p">(</span><span class="n">maxlen</span><span class="o">=</span><span class="n">window_size</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">step</span><span class="p">):</span>
        <span class="c1"># 记录当前内存</span>
        <span class="n">allocated</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">memory_allocated</span><span class="p">()</span> <span class="o">/</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">max_memory_allocated</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">memory_history</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">allocated</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">step_history</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">step</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">predict_oom_risk</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">future_steps</span><span class="o">=</span><span class="mi">10</span><span class="p">):</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">memory_history</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mi">3</span><span class="p">:</span>
            <span class="k">return</span> <span class="mf">0.0</span>

        <span class="c1"># 线性回归预测</span>
        <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">step_history</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">memory_history</span><span class="p">)</span>

        <span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span>
        <span class="n">model</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span>
        <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

        <span class="c1"># 预测未来内存使用</span>
        <span class="n">future_step</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">step_history</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">future_steps</span>
        <span class="n">predicted_memory</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">([[</span><span class="n">future_step</span><span class="p">]])[</span><span class="mi">0</span><span class="p">]</span>

        <span class="c1"># 计算 OOM 风险</span>
        <span class="k">if</span> <span class="n">predicted_memory</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">oom_threshold</span><span class="p">:</span>
            <span class="n">risk</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="mf">1.0</span><span class="p">,</span> <span class="p">(</span><span class="n">predicted_memory</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">oom_threshold</span><span class="p">)</span> <span class="o">/</span> <span class="mf">0.15</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">risk</span> <span class="o">=</span> <span class="mf">0.0</span>

        <span class="k">return</span> <span class="n">risk</span>

    <span class="k">def</span> <span class="nf">adjust_training_params</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">,</span> <span class="n">risk</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;根据风险调整参数&quot;&quot;&quot;</span>
        <span class="n">adjustments</span> <span class="o">=</span> <span class="p">{}</span>

        <span class="k">if</span> <span class="n">risk</span> <span class="o">&gt;</span> <span class="mf">0.8</span><span class="p">:</span>
            <span class="c1"># 高风险：激进调整</span>
            <span class="n">adjustments</span><span class="p">[</span><span class="s1">&#39;batch_size&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">batch_size</span> <span class="o">//</span> <span class="mi">2</span><span class="p">)</span>
            <span class="n">adjustments</span><span class="p">[</span><span class="s1">&#39;gradient_checkpointing&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>
            <span class="n">adjustments</span><span class="p">[</span><span class="s1">&#39;accumulation_steps&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">accumulation_steps</span> <span class="o">*</span> <span class="mi">2</span>

        <span class="k">elif</span> <span class="n">risk</span> <span class="o">&gt;</span> <span class="mf">0.5</span><span class="p">:</span>
            <span class="c1"># 中风险：温和调整</span>
            <span class="n">adjustments</span><span class="p">[</span><span class="s1">&#39;batch_size&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">batch_size</span> <span class="o">*</span> <span class="mf">0.75</span><span class="p">)</span>
            <span class="n">adjustments</span><span class="p">[</span><span class="s1">&#39;gradient_checkpointing&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>

        <span class="k">elif</span> <span class="n">risk</span> <span class="o">&gt;</span> <span class="mf">0.3</span><span class="p">:</span>
            <span class="c1"># 低风险：小幅优化</span>
            <span class="n">adjustments</span><span class="p">[</span><span class="s1">&#39;mixed_precision&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>

        <span class="k">return</span> <span class="n">adjustments</span>

    <span class="k">def</span> <span class="nf">emergency_cleanup</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;紧急内存清理&quot;&quot;&quot;</span>
        <span class="c1"># 1. 清空缓存</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">empty_cache</span><span class="p">()</span>

        <span class="c1"># 2. 删除不必要的张量</span>
        <span class="kn">import</span> <span class="nn">gc</span>
        <span class="k">for</span> <span class="n">obj</span> <span class="ow">in</span> <span class="n">gc</span><span class="o">.</span><span class="n">get_objects</span><span class="p">():</span>
            <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">is_tensor</span><span class="p">(</span><span class="n">obj</span><span class="p">)</span> <span class="ow">and</span> <span class="n">obj</span><span class="o">.</span><span class="n">is_cuda</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">obj</span><span class="o">.</span><span class="n">grad_fn</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>  <span class="c1"># 不在计算图中</span>
                    <span class="k">del</span> <span class="n">obj</span>

        <span class="c1"># 3. 同步并再次清理</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">synchronize</span><span class="p">()</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">empty_cache</span><span class="p">()</span>
        <span class="n">gc</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
</code></pre></div>

<p>使用该系统可以预防 OOM，而不是等到发生后再处理。</p>
</details>
<p><strong>练习 9.5</strong>：分析并优化以下 VLM 前向传播代码的内存使用：</p>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">images</span><span class="p">,</span> <span class="n">text_ids</span><span class="p">):</span>
    <span class="c1"># 视觉编码</span>
    <span class="n">B</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">W</span> <span class="o">=</span> <span class="n">images</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">all_features</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">B</span><span class="p">):</span>
        <span class="n">img_features</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">N</span><span class="p">):</span>
            <span class="n">feat</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">vision_encoder</span><span class="p">(</span><span class="n">images</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">])</span>
            <span class="n">img_features</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">feat</span><span class="p">)</span>
        <span class="n">all_features</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">img_features</span><span class="p">))</span>
    <span class="n">vision_features</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">all_features</span><span class="p">)</span>

    <span class="c1"># 文本嵌入</span>
    <span class="n">text_embeds</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">text_embedder</span><span class="p">(</span><span class="n">text_ids</span><span class="p">)</span>

    <span class="c1"># 交叉注意力</span>
    <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">cross_attention_layers</span><span class="p">:</span>
        <span class="n">text_embeds</span> <span class="o">=</span> <span class="n">layer</span><span class="p">(</span><span class="n">text_embeds</span><span class="p">,</span> <span class="n">vision_features</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">text_embeds</span>
</code></pre></div>

<p>💡 <strong>提示</strong>：考虑向量化、内存复用、梯度检查点。</p>
<details>
<summary>📝 参考答案</summary>
<p>优化后的代码：</p>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">images</span><span class="p">,</span> <span class="n">text_ids</span><span class="p">):</span>
    <span class="n">B</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">W</span> <span class="o">=</span> <span class="n">images</span><span class="o">.</span><span class="n">shape</span>

    <span class="c1"># 优化 1：向量化处理，避免循环</span>
    <span class="n">images_flat</span> <span class="o">=</span> <span class="n">images</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">B</span> <span class="o">*</span> <span class="n">N</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">W</span><span class="p">)</span>

    <span class="c1"># 优化 2：使用 checkpoint 减少激活值内存</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">training</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_checkpointing</span><span class="p">:</span>
        <span class="n">vision_features</span> <span class="o">=</span> <span class="n">checkpoint</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">vision_encoder</span><span class="p">,</span> <span class="n">images_flat</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">vision_features</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">vision_encoder</span><span class="p">(</span><span class="n">images_flat</span><span class="p">)</span>

    <span class="c1"># 优化 3：原地 reshape，避免额外内存分配</span>
    <span class="n">vision_features</span> <span class="o">=</span> <span class="n">vision_features</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">vision_features</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>

    <span class="c1"># 优化 4：如果 N 很大，考虑分块处理</span>
    <span class="k">if</span> <span class="n">N</span> <span class="o">&gt;</span> <span class="mi">4</span><span class="p">:</span>
        <span class="n">chunk_size</span> <span class="o">=</span> <span class="mi">2</span>
        <span class="n">chunks</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">chunk_size</span><span class="p">):</span>
            <span class="n">chunk</span> <span class="o">=</span> <span class="n">vision_features</span><span class="p">[:,</span> <span class="n">i</span><span class="p">:</span><span class="n">i</span><span class="o">+</span><span class="n">chunk_size</span><span class="p">]</span>
            <span class="n">chunks</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">chunk</span><span class="p">)</span>

            <span class="c1"># 优化 5：及时释放中间结果</span>
            <span class="k">if</span> <span class="n">i</span> <span class="o">+</span> <span class="n">chunk_size</span> <span class="o">&lt;</span> <span class="n">N</span><span class="p">:</span>
                <span class="k">del</span> <span class="n">chunk</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">empty_cache</span><span class="p">()</span>

        <span class="n">vision_features</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">chunks</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="c1"># 文本嵌入</span>
    <span class="n">text_embeds</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">text_embedder</span><span class="p">(</span><span class="n">text_ids</span><span class="p">)</span>

    <span class="c1"># 优化 6：重用 vision_features 的 KV cache</span>
    <span class="n">vision_k</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">vision_proj_k</span><span class="p">(</span><span class="n">vision_features</span><span class="p">)</span>
    <span class="n">vision_v</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">vision_proj_v</span><span class="p">(</span><span class="n">vision_features</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">cross_attention_layers</span><span class="p">:</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">training</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_checkpointing</span><span class="p">:</span>
            <span class="n">text_embeds</span> <span class="o">=</span> <span class="n">checkpoint</span><span class="p">(</span>
                <span class="n">layer</span><span class="p">,</span> <span class="n">text_embeds</span><span class="p">,</span> <span class="n">vision_k</span><span class="p">,</span> <span class="n">vision_v</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">text_embeds</span> <span class="o">=</span> <span class="n">layer</span><span class="p">(</span><span class="n">text_embeds</span><span class="p">,</span> <span class="n">vision_k</span><span class="p">,</span> <span class="n">vision_v</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">text_embeds</span>
</code></pre></div>

<p>内存节省：</p>
<ol>
<li>向量化：减少临时张量创建</li>
<li>Checkpointing：激活值内存降低 50-70%</li>
<li>KV 复用：避免每层重复计算</li>
<li>分块处理：峰值内存降低</li>
<li>及时清理：避免内存累积</li>
</ol>
</details>
<p><strong>练习 9.6</strong>：设计实验比较不同注意力实现的内存-速度权衡：</p>
<ul>
<li>标准注意力</li>
<li>Flash Attention</li>
<li>稀疏注意力</li>
<li>滑动窗口注意力</li>
</ul>
<p>💡 <strong>提示</strong>：固定序列长度，测量内存占用和推理时间。</p>
<details>
<summary>📝 参考答案</summary>
<div class="codehilite"><pre><span></span><code><span class="kn">import</span> <span class="nn">time</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="k">def</span> <span class="nf">benchmark_attention_methods</span><span class="p">():</span>
    <span class="n">seq_lengths</span> <span class="o">=</span> <span class="p">[</span><span class="mi">512</span><span class="p">,</span> <span class="mi">1024</span><span class="p">,</span> <span class="mi">2048</span><span class="p">,</span> <span class="mi">4096</span><span class="p">]</span>
    <span class="n">batch_size</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="n">hidden_dim</span> <span class="o">=</span> <span class="mi">1024</span>
    <span class="n">num_heads</span> <span class="o">=</span> <span class="mi">16</span>

    <span class="n">results</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s1">&#39;standard&#39;</span><span class="p">:</span> <span class="p">{</span><span class="s1">&#39;memory&#39;</span><span class="p">:</span> <span class="p">[],</span> <span class="s1">&#39;time&#39;</span><span class="p">:</span> <span class="p">[]},</span>
        <span class="s1">&#39;flash&#39;</span><span class="p">:</span> <span class="p">{</span><span class="s1">&#39;memory&#39;</span><span class="p">:</span> <span class="p">[],</span> <span class="s1">&#39;time&#39;</span><span class="p">:</span> <span class="p">[]},</span>
        <span class="s1">&#39;sparse&#39;</span><span class="p">:</span> <span class="p">{</span><span class="s1">&#39;memory&#39;</span><span class="p">:</span> <span class="p">[],</span> <span class="s1">&#39;time&#39;</span><span class="p">:</span> <span class="p">[]},</span>
        <span class="s1">&#39;sliding&#39;</span><span class="p">:</span> <span class="p">{</span><span class="s1">&#39;memory&#39;</span><span class="p">:</span> <span class="p">[],</span> <span class="s1">&#39;time&#39;</span><span class="p">:</span> <span class="p">[]}</span>
    <span class="p">}</span>

    <span class="k">for</span> <span class="n">seq_len</span> <span class="ow">in</span> <span class="n">seq_lengths</span><span class="p">:</span>
        <span class="c1"># 准备输入</span>
        <span class="n">q</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">hidden_dim</span> <span class="o">//</span> <span class="n">num_heads</span><span class="p">)</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>
        <span class="n">k</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn_like</span><span class="p">(</span><span class="n">q</span><span class="p">)</span>
        <span class="n">v</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn_like</span><span class="p">(</span><span class="n">q</span><span class="p">)</span>

        <span class="c1"># 1. 标准注意力</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">reset_peak_memory_stats</span><span class="p">()</span>
        <span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>

        <span class="n">scores</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">q</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>
        <span class="n">attn</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">attn</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>

        <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">synchronize</span><span class="p">()</span>
        <span class="n">standard_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">start</span>
        <span class="n">standard_memory</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">max_memory_allocated</span><span class="p">()</span> <span class="o">/</span> <span class="mi">1024</span><span class="o">**</span><span class="mi">3</span>

        <span class="n">results</span><span class="p">[</span><span class="s1">&#39;standard&#39;</span><span class="p">][</span><span class="s1">&#39;time&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">standard_time</span><span class="p">)</span>
        <span class="n">results</span><span class="p">[</span><span class="s1">&#39;standard&#39;</span><span class="p">][</span><span class="s1">&#39;memory&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">standard_memory</span><span class="p">)</span>

        <span class="c1"># 2. Flash Attention（模拟）</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">reset_peak_memory_stats</span><span class="p">()</span>
        <span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>

        <span class="c1"># Flash attention 内存复杂度 O(seq_len) 而非 O(seq_len^2)</span>
        <span class="c1"># 这里简化模拟</span>
        <span class="n">chunk_size</span> <span class="o">=</span> <span class="mi">64</span>
        <span class="n">out_flash</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">chunk_size</span><span class="p">):</span>
            <span class="n">q_chunk</span> <span class="o">=</span> <span class="n">q</span><span class="p">[:,</span> <span class="p">:,</span> <span class="n">i</span><span class="p">:</span><span class="n">i</span><span class="o">+</span><span class="n">chunk_size</span><span class="p">]</span>
            <span class="n">scores_chunk</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">q_chunk</span><span class="p">,</span> <span class="n">k</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">q</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>
            <span class="n">attn_chunk</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">scores_chunk</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">out_chunk</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">attn_chunk</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>
            <span class="n">out_flash</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">out_chunk</span><span class="p">)</span>

        <span class="n">out_flash</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">out_flash</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

        <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">synchronize</span><span class="p">()</span>
        <span class="n">flash_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">start</span>
        <span class="n">flash_memory</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">max_memory_allocated</span><span class="p">()</span> <span class="o">/</span> <span class="mi">1024</span><span class="o">**</span><span class="mi">3</span>

        <span class="n">results</span><span class="p">[</span><span class="s1">&#39;flash&#39;</span><span class="p">][</span><span class="s1">&#39;time&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">flash_time</span><span class="p">)</span>
        <span class="n">results</span><span class="p">[</span><span class="s1">&#39;flash&#39;</span><span class="p">][</span><span class="s1">&#39;memory&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">flash_memory</span><span class="p">)</span>

        <span class="c1"># 3. 稀疏注意力</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">reset_peak_memory_stats</span><span class="p">()</span>
        <span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>

        <span class="n">scores</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">q</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>
        <span class="c1"># 只保留 top 10%</span>
        <span class="n">k_sparse</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="mf">0.1</span> <span class="o">*</span> <span class="n">scores</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
        <span class="n">topk_scores</span><span class="p">,</span> <span class="n">topk_indices</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">topk</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">k_sparse</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">sparse_scores</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">scores</span><span class="p">)</span>
        <span class="n">sparse_scores</span><span class="o">.</span><span class="n">scatter_</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">topk_indices</span><span class="p">,</span> <span class="n">topk_scores</span><span class="p">)</span>
        <span class="n">attn</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">sparse_scores</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">attn</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>

        <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">synchronize</span><span class="p">()</span>
        <span class="n">sparse_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">start</span>
        <span class="n">sparse_memory</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">max_memory_allocated</span><span class="p">()</span> <span class="o">/</span> <span class="mi">1024</span><span class="o">**</span><span class="mi">3</span>

        <span class="n">results</span><span class="p">[</span><span class="s1">&#39;sparse&#39;</span><span class="p">][</span><span class="s1">&#39;time&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">sparse_time</span><span class="p">)</span>
        <span class="n">results</span><span class="p">[</span><span class="s1">&#39;sparse&#39;</span><span class="p">][</span><span class="s1">&#39;memory&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">sparse_memory</span><span class="p">)</span>

        <span class="c1"># 4. 滑动窗口</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">reset_peak_memory_stats</span><span class="p">()</span>
        <span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>

        <span class="n">window_size</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">)</span>
        <span class="n">out_sliding</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">window_size</span> <span class="o">//</span> <span class="mi">2</span><span class="p">):</span>
            <span class="n">start_idx</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">i</span> <span class="o">-</span> <span class="n">window_size</span> <span class="o">//</span> <span class="mi">2</span><span class="p">)</span>
            <span class="n">end_idx</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">seq_len</span><span class="p">,</span> <span class="n">i</span> <span class="o">+</span> <span class="n">window_size</span><span class="p">)</span>

            <span class="n">q_window</span> <span class="o">=</span> <span class="n">q</span><span class="p">[:,</span> <span class="p">:,</span> <span class="n">start_idx</span><span class="p">:</span><span class="n">end_idx</span><span class="p">]</span>
            <span class="n">k_window</span> <span class="o">=</span> <span class="n">k</span><span class="p">[:,</span> <span class="p">:,</span> <span class="n">start_idx</span><span class="p">:</span><span class="n">end_idx</span><span class="p">]</span>
            <span class="n">v_window</span> <span class="o">=</span> <span class="n">v</span><span class="p">[:,</span> <span class="p">:,</span> <span class="n">start_idx</span><span class="p">:</span><span class="n">end_idx</span><span class="p">]</span>

            <span class="n">scores_window</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">q_window</span><span class="p">,</span> <span class="n">k_window</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">q</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>
            <span class="n">attn_window</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">scores_window</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">out_window</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">attn_window</span><span class="p">,</span> <span class="n">v_window</span><span class="p">)</span>
            <span class="n">out_sliding</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">out_window</span><span class="p">)</span>

        <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">synchronize</span><span class="p">()</span>
        <span class="n">sliding_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">start</span>
        <span class="n">sliding_memory</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">max_memory_allocated</span><span class="p">()</span> <span class="o">/</span> <span class="mi">1024</span><span class="o">**</span><span class="mi">3</span>

        <span class="n">results</span><span class="p">[</span><span class="s1">&#39;sliding&#39;</span><span class="p">][</span><span class="s1">&#39;time&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">sliding_time</span><span class="p">)</span>
        <span class="n">results</span><span class="p">[</span><span class="s1">&#39;sliding&#39;</span><span class="p">][</span><span class="s1">&#39;memory&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">sliding_memory</span><span class="p">)</span>

        <span class="c1"># 清理</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">empty_cache</span><span class="p">()</span>

    <span class="c1"># 可视化结果</span>
    <span class="n">fig</span><span class="p">,</span> <span class="p">(</span><span class="n">ax1</span><span class="p">,</span> <span class="n">ax2</span><span class="p">)</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>

    <span class="k">for</span> <span class="n">method</span> <span class="ow">in</span> <span class="n">results</span><span class="p">:</span>
        <span class="n">ax1</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">seq_lengths</span><span class="p">,</span> <span class="n">results</span><span class="p">[</span><span class="n">method</span><span class="p">][</span><span class="s1">&#39;memory&#39;</span><span class="p">],</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="n">method</span><span class="p">)</span>
        <span class="n">ax2</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">seq_lengths</span><span class="p">,</span> <span class="n">results</span><span class="p">[</span><span class="n">method</span><span class="p">][</span><span class="s1">&#39;time&#39;</span><span class="p">],</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;s&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="n">method</span><span class="p">)</span>

    <span class="n">ax1</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;序列长度&#39;</span><span class="p">)</span>
    <span class="n">ax1</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;峰值内存 (GB)&#39;</span><span class="p">)</span>
    <span class="n">ax1</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;内存占用对比&#39;</span><span class="p">)</span>
    <span class="n">ax1</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
    <span class="n">ax1</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>

    <span class="n">ax2</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;序列长度&#39;</span><span class="p">)</span>
    <span class="n">ax2</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;推理时间 (秒)&#39;</span><span class="p">)</span>
    <span class="n">ax2</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;速度对比&#39;</span><span class="p">)</span>
    <span class="n">ax2</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
    <span class="n">ax2</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

    <span class="k">return</span> <span class="n">results</span>

<span class="c1"># 运行基准测试</span>
<span class="n">results</span> <span class="o">=</span> <span class="n">benchmark_attention_methods</span><span class="p">()</span>

<span class="c1"># 分析权衡</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">=== 内存-速度权衡分析 ===&quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">seq_len_idx</span><span class="p">,</span> <span class="n">seq_len</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">([</span><span class="mi">512</span><span class="p">,</span> <span class="mi">1024</span><span class="p">,</span> <span class="mi">2048</span><span class="p">,</span> <span class="mi">4096</span><span class="p">]):</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">序列长度 </span><span class="si">{</span><span class="n">seq_len</span><span class="si">}</span><span class="s2">:&quot;</span><span class="p">)</span>
    <span class="n">base_memory</span> <span class="o">=</span> <span class="n">results</span><span class="p">[</span><span class="s1">&#39;standard&#39;</span><span class="p">][</span><span class="s1">&#39;memory&#39;</span><span class="p">][</span><span class="n">seq_len_idx</span><span class="p">]</span>
    <span class="n">base_time</span> <span class="o">=</span> <span class="n">results</span><span class="p">[</span><span class="s1">&#39;standard&#39;</span><span class="p">][</span><span class="s1">&#39;time&#39;</span><span class="p">][</span><span class="n">seq_len_idx</span><span class="p">]</span>

    <span class="k">for</span> <span class="n">method</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;flash&#39;</span><span class="p">,</span> <span class="s1">&#39;sparse&#39;</span><span class="p">,</span> <span class="s1">&#39;sliding&#39;</span><span class="p">]:</span>
        <span class="n">memory_save</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">results</span><span class="p">[</span><span class="n">method</span><span class="p">][</span><span class="s1">&#39;memory&#39;</span><span class="p">][</span><span class="n">seq_len_idx</span><span class="p">]</span> <span class="o">/</span> <span class="n">base_memory</span><span class="p">)</span> <span class="o">*</span> <span class="mi">100</span>
        <span class="n">speed_diff</span> <span class="o">=</span> <span class="p">(</span><span class="n">results</span><span class="p">[</span><span class="n">method</span><span class="p">][</span><span class="s1">&#39;time&#39;</span><span class="p">][</span><span class="n">seq_len_idx</span><span class="p">]</span> <span class="o">/</span> <span class="n">base_time</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="mi">100</span>

        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  </span><span class="si">{</span><span class="n">method</span><span class="si">}</span><span class="s2">: 内存节省 </span><span class="si">{</span><span class="n">memory_save</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2">%, 速度变化 </span><span class="si">{</span><span class="n">speed_diff</span><span class="si">:</span><span class="s2">+.1f</span><span class="si">}</span><span class="s2">%&quot;</span><span class="p">)</span>
</code></pre></div>

<p>典型结果：</p>
<ul>
<li>Flash Attention：内存节省 60-80%，速度提升 10-30%</li>
<li>稀疏注意力：内存节省 30-50%，速度略有下降</li>
<li>滑动窗口：内存节省 70-90%，速度下降 20-40%</li>
</ul>
<p>选择建议：</p>
<ul>
<li>长序列（&gt;2048）：Flash Attention 最优</li>
<li>内存极度受限：滑动窗口</li>
<li>需要全局信息：稀疏注意力</li>
</ul>
</details>
<p><strong>练习 9.7</strong>：实现一个 VLM 专用的内存预算分配器，给定总显存预算，自动分配给不同组件。</p>
<p>💡 <strong>提示</strong>：考虑组件优先级、最小需求、性能影响。</p>
<details>
<summary>📝 参考答案</summary>
<div class="codehilite"><pre><span></span><code><span class="k">class</span> <span class="nc">VLMMemoryBudgetAllocator</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">total_memory_gb</span><span class="p">,</span> <span class="n">model_config</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">total_memory</span> <span class="o">=</span> <span class="n">total_memory_gb</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">config</span> <span class="o">=</span> <span class="n">model_config</span>

        <span class="c1"># 组件优先级（越小越重要）</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">priorities</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s1">&#39;model_params&#39;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
            <span class="s1">&#39;gradients&#39;</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span>
            <span class="s1">&#39;optimizer&#39;</span><span class="p">:</span> <span class="mi">3</span><span class="p">,</span>
            <span class="s1">&#39;vision_features&#39;</span><span class="p">:</span> <span class="mi">4</span><span class="p">,</span>
            <span class="s1">&#39;attention&#39;</span><span class="p">:</span> <span class="mi">5</span><span class="p">,</span>
            <span class="s1">&#39;activations&#39;</span><span class="p">:</span> <span class="mi">6</span>
        <span class="p">}</span>

    <span class="k">def</span> <span class="nf">calculate_minimum_requirements</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;计算各组件最小内存需求&quot;&quot;&quot;</span>
        <span class="n">min_req</span> <span class="o">=</span> <span class="p">{}</span>

        <span class="c1"># 模型参数（必须）</span>
        <span class="n">param_size</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">num_params</span> <span class="o">*</span> <span class="mi">2</span> <span class="o">/</span> <span class="mi">1024</span><span class="o">**</span><span class="mi">3</span>  <span class="c1"># FP16</span>
        <span class="n">min_req</span><span class="p">[</span><span class="s1">&#39;model_params&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">param_size</span>

        <span class="c1"># 梯度（如果训练）</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">training</span><span class="p">:</span>
            <span class="n">min_req</span><span class="p">[</span><span class="s1">&#39;gradients&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">param_size</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">trainable_ratio</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">min_req</span><span class="p">[</span><span class="s1">&#39;gradients&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="c1"># 优化器</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">optimizer</span> <span class="o">==</span> <span class="s1">&#39;adam&#39;</span><span class="p">:</span>
            <span class="n">min_req</span><span class="p">[</span><span class="s1">&#39;optimizer&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">min_req</span><span class="p">[</span><span class="s1">&#39;gradients&#39;</span><span class="p">]</span> <span class="o">*</span> <span class="mi">2</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">optimizer</span> <span class="o">==</span> <span class="s1">&#39;sgd&#39;</span><span class="p">:</span>
            <span class="n">min_req</span><span class="p">[</span><span class="s1">&#39;optimizer&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">min_req</span><span class="p">[</span><span class="s1">&#39;gradients&#39;</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">min_req</span><span class="p">[</span><span class="s1">&#39;optimizer&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="c1"># 视觉特征（最小 batch=1）</span>
        <span class="n">vision_tokens</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">image_size</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">patch_size</span> <span class="o">**</span> <span class="mi">2</span>
        <span class="n">min_req</span><span class="p">[</span><span class="s1">&#39;vision_features&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">vision_tokens</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_dim</span> <span class="o">*</span> <span class="mi">4</span> <span class="o">/</span> <span class="mi">1024</span><span class="o">**</span><span class="mi">3</span>
        <span class="p">)</span>

        <span class="c1"># 注意力（可以用 Flash Attention 压缩）</span>
        <span class="n">min_seq_len</span> <span class="o">=</span> <span class="mi">512</span>  <span class="c1"># 最小序列长度</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">use_flash_attention</span><span class="p">:</span>
            <span class="n">min_req</span><span class="p">[</span><span class="s1">&#39;attention&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">min_seq_len</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_dim</span> <span class="o">*</span> <span class="mi">4</span> <span class="o">/</span> <span class="mi">1024</span><span class="o">**</span><span class="mi">3</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">min_req</span><span class="p">[</span><span class="s1">&#39;attention&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">min_seq_len</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">*</span> <span class="mi">4</span> <span class="o">/</span> <span class="mi">1024</span><span class="o">**</span><span class="mi">3</span>

        <span class="c1"># 激活值（可以用 checkpoint 压缩）</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">gradient_checkpointing</span><span class="p">:</span>
            <span class="n">min_req</span><span class="p">[</span><span class="s1">&#39;activations&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">param_size</span> <span class="o">*</span> <span class="mf">0.5</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">min_req</span><span class="p">[</span><span class="s1">&#39;activations&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">param_size</span> <span class="o">*</span> <span class="mi">2</span>

        <span class="k">return</span> <span class="n">min_req</span>

    <span class="k">def</span> <span class="nf">allocate_budget</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;分配内存预算&quot;&quot;&quot;</span>
        <span class="n">min_requirements</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">calculate_minimum_requirements</span><span class="p">()</span>
        <span class="n">total_min</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">min_requirements</span><span class="o">.</span><span class="n">values</span><span class="p">())</span>

        <span class="k">if</span> <span class="n">total_min</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">total_memory</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">emergency_allocation</span><span class="p">(</span><span class="n">min_requirements</span><span class="p">)</span>

        <span class="c1"># 剩余预算</span>
        <span class="n">remaining</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">total_memory</span> <span class="o">-</span> <span class="n">total_min</span>

        <span class="c1"># 初始分配（满足最小需求）</span>
        <span class="n">allocation</span> <span class="o">=</span> <span class="n">min_requirements</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>

        <span class="c1"># 按优先级分配剩余内存</span>
        <span class="n">sorted_components</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span>
            <span class="n">min_requirements</span><span class="o">.</span><span class="n">keys</span><span class="p">(),</span> 
            <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">priorities</span><span class="p">[</span><span class="n">x</span><span class="p">]</span>
        <span class="p">)</span>

        <span class="c1"># 计算权重</span>
        <span class="n">weights</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="n">total_weight</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">comp</span> <span class="ow">in</span> <span class="n">sorted_components</span><span class="p">:</span>
            <span class="n">weight</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">priorities</span><span class="p">[</span><span class="n">comp</span><span class="p">]</span>
            <span class="n">weights</span><span class="p">[</span><span class="n">comp</span><span class="p">]</span> <span class="o">=</span> <span class="n">weight</span>
            <span class="n">total_weight</span> <span class="o">+=</span> <span class="n">weight</span>

        <span class="c1"># 分配剩余内存</span>
        <span class="k">for</span> <span class="n">comp</span> <span class="ow">in</span> <span class="n">sorted_components</span><span class="p">:</span>
            <span class="n">extra</span> <span class="o">=</span> <span class="n">remaining</span> <span class="o">*</span> <span class="p">(</span><span class="n">weights</span><span class="p">[</span><span class="n">comp</span><span class="p">]</span> <span class="o">/</span> <span class="n">total_weight</span><span class="p">)</span>
            <span class="n">allocation</span><span class="p">[</span><span class="n">comp</span><span class="p">]</span> <span class="o">+=</span> <span class="n">extra</span>

        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimize_allocation</span><span class="p">(</span><span class="n">allocation</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">emergency_allocation</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">min_requirements</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;紧急模式：内存不足时的分配策略&quot;&quot;&quot;</span>
        <span class="n">allocation</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="n">available</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">total_memory</span>

        <span class="c1"># 1. 必须满足模型参数</span>
        <span class="n">allocation</span><span class="p">[</span><span class="s1">&#39;model_params&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">min_requirements</span><span class="p">[</span><span class="s1">&#39;model_params&#39;</span><span class="p">]</span>
        <span class="n">available</span> <span class="o">-=</span> <span class="n">allocation</span><span class="p">[</span><span class="s1">&#39;model_params&#39;</span><span class="p">]</span>

        <span class="k">if</span> <span class="n">available</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">MemoryError</span><span class="p">(</span><span class="s2">&quot;显存不足以加载模型！&quot;</span><span class="p">)</span>

        <span class="c1"># 2. 启用所有内存优化</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">gradient_checkpointing</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">use_flash_attention</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">mixed_precision</span> <span class="o">=</span> <span class="kc">True</span>

        <span class="c1"># 3. 最小化其他组件</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">training</span><span class="p">:</span>
            <span class="c1"># 使用 Adafactor 或 8-bit Adam</span>
            <span class="n">allocation</span><span class="p">[</span><span class="s1">&#39;optimizer&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">min_requirements</span><span class="p">[</span><span class="s1">&#39;gradients&#39;</span><span class="p">]</span> <span class="o">*</span> <span class="mf">0.5</span>
            <span class="n">allocation</span><span class="p">[</span><span class="s1">&#39;gradients&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">min_requirements</span><span class="p">[</span><span class="s1">&#39;gradients&#39;</span><span class="p">]</span>
            <span class="n">available</span> <span class="o">-=</span> <span class="n">allocation</span><span class="p">[</span><span class="s1">&#39;optimizer&#39;</span><span class="p">]</span> <span class="o">+</span> <span class="n">allocation</span><span class="p">[</span><span class="s1">&#39;gradients&#39;</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">allocation</span><span class="p">[</span><span class="s1">&#39;optimizer&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="n">allocation</span><span class="p">[</span><span class="s1">&#39;gradients&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="c1"># 4. 极限压缩激活值</span>
        <span class="n">allocation</span><span class="p">[</span><span class="s1">&#39;activations&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">available</span> <span class="o">*</span> <span class="mf">0.3</span><span class="p">,</span> <span class="n">min_requirements</span><span class="p">[</span><span class="s1">&#39;model_params&#39;</span><span class="p">]</span> <span class="o">*</span> <span class="mf">0.3</span><span class="p">)</span>
        <span class="n">available</span> <span class="o">-=</span> <span class="n">allocation</span><span class="p">[</span><span class="s1">&#39;activations&#39;</span><span class="p">]</span>

        <span class="c1"># 5. 分配剩余给视觉和注意力</span>
        <span class="n">allocation</span><span class="p">[</span><span class="s1">&#39;vision_features&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">available</span> <span class="o">*</span> <span class="mf">0.4</span>
        <span class="n">allocation</span><span class="p">[</span><span class="s1">&#39;attention&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">available</span> <span class="o">*</span> <span class="mf">0.6</span>

        <span class="k">return</span> <span class="n">allocation</span>

    <span class="k">def</span> <span class="nf">optimize_allocation</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">allocation</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;优化分配以最大化性能&quot;&quot;&quot;</span>
        <span class="n">optimized</span> <span class="o">=</span> <span class="n">allocation</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>

        <span class="c1"># 规则 1：如果 batch size 可以翻倍，重新分配</span>
        <span class="n">vision_memory</span> <span class="o">=</span> <span class="n">allocation</span><span class="p">[</span><span class="s1">&#39;vision_features&#39;</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">vision_memory</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">min_vision_memory</span> <span class="o">*</span> <span class="mi">2</span><span class="p">:</span>
            <span class="c1"># 可以支持 batch size = 2</span>
            <span class="n">extra</span> <span class="o">=</span> <span class="n">vision_memory</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">min_vision_memory</span> <span class="o">*</span> <span class="mi">2</span>
            <span class="c1"># 将多余的分配给注意力</span>
            <span class="n">optimized</span><span class="p">[</span><span class="s1">&#39;attention&#39;</span><span class="p">]</span> <span class="o">+=</span> <span class="n">extra</span> <span class="o">*</span> <span class="mf">0.5</span>
            <span class="n">optimized</span><span class="p">[</span><span class="s1">&#39;activations&#39;</span><span class="p">]</span> <span class="o">+=</span> <span class="n">extra</span> <span class="o">*</span> <span class="mf">0.5</span>

        <span class="c1"># 规则 2：平衡激活值和注意力</span>
        <span class="n">ratio</span> <span class="o">=</span> <span class="n">optimized</span><span class="p">[</span><span class="s1">&#39;activations&#39;</span><span class="p">]</span> <span class="o">/</span> <span class="n">optimized</span><span class="p">[</span><span class="s1">&#39;attention&#39;</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">ratio</span> <span class="o">&gt;</span> <span class="mi">3</span><span class="p">:</span>
            <span class="c1"># 激活值过多，重新平衡</span>
            <span class="n">diff</span> <span class="o">=</span> <span class="p">(</span><span class="n">optimized</span><span class="p">[</span><span class="s1">&#39;activations&#39;</span><span class="p">]</span> <span class="o">-</span> <span class="n">optimized</span><span class="p">[</span><span class="s1">&#39;attention&#39;</span><span class="p">]</span> <span class="o">*</span> <span class="mi">2</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span>
            <span class="n">optimized</span><span class="p">[</span><span class="s1">&#39;activations&#39;</span><span class="p">]</span> <span class="o">-=</span> <span class="n">diff</span>
            <span class="n">optimized</span><span class="p">[</span><span class="s1">&#39;attention&#39;</span><span class="p">]</span> <span class="o">+=</span> <span class="n">diff</span>

        <span class="k">return</span> <span class="n">optimized</span>

    <span class="k">def</span> <span class="nf">get_training_config</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">allocation</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;根据内存分配生成训练配置&quot;&quot;&quot;</span>
        <span class="n">config</span> <span class="o">=</span> <span class="p">{}</span>

        <span class="c1"># Batch size</span>
        <span class="n">vision_batch</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">allocation</span><span class="p">[</span><span class="s1">&#39;vision_features&#39;</span><span class="p">]</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">min_vision_memory</span><span class="p">)</span>
        <span class="n">config</span><span class="p">[</span><span class="s1">&#39;batch_size&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">vision_batch</span><span class="p">)</span>

        <span class="c1"># 序列长度</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">use_flash_attention</span><span class="p">:</span>
            <span class="c1"># Flash Attention: O(L) 内存</span>
            <span class="n">max_seq_len</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">allocation</span><span class="p">[</span><span class="s1">&#39;attention&#39;</span><span class="p">]</span> <span class="o">*</span> <span class="mi">1024</span><span class="o">**</span><span class="mi">3</span> <span class="o">/</span> 
                            <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_dim</span> <span class="o">*</span> <span class="mi">4</span><span class="p">))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># 标准注意力: O(L^2) 内存</span>
            <span class="n">max_seq_len</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">allocation</span><span class="p">[</span><span class="s1">&#39;attention&#39;</span><span class="p">]</span> <span class="o">*</span> <span class="mi">1024</span><span class="o">**</span><span class="mi">3</span> <span class="o">/</span> <span class="mi">4</span><span class="p">))</span>

        <span class="n">config</span><span class="p">[</span><span class="s1">&#39;max_seq_length&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">max_seq_len</span><span class="p">,</span> <span class="mi">4096</span><span class="p">)</span>

        <span class="c1"># 梯度累积</span>
        <span class="k">if</span> <span class="n">config</span><span class="p">[</span><span class="s1">&#39;batch_size&#39;</span><span class="p">]</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">target_batch_size</span><span class="p">:</span>
            <span class="n">config</span><span class="p">[</span><span class="s1">&#39;accumulation_steps&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">target_batch_size</span> <span class="o">//</span> <span class="n">config</span><span class="p">[</span><span class="s1">&#39;batch_size&#39;</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">config</span><span class="p">[</span><span class="s1">&#39;accumulation_steps&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>

        <span class="c1"># 内存优化设置</span>
        <span class="n">config</span><span class="p">[</span><span class="s1">&#39;gradient_checkpointing&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">allocation</span><span class="p">[</span><span class="s1">&#39;activations&#39;</span><span class="p">]</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">num_params</span> <span class="o">*</span> <span class="mf">1.5</span> <span class="o">/</span> <span class="mi">1024</span><span class="o">**</span><span class="mi">3</span>
        <span class="n">config</span><span class="p">[</span><span class="s1">&#39;mixed_precision&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="n">config</span><span class="p">[</span><span class="s1">&#39;use_flash_attention&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">use_flash_attention</span>

        <span class="k">return</span> <span class="n">config</span>

<span class="c1"># 使用示例</span>
<span class="n">model_config</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;num_params&#39;</span><span class="p">:</span> <span class="mf">7e9</span><span class="p">,</span>  <span class="c1"># 7B 参数</span>
    <span class="s1">&#39;trainable_ratio&#39;</span><span class="p">:</span> <span class="mf">1.0</span><span class="p">,</span>  <span class="c1"># 全量微调</span>
    <span class="s1">&#39;training&#39;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>
    <span class="s1">&#39;optimizer&#39;</span><span class="p">:</span> <span class="s1">&#39;adam&#39;</span><span class="p">,</span>
    <span class="s1">&#39;image_size&#39;</span><span class="p">:</span> <span class="mi">336</span><span class="p">,</span>
    <span class="s1">&#39;patch_size&#39;</span><span class="p">:</span> <span class="mi">14</span><span class="p">,</span>
    <span class="s1">&#39;hidden_dim&#39;</span><span class="p">:</span> <span class="mi">4096</span><span class="p">,</span>
    <span class="s1">&#39;use_flash_attention&#39;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>
    <span class="s1">&#39;gradient_checkpointing&#39;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span>
    <span class="s1">&#39;target_batch_size&#39;</span><span class="p">:</span> <span class="mi">32</span><span class="p">,</span>
    <span class="s1">&#39;min_vision_memory&#39;</span><span class="p">:</span> <span class="mf">0.5</span>  <span class="c1"># GB</span>
<span class="p">}</span>

<span class="c1"># 24GB 显存（如 RTX 3090）</span>
<span class="n">allocator</span> <span class="o">=</span> <span class="n">VLMMemoryBudgetAllocator</span><span class="p">(</span><span class="mi">24</span><span class="p">,</span> <span class="n">model_config</span><span class="p">)</span>
<span class="n">allocation</span> <span class="o">=</span> <span class="n">allocator</span><span class="o">.</span><span class="n">allocate_budget</span><span class="p">()</span>
<span class="n">training_config</span> <span class="o">=</span> <span class="n">allocator</span><span class="o">.</span><span class="n">get_training_config</span><span class="p">(</span><span class="n">allocation</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=== 内存分配方案 ===&quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">component</span><span class="p">,</span> <span class="n">memory</span> <span class="ow">in</span> <span class="n">allocation</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">component</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">memory</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> GB&quot;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">=== 推荐训练配置 ===&quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">training_config</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">key</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">value</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</code></pre></div>

<p>这个分配器可以根据硬件自动优化训练配置，避免手动试错。</p>
</details>
<p><strong>练习 9.8</strong>：分析真实 VLM 训练日志，诊断内存泄漏问题。</p>
<p>给定以下训练日志片段：</p>
<div class="codehilite"><pre><span></span><code>Step 100: Loss=2.34, Memory=18.2GB
Step 200: Loss=2.11, Memory=18.5GB  
Step 300: Loss=1.98, Memory=18.9GB
Step 400: Loss=1.87, Memory=19.4GB
Step 500: Loss=1.76, Memory=20.1GB
Step 600: Loss=1.65, Memory=20.9GB
Step 700: Loss=1.54, Memory=21.8GB
Step 800: Loss=1.43, Memory=22.9GB
Step 900: Loss=1.32, Memory=24.2GB
Step 1000: CUDA OOM
</code></pre></div>

<p>💡 <strong>提示</strong>：计算内存增长率，分析可能的泄漏源。</p>
<details>
<summary>📝 参考答案</summary>
<p>分析过程：</p>
<ol>
<li>
<p><strong>内存增长模式</strong>：
   - 初始：18.2 GB
   - 最终：24.2 GB（OOM 前）
   - 总增长：6.0 GB
   - 平均每 100 步增长：0.67 GB
   - 增长率：线性增长，非对数增长</p>
</li>
<li>
<p><strong>增长速度分析</strong>：</p>
</li>
</ol>
<div class="codehilite"><pre><span></span><code><span class="n">steps</span> <span class="o">=</span> <span class="p">[</span><span class="mi">100</span><span class="p">,</span> <span class="mi">200</span><span class="p">,</span> <span class="mi">300</span><span class="p">,</span> <span class="mi">400</span><span class="p">,</span> <span class="mi">500</span><span class="p">,</span> <span class="mi">600</span><span class="p">,</span> <span class="mi">700</span><span class="p">,</span> <span class="mi">800</span><span class="p">,</span> <span class="mi">900</span><span class="p">]</span>
<span class="n">memory</span> <span class="o">=</span> <span class="p">[</span><span class="mf">18.2</span><span class="p">,</span> <span class="mf">18.5</span><span class="p">,</span> <span class="mf">18.9</span><span class="p">,</span> <span class="mf">19.4</span><span class="p">,</span> <span class="mf">20.1</span><span class="p">,</span> <span class="mf">20.9</span><span class="p">,</span> <span class="mf">21.8</span><span class="p">,</span> <span class="mf">22.9</span><span class="p">,</span> <span class="mf">24.2</span><span class="p">]</span>

<span class="n">growth_rates</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">memory</span><span class="p">)):</span>
    <span class="n">rate</span> <span class="o">=</span> <span class="p">(</span><span class="n">memory</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-</span> <span class="n">memory</span><span class="p">[</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span> <span class="o">/</span> <span class="mi">100</span>  <span class="c1"># GB per step</span>
    <span class="n">growth_rates</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">rate</span> <span class="o">*</span> <span class="mi">1000</span><span class="p">)</span>  <span class="c1"># MB per step</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;每 100 步内存增长（MB）:&quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">rate</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">growth_rates</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Step </span><span class="si">{</span><span class="p">(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="mi">100</span><span class="si">}</span><span class="s2">-</span><span class="si">{</span><span class="p">(</span><span class="n">i</span><span class="o">+</span><span class="mi">2</span><span class="p">)</span><span class="o">*</span><span class="mi">100</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">rate</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2"> MB&quot;</span><span class="p">)</span>
</code></pre></div>

<p>输出：
   <code>Step 100-200: 3.0 MB
   Step 200-300: 4.0 MB
   Step 300-400: 5.0 MB
   Step 400-500: 7.0 MB
   Step 500-600: 8.0 MB
   Step 600-700: 9.0 MB
   Step 700-800: 11.0 MB
   Step 800-900: 13.0 MB</code></p>
<p><strong>发现</strong>：增长速度在加速！</p>
<ol start="3">
<li><strong>可能的泄漏源</strong>：</li>
</ol>
<p>a) <strong>梯度累积未清理</strong>：</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># 错误代码</span>
<span class="n">total_loss</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">dataloader</span><span class="p">:</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>
    <span class="n">total_loss</span> <span class="o">+=</span> <span class="n">loss</span>  <span class="c1"># 危险！保留计算图</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</code></pre></div>

<p>b) <strong>列表累积张量</strong>：
   <code>python
   # 错误代码
   losses = []
   for step in range(1000):
       loss = train_step()
       losses.append(loss)  # 应该用 loss.item()</code></p>
<p>c) <strong>Hook 未释放</strong>：
   <code>python
   # 错误代码
   def register_hooks(model):
       for layer in model.layers:
           layer.register_forward_hook(save_activation)
   # 训练结束后未调用 handle.remove()</code></p>
<p>d) <strong>动态图像大小导致缓存累积</strong>：
   <code>python
   # VLM 特有问题
   image_cache = {}
   for batch in dataloader:
       h, w = batch['image'].shape[-2:]
       key = f"{h}x{w}"
       if key not in image_cache:
           image_cache[key] = process_image(batch['image'])
       # 缓存无限增长！</code></p>
<ol start="4">
<li><strong>诊断代码</strong>：</li>
</ol>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">diagnose_memory_leak</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">dataloader</span><span class="p">):</span>
    <span class="kn">import</span> <span class="nn">gc</span>
    <span class="kn">import</span> <span class="nn">weakref</span>

    <span class="c1"># 跟踪张量引用</span>
    <span class="n">tensors_before</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">obj</span> <span class="ow">in</span> <span class="n">gc</span><span class="o">.</span><span class="n">get_objects</span><span class="p">():</span>
        <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">is_tensor</span><span class="p">(</span><span class="n">obj</span><span class="p">)</span> <span class="ow">and</span> <span class="n">obj</span><span class="o">.</span><span class="n">is_cuda</span><span class="p">:</span>
            <span class="n">tensors_before</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">weakref</span><span class="o">.</span><span class="n">ref</span><span class="p">(</span><span class="n">obj</span><span class="p">))</span>

    <span class="c1"># 运行 10 步</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">batch</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">dataloader</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">i</span> <span class="o">&gt;=</span> <span class="mi">10</span><span class="p">:</span>
            <span class="k">break</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">train_step</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">batch</span><span class="p">)</span>

    <span class="c1"># 检查新增张量</span>
    <span class="n">tensors_after</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
    <span class="n">leaked_tensors</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">for</span> <span class="n">obj</span> <span class="ow">in</span> <span class="n">gc</span><span class="o">.</span><span class="n">get_objects</span><span class="p">():</span>
        <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">is_tensor</span><span class="p">(</span><span class="n">obj</span><span class="p">)</span> <span class="ow">and</span> <span class="n">obj</span><span class="o">.</span><span class="n">is_cuda</span><span class="p">:</span>
            <span class="n">ref</span> <span class="o">=</span> <span class="n">weakref</span><span class="o">.</span><span class="n">ref</span><span class="p">(</span><span class="n">obj</span><span class="p">)</span>
            <span class="n">tensors_after</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">ref</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">ref</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">tensors_before</span><span class="p">:</span>
                <span class="c1"># 新增张量</span>
                <span class="n">size_mb</span> <span class="o">=</span> <span class="n">obj</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="o">*</span> <span class="n">obj</span><span class="o">.</span><span class="n">element_size</span><span class="p">()</span> <span class="o">/</span> <span class="mi">1024</span><span class="o">**</span><span class="mi">2</span>
                <span class="k">if</span> <span class="n">size_mb</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>  <span class="c1"># 只关注 &gt; 1MB 的张量</span>
                    <span class="n">leaked_tensors</span><span class="o">.</span><span class="n">append</span><span class="p">({</span>
                        <span class="s1">&#39;shape&#39;</span><span class="p">:</span> <span class="n">obj</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span>
                        <span class="s1">&#39;dtype&#39;</span><span class="p">:</span> <span class="n">obj</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
                        <span class="s1">&#39;size_mb&#39;</span><span class="p">:</span> <span class="n">size_mb</span><span class="p">,</span>
                        <span class="s1">&#39;requires_grad&#39;</span><span class="p">:</span> <span class="n">obj</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">,</span>
                        <span class="s1">&#39;grad_fn&#39;</span><span class="p">:</span> <span class="n">obj</span><span class="o">.</span><span class="n">grad_fn</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
                    <span class="p">})</span>

    <span class="c1"># 按大小排序</span>
    <span class="n">leaked_tensors</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="s1">&#39;size_mb&#39;</span><span class="p">],</span> <span class="n">reverse</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;发现 </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">leaked_tensors</span><span class="p">)</span><span class="si">}</span><span class="s2"> 个可疑张量&quot;</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">tensor_info</span> <span class="ow">in</span> <span class="n">leaked_tensors</span><span class="p">[:</span><span class="mi">5</span><span class="p">]:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  </span><span class="si">{</span><span class="n">tensor_info</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">leaked_tensors</span>
</code></pre></div>

<ol start="5">
<li><strong>修复方案</strong>：</li>
</ol>
<div class="codehilite"><pre><span></span><code><span class="c1"># 修复 1：使用 .item() 获取标量</span>
<span class="n">total_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>

<span class="c1"># 修复 2：定期清理缓存</span>
<span class="k">if</span> <span class="n">step</span> <span class="o">%</span> <span class="mi">100</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">empty_cache</span><span class="p">()</span>

<span class="c1"># 修复 3：使用 with torch.no_grad()</span>
<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="n">metrics</span> <span class="o">=</span> <span class="n">evaluate</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">val_loader</span><span class="p">)</span>

<span class="c1"># 修复 4：限制缓存大小</span>
<span class="kn">from</span> <span class="nn">functools</span> <span class="kn">import</span> <span class="n">lru_cache</span>

<span class="nd">@lru_cache</span><span class="p">(</span><span class="n">maxsize</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">cached_process_image</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">w</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">process_image_size</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span>
</code></pre></div>

<p><strong>结论</strong>：该日志显示典型的梯度/激活值累积泄漏，每步泄漏约 6-13 MB，需要检查训练循环中的张量引用。</p>
</details>
<h2 id="gotchas">常见陷阱与错误 (Gotchas)</h2>
<h3 id="1-allocated-reserved">1. 混淆 allocated 和 reserved 内存</h3>
<div class="codehilite"><pre><span></span><code><span class="c1"># 错误理解</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;已用内存: </span><span class="si">{</span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">memory_reserved</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>  <span class="c1"># 错！这是预留的</span>

<span class="c1"># 正确</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;实际使用: </span><span class="si">{</span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">memory_allocated</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;PyTorch 预留: </span><span class="si">{</span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">memory_reserved</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;可用于分配: </span><span class="si">{</span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">memory_reserved</span><span class="p">()</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">memory_allocated</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</code></pre></div>

<h3 id="2-vlm">2. 忽视 VLM 的二次复杂度</h3>
<div class="codehilite"><pre><span></span><code><span class="c1"># 危险：注意力内存是 O(L^2)</span>
<span class="n">seq_len</span> <span class="o">=</span> <span class="mi">4096</span>
<span class="n">memory_gb</span> <span class="o">=</span> <span class="p">(</span><span class="n">seq_len</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">*</span> <span class="mi">4</span><span class="p">)</span> <span class="o">/</span> <span class="mi">1024</span><span class="o">**</span><span class="mi">3</span>  <span class="c1"># 64 MB 仅单个头！</span>

<span class="c1"># 安全：使用 Flash Attention 或分块</span>
</code></pre></div>

<h3 id="3">3. 动态图像大小的陷阱</h3>
<div class="codehilite"><pre><span></span><code><span class="c1"># 问题：batch 中一张大图导致整体 OOM</span>
<span class="n">images</span> <span class="o">=</span> <span class="p">[</span><span class="n">img1_224x224</span><span class="p">,</span> <span class="n">img2_224x224</span><span class="p">,</span> <span class="n">img3_1024x1024</span><span class="p">]</span>  <span class="c1"># 第三张导致 OOM</span>

<span class="c1"># 解决：预先排序和分组</span>
<span class="n">images</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span> <span class="o">*</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
</code></pre></div>

<h3 id="4">4. 梯度检查点的误用</h3>
<div class="codehilite"><pre><span></span><code><span class="c1"># 错误：对小模型使用反而更慢</span>
<span class="n">tiny_model</span><span class="o">.</span><span class="n">gradient_checkpointing_enable</span><span class="p">()</span>  <span class="c1"># 2 层模型，收益为负</span>

<span class="c1"># 正确：只对深层模型使用</span>
<span class="k">if</span> <span class="n">model</span><span class="o">.</span><span class="n">num_layers</span> <span class="o">&gt;=</span> <span class="mi">12</span><span class="p">:</span>
    <span class="n">model</span><span class="o">.</span><span class="n">gradient_checkpointing_enable</span><span class="p">()</span>
</code></pre></div>

<h3 id="5">5. 优化器状态的遗忘</h3>
<div class="codehilite"><pre><span></span><code><span class="c1"># 容易忽视：Adam 需要 2 倍参数内存</span>
<span class="c1"># 7B 模型 + Adam = 14GB (参数) + 14GB (梯度) + 28GB (优化器) = 56GB！</span>

<span class="c1"># 考虑使用 8-bit Adam 或 Adafactor</span>
</code></pre></div>

<h3 id="6-cpu-gpu">6. CPU-GPU 传输瓶颈</h3>
<div class="codehilite"><pre><span></span><code><span class="c1"># 慢：频繁的小批量传输</span>
<span class="k">for</span> <span class="n">img</span> <span class="ow">in</span> <span class="n">images</span><span class="p">:</span>
    <span class="n">img</span> <span class="o">=</span> <span class="n">img</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>  <span class="c1"># 每次传输开销大</span>

<span class="c1"># 快：批量传输</span>
<span class="n">images</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">images</span><span class="p">)</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>  <span class="c1"># 一次传输</span>
</code></pre></div>

<h3 id="7">7. 内存碎片化</h3>
<div class="codehilite"><pre><span></span><code><span class="c1"># 导致碎片化：频繁分配不同大小</span>
<span class="k">for</span> <span class="n">size</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">100</span><span class="p">,</span> <span class="mi">1000</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">10000</span><span class="p">,</span> <span class="mi">1</span><span class="p">]:</span>
    <span class="n">tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">size</span><span class="p">)</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>

<span class="c1"># 监控碎片化</span>
<span class="n">fragmentation</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">memory_allocated</span><span class="p">()</span> <span class="o">/</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">memory_reserved</span><span class="p">())</span>
<span class="k">if</span> <span class="n">fragmentation</span> <span class="o">&gt;</span> <span class="mf">0.3</span><span class="p">:</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">empty_cache</span><span class="p">()</span>  <span class="c1"># 整理内存</span>
</code></pre></div>

<h3 id="8">8. 多进程训练的内存重复</h3>
<div class="codehilite"><pre><span></span><code><span class="c1"># 错误：每个进程都加载完整模型</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">load_model</span><span class="p">()</span>  <span class="c1"># 每个 GPU 都有完整副本</span>

<span class="c1"># 正确：使用 DDP 或 FSDP</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">FSDP</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">sharding_strategy</span><span class="o">=</span><span class="n">ShardingStrategy</span><span class="o">.</span><span class="n">FULL_SHARD</span><span class="p">)</span>
</code></pre></div>

<h2 id="_6">最佳实践检查清单</h2>
<h3 id="_7">训练前检查</h3>
<ul>
<li>[ ] <strong>计算内存需求</strong></li>
<li>[ ] 模型参数内存</li>
<li>[ ] 梯度内存（考虑冻结层）</li>
<li>[ ] 优化器状态内存</li>
<li>
<p>[ ] 估算激活值内存</p>
</li>
<li>
<p>[ ] <strong>选择合适的优化器</strong></p>
</li>
<li>[ ] 内存充足：AdamW</li>
<li>[ ] 内存紧张：Adafactor 或 8-bit Adam</li>
<li>
<p>[ ] 极度受限：SGD with momentum</p>
</li>
<li>
<p>[ ] <strong>配置内存优化</strong></p>
</li>
<li>[ ] 启用混合精度（FP16/BF16）</li>
<li>[ ] 考虑 gradient checkpointing</li>
<li>
<p>[ ] 评估 Flash Attention 适用性</p>
</li>
<li>
<p>[ ] <strong>数据加载优化</strong></p>
</li>
<li>[ ] 设置合理的 num_workers</li>
<li>[ ] 使用 pin_memory=True</li>
<li>[ ] 预处理图像尺寸</li>
</ul>
<h3 id="_8">训练中监控</h3>
<ul>
<li>[ ] <strong>实时内存监控</strong></li>
<li>[ ] 每 N 步记录内存使用</li>
<li>[ ] 监控内存增长趋势</li>
<li>
<p>[ ] 设置 OOM 预警阈值（如 90%）</p>
</li>
<li>
<p>[ ] <strong>性能指标跟踪</strong></p>
</li>
<li>[ ] GPU 利用率</li>
<li>[ ] 内存碎片率</li>
<li>
<p>[ ] 数据加载时间占比</p>
</li>
<li>
<p>[ ] <strong>异常处理</strong></p>
</li>
<li>[ ] OOM 自动恢复机制</li>
<li>[ ] 动态 batch size 调整</li>
<li>[ ] Checkpoint 保存策略</li>
</ul>
<h3 id="oom">调试 OOM 时</h3>
<ul>
<li>[ ] <strong>快速诊断</strong>（30 秒内）</li>
<li>[ ] 运行 nvidia-smi 查看总体占用</li>
<li>[ ] 打印 torch.cuda.memory_summary()</li>
<li>[ ] 检查 batch size 和序列长度</li>
<li>
<p>[ ] 确认是否开启了优化</p>
</li>
<li>
<p>[ ] <strong>深度分析</strong>（5 分钟内）</p>
</li>
<li>[ ] 使用 Profiler 定位内存热点</li>
<li>[ ] 检查是否有内存泄漏</li>
<li>[ ] 分析注意力矩阵大小</li>
<li>
<p>[ ] 验证图像分辨率</p>
</li>
<li>
<p>[ ] <strong>优化措施</strong>（按优先级）
  1. 减小 batch size
  2. 启用 gradient checkpointing
  3. 使用混合精度训练
  4. 启用 Flash Attention
  5. 冻结部分层
  6. 使用 CPU offloading
  7. 切换到模型并行</p>
</li>
</ul>
<h3 id="_9">优化验证</h3>
<ul>
<li>[ ] <strong>内存优化效果</strong></li>
<li>[ ] 峰值内存降低 %</li>
<li>[ ] 可支持的最大 batch size</li>
<li>
<p>[ ] 训练速度变化</p>
</li>
<li>
<p>[ ] <strong>模型质量检查</strong></p>
</li>
<li>[ ] 损失收敛正常</li>
<li>[ ] 验证集指标稳定</li>
<li>
<p>[ ] 无数值溢出/下溢</p>
</li>
<li>
<p>[ ] <strong>稳定性测试</strong></p>
</li>
<li>[ ] 长时间训练无 OOM</li>
<li>[ ] 无内存泄漏</li>
<li>[ ] 恢复训练正常</li>
</ul>
<p>通过系统地执行这个检查清单，可以有效预防和解决 VLM 训练中的内存问题，确保训练顺利进行。</p>
            </article>
            
            <nav class="page-nav"><a href="chapter8.html" class="nav-link prev">← 第 8 章：模型部署与服务化</a><a href="chapter10.html" class="nav-link next">第 10 章：训练崩溃与 NaN 问题 →</a></nav>
        </main>
    </div>
</body>
</html>