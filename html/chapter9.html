<!DOCTYPE html>
<html lang="zh">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <base href="./">
    <title>ç¬¬ 9 ç« ï¼šCUDA OOM è°ƒè¯•å®Œå…¨æŒ‡å—</title>
    <link rel="stylesheet" href="assets/style.css">
    <link rel="stylesheet" href="assets/highlight.css">
    <script src="assets/script.js" defer></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>
        window.MathJax = {
            tex: {
                inlineMath: [['$', '$']],
                displayMath: [['$$', '$$']],
                processEscapes: false,
                packages: {'[+]': ['noerrors', 'ams']}
            },
            options: {
                ignoreHtmlClass: 'tex2jax_ignore',
                processHtmlClass: 'tex2jax_process'
            },
            loader: {
                load: ['[tex]/noerrors', '[tex]/ams']
            }
        };
    </script>
</head>
<body>
    <div class="container">
        <nav id="sidebar" class="sidebar">
            <div class="sidebar-header">
                <h3>ç›®å½•</h3>
                <button id="sidebar-toggle" class="sidebar-toggle">
                    <span></span>
                    <span></span>
                    <span></span>
                </button>
            </div>
            <div class="sidebar-search">
                <input type="text" id="sidebar-search-input" placeholder="æœç´¢..." autocomplete="off">
            </div>
            <div id="tree-container">
                <nav class="tree-nav" role="tree">
                    <div class="tree-item " >
                        <a href="index.html" class="tree-link">
                            <span class="tree-icon">ğŸ“„</span>
                            <span class="tree-title">è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰çš„ç›‘ç£å¾®è°ƒä¸å¼ºåŒ–å­¦ä¹ å®æˆ˜æ•™ç¨‹</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter1.html" class="tree-link">
                            <span class="tree-icon">ğŸ“„</span>
                            <span class="tree-title">ç¬¬ 1 ç« ï¼šVLM æ¶æ„ä¸åŸç†</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter2.html" class="tree-link">
                            <span class="tree-icon">ğŸ“„</span>
                            <span class="tree-title">ç¬¬ 2 ç« ï¼šæ•°æ®å‡†å¤‡ä¸é¢„å¤„ç†</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter3.html" class="tree-link">
                            <span class="tree-icon">ğŸ“„</span>
                            <span class="tree-title">ç¬¬ 3 ç« ï¼šSFT è®­ç»ƒç­–ç•¥</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter4.html" class="tree-link">
                            <span class="tree-icon">ğŸ“„</span>
                            <span class="tree-title">ç¬¬ 4 ç« ï¼šåˆ†å¸ƒå¼è®­ç»ƒä¸ä¼˜åŒ–</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter5.html" class="tree-link">
                            <span class="tree-icon">ğŸ“„</span>
                            <span class="tree-title">ç¬¬ 5 ç« ï¼šRLHF åŸºç¡€ä¸å®ç°</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter6.html" class="tree-link">
                            <span class="tree-icon">ğŸ“„</span>
                            <span class="tree-title">ç¬¬ 6 ç« ï¼šç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter7.html" class="tree-link">
                            <span class="tree-icon">ğŸ“„</span>
                            <span class="tree-title">ç¬¬ 7 ç« ï¼šè¯„ä¼°ä½“ç³»è®¾è®¡</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter8.html" class="tree-link">
                            <span class="tree-icon">ğŸ“„</span>
                            <span class="tree-title">ç¬¬ 8 ç« ï¼šæ¨¡å‹éƒ¨ç½²ä¸æœåŠ¡åŒ–</span>
                        </a>
                    </div>
                
                    <div class="tree-item active" >
                        <a href="chapter9.html" class="tree-link">
                            <span class="tree-icon">ğŸ“„</span>
                            <span class="tree-title">ç¬¬ 9 ç« ï¼šCUDA OOM è°ƒè¯•å®Œå…¨æŒ‡å—</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter10.html" class="tree-link">
                            <span class="tree-icon">ğŸ“„</span>
                            <span class="tree-title">ç¬¬ 10 ç« ï¼šè®­ç»ƒå´©æºƒä¸ NaN é—®é¢˜</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter11.html" class="tree-link">
                            <span class="tree-icon">ğŸ“„</span>
                            <span class="tree-title">ç¬¬ 11 ç« ï¼šè®­ç»ƒé€Ÿåº¦ä¼˜åŒ–å®æˆ˜</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter12.html" class="tree-link">
                            <span class="tree-icon">ğŸ“„</span>
                            <span class="tree-title">ç¬¬ 12 ç« ï¼šå¤šæœºå¤šå¡è°ƒè¯•åœ°ç‹±</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="CLAUDE.html" class="tree-link">
                            <span class="tree-icon">ğŸ“„</span>
                            <span class="tree-title">Untitled</span>
                        </a>
                    </div>
                </nav>
            </div>
        </nav>
        
        <main class="content">
            <article>
                <h1 id="9-cuda-oom">ç¬¬ 9 ç« ï¼šCUDA OOM è°ƒè¯•å®Œå…¨æŒ‡å—</h1>
<p>åœ¨ VLM è®­ç»ƒè¿‡ç¨‹ä¸­ï¼ŒCUDA Out of Memory (OOM) é”™è¯¯å¯èƒ½æ˜¯æœ€å¸¸è§ä¹Ÿæœ€ä»¤äººå¤´ç–¼çš„é—®é¢˜ã€‚å½“ä½ èŠ±è´¹æ•°å°æ—¶å‡†å¤‡æ•°æ®ã€é…ç½®ç¯å¢ƒï¼Œæ»¡æ€€æœŸå¾…åœ°å¯åŠ¨è®­ç»ƒï¼Œå´åœ¨ç¬¬ä¸€ä¸ª batch å°±é­é‡ OOM å´©æºƒæ—¶ï¼Œé‚£ç§æŒ«è´¥æ„Ÿç›¸ä¿¡æ¯ä¸ª AI å·¥ç¨‹å¸ˆéƒ½æ·±æœ‰ä½“ä¼šã€‚æœ¬ç« å°†ç³»ç»Ÿä»‹ç» VLM è®­ç»ƒä¸­çš„å†…å­˜ç®¡ç†ï¼Œå¸®åŠ©ä½ å¿«é€Ÿè¯Šæ–­å’Œè§£å†³ OOM é—®é¢˜ï¼Œè®©è®­ç»ƒè¿‡ç¨‹æ›´åŠ é¡ºç•…ã€‚</p>
<h2 id="_1">å­¦ä¹ ç›®æ ‡</h2>
<p>å®Œæˆæœ¬ç« å­¦ä¹ åï¼Œä½ å°†èƒ½å¤Ÿï¼š</p>
<ul>
<li><strong>30 ç§’å†…å®šä½</strong> OOM çš„å…·ä½“åŸå› ï¼ˆæ¨¡å‹ã€æ¢¯åº¦ã€æ¿€æ´»å€¼è¿˜æ˜¯ä¼˜åŒ–å™¨ï¼‰</li>
<li><strong>æŒæ¡ 5 ç§ç´§æ€¥å¤„ç†æ–¹æ¡ˆ</strong>ï¼Œè®©è®­ç»ƒç«‹å³æ¢å¤è¿è¡Œ</li>
<li><strong>ç²¾ç¡®è®¡ç®—</strong>ä»»æ„ VLM é…ç½®çš„å†…å­˜éœ€æ±‚ï¼Œé¿å…ç›²ç›®è¯•é”™</li>
<li><strong>è¯†åˆ«å¹¶è§„é¿</strong> VLM ç‰¹æœ‰çš„ 4 ç±»å†…å­˜é™·é˜±</li>
<li><strong>å»ºç«‹ç³»ç»Ÿçš„å†…å­˜ä¼˜åŒ–æµç¨‹</strong>ï¼Œå°†æ˜¾å­˜åˆ©ç”¨ç‡æå‡è‡³ 95% ä»¥ä¸Š</li>
</ul>
<h2 id="91">9.1 å¿«é€Ÿè¯Šæ–­å†…å­˜å ç”¨</h2>
<p>å½“é­é‡ OOM æ—¶ï¼Œé¦–è¦ä»»åŠ¡æ˜¯å¿«é€Ÿå®šä½å†…å­˜ç“¶é¢ˆã€‚VLM è®­ç»ƒçš„å†…å­˜å ç”¨ä¸»è¦åˆ†ä¸ºå››ä¸ªéƒ¨åˆ†ï¼šæ¨¡å‹å‚æ•°ã€æ¢¯åº¦ã€æ¿€æ´»å€¼å’Œä¼˜åŒ–å™¨çŠ¶æ€ã€‚è®©æˆ‘ä»¬é€ä¸€åˆ†æã€‚</p>
<h3 id="911">9.1.1 æ¨¡å‹å‚æ•°å†…å­˜è®¡ç®—</h3>
<p>VLM çš„å‚æ•°å†…å­˜åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦ç»„ä»¶ï¼š</p>
<div class="codehilite"><pre><span></span><code>æ€»å‚æ•°å†…å­˜ = è§†è§‰ç¼–ç å™¨ + è¯­è¨€æ¨¡å‹ + è¿æ¥å±‚
</code></pre></div>

<p><strong>å¿«é€Ÿä¼°ç®—å…¬å¼</strong>ï¼ˆä»¥ FP16 ä¸ºä¾‹ï¼‰ï¼š</p>
<p>$$M_{params} = 2 \times (N_{vision} + N_{language} + N_{connector}) \text{ bytes}$$
å…¶ä¸­ï¼š</p>
<ul>
<li>$N_{vision}$ï¼šè§†è§‰ç¼–ç å™¨å‚æ•°é‡ï¼ˆå¦‚ ViT-L/14 çº¦ 304Mï¼‰</li>
<li>$N_{language}$ï¼šè¯­è¨€æ¨¡å‹å‚æ•°é‡ï¼ˆå¦‚ Vicuna-7B çº¦ 7Bï¼‰</li>
<li>$N_{connector}$ï¼šè¿æ¥å±‚å‚æ•°é‡ï¼ˆé€šå¸¸ &lt; 100Mï¼‰</li>
</ul>
<p><strong>å®ä¾‹è®¡ç®—</strong>ï¼šLLaVA-1.5-7B</p>
<div class="codehilite"><pre><span></span><code>è§†è§‰ç¼–ç å™¨ (CLIP-ViT-L/14): 304M Ã— 2 bytes = 608 MB
è¯­è¨€æ¨¡å‹ (Vicuna-7B): 7B Ã— 2 bytes = 14 GB
MLP è¿æ¥å±‚: 20M Ã— 2 bytes = 40 MB
æ€»è®¡: çº¦ 14.6 GB
</code></pre></div>

<h3 id="912">9.1.2 æ¢¯åº¦å†…å­˜è®¡ç®—</h3>
<p>è®­ç»ƒæ—¶æ¯ä¸ªå‚æ•°éƒ½éœ€è¦å­˜å‚¨æ¢¯åº¦ï¼Œå†…å­˜å ç”¨ä¸å‚æ•°ç›¸åŒï¼š
$$M_{gradients} = M_{params}$$
ä½†æ³¨æ„ï¼Œå¦‚æœå†»ç»“éƒ¨åˆ†æ¨¡å—ï¼ˆå¦‚è§†è§‰ç¼–ç å™¨ï¼‰ï¼Œè¯¥éƒ¨åˆ†ä¸äº§ç”Ÿæ¢¯åº¦ï¼š</p>
<div class="codehilite"><pre><span></span><code>å¯è®­ç»ƒå‚æ•°æ¢¯åº¦ = æ€»å‚æ•° - å†»ç»“å‚æ•°
</code></pre></div>

<p><strong>ä¼˜åŒ–æŠ€å·§</strong>ï¼šåˆ†é˜¶æ®µè§£å†»</p>
<ul>
<li>Stage 1: åªè®­ç»ƒè¿æ¥å±‚ï¼ˆæ¢¯åº¦å†…å­˜ &lt; 100MBï¼‰</li>
<li>Stage 2: è§£å†»è¯­è¨€æ¨¡å‹ï¼ˆæ¢¯åº¦å†…å­˜çº¦ 14GBï¼‰</li>
<li>Stage 3: å…¨éƒ¨è§£å†»ï¼ˆæ¢¯åº¦å†…å­˜çº¦ 14.6GBï¼‰</li>
</ul>
<h3 id="913">9.1.3 æ¿€æ´»å€¼å†…å­˜åˆ†æ</h3>
<p>æ¿€æ´»å€¼ï¼ˆä¸­é—´å¼ é‡ï¼‰æ˜¯ OOM çš„ä¸»è¦å…ƒå‡¶ï¼Œå…¶å¤§å°ä¸ batch sizeã€åºåˆ—é•¿åº¦æˆæ­£æ¯”ï¼š
$$M_{activation} = O(B \times L \times H \times N_{layers})$$
å…¶ä¸­ï¼š</p>
<ul>
<li>$B$ï¼šbatch size</li>
<li>$L$ï¼šåºåˆ—é•¿åº¦</li>
<li>$H$ï¼šéšè—ç»´åº¦</li>
<li>$N_{layers}$ï¼šå±‚æ•°</li>
</ul>
<p><strong>VLM æ¿€æ´»å€¼ç‰¹ç‚¹</strong>ï¼š</p>
<ol>
<li>
<p><strong>è§†è§‰ tokens çˆ†ç‚¸</strong>ï¼š
   - å•å¼ å›¾åƒäº§ç”Ÿå¤§é‡ tokensï¼ˆå¦‚ 576 ä¸ª for ViT-L/14ï¼‰
   - å¤šå›¾åœºæ™¯ä¸‹æ¿€æ´»å€¼æ€¥å‰§å¢é•¿</p>
</li>
<li>
<p><strong>æ³¨æ„åŠ›çŸ©é˜µ</strong>ï¼š
$$M_{attention} = B \times N_{heads} \times L^2 \times 4 \text{ bytes}$$</p>
</li>
</ol>
<p>å½“ $L = 2048$ æ—¶ï¼Œå•ä¸ªæ³¨æ„åŠ›å±‚å°±éœ€è¦ $B \times 32 \times 4M \times 4 = 512B$ MBï¼</p>
<h3 id="914">9.1.4 ä¼˜åŒ–å™¨çŠ¶æ€å†…å­˜</h3>
<p>ä¸åŒä¼˜åŒ–å™¨çš„å†…å­˜å ç”¨å·®å¼‚å·¨å¤§ï¼š</p>
<p>| ä¼˜åŒ–å™¨ | çŠ¶æ€å†…å­˜ | è®¡ç®—å…¬å¼ |</p>
<table>
<thead>
<tr>
<th>ä¼˜åŒ–å™¨</th>
<th>çŠ¶æ€å†…å­˜</th>
<th>è®¡ç®—å…¬å¼</th>
</tr>
</thead>
<tbody>
<tr>
<td>SGD</td>
<td>0ï¼ˆæ— åŠ¨é‡ï¼‰æˆ– $M_{params}$ï¼ˆæœ‰åŠ¨é‡ï¼‰</td>
<td>$M_{optimizer} = M_{params}$</td>
</tr>
<tr>
<td>Adam</td>
<td>$2 \times M_{params}$</td>
<td>ä¸€é˜¶ã€äºŒé˜¶åŠ¨é‡å„å ä¸€ä»½</td>
</tr>
<tr>
<td>AdamW</td>
<td>$2 \times M_{params}$</td>
<td>åŒ Adam</td>
</tr>
<tr>
<td>Adafactor</td>
<td>$M_{params} / N$</td>
<td>åˆ†è§£äºŒé˜¶åŠ¨é‡ï¼ŒèŠ‚çœå†…å­˜</td>
</tr>
</tbody>
</table>
<p><strong>ç¤ºä¾‹</strong>ï¼š7B æ¨¡å‹ä½¿ç”¨ Adam</p>
<div class="codehilite"><pre><span></span><code>ä¼˜åŒ–å™¨çŠ¶æ€ = 14 GB Ã— 2 = 28 GB
æ€»å†…å­˜éœ€æ±‚ = 14.6 (å‚æ•°) + 14.6 (æ¢¯åº¦) + 28 (ä¼˜åŒ–å™¨) + æ¿€æ´»å€¼
           &gt; 57.2 GB + æ¿€æ´»å€¼
</code></pre></div>

<p>è¿™å°±æ˜¯ä¸ºä»€ä¹ˆå•å¡ V100 (32GB) éš¾ä»¥è®­ç»ƒ 7B æ¨¡å‹ï¼</p>
<h3 id="915">9.1.5 å†…å­˜å ç”¨å¿«é€Ÿè¯Šæ–­æµç¨‹</h3>
<div class="codehilite"><pre><span></span><code><span class="kn">import</span> <span class="nn">torch</span>

<span class="k">def</span> <span class="nf">diagnose_memory</span><span class="p">():</span>
    <span class="c1"># 1. æ£€æŸ¥å½“å‰å†…å­˜ä½¿ç”¨</span>
    <span class="n">allocated</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">memory_allocated</span><span class="p">()</span> <span class="o">/</span> <span class="mi">1024</span><span class="o">**</span><span class="mi">3</span>
    <span class="n">reserved</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">memory_reserved</span><span class="p">()</span> <span class="o">/</span> <span class="mi">1024</span><span class="o">**</span><span class="mi">3</span>

    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;å·²åˆ†é…: </span><span class="si">{</span><span class="n">allocated</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> GB&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;å·²é¢„ç•™: </span><span class="si">{</span><span class="n">reserved</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> GB&quot;</span><span class="p">)</span>

    <span class="c1"># 2. æ‰“å°è¯¦ç»†å†…å­˜å¿«ç…§</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">memory_summary</span><span class="p">())</span>

    <span class="c1"># 3. å®šä½å¤§å¼ é‡</span>
    <span class="k">for</span> <span class="n">obj</span> <span class="ow">in</span> <span class="n">gc</span><span class="o">.</span><span class="n">get_objects</span><span class="p">():</span>
        <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">is_tensor</span><span class="p">(</span><span class="n">obj</span><span class="p">)</span> <span class="ow">and</span> <span class="n">obj</span><span class="o">.</span><span class="n">is_cuda</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">obj</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">obj</span><span class="o">.</span><span class="n">dtype</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">obj</span><span class="o">.</span><span class="n">element_size</span><span class="p">()</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">obj</span><span class="o">.</span><span class="n">nelement</span><span class="p">()</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="mi">1024</span><span class="o">**</span><span class="mi">2</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> MB&quot;</span><span class="p">)</span>
</code></pre></div>

<p><strong>30 ç§’è¯Šæ–­æ¸…å•</strong>ï¼š</p>
<ol>
<li>è¿è¡Œ <code>nvidia-smi</code> æŸ¥çœ‹æ€»ä½“å ç”¨</li>
<li>è°ƒç”¨ <code>diagnose_memory()</code> å®šä½å¤§å¼ é‡</li>
<li>æ£€æŸ¥ batch size å’Œåºåˆ—é•¿åº¦</li>
<li>ç¡®è®¤ä¼˜åŒ–å™¨ç±»å‹</li>
<li>éªŒè¯æ˜¯å¦å¼€å¯æ··åˆç²¾åº¦</li>
</ol>
<h2 id="92">9.2 ç´§æ€¥å¤„ç†æ–¹æ¡ˆ</h2>
<p>å½“ OOM å‘ç”Ÿæ—¶ï¼Œä»¥ä¸‹æ–¹æ¡ˆå¯ä»¥å¿«é€Ÿæ¢å¤è®­ç»ƒï¼ŒæŒ‰ä¼˜å…ˆçº§æ’åºï¼š</p>
<h3 id="921-gradient-checkpointing">9.2.1 Gradient Checkpointingï¼ˆæ¢¯åº¦æ£€æŸ¥ç‚¹ï¼‰</h3>
<p>æœ€æœ‰æ•ˆçš„å†…å­˜ä¼˜åŒ–æŠ€æœ¯ï¼Œç”¨è®¡ç®—æ¢å†…å­˜ï¼š</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># å¼€å¯ gradient checkpointing</span>
<span class="n">model</span><span class="o">.</span><span class="n">gradient_checkpointing_enable</span><span class="p">()</span>

<span class="c1"># å¯¹äº VLMï¼Œå¯ä»¥é€‰æ‹©æ€§å¼€å¯</span>
<span class="n">vision_encoder</span><span class="o">.</span><span class="n">gradient_checkpointing_enable</span><span class="p">()</span>  <span class="c1"># è§†è§‰ç¼–ç å™¨</span>
<span class="n">language_model</span><span class="o">.</span><span class="n">gradient_checkpointing_enable</span><span class="p">()</span>   <span class="c1"># è¯­è¨€æ¨¡å‹</span>
</code></pre></div>

<p><strong>å†…å­˜èŠ‚çœ</strong>ï¼šæ¿€æ´»å€¼ä» $O(N_{layers})$ é™è‡³ $O(\sqrt{N_{layers}})$</p>
<p><strong>æ€§èƒ½å½±å“</strong>ï¼šè®­ç»ƒé€Ÿåº¦é™ä½ 15-30%</p>
<p><strong>æœ€ä½³å®è·µ</strong>ï¼š</p>
<ul>
<li>ä¼˜å…ˆåœ¨è¯­è¨€æ¨¡å‹ä¸Šå¼€å¯ï¼ˆå±‚æ•°å¤šï¼Œæ•ˆæœæ˜æ˜¾ï¼‰</li>
<li>è§†è§‰ç¼–ç å™¨å¯é€‰ï¼ˆå±‚æ•°å°‘ï¼Œæ”¶ç›Šæœ‰é™ï¼‰</li>
<li>ç»“åˆ FlashAttention ä½¿ç”¨æ•ˆæœæ›´ä½³</li>
</ul>
<h3 id="922-batch-size">9.2.2 Batch Size åŠ¨æ€è°ƒæ•´</h3>
<p>æ™ºèƒ½è°ƒæ•´ batch sizeï¼Œæœ€å¤§åŒ–æ˜¾å­˜åˆ©ç”¨ï¼š</p>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">find_optimal_batch_size</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">initial_bs</span><span class="o">=</span><span class="mi">32</span><span class="p">):</span>
    <span class="n">batch_size</span> <span class="o">=</span> <span class="n">initial_bs</span>

    <span class="k">while</span> <span class="n">batch_size</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="c1"># å°è¯•å‰å‘ä¼ æ’­</span>
            <span class="n">dummy_batch</span> <span class="o">=</span> <span class="n">create_dummy_batch</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">dummy_batch</span><span class="p">)</span>
            <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;æœ€ä½³ batch size: </span><span class="si">{</span><span class="n">batch_size</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">batch_size</span>

        <span class="k">except</span> <span class="ne">RuntimeError</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
            <span class="k">if</span> <span class="s2">&quot;out of memory&quot;</span> <span class="ow">in</span> <span class="nb">str</span><span class="p">(</span><span class="n">e</span><span class="p">):</span>
                <span class="c1"># æ¸…ç†ç¼“å­˜</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">empty_cache</span><span class="p">()</span>
                <span class="c1"># å‡åŠé‡è¯•</span>
                <span class="n">batch_size</span> <span class="o">=</span> <span class="n">batch_size</span> <span class="o">//</span> <span class="mi">2</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="n">e</span>

    <span class="k">return</span> <span class="mi">1</span>  <span class="c1"># æœ€å° batch size</span>
</code></pre></div>

<p><strong>æ¢¯åº¦ç´¯ç§¯è¡¥å¿</strong>ï¼š</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># ç›®æ ‡ï¼šç­‰æ•ˆ batch size = 32</span>
<span class="n">actual_batch_size</span> <span class="o">=</span> <span class="mi">4</span>  <span class="c1"># å—é™äºæ˜¾å­˜</span>
<span class="n">accumulation_steps</span> <span class="o">=</span> <span class="mi">32</span> <span class="o">//</span> <span class="mi">4</span>  <span class="c1"># ç´¯ç§¯ 8 æ­¥</span>

<span class="k">for</span> <span class="n">step</span><span class="p">,</span> <span class="n">batch</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">dataloader</span><span class="p">):</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span> <span class="o">/</span> <span class="n">accumulation_steps</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

    <span class="k">if</span> <span class="p">(</span><span class="n">step</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="n">accumulation_steps</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
</code></pre></div>

<h3 id="923">9.2.3 æ··åˆç²¾åº¦è®­ç»ƒä¼˜åŒ–</h3>
<p>FP16/BF16 è®­ç»ƒå¯èŠ‚çœ 50% å†…å­˜ï¼š</p>
<div class="codehilite"><pre><span></span><code><span class="kn">from</span> <span class="nn">torch.cuda.amp</span> <span class="kn">import</span> <span class="n">autocast</span><span class="p">,</span> <span class="n">GradScaler</span>

<span class="n">scaler</span> <span class="o">=</span> <span class="n">GradScaler</span><span class="p">()</span>

<span class="k">with</span> <span class="n">autocast</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">):</span>
    <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">targets</span><span class="p">)</span>

<span class="c1"># ç¼©æ”¾æ¢¯åº¦é˜²æ­¢ä¸‹æº¢</span>
<span class="n">scaler</span><span class="o">.</span><span class="n">scale</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
<span class="n">scaler</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">optimizer</span><span class="p">)</span>
<span class="n">scaler</span><span class="o">.</span><span class="n">update</span><span class="p">()</span>
</code></pre></div>

<p><strong>VLM ç‰¹æ®Šè€ƒè™‘</strong>ï¼š</p>
<ul>
<li>è§†è§‰ç¼–ç å™¨å»ºè®®ä¿æŒ FP32ï¼ˆæ•°å€¼ç¨³å®šæ€§ï¼‰</li>
<li>è¯­è¨€æ¨¡å‹å¯ä»¥å®‰å…¨ä½¿ç”¨ FP16</li>
<li>æ³¨æ„åŠ›å±‚ä½¿ç”¨ BF16 æ›´ç¨³å®š</li>
</ul>
<h3 id="924-cpu-offloading">9.2.4 CPU Offloading</h3>
<p>å°†éƒ¨åˆ†æ•°æ®è½¬ç§»åˆ° CPU å†…å­˜ï¼š</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># DeepSpeed ZeRO-Offload é…ç½®</span>
<span class="n">ds_config</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;zero_optimization&quot;</span><span class="p">:</span> <span class="p">{</span>
        <span class="s2">&quot;stage&quot;</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span>
        <span class="s2">&quot;offload_optimizer&quot;</span><span class="p">:</span> <span class="p">{</span>
            <span class="s2">&quot;device&quot;</span><span class="p">:</span> <span class="s2">&quot;cpu&quot;</span><span class="p">,</span>
            <span class="s2">&quot;pin_memory&quot;</span><span class="p">:</span> <span class="kc">True</span>
        <span class="p">},</span>
        <span class="s2">&quot;offload_param&quot;</span><span class="p">:</span> <span class="p">{</span>
            <span class="s2">&quot;device&quot;</span><span class="p">:</span> <span class="s2">&quot;cpu&quot;</span><span class="p">,</span>
            <span class="s2">&quot;pin_memory&quot;</span><span class="p">:</span> <span class="kc">True</span>
        <span class="p">}</span>
    <span class="p">}</span>
<span class="p">}</span>
</code></pre></div>

<p><strong>æƒè¡¡</strong>ï¼š</p>
<ul>
<li>ä¼˜ç‚¹ï¼šå¯è®­ç»ƒè¶…å¤§æ¨¡å‹ï¼ˆå¦‚ 65Bï¼‰</li>
<li>ç¼ºç‚¹ï¼šè®­ç»ƒé€Ÿåº¦é™ä½ 2-3 å€</li>
<li>é€‚ç”¨ï¼šå•å¡è®­ç»ƒå¤§æ¨¡å‹çš„æ— å¥ˆé€‰æ‹©</li>
</ul>
<h3 id="925">9.2.5 æ¨¡å‹å¹¶è¡Œç­–ç•¥</h3>
<p>å½“å•å¡æ— æ³•å®¹çº³æ—¶ï¼Œè€ƒè™‘æ¨¡å‹å¹¶è¡Œï¼š</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># Pipeline å¹¶è¡Œç¤ºä¾‹</span>
<span class="kn">from</span> <span class="nn">torch.distributed.pipeline.sync</span> <span class="kn">import</span> <span class="n">Pipe</span>

<span class="c1"># å°†æ¨¡å‹åˆ†å‰²ä¸ºä¸¤éƒ¨åˆ†</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
    <span class="n">vision_encoder</span><span class="p">,</span>    <span class="c1"># GPU 0</span>
    <span class="n">language_model</span>     <span class="c1"># GPU 1</span>
<span class="p">)</span>

<span class="c1"># åˆ›å»º pipeline</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">Pipe</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">balance</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">devices</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
</code></pre></div>

<p><strong>VLM å¹¶è¡Œå»ºè®®</strong>ï¼š</p>
<ul>
<li>è§†è§‰ç¼–ç å™¨å’Œè¯­è¨€æ¨¡å‹å¤©ç„¶åˆ†ç¦»ï¼Œé€‚åˆ pipeline</li>
<li>Tensor å¹¶è¡Œé€‚åˆå•ä¸ª Transformer å±‚</li>
<li>ä¼˜å…ˆä½¿ç”¨æ•°æ®å¹¶è¡Œï¼Œæ€§èƒ½æœ€ä½³</li>
</ul>
<h2 id="93">9.3 å†…å­˜åˆ†æå·¥å…·ä½¿ç”¨</h2>
<p>æŒæ¡å†…å­˜åˆ†æå·¥å…·æ˜¯è§£å†³ OOM é—®é¢˜çš„å…³é”®ã€‚æœ¬èŠ‚ä»‹ç» 4 ä¸ªå¿…å¤‡å·¥å…·åŠå…¶é«˜çº§ç”¨æ³•ã€‚</p>
<h3 id="931-torchcudamemory_summary">9.3.1 torch.cuda.memory_summary() æ·±åº¦è§£æ</h3>
<p>PyTorch å†…ç½®çš„æœ€å¼ºå¤§å†…å­˜åˆ†æå·¥å…·ï¼š</p>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">analyze_memory_detailed</span><span class="p">():</span>
    <span class="c1"># è·å–å®Œæ•´å†…å­˜æŠ¥å‘Š</span>
    <span class="n">summary</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">memory_summary</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">abbreviated</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">summary</span><span class="p">)</span>

    <span class="c1"># å…³é”®æŒ‡æ ‡è§£è¯»</span>
    <span class="n">stats</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">memory_stats</span><span class="p">()</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">=== å†…å­˜ä½¿ç”¨ç»†åˆ† ===&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;å½“å‰åˆ†é…: </span><span class="si">{</span><span class="n">stats</span><span class="p">[</span><span class="s1">&#39;allocated_bytes.all.current&#39;</span><span class="p">]</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="mi">1024</span><span class="o">**</span><span class="mi">3</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> GB&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;å³°å€¼åˆ†é…: </span><span class="si">{</span><span class="n">stats</span><span class="p">[</span><span class="s1">&#39;allocated_bytes.all.peak&#39;</span><span class="p">]</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="mi">1024</span><span class="o">**</span><span class="mi">3</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> GB&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;é¢„ç•™å†…å­˜: </span><span class="si">{</span><span class="n">stats</span><span class="p">[</span><span class="s1">&#39;reserved_bytes.all.current&#39;</span><span class="p">]</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="mi">1024</span><span class="o">**</span><span class="mi">3</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> GB&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;æ´»è·ƒå†…å­˜å—: </span><span class="si">{</span><span class="n">stats</span><span class="p">[</span><span class="s1">&#39;active_bytes.all.current&#39;</span><span class="p">]</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="mi">1024</span><span class="o">**</span><span class="mi">3</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> GB&quot;</span><span class="p">)</span>

    <span class="c1"># å†…å­˜ç¢ç‰‡åˆ†æ</span>
    <span class="n">fragmentation</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="p">(</span><span class="n">stats</span><span class="p">[</span><span class="s1">&#39;allocated_bytes.all.current&#39;</span><span class="p">]</span> <span class="o">/</span> 
                        <span class="n">stats</span><span class="p">[</span><span class="s1">&#39;reserved_bytes.all.current&#39;</span><span class="p">])</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;å†…å­˜ç¢ç‰‡ç‡: </span><span class="si">{</span><span class="n">fragmentation</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="mi">100</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2">%&quot;</span><span class="p">)</span>

    <span class="c1"># OOM æ¬¡æ•°</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;OOM é‡è¯•æ¬¡æ•°: </span><span class="si">{</span><span class="n">stats</span><span class="p">[</span><span class="s1">&#39;num_ooms&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</code></pre></div>

<p><strong>å…³é”®æŒ‡æ ‡è§£è¯»</strong>ï¼š</p>
<ul>
<li><strong>Allocated vs Reserved</strong>ï¼šReserved æ˜¯ PyTorch å‘ CUDA ç”³è¯·çš„æ€»å†…å­˜ï¼ŒAllocated æ˜¯å®é™…ä½¿ç”¨çš„</li>
<li><strong>ç¢ç‰‡ç‡ &gt; 20%</strong>ï¼šéœ€è¦è°ƒç”¨ <code>torch.cuda.empty_cache()</code> æ•´ç†å†…å­˜</li>
<li><strong>num_ooms &gt; 0</strong>ï¼šè¯´æ˜å‘ç”Ÿè¿‡ OOM å¹¶è‡ªåŠ¨é‡è¯•</li>
</ul>
<h3 id="932-nvidia-smi">9.3.2 nvidia-smi é«˜çº§ç”¨æ³•</h3>
<p>ä¸åªæ˜¯çœ‹æ˜¾å­˜å ç”¨ï¼Œæ›´å¤šé«˜çº§åŠŸèƒ½ï¼š</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># 1. æŒç»­ç›‘æ§ï¼ˆæ¯ 0.1 ç§’åˆ·æ–°ï¼‰</span>
nvidia-smi<span class="w"> </span>-l<span class="w"> </span><span class="m">0</span>.1

<span class="c1"># 2. åªæ˜¾ç¤ºå†…å­˜ä¿¡æ¯</span>
nvidia-smi<span class="w"> </span>--query-gpu<span class="o">=</span>memory.used,memory.free,memory.total<span class="w"> </span><span class="se">\</span>
<span class="w">           </span>--format<span class="o">=</span>csv,noheader,nounits<span class="w"> </span>-l<span class="w"> </span><span class="m">1</span>

<span class="c1"># 3. ç›‘æ§ç‰¹å®šè¿›ç¨‹</span>
nvidia-smi<span class="w"> </span>pmon<span class="w"> </span>-i<span class="w"> </span><span class="m">0</span>

<span class="c1"># 4. å¯¼å‡ºè¯¦ç»†æ—¥å¿—ç”¨äºåˆ†æ</span>
nvidia-smi<span class="w"> </span>--query-gpu<span class="o">=</span>timestamp,name,memory.used,memory.free,utilization.gpu<span class="w"> </span><span class="se">\</span>
<span class="w">           </span>--format<span class="o">=</span>csv<span class="w"> </span>-l<span class="w"> </span><span class="m">1</span><span class="w"> </span>&gt;<span class="w"> </span>gpu_log.csv
</code></pre></div>

<p><strong>Python é›†æˆç›‘æ§</strong>ï¼š</p>
<div class="codehilite"><pre><span></span><code><span class="kn">import</span> <span class="nn">subprocess</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="k">def</span> <span class="nf">monitor_gpu_memory</span><span class="p">():</span>
    <span class="n">result</span> <span class="o">=</span> <span class="n">subprocess</span><span class="o">.</span><span class="n">run</span><span class="p">([</span>
        <span class="s1">&#39;nvidia-smi&#39;</span><span class="p">,</span> 
        <span class="s1">&#39;--query-gpu=memory.used,memory.free,memory.total&#39;</span><span class="p">,</span>
        <span class="s1">&#39;--format=csv,noheader,nounits&#39;</span>
    <span class="p">],</span> <span class="n">capture_output</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">text</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="n">lines</span> <span class="o">=</span> <span class="n">result</span><span class="o">.</span><span class="n">stdout</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">line</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">lines</span><span class="p">):</span>
        <span class="n">used</span><span class="p">,</span> <span class="n">free</span><span class="p">,</span> <span class="n">total</span> <span class="o">=</span> <span class="nb">map</span><span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="n">line</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;, &#39;</span><span class="p">))</span>
        <span class="n">usage_percent</span> <span class="o">=</span> <span class="p">(</span><span class="n">used</span> <span class="o">/</span> <span class="n">total</span><span class="p">)</span> <span class="o">*</span> <span class="mi">100</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;GPU </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">used</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">total</span><span class="si">}</span><span class="s2"> MB (</span><span class="si">{</span><span class="n">usage_percent</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2">%)&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">usage_percent</span> <span class="o">&gt;</span> <span class="mi">90</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;âš ï¸  GPU </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2"> å†…å­˜ä½¿ç”¨è¶…è¿‡ 90%ï¼&quot;</span><span class="p">)</span>
</code></pre></div>

<h3 id="933-memory-profiler">9.3.3 Memory Profiler å®æˆ˜</h3>
<p>ä½¿ç”¨ PyTorch Profiler è¿½è¸ªå†…å­˜åˆ†é…ï¼š</p>
<div class="codehilite"><pre><span></span><code><span class="kn">from</span> <span class="nn">torch.profiler</span> <span class="kn">import</span> <span class="n">profile</span><span class="p">,</span> <span class="n">ProfilerActivity</span><span class="p">,</span> <span class="n">record_function</span>

<span class="k">def</span> <span class="nf">profile_memory_usage</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">dataloader</span><span class="p">):</span>
    <span class="k">with</span> <span class="n">profile</span><span class="p">(</span>
        <span class="n">activities</span><span class="o">=</span><span class="p">[</span><span class="n">ProfilerActivity</span><span class="o">.</span><span class="n">CPU</span><span class="p">,</span> <span class="n">ProfilerActivity</span><span class="o">.</span><span class="n">CUDA</span><span class="p">],</span>
        <span class="n">profile_memory</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">record_shapes</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">with_stack</span><span class="o">=</span><span class="kc">True</span>
    <span class="p">)</span> <span class="k">as</span> <span class="n">prof</span><span class="p">:</span>

        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">batch</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">dataloader</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">i</span> <span class="o">&gt;=</span> <span class="mi">3</span><span class="p">:</span>  <span class="c1"># åªåˆ†æå‰ 3 ä¸ª batch</span>
                <span class="k">break</span>

            <span class="k">with</span> <span class="n">record_function</span><span class="p">(</span><span class="s2">&quot;forward&quot;</span><span class="p">):</span>
                <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>

            <span class="k">with</span> <span class="n">record_function</span><span class="p">(</span><span class="s2">&quot;loss&quot;</span><span class="p">):</span>
                <span class="n">loss</span> <span class="o">=</span> <span class="n">compute_loss</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">batch</span><span class="p">[</span><span class="s1">&#39;labels&#39;</span><span class="p">])</span>

            <span class="k">with</span> <span class="n">record_function</span><span class="p">(</span><span class="s2">&quot;backward&quot;</span><span class="p">):</span>
                <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

            <span class="k">with</span> <span class="n">record_function</span><span class="p">(</span><span class="s2">&quot;optimizer&quot;</span><span class="p">):</span>
                <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
                <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

    <span class="c1"># è¾“å‡ºåˆ†æç»“æœ</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">prof</span><span class="o">.</span><span class="n">key_averages</span><span class="p">()</span><span class="o">.</span><span class="n">table</span><span class="p">(</span><span class="n">sort_by</span><span class="o">=</span><span class="s2">&quot;cuda_memory_usage&quot;</span><span class="p">,</span> <span class="n">row_limit</span><span class="o">=</span><span class="mi">10</span><span class="p">))</span>

    <span class="c1"># ç”Ÿæˆ Chrome è¿½è¸ªæ–‡ä»¶</span>
    <span class="n">prof</span><span class="o">.</span><span class="n">export_chrome_trace</span><span class="p">(</span><span class="s2">&quot;memory_trace.json&quot;</span><span class="p">)</span>

    <span class="c1"># æ‰¾å‡ºå†…å­˜çƒ­ç‚¹</span>
    <span class="k">for</span> <span class="n">evt</span> <span class="ow">in</span> <span class="n">prof</span><span class="o">.</span><span class="n">key_averages</span><span class="p">():</span>
        <span class="k">if</span> <span class="n">evt</span><span class="o">.</span><span class="n">cuda_memory_usage</span> <span class="o">&gt;</span> <span class="mi">100</span> <span class="o">*</span> <span class="mi">1024</span> <span class="o">*</span> <span class="mi">1024</span><span class="p">:</span>  <span class="c1"># &gt; 100MB</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;å†…å­˜çƒ­ç‚¹: </span><span class="si">{</span><span class="n">evt</span><span class="o">.</span><span class="n">key</span><span class="si">}</span><span class="s2">, ä½¿ç”¨: </span><span class="si">{</span><span class="n">evt</span><span class="o">.</span><span class="n">cuda_memory_usage</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="mi">1024</span><span class="o">**</span><span class="mi">2</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2"> MB&quot;</span><span class="p">)</span>
</code></pre></div>

<p><strong>åˆ†ææŠ€å·§</strong>ï¼š</p>
<ol>
<li>ç”¨ Chrome æµè§ˆå™¨æ‰“å¼€ <code>chrome://tracing</code>ï¼ŒåŠ è½½ json æ–‡ä»¶</li>
<li>æŸ¥çœ‹å†…å­˜åˆ†é…æ—¶é—´çº¿ï¼Œå®šä½å³°å€¼</li>
<li>è¯†åˆ«å†…å­˜æ³„æ¼ï¼ˆæŒç»­å¢é•¿çš„æ›²çº¿ï¼‰</li>
</ol>
<h3 id="934">9.3.4 è‡ªå®šä¹‰å†…å­˜ç›‘æ§</h3>
<p>æ„å»ºå®æ—¶å†…å­˜ç›‘æ§ç³»ç»Ÿï¼š</p>
<div class="codehilite"><pre><span></span><code><span class="kn">import</span> <span class="nn">threading</span>
<span class="kn">import</span> <span class="nn">time</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">deque</span>

<span class="k">class</span> <span class="nc">MemoryMonitor</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">interval</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">max_history</span><span class="o">=</span><span class="mi">100</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">interval</span> <span class="o">=</span> <span class="n">interval</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_history</span> <span class="o">=</span> <span class="n">max_history</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">memory_history</span> <span class="o">=</span> <span class="n">deque</span><span class="p">(</span><span class="n">maxlen</span><span class="o">=</span><span class="n">max_history</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">time_history</span> <span class="o">=</span> <span class="n">deque</span><span class="p">(</span><span class="n">maxlen</span><span class="o">=</span><span class="n">max_history</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">running</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">thread</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">def</span> <span class="nf">start</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">running</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">thread</span> <span class="o">=</span> <span class="n">threading</span><span class="o">.</span><span class="n">Thread</span><span class="p">(</span><span class="n">target</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_monitor_loop</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">thread</span><span class="o">.</span><span class="n">start</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">stop</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">running</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">thread</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">thread</span><span class="o">.</span><span class="n">join</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">_monitor_loop</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
        <span class="k">while</span> <span class="bp">self</span><span class="o">.</span><span class="n">running</span><span class="p">:</span>
            <span class="c1"># è®°å½•å†…å­˜ä½¿ç”¨</span>
            <span class="n">allocated</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">memory_allocated</span><span class="p">()</span> <span class="o">/</span> <span class="mi">1024</span><span class="o">**</span><span class="mi">3</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">memory_history</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">allocated</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">time_history</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">start_time</span><span class="p">)</span>

            <span class="c1"># æ£€æµ‹å¼‚å¸¸</span>
            <span class="k">if</span> <span class="n">allocated</span> <span class="o">&gt;</span> <span class="mf">0.9</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">get_device_properties</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">total_memory</span> <span class="o">/</span> <span class="mi">1024</span><span class="o">**</span><span class="mi">3</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;âš ï¸  å†…å­˜å‘Šè­¦: </span><span class="si">{</span><span class="n">allocated</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> GB&quot;</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_dump_tensors</span><span class="p">()</span>

            <span class="n">time</span><span class="o">.</span><span class="n">sleep</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">interval</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_dump_tensors</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;è¾“å‡ºå ç”¨å†…å­˜æœ€å¤§çš„å¼ é‡&quot;&quot;&quot;</span>
        <span class="n">tensors</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">obj</span> <span class="ow">in</span> <span class="n">gc</span><span class="o">.</span><span class="n">get_objects</span><span class="p">():</span>
            <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">is_tensor</span><span class="p">(</span><span class="n">obj</span><span class="p">)</span> <span class="ow">and</span> <span class="n">obj</span><span class="o">.</span><span class="n">is_cuda</span><span class="p">:</span>
                <span class="n">tensors</span><span class="o">.</span><span class="n">append</span><span class="p">((</span>
                    <span class="n">obj</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="o">*</span> <span class="n">obj</span><span class="o">.</span><span class="n">element_size</span><span class="p">(),</span>
                    <span class="nb">str</span><span class="p">(</span><span class="n">obj</span><span class="o">.</span><span class="n">shape</span><span class="p">),</span>
                    <span class="nb">str</span><span class="p">(</span><span class="n">obj</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
                <span class="p">))</span>

        <span class="n">tensors</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">reverse</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">=== Top 5 å†…å­˜å ç”¨å¼ é‡ ===&quot;</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">size</span><span class="p">,</span> <span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span> <span class="ow">in</span> <span class="n">tensors</span><span class="p">[:</span><span class="mi">5</span><span class="p">]:</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">size</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="mi">1024</span><span class="o">**</span><span class="mi">2</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2"> MB: </span><span class="si">{</span><span class="n">shape</span><span class="si">}</span><span class="s2"> (</span><span class="si">{</span><span class="n">dtype</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">plot</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">time_history</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">memory_history</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;æ—¶é—´ (ç§’)&#39;</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;æ˜¾å­˜ä½¿ç”¨ (GB)&#39;</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;è®­ç»ƒè¿‡ç¨‹æ˜¾å­˜ç›‘æ§&#39;</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="c1"># ä½¿ç”¨ç¤ºä¾‹</span>
<span class="n">monitor</span> <span class="o">=</span> <span class="n">MemoryMonitor</span><span class="p">(</span><span class="n">interval</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">monitor</span><span class="o">.</span><span class="n">start</span><span class="p">()</span>

<span class="c1"># è®­ç»ƒä»£ç </span>
<span class="n">train_model</span><span class="p">()</span>

<span class="n">monitor</span><span class="o">.</span><span class="n">stop</span><span class="p">()</span>
<span class="n">monitor</span><span class="o">.</span><span class="n">plot</span><span class="p">()</span>
</code></pre></div>

<h3 id="935">9.3.5 å†…å­˜æ³„æ¼æ£€æµ‹</h3>
<p>VLM è®­ç»ƒä¸­å¸¸è§çš„å†…å­˜æ³„æ¼æ¨¡å¼ï¼š</p>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">detect_memory_leak</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">dataloader</span><span class="p">,</span> <span class="n">num_iterations</span><span class="o">=</span><span class="mi">50</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;æ£€æµ‹è®­ç»ƒè¿‡ç¨‹ä¸­çš„å†…å­˜æ³„æ¼&quot;&quot;&quot;</span>

    <span class="n">memory_usage</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">batch</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">dataloader</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">i</span> <span class="o">&gt;=</span> <span class="n">num_iterations</span><span class="p">:</span>
            <span class="k">break</span>

        <span class="c1"># è®­ç»ƒæ­¥éª¤</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">compute_loss</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">batch</span><span class="p">[</span><span class="s1">&#39;labels&#39;</span><span class="p">])</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

        <span class="c1"># è®°å½•å†…å­˜</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">synchronize</span><span class="p">()</span>
        <span class="n">memory_usage</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">memory_allocated</span><span class="p">())</span>

        <span class="c1"># æ¯ 10 æ­¥æ£€æŸ¥ä¸€æ¬¡</span>
        <span class="k">if</span> <span class="n">i</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">i</span> <span class="o">%</span> <span class="mi">10</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="c1"># è®¡ç®—å†…å­˜å¢é•¿ç‡</span>
            <span class="n">recent_memory</span> <span class="o">=</span> <span class="n">memory_usage</span><span class="p">[</span><span class="o">-</span><span class="mi">10</span><span class="p">:]</span>
            <span class="n">growth_rate</span> <span class="o">=</span> <span class="p">(</span><span class="n">recent_memory</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">recent_memory</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="o">/</span> <span class="n">recent_memory</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

            <span class="k">if</span> <span class="n">growth_rate</span> <span class="o">&gt;</span> <span class="mf">0.05</span><span class="p">:</span>  <span class="c1"># å¢é•¿è¶…è¿‡ 5%</span>
                <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;âš ï¸  å¯èƒ½å­˜åœ¨å†…å­˜æ³„æ¼ï¼æ­¥éª¤ </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">, å¢é•¿ç‡: </span><span class="si">{</span><span class="n">growth_rate</span><span class="si">:</span><span class="s2">.2%</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

                <span class="c1"># å°è¯•å®šä½æ³„æ¼æº</span>
                <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">():</span>
                    <span class="k">if</span> <span class="n">param</span><span class="o">.</span><span class="n">grad</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">param</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">data_ptr</span><span class="p">()</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
                        <span class="c1"># æ£€æŸ¥æ¢¯åº¦æ˜¯å¦å¼‚å¸¸ç´¯ç§¯</span>
                        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">param</span><span class="p">,</span> <span class="s1">&#39;_grad_accumulation_count&#39;</span><span class="p">):</span>
                            <span class="k">if</span> <span class="n">param</span><span class="o">.</span><span class="n">_grad_accumulation_count</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
                                <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  æ¢¯åº¦ç´¯ç§¯å¼‚å¸¸: </span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">memory_usage</span>

<span class="c1"># å¸¸è§æ³„æ¼åŸå› åŠè§£å†³æ–¹æ¡ˆ</span>
<span class="sd">&quot;&quot;&quot;</span>

<span class="sd">1. ä¿å­˜äº†è®¡ç®—å›¾ï¼šä½¿ç”¨ loss.item() è€Œä¸æ˜¯ loss</span>
<span class="sd">2. åˆ—è¡¨ç´¯ç§¯å¼ é‡ï¼šå®šæœŸæ¸…ç†æˆ–ä½¿ç”¨ .detach()</span>
<span class="sd">3. è‡ªå®šä¹‰ autograd å‡½æ•°ï¼šç¡®ä¿æ­£ç¡®å®ç° backward</span>
<span class="sd">4. hook æœªé‡Šæ”¾ï¼šè®­ç»ƒç»“æŸåè°ƒç”¨ handle.remove()</span>
<span class="sd">&quot;&quot;&quot;</span>
</code></pre></div>

<h2 id="94-vlm">9.4 VLM ç‰¹æœ‰çš„å†…å­˜é™·é˜±</h2>
<p>VLM ç›¸æ¯”çº¯è¯­è¨€æ¨¡å‹ï¼Œæœ‰å…¶ç‹¬ç‰¹çš„å†…å­˜æŒ‘æˆ˜ã€‚æœ¬èŠ‚æ·±å…¥å‰–æ 4 ç±»å¸¸è§é™·é˜±åŠè§£å†³æ–¹æ¡ˆã€‚</p>
<h3 id="941">9.4.1 è§†è§‰ç¼–ç å™¨å†…å­˜çˆ†ç‚¸</h3>
<p><strong>é—®é¢˜ç°è±¡</strong>ï¼š</p>
<ul>
<li>å•å¼ é«˜åˆ†è¾¨ç‡å›¾åƒå°± OOM</li>
<li>å¤šå›¾è¾“å…¥æ—¶å†…å­˜æŒ‡æ•°å¢é•¿</li>
<li>åŠ¨æ€åˆ†è¾¨ç‡å¯¼è‡´å†…å­˜ä¸å¯é¢„æµ‹</li>
</ul>
<p><strong>æ ¹æœ¬åŸå› </strong>ï¼š</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># é—®é¢˜ä»£ç ç¤ºä¾‹</span>
<span class="k">def</span> <span class="nf">process_images</span><span class="p">(</span><span class="n">images</span><span class="p">,</span> <span class="n">vision_encoder</span><span class="p">):</span>
    <span class="c1"># å±é™©ï¼æ‰€æœ‰å›¾åƒåŒæ—¶ç¼–ç </span>
    <span class="n">features</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">img</span> <span class="ow">in</span> <span class="n">images</span><span class="p">:</span>  <span class="c1"># images: [B, N, C, H, W]</span>
        <span class="n">feat</span> <span class="o">=</span> <span class="n">vision_encoder</span><span class="p">(</span><span class="n">img</span><span class="p">)</span>  <span class="c1"># æ¯æ¬¡éƒ½ä¿ç•™åœ¨æ˜¾å­˜ä¸­</span>
        <span class="n">features</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">feat</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">features</span><span class="p">)</span>
</code></pre></div>

<p><strong>å†…å­˜è®¡ç®—</strong>ï¼š</p>
<div class="codehilite"><pre><span></span><code>å•å¼ å›¾åƒ tokens = (H/patch_size) Ã— (W/patch_size)
ViT-L/14: 1024Ã—1024 å›¾åƒ â†’ 5184 tokensï¼
å†…å­˜ = B Ã— N_images Ã— tokens Ã— hidden_dim Ã— 4 bytes
     = 1 Ã— 4 Ã— 5184 Ã— 1024 Ã— 4 = 84.9 MBï¼ˆä»…æ¿€æ´»å€¼ï¼‰
</code></pre></div>

<p><strong>è§£å†³æ–¹æ¡ˆ</strong>ï¼š</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># æ–¹æ¡ˆ 1ï¼šæ‰¹å¤„ç†ä¼˜åŒ–</span>
<span class="k">def</span> <span class="nf">process_images_optimized</span><span class="p">(</span><span class="n">images</span><span class="p">,</span> <span class="n">vision_encoder</span><span class="p">,</span> <span class="n">max_batch</span><span class="o">=</span><span class="mi">2</span><span class="p">):</span>
    <span class="n">B</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">W</span> <span class="o">=</span> <span class="n">images</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">features</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="c1"># åˆ†æ‰¹å¤„ç†</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">max_batch</span><span class="p">):</span>
        <span class="n">batch_images</span> <span class="o">=</span> <span class="n">images</span><span class="p">[:,</span> <span class="n">i</span><span class="p">:</span><span class="n">i</span><span class="o">+</span><span class="n">max_batch</span><span class="p">]</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">amp</span><span class="o">.</span><span class="n">autocast</span><span class="p">():</span>  <span class="c1"># ä½¿ç”¨æ··åˆç²¾åº¦</span>
            <span class="n">feat</span> <span class="o">=</span> <span class="n">vision_encoder</span><span class="p">(</span><span class="n">batch_images</span><span class="p">)</span>
        <span class="n">features</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">feat</span><span class="p">)</span>

        <span class="c1"># åŠæ—¶æ¸…ç†</span>
        <span class="k">if</span> <span class="n">i</span> <span class="o">+</span> <span class="n">max_batch</span> <span class="o">&lt;</span> <span class="n">N</span><span class="p">:</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">empty_cache</span><span class="p">()</span>

    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">features</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># æ–¹æ¡ˆ 2ï¼šåŠ¨æ€åˆ†è¾¨ç‡ç­–ç•¥</span>
<span class="k">def</span> <span class="nf">adaptive_resolution</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">base_resolution</span><span class="o">=</span><span class="mi">336</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;æ ¹æ®æ˜¾å­˜åŠ¨æ€è°ƒæ•´åˆ†è¾¨ç‡&quot;&quot;&quot;</span>
    <span class="n">available_memory</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">mem_get_info</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span> <span class="o">/</span> <span class="mi">1024</span><span class="o">**</span><span class="mi">3</span>  <span class="c1"># GB</span>

    <span class="k">if</span> <span class="n">available_memory</span> <span class="o">&lt;</span> <span class="mi">4</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">interpolate</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">base_resolution</span><span class="p">,</span> <span class="n">base_resolution</span><span class="p">))</span>
    <span class="k">elif</span> <span class="n">available_memory</span> <span class="o">&lt;</span> <span class="mi">8</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">interpolate</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">base_resolution</span><span class="o">*</span><span class="mi">2</span><span class="p">,</span> <span class="n">base_resolution</span><span class="o">*</span><span class="mi">2</span><span class="p">))</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">image</span>  <span class="c1"># åŸå§‹åˆ†è¾¨ç‡</span>
</code></pre></div>

<h3 id="942">9.4.2 æ³¨æ„åŠ›çŸ©é˜µå†…å­˜é—®é¢˜</h3>
<p><strong>é—®é¢˜ç°è±¡</strong>ï¼š</p>
<ul>
<li>é•¿åºåˆ—ï¼ˆ&gt;2048 tokensï¼‰ç›´æ¥ OOM</li>
<li>å¤šæ¨¡æ€ token æ··åˆå¯¼è‡´å†…å­˜æ¿€å¢</li>
<li>Cross-attention å†…å­˜å¼€é”€å·¨å¤§</li>
</ul>
<p><strong>å†…å­˜åˆ†æ</strong>ï¼š</p>
<p>æ ‡å‡†æ³¨æ„åŠ›å†…å­˜å¤æ‚åº¦ï¼š$O(L^2)$</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># æ³¨æ„åŠ›çŸ©é˜µå¤§å°è®¡ç®—</span>
<span class="k">def</span> <span class="nf">attention_memory</span><span class="p">(</span><span class="n">seq_len</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">):</span>
    <span class="c1"># Q @ K^T çš„å¤§å°</span>
    <span class="n">memory_bytes</span> <span class="o">=</span> <span class="n">batch_size</span> <span class="o">*</span> <span class="n">num_heads</span> <span class="o">*</span> <span class="n">seq_len</span> <span class="o">*</span> <span class="n">seq_len</span> <span class="o">*</span> <span class="mi">4</span>
    <span class="k">return</span> <span class="n">memory_bytes</span> <span class="o">/</span> <span class="mi">1024</span><span class="o">**</span><span class="mi">3</span>  <span class="c1"># GB</span>

<span class="c1"># ç¤ºä¾‹ï¼š2048 tokens, 32 heads, batch_size=1</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;æ³¨æ„åŠ›çŸ©é˜µ: </span><span class="si">{</span><span class="n">attention_memory</span><span class="p">(</span><span class="mi">2048</span><span class="p">,</span><span class="w"> </span><span class="mi">32</span><span class="p">,</span><span class="w"> </span><span class="mi">1</span><span class="p">)</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> GB&quot;</span><span class="p">)</span>
<span class="c1"># è¾“å‡º: 0.50 GBï¼ˆå•å±‚ï¼ï¼‰</span>
</code></pre></div>

<p><strong>è§£å†³æ–¹æ¡ˆ</strong>ï¼š</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># æ–¹æ¡ˆ 1ï¼šFlash Attention</span>
<span class="kn">from</span> <span class="nn">flash_attn</span> <span class="kn">import</span> <span class="n">flash_attn_func</span>

<span class="k">class</span> <span class="nc">FlashAttentionVLM</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">):</span>
        <span class="c1"># Flash Attentionï¼šå†…å­˜ä» O(L^2) é™è‡³ O(L)</span>
        <span class="k">return</span> <span class="n">flash_attn_func</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">causal</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="c1"># æ–¹æ¡ˆ 2ï¼šæ»‘åŠ¨çª—å£æ³¨æ„åŠ›</span>
<span class="k">def</span> <span class="nf">sliding_window_attention</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">window_size</span><span class="o">=</span><span class="mi">512</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;åªè®¡ç®—å±€éƒ¨çª—å£å†…çš„æ³¨æ„åŠ›&quot;&quot;&quot;</span>
    <span class="n">B</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">L</span><span class="p">,</span> <span class="n">D</span> <span class="o">=</span> <span class="n">q</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">attention_scores</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">L</span><span class="p">,</span> <span class="n">window_size</span> <span class="o">//</span> <span class="mi">2</span><span class="p">):</span>  <span class="c1"># 50% é‡å </span>
        <span class="n">start</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">i</span> <span class="o">-</span> <span class="n">window_size</span> <span class="o">//</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">end</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">L</span><span class="p">,</span> <span class="n">i</span> <span class="o">+</span> <span class="n">window_size</span><span class="p">)</span>

        <span class="n">q_window</span> <span class="o">=</span> <span class="n">q</span><span class="p">[:,</span> <span class="p">:,</span> <span class="n">start</span><span class="p">:</span><span class="n">end</span><span class="p">]</span>
        <span class="n">k_window</span> <span class="o">=</span> <span class="n">k</span><span class="p">[:,</span> <span class="p">:,</span> <span class="n">start</span><span class="p">:</span><span class="n">end</span><span class="p">]</span>
        <span class="n">v_window</span> <span class="o">=</span> <span class="n">v</span><span class="p">[:,</span> <span class="p">:,</span> <span class="n">start</span><span class="p">:</span><span class="n">end</span><span class="p">]</span>

        <span class="n">scores</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">q_window</span><span class="p">,</span> <span class="n">k_window</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>
        <span class="n">scores</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">scores</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">D</span><span class="p">),</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">v_window</span><span class="p">)</span>
        <span class="n">attention_scores</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">combine_windows</span><span class="p">(</span><span class="n">attention_scores</span><span class="p">)</span>

<span class="c1"># æ–¹æ¡ˆ 3ï¼šç¨€ç–æ³¨æ„åŠ›</span>
<span class="k">class</span> <span class="nc">SparseAttentionVLM</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sparsity_ratio</span><span class="o">=</span><span class="mf">0.9</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sparsity_ratio</span> <span class="o">=</span> <span class="n">sparsity_ratio</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">):</span>
        <span class="c1"># åªä¿ç•™ top-k æ³¨æ„åŠ›æƒé‡</span>
        <span class="n">scores</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>

        <span class="c1"># ä¿ç•™ top 10% çš„å€¼</span>
        <span class="n">k_val</span> <span class="o">=</span> <span class="nb">int</span><span class="p">((</span><span class="mi">1</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">sparsity_ratio</span><span class="p">)</span> <span class="o">*</span> <span class="n">scores</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
        <span class="n">topk_scores</span><span class="p">,</span> <span class="n">topk_indices</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">topk</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">k_val</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

        <span class="c1"># åˆ›å»ºç¨€ç–çŸ©é˜µ</span>
        <span class="n">sparse_scores</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">scores</span><span class="p">)</span>
        <span class="n">sparse_scores</span><span class="o">.</span><span class="n">scatter_</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">topk_indices</span><span class="p">,</span> <span class="n">topk_scores</span><span class="p">)</span>

        <span class="n">attn_weights</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">sparse_scores</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">attn_weights</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>
</code></pre></div>

<h3 id="943">9.4.3 å¤šåˆ†è¾¨ç‡å¤„ç†é™·é˜±</h3>
<p><strong>é—®é¢˜ç°è±¡</strong>ï¼š</p>
<ul>
<li>ä¸åŒåˆ†è¾¨ç‡å›¾åƒå¯¼è‡´å†…å­˜æ³¢åŠ¨</li>
<li>åŠ¨æ€ padding é€ æˆå†…å­˜æµªè´¹</li>
<li>æ‰¹å¤„ç†æ—¶æœ€å¤§åˆ†è¾¨ç‡å†³å®šæ•´ä½“å†…å­˜</li>
</ul>
<p><strong>ç¤ºä¾‹é—®é¢˜</strong>ï¼š</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># é—®é¢˜ä»£ç </span>
<span class="k">def</span> <span class="nf">batch_images_naive</span><span class="p">(</span><span class="n">image_list</span><span class="p">):</span>
    <span class="c1"># æ‰€æœ‰å›¾åƒ pad åˆ°æœ€å¤§å°ºå¯¸ â†’ å†…å­˜æµªè´¹ï¼</span>
    <span class="n">max_h</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">img</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span> <span class="k">for</span> <span class="n">img</span> <span class="ow">in</span> <span class="n">image_list</span><span class="p">)</span>
    <span class="n">max_w</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">img</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">img</span> <span class="ow">in</span> <span class="n">image_list</span><span class="p">)</span>

    <span class="n">padded_images</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">img</span> <span class="ow">in</span> <span class="n">image_list</span><span class="p">:</span>
        <span class="n">pad_h</span> <span class="o">=</span> <span class="n">max_h</span> <span class="o">-</span> <span class="n">img</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span>
        <span class="n">pad_w</span> <span class="o">=</span> <span class="n">max_w</span> <span class="o">-</span> <span class="n">img</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">padded</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">pad_w</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">pad_h</span><span class="p">))</span>
        <span class="n">padded_images</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">padded</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">padded_images</span><span class="p">)</span>
</code></pre></div>

<p><strong>ä¼˜åŒ–æ–¹æ¡ˆ</strong>ï¼š</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># æ–¹æ¡ˆ 1ï¼šåˆ†ç»„æ‰¹å¤„ç†</span>
<span class="k">def</span> <span class="nf">group_by_resolution</span><span class="p">(</span><span class="n">images</span><span class="p">,</span> <span class="n">num_groups</span><span class="o">=</span><span class="mi">3</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;æŒ‰åˆ†è¾¨ç‡åˆ†ç»„ï¼Œå‡å°‘ padding æµªè´¹&quot;&quot;&quot;</span>
    <span class="c1"># è®¡ç®—æ¯å¼ å›¾åƒçš„åƒç´ æ•°</span>
    <span class="n">resolutions</span> <span class="o">=</span> <span class="p">[</span><span class="n">img</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span> <span class="o">*</span> <span class="n">img</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">img</span> <span class="ow">in</span> <span class="n">images</span><span class="p">]</span>

    <span class="c1"># K-means èšç±»</span>
    <span class="n">groups</span> <span class="o">=</span> <span class="n">defaultdict</span><span class="p">(</span><span class="nb">list</span><span class="p">)</span>
    <span class="n">sorted_indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">resolutions</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">idx</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">sorted_indices</span><span class="p">):</span>
        <span class="n">group_id</span> <span class="o">=</span> <span class="n">i</span> <span class="o">*</span> <span class="n">num_groups</span> <span class="o">//</span> <span class="nb">len</span><span class="p">(</span><span class="n">sorted_indices</span><span class="p">)</span>
        <span class="n">groups</span><span class="p">[</span><span class="n">group_id</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">images</span><span class="p">[</span><span class="n">idx</span><span class="p">])</span>

    <span class="c1"># æ¯ç»„å•ç‹¬å¤„ç†</span>
    <span class="n">processed_groups</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">group_images</span> <span class="ow">in</span> <span class="n">groups</span><span class="o">.</span><span class="n">values</span><span class="p">():</span>
        <span class="n">batch</span> <span class="o">=</span> <span class="n">batch_images_naive</span><span class="p">(</span><span class="n">group_images</span><span class="p">)</span>  <span class="c1"># ç»„å†… padding</span>
        <span class="n">processed_groups</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">processed_groups</span>

<span class="c1"># æ–¹æ¡ˆ 2ï¼šåŠ¨æ€åˆ†å—å¤„ç†</span>
<span class="k">class</span> <span class="nc">DynamicPatchProcessor</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">base_size</span><span class="o">=</span><span class="mi">224</span><span class="p">,</span> <span class="n">max_patches</span><span class="o">=</span><span class="mi">16</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">base_size</span> <span class="o">=</span> <span class="n">base_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_patches</span> <span class="o">=</span> <span class="n">max_patches</span>

    <span class="k">def</span> <span class="nf">process</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">image</span><span class="p">):</span>
        <span class="n">H</span><span class="p">,</span> <span class="n">W</span> <span class="o">=</span> <span class="n">image</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">:]</span>

        <span class="c1"># è®¡ç®—éœ€è¦çš„ patch æ•°é‡</span>
        <span class="n">n_h</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="n">H</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">base_size</span><span class="p">)</span>
        <span class="n">n_w</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="n">W</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">base_size</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">n_h</span> <span class="o">*</span> <span class="n">n_w</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_patches</span><span class="p">:</span>
            <span class="c1"># é™é‡‡æ ·ä»¥æ»¡è¶³å†…å­˜é™åˆ¶</span>
            <span class="n">scale</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">max_patches</span> <span class="o">/</span> <span class="p">(</span><span class="n">n_h</span> <span class="o">*</span> <span class="n">n_w</span><span class="p">))</span>
            <span class="n">new_h</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">H</span> <span class="o">*</span> <span class="n">scale</span><span class="p">)</span>
            <span class="n">new_w</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">W</span> <span class="o">*</span> <span class="n">scale</span><span class="p">)</span>
            <span class="n">image</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">interpolate</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">new_h</span><span class="p">,</span> <span class="n">new_w</span><span class="p">))</span>
            <span class="n">n_h</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="n">new_h</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">base_size</span><span class="p">)</span>
            <span class="n">n_w</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="n">new_w</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">base_size</span><span class="p">)</span>

        <span class="c1"># åˆ†å—å¤„ç†</span>
        <span class="n">patches</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_h</span><span class="p">):</span>
            <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_w</span><span class="p">):</span>
                <span class="n">patch</span> <span class="o">=</span> <span class="n">image</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> 
                             <span class="n">i</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">base_size</span><span class="p">:(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">base_size</span><span class="p">,</span>
                             <span class="n">j</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">base_size</span><span class="p">:(</span><span class="n">j</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">base_size</span><span class="p">]</span>
                <span class="n">patches</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">patch</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">patches</span><span class="p">,</span> <span class="p">(</span><span class="n">n_h</span><span class="p">,</span> <span class="n">n_w</span><span class="p">)</span>
</code></pre></div>

<h3 id="944">9.4.4 äº¤å‰æ³¨æ„åŠ›å†…å­˜ä¼˜åŒ–</h3>
<p><strong>é—®é¢˜ç°è±¡</strong>ï¼š</p>
<ul>
<li>è§†è§‰-è¯­è¨€äº¤å‰æ³¨æ„åŠ›å†…å­˜å¼€é”€å·¨å¤§</li>
<li>å¤šå±‚äº¤å‰æ³¨æ„åŠ›ç´¯ç§¯å¯¼è‡´ OOM</li>
<li>Cache æœºåˆ¶å¤±æ•ˆ</li>
</ul>
<p><strong>å†…å­˜åˆ†æ</strong>ï¼š</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># äº¤å‰æ³¨æ„åŠ›å†…å­˜è®¡ç®—</span>
<span class="k">def</span> <span class="nf">cross_attention_memory</span><span class="p">(</span><span class="n">text_len</span><span class="p">,</span> <span class="n">image_tokens</span><span class="p">,</span> <span class="n">num_layers</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">):</span>
    <span class="c1"># æ¯å±‚éƒ½éœ€è¦å­˜å‚¨ K, V</span>
    <span class="n">kv_memory</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">image_tokens</span> <span class="o">*</span> <span class="n">hidden_dim</span> <span class="o">*</span> <span class="mi">4</span>  <span class="c1"># bytes</span>

    <span class="c1"># æ³¨æ„åŠ›çŸ©é˜µ</span>
    <span class="n">attn_memory</span> <span class="o">=</span> <span class="n">text_len</span> <span class="o">*</span> <span class="n">image_tokens</span> <span class="o">*</span> <span class="mi">4</span>  <span class="c1"># bytes</span>

    <span class="n">total</span> <span class="o">=</span> <span class="n">num_layers</span> <span class="o">*</span> <span class="p">(</span><span class="n">kv_memory</span> <span class="o">+</span> <span class="n">attn_memory</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">total</span> <span class="o">/</span> <span class="mi">1024</span><span class="o">**</span><span class="mi">3</span>  <span class="c1"># GB</span>

<span class="c1"># ç¤ºä¾‹ï¼š1024 text tokens, 576 image tokens, 24 layers</span>
<span class="n">memory</span> <span class="o">=</span> <span class="n">cross_attention_memory</span><span class="p">(</span><span class="mi">1024</span><span class="p">,</span> <span class="mi">576</span><span class="p">,</span> <span class="mi">24</span><span class="p">,</span> <span class="mi">4096</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;äº¤å‰æ³¨æ„åŠ›å†…å­˜: </span><span class="si">{</span><span class="n">memory</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> GB&quot;</span><span class="p">)</span>
</code></pre></div>

<p><strong>ä¼˜åŒ–ç­–ç•¥</strong>ï¼š</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># æ–¹æ¡ˆ 1ï¼šå…±äº« KV cache</span>
<span class="k">class</span> <span class="nc">SharedCrossAttention</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_layers</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="c1"># åªåœ¨ç¬¬ä¸€å±‚è®¡ç®— image KVï¼Œåç»­å±‚å¤ç”¨</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">image_proj_k</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">image_proj_v</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layers</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">([</span>
            <span class="n">CrossAttentionLayer</span><span class="p">(</span><span class="n">hidden_dim</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_layers</span><span class="p">)</span>
        <span class="p">])</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">text_hidden</span><span class="p">,</span> <span class="n">image_features</span><span class="p">):</span>
        <span class="c1"># ä¸€æ¬¡æ€§è®¡ç®—æ‰€æœ‰å±‚çš„ KV</span>
        <span class="n">image_k</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">image_proj_k</span><span class="p">(</span><span class="n">image_features</span><span class="p">)</span>
        <span class="n">image_v</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">image_proj_v</span><span class="p">(</span><span class="n">image_features</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">:</span>
            <span class="n">text_hidden</span> <span class="o">=</span> <span class="n">layer</span><span class="p">(</span><span class="n">text_hidden</span><span class="p">,</span> <span class="n">image_k</span><span class="p">,</span> <span class="n">image_v</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">text_hidden</span>

<span class="c1"># æ–¹æ¡ˆ 2ï¼šé—¨æ§äº¤å‰æ³¨æ„åŠ›</span>
<span class="k">class</span> <span class="nc">GatedCrossAttention</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;åªåœ¨å¿…è¦æ—¶è¿›è¡Œäº¤å‰æ³¨æ„åŠ›&quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">,</span> <span class="n">threshold</span><span class="o">=</span><span class="mf">0.5</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gate</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">threshold</span> <span class="o">=</span> <span class="n">threshold</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cross_attn</span> <span class="o">=</span> <span class="n">CrossAttentionLayer</span><span class="p">(</span><span class="n">hidden_dim</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">text_hidden</span><span class="p">,</span> <span class="n">image_features</span><span class="p">):</span>
        <span class="c1"># è®¡ç®—é—¨æ§å€¼</span>
        <span class="n">gate_scores</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">gate</span><span class="p">(</span><span class="n">text_hidden</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)))</span>

        <span class="k">if</span> <span class="n">gate_scores</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">threshold</span><span class="p">:</span>
            <span class="c1"># æ‰§è¡Œäº¤å‰æ³¨æ„åŠ›</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">cross_attn</span><span class="p">(</span><span class="n">text_hidden</span><span class="p">,</span> <span class="n">image_features</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># è·³è¿‡ï¼ŒèŠ‚çœå†…å­˜</span>
            <span class="k">return</span> <span class="n">text_hidden</span>

<span class="c1"># æ–¹æ¡ˆ 3ï¼šä½ç§©åˆ†è§£</span>
<span class="k">class</span> <span class="nc">LowRankCrossAttention</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;ä½¿ç”¨ä½ç§©åˆ†è§£å‡å°‘å‚æ•°å’Œå†…å­˜&quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">,</span> <span class="n">rank</span><span class="o">=</span><span class="mi">64</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">rank</span> <span class="o">=</span> <span class="n">rank</span>

        <span class="c1"># åˆ†è§£ W_q, W_k, W_v</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">q_down</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="n">rank</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">q_up</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">rank</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">kv_down</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="n">rank</span> <span class="o">*</span> <span class="mi">2</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">kv_up</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">rank</span> <span class="o">*</span> <span class="mi">2</span><span class="p">,</span> <span class="n">hidden_dim</span> <span class="o">*</span> <span class="mi">2</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">text_hidden</span><span class="p">,</span> <span class="n">image_features</span><span class="p">):</span>
        <span class="c1"># ä½ç§©æŠ•å½±</span>
        <span class="n">q</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">q_up</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">q_down</span><span class="p">(</span><span class="n">text_hidden</span><span class="p">))</span>
        <span class="n">kv</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">kv_up</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">kv_down</span><span class="p">(</span><span class="n">image_features</span><span class="p">))</span>
        <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="o">=</span> <span class="n">kv</span><span class="o">.</span><span class="n">chunk</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

        <span class="c1"># æ ‡å‡†æ³¨æ„åŠ›è®¡ç®—</span>
        <span class="n">scores</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">q</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>
        <span class="n">attn</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">attn</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>
</code></pre></div>

<h3 id="945">9.4.5 å†…å­˜ä¼˜åŒ–æœ€ä½³å®è·µæ±‡æ€»</h3>
<div class="codehilite"><pre><span></span><code><span class="k">class</span> <span class="nc">MemoryOptimizedVLM</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;é›†æˆæ‰€æœ‰å†…å­˜ä¼˜åŒ–æŠ€æœ¯çš„ VLM&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">config</span> <span class="o">=</span> <span class="n">config</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">setup_memory_optimization</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">setup_memory_optimization</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># 1. å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">gradient_checkpointing</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">gradient_checkpointing_enable</span><span class="p">()</span>

        <span class="c1"># 2. ä½¿ç”¨ Flash Attention</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">use_flash_attention</span><span class="p">:</span>
            <span class="n">replace_attention_with_flash_attention</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">)</span>

        <span class="c1"># 3. æ··åˆç²¾åº¦è®­ç»ƒ</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">mixed_precision</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">scaler</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">amp</span><span class="o">.</span><span class="n">GradScaler</span><span class="p">()</span>

        <span class="c1"># 4. å†…å­˜ç›‘æ§</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">memory_monitor</span> <span class="o">=</span> <span class="n">MemoryMonitor</span><span class="p">(</span><span class="n">interval</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">train_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">):</span>
        <span class="c1"># åŠ¨æ€è°ƒæ•´ batch size</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">should_reduce_batch_size</span><span class="p">():</span>
            <span class="n">batch</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">split_batch</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>

        <span class="c1"># åˆ†ç»„å¤„ç†å¤šåˆ†è¾¨ç‡å›¾åƒ</span>
        <span class="n">image_groups</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">group_images_by_resolution</span><span class="p">(</span><span class="n">batch</span><span class="p">[</span><span class="s1">&#39;images&#39;</span><span class="p">])</span>

        <span class="n">total_loss</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">images</span> <span class="ow">in</span> <span class="n">image_groups</span><span class="p">:</span>
            <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">amp</span><span class="o">.</span><span class="n">autocast</span><span class="p">(</span><span class="n">enabled</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">mixed_precision</span><span class="p">):</span>
                <span class="c1"># å‰å‘ä¼ æ’­</span>
                <span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">(</span><span class="n">images</span><span class="p">,</span> <span class="n">batch</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">])</span>
                <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">criterion</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">batch</span><span class="p">[</span><span class="s1">&#39;labels&#39;</span><span class="p">])</span>

            <span class="c1"># åå‘ä¼ æ’­</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">mixed_precision</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">scaler</span><span class="o">.</span><span class="n">scale</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

            <span class="n">total_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>

            <span class="c1"># åŠæ—¶æ¸…ç†</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">image_groups</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">empty_cache</span><span class="p">()</span>

        <span class="k">return</span> <span class="n">total_loss</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">image_groups</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">should_reduce_batch_size</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;åŠ¨æ€æ£€æµ‹æ˜¯å¦éœ€è¦å‡å° batch size&quot;&quot;&quot;</span>
        <span class="n">memory_usage</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">memory_allocated</span><span class="p">()</span> <span class="o">/</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">max_memory_allocated</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">memory_usage</span> <span class="o">&gt;</span> <span class="mf">0.9</span>
</code></pre></div>

<h2 id="_2">æœ¬ç« å°ç»“</h2>
<p>æœ¬ç« ç³»ç»Ÿä»‹ç»äº† VLM è®­ç»ƒä¸­ CUDA OOM é—®é¢˜çš„è¯Šæ–­å’Œè§£å†³æ–¹æ³•ã€‚æˆ‘ä»¬å­¦ä¹ äº†ï¼š</p>
<p><strong>æ ¸å¿ƒæ¦‚å¿µ</strong>ï¼š</p>
<ul>
<li>VLM å†…å­˜å ç”¨çš„å››å¤§ç»„æˆï¼šæ¨¡å‹å‚æ•°ã€æ¢¯åº¦ã€æ¿€æ´»å€¼ã€ä¼˜åŒ–å™¨çŠ¶æ€</li>
<li>å†…å­˜è®¡ç®—å…¬å¼ï¼šç²¾ç¡®é¢„ä¼°ä»»æ„é…ç½®çš„å†…å­˜éœ€æ±‚</li>
<li>VLM ç‰¹æœ‰æŒ‘æˆ˜ï¼šè§†è§‰ tokens çˆ†ç‚¸ã€æ³¨æ„åŠ›äºŒæ¬¡å¤æ‚åº¦ã€å¤šåˆ†è¾¨ç‡å¤„ç†</li>
</ul>
<p><strong>å…³é”®æŠ€æœ¯</strong>ï¼š</p>
<ul>
<li><strong>Gradient Checkpointing</strong>ï¼šç”¨è®¡ç®—æ¢å†…å­˜ï¼Œæ¿€æ´»å€¼ä» $O(N)$ é™è‡³ $O(\sqrt{N})$</li>
<li><strong>Flash Attention</strong>ï¼šæ³¨æ„åŠ›å†…å­˜ä» $O(L^2)$ é™è‡³ $O(L)$</li>
<li><strong>åŠ¨æ€æ‰¹å¤„ç†</strong>ï¼šæ ¹æ®æ˜¾å­˜å®æ—¶è°ƒæ•´ batch size</li>
<li><strong>æ··åˆç²¾åº¦è®­ç»ƒ</strong>ï¼šFP16/BF16 èŠ‚çœ 50% å†…å­˜</li>
</ul>
<p><strong>å®ç”¨å·¥å…·</strong>ï¼š</p>
<ul>
<li><code>torch.cuda.memory_summary()</code>ï¼šæ·±åº¦å†…å­˜åˆ†æ</li>
<li><code>nvidia-smi</code> é«˜çº§ç”¨æ³•ï¼šæŒç»­ç›‘æ§å’Œæ—¥å¿—å¯¼å‡º</li>
<li>PyTorch Profilerï¼šå†…å­˜çƒ­ç‚¹å®šä½</li>
<li>è‡ªå®šä¹‰ç›‘æ§ç³»ç»Ÿï¼šå®æ—¶é¢„è­¦å’Œè‡ªåŠ¨è°ƒæ•´</li>
</ul>
<p><strong>VLM ä¼˜åŒ–ç­–ç•¥</strong>ï¼š</p>
<ol>
<li>è§†è§‰ç¼–ç å™¨ï¼šåˆ†æ‰¹å¤„ç†ã€åŠ¨æ€åˆ†è¾¨ç‡</li>
<li>æ³¨æ„åŠ›ä¼˜åŒ–ï¼šFlash/ç¨€ç–/æ»‘åŠ¨çª—å£æ³¨æ„åŠ›</li>
<li>å¤šåˆ†è¾¨ç‡ï¼šåˆ†ç»„æ‰¹å¤„ç†ã€åŠ¨æ€åˆ†å—</li>
<li>äº¤å‰æ³¨æ„åŠ›ï¼šKV å…±äº«ã€é—¨æ§æœºåˆ¶ã€ä½ç§©åˆ†è§£</li>
</ol>
<p>è®°ä½ï¼š<strong>OOM ä¸æ˜¯æ— è§£çš„</strong>ã€‚é€šè¿‡ç³»ç»Ÿçš„åˆ†æå’Œåˆç†çš„ä¼˜åŒ–ï¼Œå³ä½¿åœ¨æœ‰é™çš„ç¡¬ä»¶ä¸Šä¹Ÿèƒ½è®­ç»ƒå¤§è§„æ¨¡ VLMã€‚å…³é”®æ˜¯ç†è§£å†…å­˜åˆ†é…æœºåˆ¶ï¼Œé€‰æ‹©åˆé€‚çš„ä¼˜åŒ–ç­–ç•¥ï¼Œå¹¶å»ºç«‹å®Œå–„çš„ç›‘æ§ä½“ç³»ã€‚</p>
<h2 id="_3">ç»ƒä¹ é¢˜</h2>
<h3 id="_4">åŸºç¡€é¢˜</h3>
<p><strong>ç»ƒä¹  9.1</strong>ï¼šè®¡ç®— LLaVA-1.5-13B åœ¨ä»¥ä¸‹é…ç½®ä¸‹çš„æœ€å°æ˜¾å­˜éœ€æ±‚ï¼š</p>
<ul>
<li>è§†è§‰ç¼–ç å™¨ï¼šCLIP-ViT-L/14ï¼ˆ304M å‚æ•°ï¼‰</li>
<li>è¯­è¨€æ¨¡å‹ï¼šVicuna-13B</li>
<li>ä¼˜åŒ–å™¨ï¼šAdamW</li>
<li>æ‰¹å¤§å°ï¼š1</li>
<li>åºåˆ—é•¿åº¦ï¼š2048</li>
<li>æ··åˆç²¾åº¦ï¼šFP16</li>
</ul>
<p>ğŸ’¡ <strong>æç¤º</strong>ï¼šåˆ†åˆ«è®¡ç®—å‚æ•°ã€æ¢¯åº¦ã€ä¼˜åŒ–å™¨çŠ¶æ€çš„å†…å­˜ï¼Œæ¿€æ´»å€¼å¯æŒ‰ç»éªŒä¼°ç®—ä¸ºå‚æ•°çš„ 2-3 å€ã€‚</p>
<details>
<summary>ğŸ“ å‚è€ƒç­”æ¡ˆ</summary>
<p>å†…å­˜è®¡ç®—ï¼š</p>
<ol>
<li>
<p>å‚æ•°å†…å­˜ï¼ˆFP16ï¼‰ï¼š
   - è§†è§‰ç¼–ç å™¨ï¼š304M Ã— 2 = 0.61 GB
   - è¯­è¨€æ¨¡å‹ï¼š13B Ã— 2 = 26 GB
   - è¿æ¥å±‚ï¼šçº¦ 50M Ã— 2 = 0.1 GB
   - æ€»è®¡ï¼š26.71 GB</p>
</li>
<li>
<p>æ¢¯åº¦å†…å­˜ï¼šç­‰äºå‚æ•°å†…å­˜ = 26.71 GB</p>
</li>
<li>
<p>ä¼˜åŒ–å™¨çŠ¶æ€ï¼ˆAdamWï¼‰ï¼š
   - ä¸€é˜¶åŠ¨é‡ï¼š26.71 GB
   - äºŒé˜¶åŠ¨é‡ï¼š26.71 GB
   - æ€»è®¡ï¼š53.42 GB</p>
</li>
<li>
<p>æ¿€æ´»å€¼ï¼ˆç»éªŒä¼°ç®—ï¼‰ï¼š
   - çº¦å‚æ•°çš„ 2.5 å€ = 66.78 GB</p>
</li>
</ol>
<p>æœ€å°æ˜¾å­˜éœ€æ±‚ï¼š26.71 + 26.71 + 53.42 + 66.78 = <strong>173.62 GB</strong></p>
<p>è¿™å°±æ˜¯ä¸ºä»€ä¹ˆéœ€è¦å¤šå¡è®­ç»ƒæˆ–ä½¿ç”¨å†…å­˜ä¼˜åŒ–æŠ€æœ¯ï¼</p>
</details>
<p><strong>ç»ƒä¹  9.2</strong>ï¼šç»™å®šä¸€ä¸ª OOM é”™è¯¯ä¿¡æ¯ï¼Œè¯†åˆ«é—®é¢˜åŸå› å¹¶æå‡ºè§£å†³æ–¹æ¡ˆï¼š</p>
<div class="codehilite"><pre><span></span><code><span class="n">RuntimeError</span><span class="o">:</span><span class="w"> </span><span class="n">CUDA</span><span class="w"> </span><span class="n">out</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">memory</span><span class="o">.</span><span class="w"> </span><span class="n">Tried</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">allocate</span><span class="w"> </span><span class="mf">2.50</span><span class="w"> </span><span class="n">GiB</span><span class="w"> </span>
<span class="o">(</span><span class="n">GPU</span><span class="w"> </span><span class="mi">0</span><span class="o">;</span><span class="w"> </span><span class="mf">23.69</span><span class="w"> </span><span class="n">GiB</span><span class="w"> </span><span class="n">total</span><span class="w"> </span><span class="n">capacity</span><span class="o">;</span><span class="w"> </span><span class="mf">21.45</span><span class="w"> </span><span class="n">GiB</span><span class="w"> </span><span class="n">already</span><span class="w"> </span><span class="n">allocated</span><span class="o">;</span><span class="w"> </span>
<span class="mf">1.89</span><span class="w"> </span><span class="n">GiB</span><span class="w"> </span><span class="n">free</span><span class="o">;</span><span class="w"> </span><span class="mf">21.50</span><span class="w"> </span><span class="n">GiB</span><span class="w"> </span><span class="n">reserved</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="n">total</span><span class="w"> </span><span class="n">by</span><span class="w"> </span><span class="n">PyTorch</span><span class="o">)</span>
</code></pre></div>

<p>ğŸ’¡ <strong>æç¤º</strong>ï¼šæ³¨æ„ allocated vs reserved çš„å·®å¼‚ï¼Œä»¥åŠè¯·æ±‚åˆ†é…çš„å¤§å°ã€‚</p>
<details>
<summary>ğŸ“ å‚è€ƒç­”æ¡ˆ</summary>
<p>é—®é¢˜åˆ†æï¼š</p>
<ol>
<li>å·²åˆ†é…ï¼š21.45 GBï¼Œå·²é¢„ç•™ï¼š21.50 GB</li>
<li>ç¢ç‰‡ç‡å¾ˆä½ï¼š(21.50 - 21.45) / 21.50 = 0.23%</li>
<li>å‰©ä½™ç©ºé—´ï¼š1.89 GB &lt; 2.50 GBï¼ˆè¯·æ±‚ï¼‰</li>
</ol>
<p>åŸå› ï¼šå†…å­˜å·²åŸºæœ¬ç”¨å°½ï¼Œæ— æ³•æ»¡è¶³æ–°çš„å¤§å—åˆ†é…è¯·æ±‚ï¼ˆå¯èƒ½æ˜¯æ³¨æ„åŠ›çŸ©é˜µï¼‰ã€‚</p>
<p>è§£å†³æ–¹æ¡ˆï¼š</p>
<ol>
<li>
<p>ç«‹å³æªæ–½ï¼š
   - å‡å° batch sizeï¼ˆå¦‚æœ &gt; 1ï¼‰
   - å¯ç”¨ gradient checkpointing
   - è°ƒç”¨ torch.cuda.empty_cache()</p>
</li>
<li>
<p>ä¼˜åŒ–æªæ–½ï¼š
   - ä½¿ç”¨ Flash Attentionï¼ˆ2.50 GB æš—ç¤ºæ˜¯æ³¨æ„åŠ›çŸ©é˜µï¼‰
   - å¯ç”¨æ··åˆç²¾åº¦è®­ç»ƒ
   - è€ƒè™‘æ¨¡å‹å¹¶è¡Œæˆ– CPU offloading</p>
</li>
</ol>
</details>
<p><strong>ç»ƒä¹  9.3</strong>ï¼šç¼–å†™ä»£ç ï¼Œå®ç°ä¸€ä¸ªå‡½æ•°è‡ªåŠ¨æ‰¾åˆ°æœ€å¤§å¯ç”¨ batch sizeï¼š</p>
<p>ğŸ’¡ <strong>æç¤º</strong>ï¼šä½¿ç”¨äºŒåˆ†æœç´¢ï¼Œå¤„ç† OOM å¼‚å¸¸ã€‚</p>
<details>
<summary>ğŸ“ å‚è€ƒç­”æ¡ˆ</summary>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">find_max_batch_size</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">create_batch_fn</span><span class="p">,</span> <span class="n">min_bs</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">max_bs</span><span class="o">=</span><span class="mi">128</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;äºŒåˆ†æœç´¢æ‰¾åˆ°æœ€å¤§ batch size&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">can_run</span><span class="p">(</span><span class="n">batch_size</span><span class="p">):</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">batch</span> <span class="o">=</span> <span class="n">create_batch_fn</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)</span>
            <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">loss</span>
            <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

            <span class="c1"># æ¸…ç†</span>
            <span class="k">del</span> <span class="n">output</span><span class="p">,</span> <span class="n">loss</span><span class="p">,</span> <span class="n">batch</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">empty_cache</span><span class="p">()</span>
            <span class="k">return</span> <span class="kc">True</span>

        <span class="k">except</span> <span class="ne">RuntimeError</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
            <span class="k">if</span> <span class="s2">&quot;out of memory&quot;</span> <span class="ow">in</span> <span class="nb">str</span><span class="p">(</span><span class="n">e</span><span class="p">):</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">empty_cache</span><span class="p">()</span>
                <span class="k">return</span> <span class="kc">False</span>
            <span class="k">raise</span> <span class="n">e</span>

    <span class="c1"># äºŒåˆ†æœç´¢</span>
    <span class="n">left</span><span class="p">,</span> <span class="n">right</span> <span class="o">=</span> <span class="n">min_bs</span><span class="p">,</span> <span class="n">max_bs</span>
    <span class="n">best_bs</span> <span class="o">=</span> <span class="n">min_bs</span>

    <span class="k">while</span> <span class="n">left</span> <span class="o">&lt;=</span> <span class="n">right</span><span class="p">:</span>
        <span class="n">mid</span> <span class="o">=</span> <span class="p">(</span><span class="n">left</span> <span class="o">+</span> <span class="n">right</span><span class="p">)</span> <span class="o">//</span> <span class="mi">2</span>

        <span class="k">if</span> <span class="n">can_run</span><span class="p">(</span><span class="n">mid</span><span class="p">):</span>
            <span class="n">best_bs</span> <span class="o">=</span> <span class="n">mid</span>
            <span class="n">left</span> <span class="o">=</span> <span class="n">mid</span> <span class="o">+</span> <span class="mi">1</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">right</span> <span class="o">=</span> <span class="n">mid</span> <span class="o">-</span> <span class="mi">1</span>

    <span class="c1"># éªŒè¯æœ€ç»ˆç»“æœ</span>
    <span class="k">if</span> <span class="n">can_run</span><span class="p">(</span><span class="n">best_bs</span><span class="p">):</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;æœ€å¤§ batch size: </span><span class="si">{</span><span class="n">best_bs</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

        <span class="c1"># ç•™å‡ºå®‰å…¨è¾¹é™…</span>
        <span class="n">safe_bs</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">best_bs</span> <span class="o">*</span> <span class="mf">0.9</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;æ¨è batch size: </span><span class="si">{</span><span class="n">safe_bs</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">safe_bs</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">best_bs</span> <span class="o">-</span> <span class="mi">1</span>
</code></pre></div>

</details>
<h3 id="_5">æŒ‘æˆ˜é¢˜</h3>
<p><strong>ç»ƒä¹  9.4</strong>ï¼šè®¾è®¡ä¸€ä¸ªè‡ªé€‚åº”å†…å­˜ç®¡ç†ç³»ç»Ÿï¼Œèƒ½å¤Ÿï¼š</p>
<ul>
<li>ç›‘æ§å†…å­˜ä½¿ç”¨è¶‹åŠ¿</li>
<li>é¢„æµ‹ OOM é£é™©</li>
<li>è‡ªåŠ¨è°ƒæ•´è®­ç»ƒå‚æ•°</li>
</ul>
<p>ğŸ’¡ <strong>æç¤º</strong>ï¼šè€ƒè™‘ä½¿ç”¨æ»‘åŠ¨çª—å£å’Œçº¿æ€§å›å½’é¢„æµ‹å†…å­˜å¢é•¿ã€‚</p>
<details>
<summary>ğŸ“ å‚è€ƒç­”æ¡ˆ</summary>
<div class="codehilite"><pre><span></span><code><span class="k">class</span> <span class="nc">AdaptiveMemoryManager</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">window_size</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">oom_threshold</span><span class="o">=</span><span class="mf">0.85</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">window_size</span> <span class="o">=</span> <span class="n">window_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">oom_threshold</span> <span class="o">=</span> <span class="n">oom_threshold</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">memory_history</span> <span class="o">=</span> <span class="n">deque</span><span class="p">(</span><span class="n">maxlen</span><span class="o">=</span><span class="n">window_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">step_history</span> <span class="o">=</span> <span class="n">deque</span><span class="p">(</span><span class="n">maxlen</span><span class="o">=</span><span class="n">window_size</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">step</span><span class="p">):</span>
        <span class="c1"># è®°å½•å½“å‰å†…å­˜</span>
        <span class="n">allocated</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">memory_allocated</span><span class="p">()</span> <span class="o">/</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">max_memory_allocated</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">memory_history</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">allocated</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">step_history</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">step</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">predict_oom_risk</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">future_steps</span><span class="o">=</span><span class="mi">10</span><span class="p">):</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">memory_history</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mi">3</span><span class="p">:</span>
            <span class="k">return</span> <span class="mf">0.0</span>

        <span class="c1"># çº¿æ€§å›å½’é¢„æµ‹</span>
        <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">step_history</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">memory_history</span><span class="p">)</span>

        <span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span>
        <span class="n">model</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span>
        <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

        <span class="c1"># é¢„æµ‹æœªæ¥å†…å­˜ä½¿ç”¨</span>
        <span class="n">future_step</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">step_history</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">future_steps</span>
        <span class="n">predicted_memory</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">([[</span><span class="n">future_step</span><span class="p">]])[</span><span class="mi">0</span><span class="p">]</span>

        <span class="c1"># è®¡ç®— OOM é£é™©</span>
        <span class="k">if</span> <span class="n">predicted_memory</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">oom_threshold</span><span class="p">:</span>
            <span class="n">risk</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="mf">1.0</span><span class="p">,</span> <span class="p">(</span><span class="n">predicted_memory</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">oom_threshold</span><span class="p">)</span> <span class="o">/</span> <span class="mf">0.15</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">risk</span> <span class="o">=</span> <span class="mf">0.0</span>

        <span class="k">return</span> <span class="n">risk</span>

    <span class="k">def</span> <span class="nf">adjust_training_params</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">,</span> <span class="n">risk</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;æ ¹æ®é£é™©è°ƒæ•´å‚æ•°&quot;&quot;&quot;</span>
        <span class="n">adjustments</span> <span class="o">=</span> <span class="p">{}</span>

        <span class="k">if</span> <span class="n">risk</span> <span class="o">&gt;</span> <span class="mf">0.8</span><span class="p">:</span>
            <span class="c1"># é«˜é£é™©ï¼šæ¿€è¿›è°ƒæ•´</span>
            <span class="n">adjustments</span><span class="p">[</span><span class="s1">&#39;batch_size&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">batch_size</span> <span class="o">//</span> <span class="mi">2</span><span class="p">)</span>
            <span class="n">adjustments</span><span class="p">[</span><span class="s1">&#39;gradient_checkpointing&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>
            <span class="n">adjustments</span><span class="p">[</span><span class="s1">&#39;accumulation_steps&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">accumulation_steps</span> <span class="o">*</span> <span class="mi">2</span>

        <span class="k">elif</span> <span class="n">risk</span> <span class="o">&gt;</span> <span class="mf">0.5</span><span class="p">:</span>
            <span class="c1"># ä¸­é£é™©ï¼šæ¸©å’Œè°ƒæ•´</span>
            <span class="n">adjustments</span><span class="p">[</span><span class="s1">&#39;batch_size&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">batch_size</span> <span class="o">*</span> <span class="mf">0.75</span><span class="p">)</span>
            <span class="n">adjustments</span><span class="p">[</span><span class="s1">&#39;gradient_checkpointing&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>

        <span class="k">elif</span> <span class="n">risk</span> <span class="o">&gt;</span> <span class="mf">0.3</span><span class="p">:</span>
            <span class="c1"># ä½é£é™©ï¼šå°å¹…ä¼˜åŒ–</span>
            <span class="n">adjustments</span><span class="p">[</span><span class="s1">&#39;mixed_precision&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>

        <span class="k">return</span> <span class="n">adjustments</span>

    <span class="k">def</span> <span class="nf">emergency_cleanup</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;ç´§æ€¥å†…å­˜æ¸…ç†&quot;&quot;&quot;</span>
        <span class="c1"># 1. æ¸…ç©ºç¼“å­˜</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">empty_cache</span><span class="p">()</span>

        <span class="c1"># 2. åˆ é™¤ä¸å¿…è¦çš„å¼ é‡</span>
        <span class="kn">import</span> <span class="nn">gc</span>
        <span class="k">for</span> <span class="n">obj</span> <span class="ow">in</span> <span class="n">gc</span><span class="o">.</span><span class="n">get_objects</span><span class="p">():</span>
            <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">is_tensor</span><span class="p">(</span><span class="n">obj</span><span class="p">)</span> <span class="ow">and</span> <span class="n">obj</span><span class="o">.</span><span class="n">is_cuda</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">obj</span><span class="o">.</span><span class="n">grad_fn</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>  <span class="c1"># ä¸åœ¨è®¡ç®—å›¾ä¸­</span>
                    <span class="k">del</span> <span class="n">obj</span>

        <span class="c1"># 3. åŒæ­¥å¹¶å†æ¬¡æ¸…ç†</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">synchronize</span><span class="p">()</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">empty_cache</span><span class="p">()</span>
        <span class="n">gc</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
</code></pre></div>

<p>ä½¿ç”¨è¯¥ç³»ç»Ÿå¯ä»¥é¢„é˜² OOMï¼Œè€Œä¸æ˜¯ç­‰åˆ°å‘ç”Ÿåå†å¤„ç†ã€‚</p>
</details>
<p><strong>ç»ƒä¹  9.5</strong>ï¼šåˆ†æå¹¶ä¼˜åŒ–ä»¥ä¸‹ VLM å‰å‘ä¼ æ’­ä»£ç çš„å†…å­˜ä½¿ç”¨ï¼š</p>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">images</span><span class="p">,</span> <span class="n">text_ids</span><span class="p">):</span>
    <span class="c1"># è§†è§‰ç¼–ç </span>
    <span class="n">B</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">W</span> <span class="o">=</span> <span class="n">images</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">all_features</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">B</span><span class="p">):</span>
        <span class="n">img_features</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">N</span><span class="p">):</span>
            <span class="n">feat</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">vision_encoder</span><span class="p">(</span><span class="n">images</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">])</span>
            <span class="n">img_features</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">feat</span><span class="p">)</span>
        <span class="n">all_features</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">img_features</span><span class="p">))</span>
    <span class="n">vision_features</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">all_features</span><span class="p">)</span>

    <span class="c1"># æ–‡æœ¬åµŒå…¥</span>
    <span class="n">text_embeds</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">text_embedder</span><span class="p">(</span><span class="n">text_ids</span><span class="p">)</span>

    <span class="c1"># äº¤å‰æ³¨æ„åŠ›</span>
    <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">cross_attention_layers</span><span class="p">:</span>
        <span class="n">text_embeds</span> <span class="o">=</span> <span class="n">layer</span><span class="p">(</span><span class="n">text_embeds</span><span class="p">,</span> <span class="n">vision_features</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">text_embeds</span>
</code></pre></div>

<p>ğŸ’¡ <strong>æç¤º</strong>ï¼šè€ƒè™‘å‘é‡åŒ–ã€å†…å­˜å¤ç”¨ã€æ¢¯åº¦æ£€æŸ¥ç‚¹ã€‚</p>
<details>
<summary>ğŸ“ å‚è€ƒç­”æ¡ˆ</summary>
<p>ä¼˜åŒ–åçš„ä»£ç ï¼š</p>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">images</span><span class="p">,</span> <span class="n">text_ids</span><span class="p">):</span>
    <span class="n">B</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">W</span> <span class="o">=</span> <span class="n">images</span><span class="o">.</span><span class="n">shape</span>

    <span class="c1"># ä¼˜åŒ– 1ï¼šå‘é‡åŒ–å¤„ç†ï¼Œé¿å…å¾ªç¯</span>
    <span class="n">images_flat</span> <span class="o">=</span> <span class="n">images</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">B</span> <span class="o">*</span> <span class="n">N</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">W</span><span class="p">)</span>

    <span class="c1"># ä¼˜åŒ– 2ï¼šä½¿ç”¨ checkpoint å‡å°‘æ¿€æ´»å€¼å†…å­˜</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">training</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_checkpointing</span><span class="p">:</span>
        <span class="n">vision_features</span> <span class="o">=</span> <span class="n">checkpoint</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">vision_encoder</span><span class="p">,</span> <span class="n">images_flat</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">vision_features</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">vision_encoder</span><span class="p">(</span><span class="n">images_flat</span><span class="p">)</span>

    <span class="c1"># ä¼˜åŒ– 3ï¼šåŸåœ° reshapeï¼Œé¿å…é¢å¤–å†…å­˜åˆ†é…</span>
    <span class="n">vision_features</span> <span class="o">=</span> <span class="n">vision_features</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">vision_features</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>

    <span class="c1"># ä¼˜åŒ– 4ï¼šå¦‚æœ N å¾ˆå¤§ï¼Œè€ƒè™‘åˆ†å—å¤„ç†</span>
    <span class="k">if</span> <span class="n">N</span> <span class="o">&gt;</span> <span class="mi">4</span><span class="p">:</span>
        <span class="n">chunk_size</span> <span class="o">=</span> <span class="mi">2</span>
        <span class="n">chunks</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">chunk_size</span><span class="p">):</span>
            <span class="n">chunk</span> <span class="o">=</span> <span class="n">vision_features</span><span class="p">[:,</span> <span class="n">i</span><span class="p">:</span><span class="n">i</span><span class="o">+</span><span class="n">chunk_size</span><span class="p">]</span>
            <span class="n">chunks</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">chunk</span><span class="p">)</span>

            <span class="c1"># ä¼˜åŒ– 5ï¼šåŠæ—¶é‡Šæ”¾ä¸­é—´ç»“æœ</span>
            <span class="k">if</span> <span class="n">i</span> <span class="o">+</span> <span class="n">chunk_size</span> <span class="o">&lt;</span> <span class="n">N</span><span class="p">:</span>
                <span class="k">del</span> <span class="n">chunk</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">empty_cache</span><span class="p">()</span>

        <span class="n">vision_features</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">chunks</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="c1"># æ–‡æœ¬åµŒå…¥</span>
    <span class="n">text_embeds</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">text_embedder</span><span class="p">(</span><span class="n">text_ids</span><span class="p">)</span>

    <span class="c1"># ä¼˜åŒ– 6ï¼šé‡ç”¨ vision_features çš„ KV cache</span>
    <span class="n">vision_k</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">vision_proj_k</span><span class="p">(</span><span class="n">vision_features</span><span class="p">)</span>
    <span class="n">vision_v</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">vision_proj_v</span><span class="p">(</span><span class="n">vision_features</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">cross_attention_layers</span><span class="p">:</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">training</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_checkpointing</span><span class="p">:</span>
            <span class="n">text_embeds</span> <span class="o">=</span> <span class="n">checkpoint</span><span class="p">(</span>
                <span class="n">layer</span><span class="p">,</span> <span class="n">text_embeds</span><span class="p">,</span> <span class="n">vision_k</span><span class="p">,</span> <span class="n">vision_v</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">text_embeds</span> <span class="o">=</span> <span class="n">layer</span><span class="p">(</span><span class="n">text_embeds</span><span class="p">,</span> <span class="n">vision_k</span><span class="p">,</span> <span class="n">vision_v</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">text_embeds</span>
</code></pre></div>

<p>å†…å­˜èŠ‚çœï¼š</p>
<ol>
<li>å‘é‡åŒ–ï¼šå‡å°‘ä¸´æ—¶å¼ é‡åˆ›å»º</li>
<li>Checkpointingï¼šæ¿€æ´»å€¼å†…å­˜é™ä½ 50-70%</li>
<li>KV å¤ç”¨ï¼šé¿å…æ¯å±‚é‡å¤è®¡ç®—</li>
<li>åˆ†å—å¤„ç†ï¼šå³°å€¼å†…å­˜é™ä½</li>
<li>åŠæ—¶æ¸…ç†ï¼šé¿å…å†…å­˜ç´¯ç§¯</li>
</ol>
</details>
<p><strong>ç»ƒä¹  9.6</strong>ï¼šè®¾è®¡å®éªŒæ¯”è¾ƒä¸åŒæ³¨æ„åŠ›å®ç°çš„å†…å­˜-é€Ÿåº¦æƒè¡¡ï¼š</p>
<ul>
<li>æ ‡å‡†æ³¨æ„åŠ›</li>
<li>Flash Attention</li>
<li>ç¨€ç–æ³¨æ„åŠ›</li>
<li>æ»‘åŠ¨çª—å£æ³¨æ„åŠ›</li>
</ul>
<p>ğŸ’¡ <strong>æç¤º</strong>ï¼šå›ºå®šåºåˆ—é•¿åº¦ï¼Œæµ‹é‡å†…å­˜å ç”¨å’Œæ¨ç†æ—¶é—´ã€‚</p>
<details>
<summary>ğŸ“ å‚è€ƒç­”æ¡ˆ</summary>
<div class="codehilite"><pre><span></span><code><span class="kn">import</span> <span class="nn">time</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="k">def</span> <span class="nf">benchmark_attention_methods</span><span class="p">():</span>
    <span class="n">seq_lengths</span> <span class="o">=</span> <span class="p">[</span><span class="mi">512</span><span class="p">,</span> <span class="mi">1024</span><span class="p">,</span> <span class="mi">2048</span><span class="p">,</span> <span class="mi">4096</span><span class="p">]</span>
    <span class="n">batch_size</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="n">hidden_dim</span> <span class="o">=</span> <span class="mi">1024</span>
    <span class="n">num_heads</span> <span class="o">=</span> <span class="mi">16</span>

    <span class="n">results</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s1">&#39;standard&#39;</span><span class="p">:</span> <span class="p">{</span><span class="s1">&#39;memory&#39;</span><span class="p">:</span> <span class="p">[],</span> <span class="s1">&#39;time&#39;</span><span class="p">:</span> <span class="p">[]},</span>
        <span class="s1">&#39;flash&#39;</span><span class="p">:</span> <span class="p">{</span><span class="s1">&#39;memory&#39;</span><span class="p">:</span> <span class="p">[],</span> <span class="s1">&#39;time&#39;</span><span class="p">:</span> <span class="p">[]},</span>
        <span class="s1">&#39;sparse&#39;</span><span class="p">:</span> <span class="p">{</span><span class="s1">&#39;memory&#39;</span><span class="p">:</span> <span class="p">[],</span> <span class="s1">&#39;time&#39;</span><span class="p">:</span> <span class="p">[]},</span>
        <span class="s1">&#39;sliding&#39;</span><span class="p">:</span> <span class="p">{</span><span class="s1">&#39;memory&#39;</span><span class="p">:</span> <span class="p">[],</span> <span class="s1">&#39;time&#39;</span><span class="p">:</span> <span class="p">[]}</span>
    <span class="p">}</span>

    <span class="k">for</span> <span class="n">seq_len</span> <span class="ow">in</span> <span class="n">seq_lengths</span><span class="p">:</span>
        <span class="c1"># å‡†å¤‡è¾“å…¥</span>
        <span class="n">q</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">hidden_dim</span> <span class="o">//</span> <span class="n">num_heads</span><span class="p">)</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>
        <span class="n">k</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn_like</span><span class="p">(</span><span class="n">q</span><span class="p">)</span>
        <span class="n">v</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn_like</span><span class="p">(</span><span class="n">q</span><span class="p">)</span>

        <span class="c1"># 1. æ ‡å‡†æ³¨æ„åŠ›</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">reset_peak_memory_stats</span><span class="p">()</span>
        <span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>

        <span class="n">scores</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">q</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>
        <span class="n">attn</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">attn</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>

        <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">synchronize</span><span class="p">()</span>
        <span class="n">standard_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">start</span>
        <span class="n">standard_memory</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">max_memory_allocated</span><span class="p">()</span> <span class="o">/</span> <span class="mi">1024</span><span class="o">**</span><span class="mi">3</span>

        <span class="n">results</span><span class="p">[</span><span class="s1">&#39;standard&#39;</span><span class="p">][</span><span class="s1">&#39;time&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">standard_time</span><span class="p">)</span>
        <span class="n">results</span><span class="p">[</span><span class="s1">&#39;standard&#39;</span><span class="p">][</span><span class="s1">&#39;memory&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">standard_memory</span><span class="p">)</span>

        <span class="c1"># 2. Flash Attentionï¼ˆæ¨¡æ‹Ÿï¼‰</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">reset_peak_memory_stats</span><span class="p">()</span>
        <span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>

        <span class="c1"># Flash attention å†…å­˜å¤æ‚åº¦ O(seq_len) è€Œé O(seq_len^2)</span>
        <span class="c1"># è¿™é‡Œç®€åŒ–æ¨¡æ‹Ÿ</span>
        <span class="n">chunk_size</span> <span class="o">=</span> <span class="mi">64</span>
        <span class="n">out_flash</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">chunk_size</span><span class="p">):</span>
            <span class="n">q_chunk</span> <span class="o">=</span> <span class="n">q</span><span class="p">[:,</span> <span class="p">:,</span> <span class="n">i</span><span class="p">:</span><span class="n">i</span><span class="o">+</span><span class="n">chunk_size</span><span class="p">]</span>
            <span class="n">scores_chunk</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">q_chunk</span><span class="p">,</span> <span class="n">k</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">q</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>
            <span class="n">attn_chunk</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">scores_chunk</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">out_chunk</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">attn_chunk</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>
            <span class="n">out_flash</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">out_chunk</span><span class="p">)</span>

        <span class="n">out_flash</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">out_flash</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

        <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">synchronize</span><span class="p">()</span>
        <span class="n">flash_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">start</span>
        <span class="n">flash_memory</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">max_memory_allocated</span><span class="p">()</span> <span class="o">/</span> <span class="mi">1024</span><span class="o">**</span><span class="mi">3</span>

        <span class="n">results</span><span class="p">[</span><span class="s1">&#39;flash&#39;</span><span class="p">][</span><span class="s1">&#39;time&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">flash_time</span><span class="p">)</span>
        <span class="n">results</span><span class="p">[</span><span class="s1">&#39;flash&#39;</span><span class="p">][</span><span class="s1">&#39;memory&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">flash_memory</span><span class="p">)</span>

        <span class="c1"># 3. ç¨€ç–æ³¨æ„åŠ›</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">reset_peak_memory_stats</span><span class="p">()</span>
        <span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>

        <span class="n">scores</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">q</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>
        <span class="c1"># åªä¿ç•™ top 10%</span>
        <span class="n">k_sparse</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="mf">0.1</span> <span class="o">*</span> <span class="n">scores</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
        <span class="n">topk_scores</span><span class="p">,</span> <span class="n">topk_indices</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">topk</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">k_sparse</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">sparse_scores</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">scores</span><span class="p">)</span>
        <span class="n">sparse_scores</span><span class="o">.</span><span class="n">scatter_</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">topk_indices</span><span class="p">,</span> <span class="n">topk_scores</span><span class="p">)</span>
        <span class="n">attn</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">sparse_scores</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">attn</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>

        <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">synchronize</span><span class="p">()</span>
        <span class="n">sparse_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">start</span>
        <span class="n">sparse_memory</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">max_memory_allocated</span><span class="p">()</span> <span class="o">/</span> <span class="mi">1024</span><span class="o">**</span><span class="mi">3</span>

        <span class="n">results</span><span class="p">[</span><span class="s1">&#39;sparse&#39;</span><span class="p">][</span><span class="s1">&#39;time&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">sparse_time</span><span class="p">)</span>
        <span class="n">results</span><span class="p">[</span><span class="s1">&#39;sparse&#39;</span><span class="p">][</span><span class="s1">&#39;memory&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">sparse_memory</span><span class="p">)</span>

        <span class="c1"># 4. æ»‘åŠ¨çª—å£</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">reset_peak_memory_stats</span><span class="p">()</span>
        <span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>

        <span class="n">window_size</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">)</span>
        <span class="n">out_sliding</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">window_size</span> <span class="o">//</span> <span class="mi">2</span><span class="p">):</span>
            <span class="n">start_idx</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">i</span> <span class="o">-</span> <span class="n">window_size</span> <span class="o">//</span> <span class="mi">2</span><span class="p">)</span>
            <span class="n">end_idx</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">seq_len</span><span class="p">,</span> <span class="n">i</span> <span class="o">+</span> <span class="n">window_size</span><span class="p">)</span>

            <span class="n">q_window</span> <span class="o">=</span> <span class="n">q</span><span class="p">[:,</span> <span class="p">:,</span> <span class="n">start_idx</span><span class="p">:</span><span class="n">end_idx</span><span class="p">]</span>
            <span class="n">k_window</span> <span class="o">=</span> <span class="n">k</span><span class="p">[:,</span> <span class="p">:,</span> <span class="n">start_idx</span><span class="p">:</span><span class="n">end_idx</span><span class="p">]</span>
            <span class="n">v_window</span> <span class="o">=</span> <span class="n">v</span><span class="p">[:,</span> <span class="p">:,</span> <span class="n">start_idx</span><span class="p">:</span><span class="n">end_idx</span><span class="p">]</span>

            <span class="n">scores_window</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">q_window</span><span class="p">,</span> <span class="n">k_window</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">q</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>
            <span class="n">attn_window</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">scores_window</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">out_window</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">attn_window</span><span class="p">,</span> <span class="n">v_window</span><span class="p">)</span>
            <span class="n">out_sliding</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">out_window</span><span class="p">)</span>

        <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">synchronize</span><span class="p">()</span>
        <span class="n">sliding_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">start</span>
        <span class="n">sliding_memory</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">max_memory_allocated</span><span class="p">()</span> <span class="o">/</span> <span class="mi">1024</span><span class="o">**</span><span class="mi">3</span>

        <span class="n">results</span><span class="p">[</span><span class="s1">&#39;sliding&#39;</span><span class="p">][</span><span class="s1">&#39;time&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">sliding_time</span><span class="p">)</span>
        <span class="n">results</span><span class="p">[</span><span class="s1">&#39;sliding&#39;</span><span class="p">][</span><span class="s1">&#39;memory&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">sliding_memory</span><span class="p">)</span>

        <span class="c1"># æ¸…ç†</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">empty_cache</span><span class="p">()</span>

    <span class="c1"># å¯è§†åŒ–ç»“æœ</span>
    <span class="n">fig</span><span class="p">,</span> <span class="p">(</span><span class="n">ax1</span><span class="p">,</span> <span class="n">ax2</span><span class="p">)</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>

    <span class="k">for</span> <span class="n">method</span> <span class="ow">in</span> <span class="n">results</span><span class="p">:</span>
        <span class="n">ax1</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">seq_lengths</span><span class="p">,</span> <span class="n">results</span><span class="p">[</span><span class="n">method</span><span class="p">][</span><span class="s1">&#39;memory&#39;</span><span class="p">],</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="n">method</span><span class="p">)</span>
        <span class="n">ax2</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">seq_lengths</span><span class="p">,</span> <span class="n">results</span><span class="p">[</span><span class="n">method</span><span class="p">][</span><span class="s1">&#39;time&#39;</span><span class="p">],</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;s&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="n">method</span><span class="p">)</span>

    <span class="n">ax1</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;åºåˆ—é•¿åº¦&#39;</span><span class="p">)</span>
    <span class="n">ax1</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;å³°å€¼å†…å­˜ (GB)&#39;</span><span class="p">)</span>
    <span class="n">ax1</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;å†…å­˜å ç”¨å¯¹æ¯”&#39;</span><span class="p">)</span>
    <span class="n">ax1</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
    <span class="n">ax1</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>

    <span class="n">ax2</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;åºåˆ—é•¿åº¦&#39;</span><span class="p">)</span>
    <span class="n">ax2</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;æ¨ç†æ—¶é—´ (ç§’)&#39;</span><span class="p">)</span>
    <span class="n">ax2</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;é€Ÿåº¦å¯¹æ¯”&#39;</span><span class="p">)</span>
    <span class="n">ax2</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
    <span class="n">ax2</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

    <span class="k">return</span> <span class="n">results</span>

<span class="c1"># è¿è¡ŒåŸºå‡†æµ‹è¯•</span>
<span class="n">results</span> <span class="o">=</span> <span class="n">benchmark_attention_methods</span><span class="p">()</span>

<span class="c1"># åˆ†ææƒè¡¡</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">=== å†…å­˜-é€Ÿåº¦æƒè¡¡åˆ†æ ===&quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">seq_len_idx</span><span class="p">,</span> <span class="n">seq_len</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">([</span><span class="mi">512</span><span class="p">,</span> <span class="mi">1024</span><span class="p">,</span> <span class="mi">2048</span><span class="p">,</span> <span class="mi">4096</span><span class="p">]):</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">åºåˆ—é•¿åº¦ </span><span class="si">{</span><span class="n">seq_len</span><span class="si">}</span><span class="s2">:&quot;</span><span class="p">)</span>
    <span class="n">base_memory</span> <span class="o">=</span> <span class="n">results</span><span class="p">[</span><span class="s1">&#39;standard&#39;</span><span class="p">][</span><span class="s1">&#39;memory&#39;</span><span class="p">][</span><span class="n">seq_len_idx</span><span class="p">]</span>
    <span class="n">base_time</span> <span class="o">=</span> <span class="n">results</span><span class="p">[</span><span class="s1">&#39;standard&#39;</span><span class="p">][</span><span class="s1">&#39;time&#39;</span><span class="p">][</span><span class="n">seq_len_idx</span><span class="p">]</span>

    <span class="k">for</span> <span class="n">method</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;flash&#39;</span><span class="p">,</span> <span class="s1">&#39;sparse&#39;</span><span class="p">,</span> <span class="s1">&#39;sliding&#39;</span><span class="p">]:</span>
        <span class="n">memory_save</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">results</span><span class="p">[</span><span class="n">method</span><span class="p">][</span><span class="s1">&#39;memory&#39;</span><span class="p">][</span><span class="n">seq_len_idx</span><span class="p">]</span> <span class="o">/</span> <span class="n">base_memory</span><span class="p">)</span> <span class="o">*</span> <span class="mi">100</span>
        <span class="n">speed_diff</span> <span class="o">=</span> <span class="p">(</span><span class="n">results</span><span class="p">[</span><span class="n">method</span><span class="p">][</span><span class="s1">&#39;time&#39;</span><span class="p">][</span><span class="n">seq_len_idx</span><span class="p">]</span> <span class="o">/</span> <span class="n">base_time</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="mi">100</span>

        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  </span><span class="si">{</span><span class="n">method</span><span class="si">}</span><span class="s2">: å†…å­˜èŠ‚çœ </span><span class="si">{</span><span class="n">memory_save</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2">%, é€Ÿåº¦å˜åŒ– </span><span class="si">{</span><span class="n">speed_diff</span><span class="si">:</span><span class="s2">+.1f</span><span class="si">}</span><span class="s2">%&quot;</span><span class="p">)</span>
</code></pre></div>

<p>å…¸å‹ç»“æœï¼š</p>
<ul>
<li>Flash Attentionï¼šå†…å­˜èŠ‚çœ 60-80%ï¼Œé€Ÿåº¦æå‡ 10-30%</li>
<li>ç¨€ç–æ³¨æ„åŠ›ï¼šå†…å­˜èŠ‚çœ 30-50%ï¼Œé€Ÿåº¦ç•¥æœ‰ä¸‹é™</li>
<li>æ»‘åŠ¨çª—å£ï¼šå†…å­˜èŠ‚çœ 70-90%ï¼Œé€Ÿåº¦ä¸‹é™ 20-40%</li>
</ul>
<p>é€‰æ‹©å»ºè®®ï¼š</p>
<ul>
<li>é•¿åºåˆ—ï¼ˆ&gt;2048ï¼‰ï¼šFlash Attention æœ€ä¼˜</li>
<li>å†…å­˜æåº¦å—é™ï¼šæ»‘åŠ¨çª—å£</li>
<li>éœ€è¦å…¨å±€ä¿¡æ¯ï¼šç¨€ç–æ³¨æ„åŠ›</li>
</ul>
</details>
<p><strong>ç»ƒä¹  9.7</strong>ï¼šå®ç°ä¸€ä¸ª VLM ä¸“ç”¨çš„å†…å­˜é¢„ç®—åˆ†é…å™¨ï¼Œç»™å®šæ€»æ˜¾å­˜é¢„ç®—ï¼Œè‡ªåŠ¨åˆ†é…ç»™ä¸åŒç»„ä»¶ã€‚</p>
<p>ğŸ’¡ <strong>æç¤º</strong>ï¼šè€ƒè™‘ç»„ä»¶ä¼˜å…ˆçº§ã€æœ€å°éœ€æ±‚ã€æ€§èƒ½å½±å“ã€‚</p>
<details>
<summary>ğŸ“ å‚è€ƒç­”æ¡ˆ</summary>
<div class="codehilite"><pre><span></span><code><span class="k">class</span> <span class="nc">VLMMemoryBudgetAllocator</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">total_memory_gb</span><span class="p">,</span> <span class="n">model_config</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">total_memory</span> <span class="o">=</span> <span class="n">total_memory_gb</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">config</span> <span class="o">=</span> <span class="n">model_config</span>

        <span class="c1"># ç»„ä»¶ä¼˜å…ˆçº§ï¼ˆè¶Šå°è¶Šé‡è¦ï¼‰</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">priorities</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s1">&#39;model_params&#39;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
            <span class="s1">&#39;gradients&#39;</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span>
            <span class="s1">&#39;optimizer&#39;</span><span class="p">:</span> <span class="mi">3</span><span class="p">,</span>
            <span class="s1">&#39;vision_features&#39;</span><span class="p">:</span> <span class="mi">4</span><span class="p">,</span>
            <span class="s1">&#39;attention&#39;</span><span class="p">:</span> <span class="mi">5</span><span class="p">,</span>
            <span class="s1">&#39;activations&#39;</span><span class="p">:</span> <span class="mi">6</span>
        <span class="p">}</span>

    <span class="k">def</span> <span class="nf">calculate_minimum_requirements</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;è®¡ç®—å„ç»„ä»¶æœ€å°å†…å­˜éœ€æ±‚&quot;&quot;&quot;</span>
        <span class="n">min_req</span> <span class="o">=</span> <span class="p">{}</span>

        <span class="c1"># æ¨¡å‹å‚æ•°ï¼ˆå¿…é¡»ï¼‰</span>
        <span class="n">param_size</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">num_params</span> <span class="o">*</span> <span class="mi">2</span> <span class="o">/</span> <span class="mi">1024</span><span class="o">**</span><span class="mi">3</span>  <span class="c1"># FP16</span>
        <span class="n">min_req</span><span class="p">[</span><span class="s1">&#39;model_params&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">param_size</span>

        <span class="c1"># æ¢¯åº¦ï¼ˆå¦‚æœè®­ç»ƒï¼‰</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">training</span><span class="p">:</span>
            <span class="n">min_req</span><span class="p">[</span><span class="s1">&#39;gradients&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">param_size</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">trainable_ratio</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">min_req</span><span class="p">[</span><span class="s1">&#39;gradients&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="c1"># ä¼˜åŒ–å™¨</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">optimizer</span> <span class="o">==</span> <span class="s1">&#39;adam&#39;</span><span class="p">:</span>
            <span class="n">min_req</span><span class="p">[</span><span class="s1">&#39;optimizer&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">min_req</span><span class="p">[</span><span class="s1">&#39;gradients&#39;</span><span class="p">]</span> <span class="o">*</span> <span class="mi">2</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">optimizer</span> <span class="o">==</span> <span class="s1">&#39;sgd&#39;</span><span class="p">:</span>
            <span class="n">min_req</span><span class="p">[</span><span class="s1">&#39;optimizer&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">min_req</span><span class="p">[</span><span class="s1">&#39;gradients&#39;</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">min_req</span><span class="p">[</span><span class="s1">&#39;optimizer&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="c1"># è§†è§‰ç‰¹å¾ï¼ˆæœ€å° batch=1ï¼‰</span>
        <span class="n">vision_tokens</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">image_size</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">patch_size</span> <span class="o">**</span> <span class="mi">2</span>
        <span class="n">min_req</span><span class="p">[</span><span class="s1">&#39;vision_features&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">vision_tokens</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_dim</span> <span class="o">*</span> <span class="mi">4</span> <span class="o">/</span> <span class="mi">1024</span><span class="o">**</span><span class="mi">3</span>
        <span class="p">)</span>

        <span class="c1"># æ³¨æ„åŠ›ï¼ˆå¯ä»¥ç”¨ Flash Attention å‹ç¼©ï¼‰</span>
        <span class="n">min_seq_len</span> <span class="o">=</span> <span class="mi">512</span>  <span class="c1"># æœ€å°åºåˆ—é•¿åº¦</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">use_flash_attention</span><span class="p">:</span>
            <span class="n">min_req</span><span class="p">[</span><span class="s1">&#39;attention&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">min_seq_len</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_dim</span> <span class="o">*</span> <span class="mi">4</span> <span class="o">/</span> <span class="mi">1024</span><span class="o">**</span><span class="mi">3</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">min_req</span><span class="p">[</span><span class="s1">&#39;attention&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">min_seq_len</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">*</span> <span class="mi">4</span> <span class="o">/</span> <span class="mi">1024</span><span class="o">**</span><span class="mi">3</span>

        <span class="c1"># æ¿€æ´»å€¼ï¼ˆå¯ä»¥ç”¨ checkpoint å‹ç¼©ï¼‰</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">gradient_checkpointing</span><span class="p">:</span>
            <span class="n">min_req</span><span class="p">[</span><span class="s1">&#39;activations&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">param_size</span> <span class="o">*</span> <span class="mf">0.5</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">min_req</span><span class="p">[</span><span class="s1">&#39;activations&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">param_size</span> <span class="o">*</span> <span class="mi">2</span>

        <span class="k">return</span> <span class="n">min_req</span>

    <span class="k">def</span> <span class="nf">allocate_budget</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;åˆ†é…å†…å­˜é¢„ç®—&quot;&quot;&quot;</span>
        <span class="n">min_requirements</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">calculate_minimum_requirements</span><span class="p">()</span>
        <span class="n">total_min</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">min_requirements</span><span class="o">.</span><span class="n">values</span><span class="p">())</span>

        <span class="k">if</span> <span class="n">total_min</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">total_memory</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">emergency_allocation</span><span class="p">(</span><span class="n">min_requirements</span><span class="p">)</span>

        <span class="c1"># å‰©ä½™é¢„ç®—</span>
        <span class="n">remaining</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">total_memory</span> <span class="o">-</span> <span class="n">total_min</span>

        <span class="c1"># åˆå§‹åˆ†é…ï¼ˆæ»¡è¶³æœ€å°éœ€æ±‚ï¼‰</span>
        <span class="n">allocation</span> <span class="o">=</span> <span class="n">min_requirements</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>

        <span class="c1"># æŒ‰ä¼˜å…ˆçº§åˆ†é…å‰©ä½™å†…å­˜</span>
        <span class="n">sorted_components</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span>
            <span class="n">min_requirements</span><span class="o">.</span><span class="n">keys</span><span class="p">(),</span> 
            <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">priorities</span><span class="p">[</span><span class="n">x</span><span class="p">]</span>
        <span class="p">)</span>

        <span class="c1"># è®¡ç®—æƒé‡</span>
        <span class="n">weights</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="n">total_weight</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">comp</span> <span class="ow">in</span> <span class="n">sorted_components</span><span class="p">:</span>
            <span class="n">weight</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">priorities</span><span class="p">[</span><span class="n">comp</span><span class="p">]</span>
            <span class="n">weights</span><span class="p">[</span><span class="n">comp</span><span class="p">]</span> <span class="o">=</span> <span class="n">weight</span>
            <span class="n">total_weight</span> <span class="o">+=</span> <span class="n">weight</span>

        <span class="c1"># åˆ†é…å‰©ä½™å†…å­˜</span>
        <span class="k">for</span> <span class="n">comp</span> <span class="ow">in</span> <span class="n">sorted_components</span><span class="p">:</span>
            <span class="n">extra</span> <span class="o">=</span> <span class="n">remaining</span> <span class="o">*</span> <span class="p">(</span><span class="n">weights</span><span class="p">[</span><span class="n">comp</span><span class="p">]</span> <span class="o">/</span> <span class="n">total_weight</span><span class="p">)</span>
            <span class="n">allocation</span><span class="p">[</span><span class="n">comp</span><span class="p">]</span> <span class="o">+=</span> <span class="n">extra</span>

        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimize_allocation</span><span class="p">(</span><span class="n">allocation</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">emergency_allocation</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">min_requirements</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;ç´§æ€¥æ¨¡å¼ï¼šå†…å­˜ä¸è¶³æ—¶çš„åˆ†é…ç­–ç•¥&quot;&quot;&quot;</span>
        <span class="n">allocation</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="n">available</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">total_memory</span>

        <span class="c1"># 1. å¿…é¡»æ»¡è¶³æ¨¡å‹å‚æ•°</span>
        <span class="n">allocation</span><span class="p">[</span><span class="s1">&#39;model_params&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">min_requirements</span><span class="p">[</span><span class="s1">&#39;model_params&#39;</span><span class="p">]</span>
        <span class="n">available</span> <span class="o">-=</span> <span class="n">allocation</span><span class="p">[</span><span class="s1">&#39;model_params&#39;</span><span class="p">]</span>

        <span class="k">if</span> <span class="n">available</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">MemoryError</span><span class="p">(</span><span class="s2">&quot;æ˜¾å­˜ä¸è¶³ä»¥åŠ è½½æ¨¡å‹ï¼&quot;</span><span class="p">)</span>

        <span class="c1"># 2. å¯ç”¨æ‰€æœ‰å†…å­˜ä¼˜åŒ–</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">gradient_checkpointing</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">use_flash_attention</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">mixed_precision</span> <span class="o">=</span> <span class="kc">True</span>

        <span class="c1"># 3. æœ€å°åŒ–å…¶ä»–ç»„ä»¶</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">training</span><span class="p">:</span>
            <span class="c1"># ä½¿ç”¨ Adafactor æˆ– 8-bit Adam</span>
            <span class="n">allocation</span><span class="p">[</span><span class="s1">&#39;optimizer&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">min_requirements</span><span class="p">[</span><span class="s1">&#39;gradients&#39;</span><span class="p">]</span> <span class="o">*</span> <span class="mf">0.5</span>
            <span class="n">allocation</span><span class="p">[</span><span class="s1">&#39;gradients&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">min_requirements</span><span class="p">[</span><span class="s1">&#39;gradients&#39;</span><span class="p">]</span>
            <span class="n">available</span> <span class="o">-=</span> <span class="n">allocation</span><span class="p">[</span><span class="s1">&#39;optimizer&#39;</span><span class="p">]</span> <span class="o">+</span> <span class="n">allocation</span><span class="p">[</span><span class="s1">&#39;gradients&#39;</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">allocation</span><span class="p">[</span><span class="s1">&#39;optimizer&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="n">allocation</span><span class="p">[</span><span class="s1">&#39;gradients&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="c1"># 4. æé™å‹ç¼©æ¿€æ´»å€¼</span>
        <span class="n">allocation</span><span class="p">[</span><span class="s1">&#39;activations&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">available</span> <span class="o">*</span> <span class="mf">0.3</span><span class="p">,</span> <span class="n">min_requirements</span><span class="p">[</span><span class="s1">&#39;model_params&#39;</span><span class="p">]</span> <span class="o">*</span> <span class="mf">0.3</span><span class="p">)</span>
        <span class="n">available</span> <span class="o">-=</span> <span class="n">allocation</span><span class="p">[</span><span class="s1">&#39;activations&#39;</span><span class="p">]</span>

        <span class="c1"># 5. åˆ†é…å‰©ä½™ç»™è§†è§‰å’Œæ³¨æ„åŠ›</span>
        <span class="n">allocation</span><span class="p">[</span><span class="s1">&#39;vision_features&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">available</span> <span class="o">*</span> <span class="mf">0.4</span>
        <span class="n">allocation</span><span class="p">[</span><span class="s1">&#39;attention&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">available</span> <span class="o">*</span> <span class="mf">0.6</span>

        <span class="k">return</span> <span class="n">allocation</span>

    <span class="k">def</span> <span class="nf">optimize_allocation</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">allocation</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;ä¼˜åŒ–åˆ†é…ä»¥æœ€å¤§åŒ–æ€§èƒ½&quot;&quot;&quot;</span>
        <span class="n">optimized</span> <span class="o">=</span> <span class="n">allocation</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>

        <span class="c1"># è§„åˆ™ 1ï¼šå¦‚æœ batch size å¯ä»¥ç¿»å€ï¼Œé‡æ–°åˆ†é…</span>
        <span class="n">vision_memory</span> <span class="o">=</span> <span class="n">allocation</span><span class="p">[</span><span class="s1">&#39;vision_features&#39;</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">vision_memory</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">min_vision_memory</span> <span class="o">*</span> <span class="mi">2</span><span class="p">:</span>
            <span class="c1"># å¯ä»¥æ”¯æŒ batch size = 2</span>
            <span class="n">extra</span> <span class="o">=</span> <span class="n">vision_memory</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">min_vision_memory</span> <span class="o">*</span> <span class="mi">2</span>
            <span class="c1"># å°†å¤šä½™çš„åˆ†é…ç»™æ³¨æ„åŠ›</span>
            <span class="n">optimized</span><span class="p">[</span><span class="s1">&#39;attention&#39;</span><span class="p">]</span> <span class="o">+=</span> <span class="n">extra</span> <span class="o">*</span> <span class="mf">0.5</span>
            <span class="n">optimized</span><span class="p">[</span><span class="s1">&#39;activations&#39;</span><span class="p">]</span> <span class="o">+=</span> <span class="n">extra</span> <span class="o">*</span> <span class="mf">0.5</span>

        <span class="c1"># è§„åˆ™ 2ï¼šå¹³è¡¡æ¿€æ´»å€¼å’Œæ³¨æ„åŠ›</span>
        <span class="n">ratio</span> <span class="o">=</span> <span class="n">optimized</span><span class="p">[</span><span class="s1">&#39;activations&#39;</span><span class="p">]</span> <span class="o">/</span> <span class="n">optimized</span><span class="p">[</span><span class="s1">&#39;attention&#39;</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">ratio</span> <span class="o">&gt;</span> <span class="mi">3</span><span class="p">:</span>
            <span class="c1"># æ¿€æ´»å€¼è¿‡å¤šï¼Œé‡æ–°å¹³è¡¡</span>
            <span class="n">diff</span> <span class="o">=</span> <span class="p">(</span><span class="n">optimized</span><span class="p">[</span><span class="s1">&#39;activations&#39;</span><span class="p">]</span> <span class="o">-</span> <span class="n">optimized</span><span class="p">[</span><span class="s1">&#39;attention&#39;</span><span class="p">]</span> <span class="o">*</span> <span class="mi">2</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span>
            <span class="n">optimized</span><span class="p">[</span><span class="s1">&#39;activations&#39;</span><span class="p">]</span> <span class="o">-=</span> <span class="n">diff</span>
            <span class="n">optimized</span><span class="p">[</span><span class="s1">&#39;attention&#39;</span><span class="p">]</span> <span class="o">+=</span> <span class="n">diff</span>

        <span class="k">return</span> <span class="n">optimized</span>

    <span class="k">def</span> <span class="nf">get_training_config</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">allocation</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;æ ¹æ®å†…å­˜åˆ†é…ç”Ÿæˆè®­ç»ƒé…ç½®&quot;&quot;&quot;</span>
        <span class="n">config</span> <span class="o">=</span> <span class="p">{}</span>

        <span class="c1"># Batch size</span>
        <span class="n">vision_batch</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">allocation</span><span class="p">[</span><span class="s1">&#39;vision_features&#39;</span><span class="p">]</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">min_vision_memory</span><span class="p">)</span>
        <span class="n">config</span><span class="p">[</span><span class="s1">&#39;batch_size&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">vision_batch</span><span class="p">)</span>

        <span class="c1"># åºåˆ—é•¿åº¦</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">use_flash_attention</span><span class="p">:</span>
            <span class="c1"># Flash Attention: O(L) å†…å­˜</span>
            <span class="n">max_seq_len</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">allocation</span><span class="p">[</span><span class="s1">&#39;attention&#39;</span><span class="p">]</span> <span class="o">*</span> <span class="mi">1024</span><span class="o">**</span><span class="mi">3</span> <span class="o">/</span> 
                            <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_dim</span> <span class="o">*</span> <span class="mi">4</span><span class="p">))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># æ ‡å‡†æ³¨æ„åŠ›: O(L^2) å†…å­˜</span>
            <span class="n">max_seq_len</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">allocation</span><span class="p">[</span><span class="s1">&#39;attention&#39;</span><span class="p">]</span> <span class="o">*</span> <span class="mi">1024</span><span class="o">**</span><span class="mi">3</span> <span class="o">/</span> <span class="mi">4</span><span class="p">))</span>

        <span class="n">config</span><span class="p">[</span><span class="s1">&#39;max_seq_length&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">max_seq_len</span><span class="p">,</span> <span class="mi">4096</span><span class="p">)</span>

        <span class="c1"># æ¢¯åº¦ç´¯ç§¯</span>
        <span class="k">if</span> <span class="n">config</span><span class="p">[</span><span class="s1">&#39;batch_size&#39;</span><span class="p">]</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">target_batch_size</span><span class="p">:</span>
            <span class="n">config</span><span class="p">[</span><span class="s1">&#39;accumulation_steps&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">target_batch_size</span> <span class="o">//</span> <span class="n">config</span><span class="p">[</span><span class="s1">&#39;batch_size&#39;</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">config</span><span class="p">[</span><span class="s1">&#39;accumulation_steps&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>

        <span class="c1"># å†…å­˜ä¼˜åŒ–è®¾ç½®</span>
        <span class="n">config</span><span class="p">[</span><span class="s1">&#39;gradient_checkpointing&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">allocation</span><span class="p">[</span><span class="s1">&#39;activations&#39;</span><span class="p">]</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">num_params</span> <span class="o">*</span> <span class="mf">1.5</span> <span class="o">/</span> <span class="mi">1024</span><span class="o">**</span><span class="mi">3</span>
        <span class="n">config</span><span class="p">[</span><span class="s1">&#39;mixed_precision&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="n">config</span><span class="p">[</span><span class="s1">&#39;use_flash_attention&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">use_flash_attention</span>

        <span class="k">return</span> <span class="n">config</span>

<span class="c1"># ä½¿ç”¨ç¤ºä¾‹</span>
<span class="n">model_config</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;num_params&#39;</span><span class="p">:</span> <span class="mf">7e9</span><span class="p">,</span>  <span class="c1"># 7B å‚æ•°</span>
    <span class="s1">&#39;trainable_ratio&#39;</span><span class="p">:</span> <span class="mf">1.0</span><span class="p">,</span>  <span class="c1"># å…¨é‡å¾®è°ƒ</span>
    <span class="s1">&#39;training&#39;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>
    <span class="s1">&#39;optimizer&#39;</span><span class="p">:</span> <span class="s1">&#39;adam&#39;</span><span class="p">,</span>
    <span class="s1">&#39;image_size&#39;</span><span class="p">:</span> <span class="mi">336</span><span class="p">,</span>
    <span class="s1">&#39;patch_size&#39;</span><span class="p">:</span> <span class="mi">14</span><span class="p">,</span>
    <span class="s1">&#39;hidden_dim&#39;</span><span class="p">:</span> <span class="mi">4096</span><span class="p">,</span>
    <span class="s1">&#39;use_flash_attention&#39;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>
    <span class="s1">&#39;gradient_checkpointing&#39;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span>
    <span class="s1">&#39;target_batch_size&#39;</span><span class="p">:</span> <span class="mi">32</span><span class="p">,</span>
    <span class="s1">&#39;min_vision_memory&#39;</span><span class="p">:</span> <span class="mf">0.5</span>  <span class="c1"># GB</span>
<span class="p">}</span>

<span class="c1"># 24GB æ˜¾å­˜ï¼ˆå¦‚ RTX 3090ï¼‰</span>
<span class="n">allocator</span> <span class="o">=</span> <span class="n">VLMMemoryBudgetAllocator</span><span class="p">(</span><span class="mi">24</span><span class="p">,</span> <span class="n">model_config</span><span class="p">)</span>
<span class="n">allocation</span> <span class="o">=</span> <span class="n">allocator</span><span class="o">.</span><span class="n">allocate_budget</span><span class="p">()</span>
<span class="n">training_config</span> <span class="o">=</span> <span class="n">allocator</span><span class="o">.</span><span class="n">get_training_config</span><span class="p">(</span><span class="n">allocation</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=== å†…å­˜åˆ†é…æ–¹æ¡ˆ ===&quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">component</span><span class="p">,</span> <span class="n">memory</span> <span class="ow">in</span> <span class="n">allocation</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">component</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">memory</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> GB&quot;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">=== æ¨èè®­ç»ƒé…ç½® ===&quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">training_config</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">key</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">value</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</code></pre></div>

<p>è¿™ä¸ªåˆ†é…å™¨å¯ä»¥æ ¹æ®ç¡¬ä»¶è‡ªåŠ¨ä¼˜åŒ–è®­ç»ƒé…ç½®ï¼Œé¿å…æ‰‹åŠ¨è¯•é”™ã€‚</p>
</details>
<p><strong>ç»ƒä¹  9.8</strong>ï¼šåˆ†æçœŸå® VLM è®­ç»ƒæ—¥å¿—ï¼Œè¯Šæ–­å†…å­˜æ³„æ¼é—®é¢˜ã€‚</p>
<p>ç»™å®šä»¥ä¸‹è®­ç»ƒæ—¥å¿—ç‰‡æ®µï¼š</p>
<div class="codehilite"><pre><span></span><code>Step 100: Loss=2.34, Memory=18.2GB
Step 200: Loss=2.11, Memory=18.5GB  
Step 300: Loss=1.98, Memory=18.9GB
Step 400: Loss=1.87, Memory=19.4GB
Step 500: Loss=1.76, Memory=20.1GB
Step 600: Loss=1.65, Memory=20.9GB
Step 700: Loss=1.54, Memory=21.8GB
Step 800: Loss=1.43, Memory=22.9GB
Step 900: Loss=1.32, Memory=24.2GB
Step 1000: CUDA OOM
</code></pre></div>

<p>ğŸ’¡ <strong>æç¤º</strong>ï¼šè®¡ç®—å†…å­˜å¢é•¿ç‡ï¼Œåˆ†æå¯èƒ½çš„æ³„æ¼æºã€‚</p>
<details>
<summary>ğŸ“ å‚è€ƒç­”æ¡ˆ</summary>
<p>åˆ†æè¿‡ç¨‹ï¼š</p>
<ol>
<li>
<p><strong>å†…å­˜å¢é•¿æ¨¡å¼</strong>ï¼š
   - åˆå§‹ï¼š18.2 GB
   - æœ€ç»ˆï¼š24.2 GBï¼ˆOOM å‰ï¼‰
   - æ€»å¢é•¿ï¼š6.0 GB
   - å¹³å‡æ¯ 100 æ­¥å¢é•¿ï¼š0.67 GB
   - å¢é•¿ç‡ï¼šçº¿æ€§å¢é•¿ï¼Œéå¯¹æ•°å¢é•¿</p>
</li>
<li>
<p><strong>å¢é•¿é€Ÿåº¦åˆ†æ</strong>ï¼š</p>
</li>
</ol>
<div class="codehilite"><pre><span></span><code><span class="n">steps</span> <span class="o">=</span> <span class="p">[</span><span class="mi">100</span><span class="p">,</span> <span class="mi">200</span><span class="p">,</span> <span class="mi">300</span><span class="p">,</span> <span class="mi">400</span><span class="p">,</span> <span class="mi">500</span><span class="p">,</span> <span class="mi">600</span><span class="p">,</span> <span class="mi">700</span><span class="p">,</span> <span class="mi">800</span><span class="p">,</span> <span class="mi">900</span><span class="p">]</span>
<span class="n">memory</span> <span class="o">=</span> <span class="p">[</span><span class="mf">18.2</span><span class="p">,</span> <span class="mf">18.5</span><span class="p">,</span> <span class="mf">18.9</span><span class="p">,</span> <span class="mf">19.4</span><span class="p">,</span> <span class="mf">20.1</span><span class="p">,</span> <span class="mf">20.9</span><span class="p">,</span> <span class="mf">21.8</span><span class="p">,</span> <span class="mf">22.9</span><span class="p">,</span> <span class="mf">24.2</span><span class="p">]</span>

<span class="n">growth_rates</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">memory</span><span class="p">)):</span>
    <span class="n">rate</span> <span class="o">=</span> <span class="p">(</span><span class="n">memory</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-</span> <span class="n">memory</span><span class="p">[</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span> <span class="o">/</span> <span class="mi">100</span>  <span class="c1"># GB per step</span>
    <span class="n">growth_rates</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">rate</span> <span class="o">*</span> <span class="mi">1000</span><span class="p">)</span>  <span class="c1"># MB per step</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;æ¯ 100 æ­¥å†…å­˜å¢é•¿ï¼ˆMBï¼‰:&quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">rate</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">growth_rates</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Step </span><span class="si">{</span><span class="p">(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="mi">100</span><span class="si">}</span><span class="s2">-</span><span class="si">{</span><span class="p">(</span><span class="n">i</span><span class="o">+</span><span class="mi">2</span><span class="p">)</span><span class="o">*</span><span class="mi">100</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">rate</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2"> MB&quot;</span><span class="p">)</span>
</code></pre></div>

<p>è¾“å‡ºï¼š
   <code>Step 100-200: 3.0 MB
   Step 200-300: 4.0 MB
   Step 300-400: 5.0 MB
   Step 400-500: 7.0 MB
   Step 500-600: 8.0 MB
   Step 600-700: 9.0 MB
   Step 700-800: 11.0 MB
   Step 800-900: 13.0 MB</code></p>
<p><strong>å‘ç°</strong>ï¼šå¢é•¿é€Ÿåº¦åœ¨åŠ é€Ÿï¼</p>
<ol start="3">
<li><strong>å¯èƒ½çš„æ³„æ¼æº</strong>ï¼š</li>
</ol>
<p>a) <strong>æ¢¯åº¦ç´¯ç§¯æœªæ¸…ç†</strong>ï¼š</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># é”™è¯¯ä»£ç </span>
<span class="n">total_loss</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">dataloader</span><span class="p">:</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>
    <span class="n">total_loss</span> <span class="o">+=</span> <span class="n">loss</span>  <span class="c1"># å±é™©ï¼ä¿ç•™è®¡ç®—å›¾</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</code></pre></div>

<p>b) <strong>åˆ—è¡¨ç´¯ç§¯å¼ é‡</strong>ï¼š
   <code>python
   # é”™è¯¯ä»£ç 
   losses = []
   for step in range(1000):
       loss = train_step()
       losses.append(loss)  # åº”è¯¥ç”¨ loss.item()</code></p>
<p>c) <strong>Hook æœªé‡Šæ”¾</strong>ï¼š
   <code>python
   # é”™è¯¯ä»£ç 
   def register_hooks(model):
       for layer in model.layers:
           layer.register_forward_hook(save_activation)
   # è®­ç»ƒç»“æŸåæœªè°ƒç”¨ handle.remove()</code></p>
<p>d) <strong>åŠ¨æ€å›¾åƒå¤§å°å¯¼è‡´ç¼“å­˜ç´¯ç§¯</strong>ï¼š
   <code>python
   # VLM ç‰¹æœ‰é—®é¢˜
   image_cache = {}
   for batch in dataloader:
       h, w = batch['image'].shape[-2:]
       key = f"{h}x{w}"
       if key not in image_cache:
           image_cache[key] = process_image(batch['image'])
       # ç¼“å­˜æ— é™å¢é•¿ï¼</code></p>
<ol start="4">
<li><strong>è¯Šæ–­ä»£ç </strong>ï¼š</li>
</ol>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">diagnose_memory_leak</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">dataloader</span><span class="p">):</span>
    <span class="kn">import</span> <span class="nn">gc</span>
    <span class="kn">import</span> <span class="nn">weakref</span>

    <span class="c1"># è·Ÿè¸ªå¼ é‡å¼•ç”¨</span>
    <span class="n">tensors_before</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">obj</span> <span class="ow">in</span> <span class="n">gc</span><span class="o">.</span><span class="n">get_objects</span><span class="p">():</span>
        <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">is_tensor</span><span class="p">(</span><span class="n">obj</span><span class="p">)</span> <span class="ow">and</span> <span class="n">obj</span><span class="o">.</span><span class="n">is_cuda</span><span class="p">:</span>
            <span class="n">tensors_before</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">weakref</span><span class="o">.</span><span class="n">ref</span><span class="p">(</span><span class="n">obj</span><span class="p">))</span>

    <span class="c1"># è¿è¡Œ 10 æ­¥</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">batch</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">dataloader</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">i</span> <span class="o">&gt;=</span> <span class="mi">10</span><span class="p">:</span>
            <span class="k">break</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">train_step</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">batch</span><span class="p">)</span>

    <span class="c1"># æ£€æŸ¥æ–°å¢å¼ é‡</span>
    <span class="n">tensors_after</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
    <span class="n">leaked_tensors</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">for</span> <span class="n">obj</span> <span class="ow">in</span> <span class="n">gc</span><span class="o">.</span><span class="n">get_objects</span><span class="p">():</span>
        <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">is_tensor</span><span class="p">(</span><span class="n">obj</span><span class="p">)</span> <span class="ow">and</span> <span class="n">obj</span><span class="o">.</span><span class="n">is_cuda</span><span class="p">:</span>
            <span class="n">ref</span> <span class="o">=</span> <span class="n">weakref</span><span class="o">.</span><span class="n">ref</span><span class="p">(</span><span class="n">obj</span><span class="p">)</span>
            <span class="n">tensors_after</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">ref</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">ref</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">tensors_before</span><span class="p">:</span>
                <span class="c1"># æ–°å¢å¼ é‡</span>
                <span class="n">size_mb</span> <span class="o">=</span> <span class="n">obj</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="o">*</span> <span class="n">obj</span><span class="o">.</span><span class="n">element_size</span><span class="p">()</span> <span class="o">/</span> <span class="mi">1024</span><span class="o">**</span><span class="mi">2</span>
                <span class="k">if</span> <span class="n">size_mb</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>  <span class="c1"># åªå…³æ³¨ &gt; 1MB çš„å¼ é‡</span>
                    <span class="n">leaked_tensors</span><span class="o">.</span><span class="n">append</span><span class="p">({</span>
                        <span class="s1">&#39;shape&#39;</span><span class="p">:</span> <span class="n">obj</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span>
                        <span class="s1">&#39;dtype&#39;</span><span class="p">:</span> <span class="n">obj</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
                        <span class="s1">&#39;size_mb&#39;</span><span class="p">:</span> <span class="n">size_mb</span><span class="p">,</span>
                        <span class="s1">&#39;requires_grad&#39;</span><span class="p">:</span> <span class="n">obj</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">,</span>
                        <span class="s1">&#39;grad_fn&#39;</span><span class="p">:</span> <span class="n">obj</span><span class="o">.</span><span class="n">grad_fn</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
                    <span class="p">})</span>

    <span class="c1"># æŒ‰å¤§å°æ’åº</span>
    <span class="n">leaked_tensors</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="s1">&#39;size_mb&#39;</span><span class="p">],</span> <span class="n">reverse</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;å‘ç° </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">leaked_tensors</span><span class="p">)</span><span class="si">}</span><span class="s2"> ä¸ªå¯ç–‘å¼ é‡&quot;</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">tensor_info</span> <span class="ow">in</span> <span class="n">leaked_tensors</span><span class="p">[:</span><span class="mi">5</span><span class="p">]:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  </span><span class="si">{</span><span class="n">tensor_info</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">leaked_tensors</span>
</code></pre></div>

<ol start="5">
<li><strong>ä¿®å¤æ–¹æ¡ˆ</strong>ï¼š</li>
</ol>
<div class="codehilite"><pre><span></span><code><span class="c1"># ä¿®å¤ 1ï¼šä½¿ç”¨ .item() è·å–æ ‡é‡</span>
<span class="n">total_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>

<span class="c1"># ä¿®å¤ 2ï¼šå®šæœŸæ¸…ç†ç¼“å­˜</span>
<span class="k">if</span> <span class="n">step</span> <span class="o">%</span> <span class="mi">100</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">empty_cache</span><span class="p">()</span>

<span class="c1"># ä¿®å¤ 3ï¼šä½¿ç”¨ with torch.no_grad()</span>
<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="n">metrics</span> <span class="o">=</span> <span class="n">evaluate</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">val_loader</span><span class="p">)</span>

<span class="c1"># ä¿®å¤ 4ï¼šé™åˆ¶ç¼“å­˜å¤§å°</span>
<span class="kn">from</span> <span class="nn">functools</span> <span class="kn">import</span> <span class="n">lru_cache</span>

<span class="nd">@lru_cache</span><span class="p">(</span><span class="n">maxsize</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">cached_process_image</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">w</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">process_image_size</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span>
</code></pre></div>

<p><strong>ç»“è®º</strong>ï¼šè¯¥æ—¥å¿—æ˜¾ç¤ºå…¸å‹çš„æ¢¯åº¦/æ¿€æ´»å€¼ç´¯ç§¯æ³„æ¼ï¼Œæ¯æ­¥æ³„æ¼çº¦ 6-13 MBï¼Œéœ€è¦æ£€æŸ¥è®­ç»ƒå¾ªç¯ä¸­çš„å¼ é‡å¼•ç”¨ã€‚</p>
</details>
<h2 id="gotchas">å¸¸è§é™·é˜±ä¸é”™è¯¯ (Gotchas)</h2>
<h3 id="1-allocated-reserved">1. æ··æ·† allocated å’Œ reserved å†…å­˜</h3>
<div class="codehilite"><pre><span></span><code><span class="c1"># é”™è¯¯ç†è§£</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;å·²ç”¨å†…å­˜: </span><span class="si">{</span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">memory_reserved</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>  <span class="c1"># é”™ï¼è¿™æ˜¯é¢„ç•™çš„</span>

<span class="c1"># æ­£ç¡®</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;å®é™…ä½¿ç”¨: </span><span class="si">{</span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">memory_allocated</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;PyTorch é¢„ç•™: </span><span class="si">{</span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">memory_reserved</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;å¯ç”¨äºåˆ†é…: </span><span class="si">{</span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">memory_reserved</span><span class="p">()</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">memory_allocated</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</code></pre></div>

<h3 id="2-vlm">2. å¿½è§† VLM çš„äºŒæ¬¡å¤æ‚åº¦</h3>
<div class="codehilite"><pre><span></span><code><span class="c1"># å±é™©ï¼šæ³¨æ„åŠ›å†…å­˜æ˜¯ O(L^2)</span>
<span class="n">seq_len</span> <span class="o">=</span> <span class="mi">4096</span>
<span class="n">memory_gb</span> <span class="o">=</span> <span class="p">(</span><span class="n">seq_len</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">*</span> <span class="mi">4</span><span class="p">)</span> <span class="o">/</span> <span class="mi">1024</span><span class="o">**</span><span class="mi">3</span>  <span class="c1"># 64 MB ä»…å•ä¸ªå¤´ï¼</span>

<span class="c1"># å®‰å…¨ï¼šä½¿ç”¨ Flash Attention æˆ–åˆ†å—</span>
</code></pre></div>

<h3 id="3">3. åŠ¨æ€å›¾åƒå¤§å°çš„é™·é˜±</h3>
<div class="codehilite"><pre><span></span><code><span class="c1"># é—®é¢˜ï¼šbatch ä¸­ä¸€å¼ å¤§å›¾å¯¼è‡´æ•´ä½“ OOM</span>
<span class="n">images</span> <span class="o">=</span> <span class="p">[</span><span class="n">img1_224x224</span><span class="p">,</span> <span class="n">img2_224x224</span><span class="p">,</span> <span class="n">img3_1024x1024</span><span class="p">]</span>  <span class="c1"># ç¬¬ä¸‰å¼ å¯¼è‡´ OOM</span>

<span class="c1"># è§£å†³ï¼šé¢„å…ˆæ’åºå’Œåˆ†ç»„</span>
<span class="n">images</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span> <span class="o">*</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
</code></pre></div>

<h3 id="4">4. æ¢¯åº¦æ£€æŸ¥ç‚¹çš„è¯¯ç”¨</h3>
<div class="codehilite"><pre><span></span><code><span class="c1"># é”™è¯¯ï¼šå¯¹å°æ¨¡å‹ä½¿ç”¨åè€Œæ›´æ…¢</span>
<span class="n">tiny_model</span><span class="o">.</span><span class="n">gradient_checkpointing_enable</span><span class="p">()</span>  <span class="c1"># 2 å±‚æ¨¡å‹ï¼Œæ”¶ç›Šä¸ºè´Ÿ</span>

<span class="c1"># æ­£ç¡®ï¼šåªå¯¹æ·±å±‚æ¨¡å‹ä½¿ç”¨</span>
<span class="k">if</span> <span class="n">model</span><span class="o">.</span><span class="n">num_layers</span> <span class="o">&gt;=</span> <span class="mi">12</span><span class="p">:</span>
    <span class="n">model</span><span class="o">.</span><span class="n">gradient_checkpointing_enable</span><span class="p">()</span>
</code></pre></div>

<h3 id="5">5. ä¼˜åŒ–å™¨çŠ¶æ€çš„é—å¿˜</h3>
<div class="codehilite"><pre><span></span><code><span class="c1"># å®¹æ˜“å¿½è§†ï¼šAdam éœ€è¦ 2 å€å‚æ•°å†…å­˜</span>
<span class="c1"># 7B æ¨¡å‹ + Adam = 14GB (å‚æ•°) + 14GB (æ¢¯åº¦) + 28GB (ä¼˜åŒ–å™¨) = 56GBï¼</span>

<span class="c1"># è€ƒè™‘ä½¿ç”¨ 8-bit Adam æˆ– Adafactor</span>
</code></pre></div>

<h3 id="6-cpu-gpu">6. CPU-GPU ä¼ è¾“ç“¶é¢ˆ</h3>
<div class="codehilite"><pre><span></span><code><span class="c1"># æ…¢ï¼šé¢‘ç¹çš„å°æ‰¹é‡ä¼ è¾“</span>
<span class="k">for</span> <span class="n">img</span> <span class="ow">in</span> <span class="n">images</span><span class="p">:</span>
    <span class="n">img</span> <span class="o">=</span> <span class="n">img</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>  <span class="c1"># æ¯æ¬¡ä¼ è¾“å¼€é”€å¤§</span>

<span class="c1"># å¿«ï¼šæ‰¹é‡ä¼ è¾“</span>
<span class="n">images</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">images</span><span class="p">)</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>  <span class="c1"># ä¸€æ¬¡ä¼ è¾“</span>
</code></pre></div>

<h3 id="7">7. å†…å­˜ç¢ç‰‡åŒ–</h3>
<div class="codehilite"><pre><span></span><code><span class="c1"># å¯¼è‡´ç¢ç‰‡åŒ–ï¼šé¢‘ç¹åˆ†é…ä¸åŒå¤§å°</span>
<span class="k">for</span> <span class="n">size</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">100</span><span class="p">,</span> <span class="mi">1000</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">10000</span><span class="p">,</span> <span class="mi">1</span><span class="p">]:</span>
    <span class="n">tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">size</span><span class="p">)</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>

<span class="c1"># ç›‘æ§ç¢ç‰‡åŒ–</span>
<span class="n">fragmentation</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">memory_allocated</span><span class="p">()</span> <span class="o">/</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">memory_reserved</span><span class="p">())</span>
<span class="k">if</span> <span class="n">fragmentation</span> <span class="o">&gt;</span> <span class="mf">0.3</span><span class="p">:</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">empty_cache</span><span class="p">()</span>  <span class="c1"># æ•´ç†å†…å­˜</span>
</code></pre></div>

<h3 id="8">8. å¤šè¿›ç¨‹è®­ç»ƒçš„å†…å­˜é‡å¤</h3>
<div class="codehilite"><pre><span></span><code><span class="c1"># é”™è¯¯ï¼šæ¯ä¸ªè¿›ç¨‹éƒ½åŠ è½½å®Œæ•´æ¨¡å‹</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">load_model</span><span class="p">()</span>  <span class="c1"># æ¯ä¸ª GPU éƒ½æœ‰å®Œæ•´å‰¯æœ¬</span>

<span class="c1"># æ­£ç¡®ï¼šä½¿ç”¨ DDP æˆ– FSDP</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">FSDP</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">sharding_strategy</span><span class="o">=</span><span class="n">ShardingStrategy</span><span class="o">.</span><span class="n">FULL_SHARD</span><span class="p">)</span>
</code></pre></div>

<h2 id="_6">æœ€ä½³å®è·µæ£€æŸ¥æ¸…å•</h2>
<h3 id="_7">è®­ç»ƒå‰æ£€æŸ¥</h3>
<ul>
<li>[ ] <strong>è®¡ç®—å†…å­˜éœ€æ±‚</strong></li>
<li>[ ] æ¨¡å‹å‚æ•°å†…å­˜</li>
<li>[ ] æ¢¯åº¦å†…å­˜ï¼ˆè€ƒè™‘å†»ç»“å±‚ï¼‰</li>
<li>[ ] ä¼˜åŒ–å™¨çŠ¶æ€å†…å­˜</li>
<li>
<p>[ ] ä¼°ç®—æ¿€æ´»å€¼å†…å­˜</p>
</li>
<li>
<p>[ ] <strong>é€‰æ‹©åˆé€‚çš„ä¼˜åŒ–å™¨</strong></p>
</li>
<li>[ ] å†…å­˜å……è¶³ï¼šAdamW</li>
<li>[ ] å†…å­˜ç´§å¼ ï¼šAdafactor æˆ– 8-bit Adam</li>
<li>
<p>[ ] æåº¦å—é™ï¼šSGD with momentum</p>
</li>
<li>
<p>[ ] <strong>é…ç½®å†…å­˜ä¼˜åŒ–</strong></p>
</li>
<li>[ ] å¯ç”¨æ··åˆç²¾åº¦ï¼ˆFP16/BF16ï¼‰</li>
<li>[ ] è€ƒè™‘ gradient checkpointing</li>
<li>
<p>[ ] è¯„ä¼° Flash Attention é€‚ç”¨æ€§</p>
</li>
<li>
<p>[ ] <strong>æ•°æ®åŠ è½½ä¼˜åŒ–</strong></p>
</li>
<li>[ ] è®¾ç½®åˆç†çš„ num_workers</li>
<li>[ ] ä½¿ç”¨ pin_memory=True</li>
<li>[ ] é¢„å¤„ç†å›¾åƒå°ºå¯¸</li>
</ul>
<h3 id="_8">è®­ç»ƒä¸­ç›‘æ§</h3>
<ul>
<li>[ ] <strong>å®æ—¶å†…å­˜ç›‘æ§</strong></li>
<li>[ ] æ¯ N æ­¥è®°å½•å†…å­˜ä½¿ç”¨</li>
<li>[ ] ç›‘æ§å†…å­˜å¢é•¿è¶‹åŠ¿</li>
<li>
<p>[ ] è®¾ç½® OOM é¢„è­¦é˜ˆå€¼ï¼ˆå¦‚ 90%ï¼‰</p>
</li>
<li>
<p>[ ] <strong>æ€§èƒ½æŒ‡æ ‡è·Ÿè¸ª</strong></p>
</li>
<li>[ ] GPU åˆ©ç”¨ç‡</li>
<li>[ ] å†…å­˜ç¢ç‰‡ç‡</li>
<li>
<p>[ ] æ•°æ®åŠ è½½æ—¶é—´å æ¯”</p>
</li>
<li>
<p>[ ] <strong>å¼‚å¸¸å¤„ç†</strong></p>
</li>
<li>[ ] OOM è‡ªåŠ¨æ¢å¤æœºåˆ¶</li>
<li>[ ] åŠ¨æ€ batch size è°ƒæ•´</li>
<li>[ ] Checkpoint ä¿å­˜ç­–ç•¥</li>
</ul>
<h3 id="oom">è°ƒè¯• OOM æ—¶</h3>
<ul>
<li>[ ] <strong>å¿«é€Ÿè¯Šæ–­</strong>ï¼ˆ30 ç§’å†…ï¼‰</li>
<li>[ ] è¿è¡Œ nvidia-smi æŸ¥çœ‹æ€»ä½“å ç”¨</li>
<li>[ ] æ‰“å° torch.cuda.memory_summary()</li>
<li>[ ] æ£€æŸ¥ batch size å’Œåºåˆ—é•¿åº¦</li>
<li>
<p>[ ] ç¡®è®¤æ˜¯å¦å¼€å¯äº†ä¼˜åŒ–</p>
</li>
<li>
<p>[ ] <strong>æ·±åº¦åˆ†æ</strong>ï¼ˆ5 åˆ†é’Ÿå†…ï¼‰</p>
</li>
<li>[ ] ä½¿ç”¨ Profiler å®šä½å†…å­˜çƒ­ç‚¹</li>
<li>[ ] æ£€æŸ¥æ˜¯å¦æœ‰å†…å­˜æ³„æ¼</li>
<li>[ ] åˆ†ææ³¨æ„åŠ›çŸ©é˜µå¤§å°</li>
<li>
<p>[ ] éªŒè¯å›¾åƒåˆ†è¾¨ç‡</p>
</li>
<li>
<p>[ ] <strong>ä¼˜åŒ–æªæ–½</strong>ï¼ˆæŒ‰ä¼˜å…ˆçº§ï¼‰
  1. å‡å° batch size
  2. å¯ç”¨ gradient checkpointing
  3. ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒ
  4. å¯ç”¨ Flash Attention
  5. å†»ç»“éƒ¨åˆ†å±‚
  6. ä½¿ç”¨ CPU offloading
  7. åˆ‡æ¢åˆ°æ¨¡å‹å¹¶è¡Œ</p>
</li>
</ul>
<h3 id="_9">ä¼˜åŒ–éªŒè¯</h3>
<ul>
<li>[ ] <strong>å†…å­˜ä¼˜åŒ–æ•ˆæœ</strong></li>
<li>[ ] å³°å€¼å†…å­˜é™ä½ %</li>
<li>[ ] å¯æ”¯æŒçš„æœ€å¤§ batch size</li>
<li>
<p>[ ] è®­ç»ƒé€Ÿåº¦å˜åŒ–</p>
</li>
<li>
<p>[ ] <strong>æ¨¡å‹è´¨é‡æ£€æŸ¥</strong></p>
</li>
<li>[ ] æŸå¤±æ”¶æ•›æ­£å¸¸</li>
<li>[ ] éªŒè¯é›†æŒ‡æ ‡ç¨³å®š</li>
<li>
<p>[ ] æ— æ•°å€¼æº¢å‡º/ä¸‹æº¢</p>
</li>
<li>
<p>[ ] <strong>ç¨³å®šæ€§æµ‹è¯•</strong></p>
</li>
<li>[ ] é•¿æ—¶é—´è®­ç»ƒæ—  OOM</li>
<li>[ ] æ— å†…å­˜æ³„æ¼</li>
<li>[ ] æ¢å¤è®­ç»ƒæ­£å¸¸</li>
</ul>
<p>é€šè¿‡ç³»ç»Ÿåœ°æ‰§è¡Œè¿™ä¸ªæ£€æŸ¥æ¸…å•ï¼Œå¯ä»¥æœ‰æ•ˆé¢„é˜²å’Œè§£å†³ VLM è®­ç»ƒä¸­çš„å†…å­˜é—®é¢˜ï¼Œç¡®ä¿è®­ç»ƒé¡ºåˆ©è¿›è¡Œã€‚</p>
            </article>
            
            <nav class="page-nav"><a href="chapter8.html" class="nav-link prev">â† ç¬¬ 8 ç« ï¼šæ¨¡å‹éƒ¨ç½²ä¸æœåŠ¡åŒ–</a><a href="chapter10.html" class="nav-link next">ç¬¬ 10 ç« ï¼šè®­ç»ƒå´©æºƒä¸ NaN é—®é¢˜ â†’</a></nav>
        </main>
    </div>
</body>
</html>